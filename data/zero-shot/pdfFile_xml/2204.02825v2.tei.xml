<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Study of Remote Sensing Pretraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">An Empirical Study of Remote Sensing Pretraining</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Remote Sening Pretraining</term>
					<term>CNN</term>
					<term>Vision Trans- former</term>
					<term>Classification</term>
					<term>Detection</term>
					<term>Semantic Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has largely reshaped remote sensing (RS) research for aerial image understanding and made a great success. Nevertheless, most of the existing deep models are initialized with the ImageNet pretrained weights. Since natural images inevitably present a large domain gap relative to aerial images, probably limiting the finetuning performance on downstream aerial scene tasks. This issue motivates us to conduct an empirical study of remote sensing pretraining (RSP) on aerial images. To this end, we train different networks from scratch with the help of the largest RS scene recognition dataset up to now -MillionAID, to obtain a series of RS pretrained backbones, including both convolutional neural networks (CNN) and vision transformers such as Swin and ViTAE, which have shown promising performance on computer vision tasks. Then, we investigate the impact of RSP on representative downstream tasks including scene recognition, semantic segmentation, object detection, and change detection using these CNN and vision transformer backbones. Empirical study shows that RSP can help deliver distinctive performances in scene recognition tasks and in perceiving RS related semantics such as "Bridge" and "Airplane". We also find that, although RSP mitigates the data discrepancies of traditional ImageNet pretraining on RS images, it may still suffer from task discrepancies, where downstream tasks require different representations from scene recognition tasks. These findings call for further research efforts on both large-scale pretraining datasets and effective pretraining methods. The codes and pretrained models will be released at https://github.com/ViTAE-Transformer/ViTAE-Transformer-Remote-Sensing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Deep learning has largely reshaped remote sensing (RS) research for aerial image understanding and made a great success. Nevertheless, most of the existing deep models are initialized with the ImageNet pretrained weights. Since natural images inevitably present a large domain gap relative to aerial images, probably limiting the finetuning performance on downstream aerial scene tasks. This issue motivates us to conduct an empirical study of remote sensing pretraining (RSP) on aerial images. To this end, we train different networks from scratch with the help of the largest RS scene recognition dataset up to now -MillionAID, to obtain a series of RS pretrained backbones, including both convolutional neural networks (CNN) and vision transformers such as Swin and ViTAE, which have shown promising performance on computer vision tasks. Then, we investigate the impact of RSP on representative downstream tasks including scene recognition, semantic segmentation, object detection, and change detection using these CNN and vision transformer backbones. Empirical study shows that RSP can help deliver distinctive performances in scene recognition tasks and in perceiving RS related semantics such as "Bridge" and "Airplane". We also find that, although RSP mitigates the data discrepancies of traditional ImageNet pretraining on RS images, it may still suffer from task discrepancies, where downstream tasks require different representations from scene recognition tasks. These findings call for further research efforts on both large-scale pretraining datasets and effective pretraining methods. The codes and pretrained models will be released at https://github.com/ViTAE-Transformer/ViTAE-Transformer-Remote-Sensing.</p><p>Index Terms-Remote Sening Pretraining, CNN, Vision Transformer, Classification, Detection, Semantic Segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ITH the development of geoinformatics technology, the earth observation fields have witnessed significant progress, where various remote sensing (RS) sensors and devices have been widely used. Among them, with the advantages of real-time, abundant amount, and easy access, the aerial image has become one of the most important data sources in earth vision to serve the requirements of a series of practical tasks, such as precision agriculture <ref type="bibr">[1]</ref>, <ref type="bibr">[2]</ref> and environmental monitoring <ref type="bibr">[3]</ref>. In these applications, aerial scene recognition is a fundamental and active research topic over the past years. However, because of the own characteristics of aerial images, it is still challenging to efficiently understand the aerial scene.</p><p>The aerial images are usually obtained by a camera in a birdview perspective lying on the planes or satellites, perceiving a large scope of land uses and land covers. The obtained aerial scene is usually difficult to be interpreted since the interference of the scene-irrelevant regions and the complicated spatial distribution of land objects. Specifically, it causes the issue of inter-class similarity in aerial scene understanding, i.e., some different scenes present similar characteristics, as well as the issue of large intra-class variations, where the scenes in the same category have discrepancies, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>To tackle the above problems, it is necessary to obtain discriminative feature representations for different categories of aerial scenes. According to the difference in feature extraction methods, they can be divided into three types, i.e., the handcrafted features, the unsupervised learning features, and the supervised deep learning (DL) features. Initially, researchers directly utilize simple properties, such as color <ref type="bibr">[4]</ref>, texture <ref type="bibr">[5]</ref>, contour <ref type="bibr">[6]</ref>, spectral or their combination <ref type="bibr">[7]</ref> to recognize different aerial scenes. Besides these intuitive attributes, there are also some well-designed feature descriptors. For instance, the scale-invariant feature transformation and histogram of oriented gradients. These handcrafted features usually perform well in simple scenes while being ineffective in complex scenes. They are usually regarded as shallow features from a modern view in the DL era, while interpreting complex scenes requires more semantic information, which can not be efficiently extracted by shallow-layer methods <ref type="bibr">[8]</ref>. Compared with the above approaches, the unsupervised learning methods provide a feasible way to automatically extract the suitable features by adaptively learning the mapping functions or filters based on a set of handcrafted features or the raw pixel intensity values. The typical unsupervised learning methods include potential latent semantic analysis <ref type="bibr">[9]</ref> and bag-of-visual-words <ref type="bibr">[10]</ref>. Some simple feature enhancement methods such as the principal component analysis also belong to this category. Nonetheless, the encoded unsupervised features still have limited performance since no category supervision is explicitly used, which is useful for feature discrimination.</p><p>In recent years, with the superiority of automatically extracting deep features that reflect the inherent properties of objects, DL has achieved an impressive breakthrough in the computer vision (CV) field <ref type="bibr">[11]</ref>- <ref type="bibr">[17]</ref>, as well as the RS field <ref type="bibr">[18]</ref>- <ref type="bibr">[20]</ref>. In the aerial scene recognition domain, the most commonly used deep models are the convolutional neural networks (CNN), which have a good ability for local perception and global perception, where the former is achieved via applying sliding window convolutions on the input image to extract the local features while the latter is achieved by stacking multiple convolutional layers to increase the receptive field. According to the training scheme, the ways of using CNN in aerial scene recognition methods can be divided into three categories, i.e., training from scratch, finetuning, and adoption as the feature extractor. The first scheme does not involve any external data, meaning there is no prior knowledge that can be leveraged. To remedy this issue, the finetuning scheme uses the networks pretrained on a large-scale dataset as the start point for further training (i.e., finetuning). The last scheme directly extracts the feature from the pretrained CNN without further finetuning, therefore lacking the flexibility of adapting to aerial images from different downstream tasks.</p><p>The existing literature <ref type="bibr">[21]</ref> reveals that the finetuning strategy performs better than the other ones. We attribute it to the capacity of the used pretraining dataset, including the sample size and the number of categories. In the current RS field including the aerial scene recognition task, almost all finetuned models are pretrained on the ImageNet-1K dataset <ref type="bibr">[22]</ref>, which is the most famous image dataset in the CV field. Its millions of real-world images from 1,000 different categories enable the models to learn a powerful representation. Usually, offthe-shelf deep models like VGG <ref type="bibr">[11]</ref> and ResNet <ref type="bibr">[12]</ref> are used as backbone networks for aerial scene recognition, since training a new network on the ImageNet from scratch is time-consuming and requires a large number of computational resources. To further improve the classification performance, some methods <ref type="bibr">[23]</ref>, <ref type="bibr">[24]</ref> adopt the ImageNet pretrained models as the backbone network and employ multi-level features from it. In addition, many other components or strategies are specially designed for the aerial recognition task, such as the distillation <ref type="bibr">[25]</ref> and feature partitioning <ref type="bibr">[26]</ref>.</p><p>Although the aforementioned methods have achieved remarkable performance for aerial scene recognition, there are still some issues needed to be further investigated. Intuitively, when considering the characteristics of aerial images, there exists a large domain gap compared with the natural images in terms of view, color, texture, layout, object, etc. Previous methods attempt to narrow this gap by further finetuning the pretrained model on the RS image dataset. Nevertheless, the systematic bias introduced by ImageNet Pretraining (IMP) has a non-negligible side impact on the performance <ref type="bibr">[27]</ref>. On the other hand, we notice that there are abundant aerial images captured by diverse multiple sensors with the progress of RS technology, which can be used for pretraining. As a representative example, MillionAID <ref type="bibr">[28]</ref> is so far the largest aerial image dataset and has a million-level volume similar to ImageNet-1K, making the Remote Sensing Pretraining (RSP) become possible.</p><p>RSP enables training a deep model from scratch, implying that the candidate model is not necessary to be limited to the off-the-shelf CNN. In this paper, we also investigate the impact of RSP together with vision transformers, which have shown surprisingly good performance in the CV domain. Compared with convolutions in CNN which are skilled in modeling locality, the multi-head self-attention (MHSA) in transformers, e.g., Swin transformer <ref type="bibr">[13]</ref>, can flexibly capture diverse global contexts. Recently, ViTAE <ref type="bibr">[14]</ref>, <ref type="bibr">[29]</ref> explores both convolutions and MHSA for modelling locality and long-range dependency simultaneously, achieving state-of-theart (SOTA) performance on the ImageNet classification task and downstream vision tasks. In addition, it also extracts multi-scale features through a dilated convolution module and the stage-wise design, which have been shown effective in previous works, especially for aerial image interpretation <ref type="bibr">[30]</ref>. Since both the CNN and aforementioned vision transformers can also produce intermediate features in different stages, which are useful for many downstream tasks, we also investigate their finetuning performance of them after RSP on semantic segmentation, object detection, and change detection. To achieve these goals, we conduct extensive experiments on nine popular datasets and have some findings. The RSP is an emerging research direction in aerial image understanding, which however is still underexplored, especially in the context of vision transformers. We hope this study could fill this gap and provide useful insights for future research.</p><p>The main contribution of this paper is three-fold. (1) We investigate the impact of remote sensing pretraining by training on a large-scale remote sensing dataset using three types of backbone networks, including traditional CNN, competitive visual transformer models, and the advanced ViTAE transformers. (2) We further finetune the above models that are initialized with the remote sensing or ImageNet pretraining weights on four kinds of tasks including scene recognition, semantic segmentation, object detection, and change detection using a total of nine datasets, and compare them with other methods. (3) Experimental results show that typical vision transformer models can obtain competitive performance or perform better than CNN. Especially, the ViTAE achieves the best performance on almost all settings even if compared with the existing state-of-the-art methods. In addition, a series of findings of remote sensing pretraining will be presented, including the comparison with the traditional ImageNet pretraining and the performances on different downstream tasks. These findings provide useful insights for future research. The remainder of this paper is organized as follows. Section II introduces the related works, including the aerial scene recognition methods, especially CNN and vision transformer related ones, and the existing works of RSP. Section III describes the implementations of RSP, as well as the employed large capacity MillionAID dataset and the adopted ViTAE network. The experiment results on the four tasks and the related comprehensive analyses are presented in section IV. Finally, Section V concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Aerial Scene Recognition</head><p>There are a large number of CNN-based approaches to aerial scene recognition. Many off-the-shelf CNN classification models that are pretrained on ImageNet, such as the VGG <ref type="bibr">[11]</ref>, ResNet <ref type="bibr">[12]</ref>, and DenseNet <ref type="bibr">[31]</ref>, have been used and further finetuned on aerial images. Nonetheless, the challenging aerial scene that possesses inter-class similarity and intra-class diversity can not be easily interpreted by only using features of the last layer, which are also considered as the "global features" compared with the features in previous layers, since it is also useful to highlight the important scenerelevant local regions for scene understanding. To address this issue, <ref type="bibr">[23]</ref>, <ref type="bibr" target="#b62">[32]</ref> jointly exploits the multi-level CNN features, where the high-level features from deep layers usually have abundant semantic information, while the low-level features of the shallow layers tend to provide visual structures. For example, <ref type="bibr" target="#b62">[32]</ref> conducts varied dilated convolutions on multiple features of VGG to obtain the more effective multiscale features. In addition, they optimize the category probabilities by preserving the local maximum and revising others with the two-dimensional Gaussian-like distribution in a window to strengthen the local regions. <ref type="bibr">[23]</ref> separately applies graph convolutions on multilayer VGG features, where each pixel representation is regarded as a node, and the extracted graph features are concatenated with the last global visual features.</p><p>Apart from feature fusion, the attention mechanisms have also been commonly used in aerial scene recognition since they can enhance the local features by simulating the human vision that directly assigns different weights to various areas of the current scene. The attention modules can be easily inserted into the CNN <ref type="bibr" target="#b63">[33]</ref>. For example, <ref type="bibr" target="#b64">[34]</ref> adopts channel attention and spatial attention modules in parallel like <ref type="bibr" target="#b65">[35]</ref> to form a complementary attention. <ref type="bibr" target="#b62">[32]</ref> also employs spatial attention to further adjust the optimized category probabilities. Another point for aerial scene recognition is to model the relationships of different regions. For example, <ref type="bibr">[23]</ref> captures the topological relations of different objects, where the adjacency matrices are carefully designed. Besides, some interesting topics such as the multiple instance learning <ref type="bibr" target="#b66">[36]</ref>, self-distillation <ref type="bibr">[25]</ref> and feature partition <ref type="bibr">[26]</ref> have also been explored in aerial scene recognition research.</p><p>Networks before linear layers in scene recognition task can be used as feature encoders for many downstream tasks, where the most representative ones for aerial images are semantic segmentation, object detection, and change detection. While semantic segmentation and object detection are common tasks in CV, change detection is a specific task in RS. So far, a large number of related approaches in the aforementioned fields have been developed. Please refer to <ref type="bibr" target="#b67">[37]</ref>- <ref type="bibr" target="#b70">[40]</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Vision Transformer</head><p>Transformer is first proposed in <ref type="bibr" target="#b71">[41]</ref>, and has been widely used in the natural language processing (NLP) field <ref type="bibr" target="#b72">[42]</ref>, <ref type="bibr" target="#b73">[43]</ref>. Besides NLP, the recently proposed vision transformers inspire a wave of researches <ref type="bibr">[13]</ref>, <ref type="bibr">[14]</ref>, <ref type="bibr">[29]</ref>, <ref type="bibr" target="#b74">[44]</ref>- <ref type="bibr" target="#b81">[51]</ref> in the CV field. The core component of the vision transformer is the MHSA, which is the extension of self-attention (SA). Compared with the convolution operation, the SA can capture long-range context and the relationships between any different positions. On the foundation of SA, the MHSA separately conducts the SAs in different projected subspaces, possessing more powerful representative abilities. ViT <ref type="bibr" target="#b74">[44]</ref> is the pioneer vision transformer, where the input image is split into fixed-size patches to form tokens, which are then fed into the MHSA. However, the fixed receptive field restricts its applications on downstream tasks, and the global MHSA brings high computational complexities. To address the former issue, PVT <ref type="bibr" target="#b76">[46]</ref> adopts the classical pyramid structure, improving the model transferability with the generated hierarchical multiscale features. Swin <ref type="bibr">[13]</ref> further substitutes the global MHSA to the shiftable window MHSA (WMHSA), it reduces the computational overhead significantly, achieving excellent performance on many CV tasks. Nonetheless, it still suffers from the common issues of vision transformers, such as being inefficient in modeling locality and scale invariance, which are exactly the advantages of CNN. Thus, besides Swin, we also employ another advanced vision transformer named ViTAE <ref type="bibr">[14]</ref>, <ref type="bibr">[29]</ref>, where the intrinsic biases of CNN are introduced into the transformer. It also adopts a pyramid structure to generate hierarchical features and local window attention, achieving better performance on many CV tasks while having a reasonable computational overhead and memory footprint. Using vision transformers as the backbone for aerial scene recognition is still under-explored, while the existing method merely takes ViT as a branch parallel to CNN <ref type="bibr" target="#b82">[52]</ref>, implying an urgent need to further explore the applications of vision transformers on aerial scene tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Remote Sensing Pretraining</head><p>Pretraining using RS dataset for aerial scene recognition is a very intuitive idea. However, to our best knowledge, there are few explorations in this direction since the insufficiency of large-scale RS datasets like ImageNet. Nevertheless, researchers have attempted to obtain the RS representations from other resources. For example, GeoKR <ref type="bibr" target="#b83">[53]</ref> leverages the global land cover product as the labels, and they use the mean-teacher framework to alleviate the influences of imaging time and resolution differences between RS images and geographical ones. However, forcing alignment of different datasets inevitably brings errors due to the intrinsic different data distributions. The scarcity of large capacity RS dataset is mainly in the aspect of category labels instead of images. In this case, it </p><formula xml:id="formula_0">? ? ? ? Fig. 2.</formula><p>The diagram of the adopted ViTAE models. (a) Original ViTAE <ref type="bibr">[14]</ref>. (b) ViTAEv2 <ref type="bibr">[29]</ref>.</p><p>is promising to develop self-supervised pretraining methods <ref type="bibr" target="#b84">[54]</ref>- <ref type="bibr" target="#b87">[57]</ref> and some related methods have been developed in the RS area <ref type="bibr">[27]</ref>, <ref type="bibr" target="#b88">[58]</ref>- <ref type="bibr" target="#b90">[60]</ref>. For instance, SeCo <ref type="bibr" target="#b88">[58]</ref> leverages the seasonal changes to enforce consistency between positive samples, which are the unique characteristics of aerial scenes, while <ref type="bibr" target="#b89">[59]</ref> simultaneously fuses the temporal information and geographical location into the MoCo-V2 <ref type="bibr" target="#b85">[55]</ref> framework. Moreover, the channel properties <ref type="bibr" target="#b90">[60]</ref> and spatial variance <ref type="bibr">[27]</ref> are also explored in some approaches. In this study, since the adopted MillionAID dataset has the ground truth labels annotated by experts and does not contain any temporal information, we directly conduct the supervised pretraining like the conventional IMP</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. REMOTE SENSING PRETRAINING</head><p>In this section, we first provide a brief introduction of the adopted large-scale RS dataset-MillionAID. Then, we describe the details of the utilized ViTAE transformer. The whole RSP procedure will be presented finally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MillionAID</head><p>To our best knowledge, the MillionAID is by far the largest dataset in the RS area. It contains 100,0848 non-overlapping scenes, exceeding the competitive counterpart fMoW <ref type="bibr" target="#b91">[61]</ref> and BigEarthNet <ref type="bibr" target="#b92">[62]</ref>, which separately includes 132,716 and 590,326 scenes. Note that the fMoW contains 1,047,691 images since they provide multiple temporal views for each scene. In addition, it should be noted that the fMoW and BigEarthNet are multispectral datasets, while the MillionAID is an RGB dataset, which is more suitable for existing deep vision models. The categories of MillionAID consist of a hierarchical tree that has 51 leaves, which locate on 28 parent nodes on the second level, while the 28 groups are belonging to 8 base classes: agriculture land, commercial land, industrial land, public service land, residential land, transportation land, unutilized land, and water area, and each leaf category has about 2,000?45,000 images. This dataset is obtained from the Google Earth that is made up of diverse sensors including but not limited to SPOT, IKONOS, WorldView, and Landsat series, resulting in different resolutions. The maximum resolution can reach 0.5m, while the smallest is 153m. The image size ranges from 110 ? 110 to 31,672 ? 31,672.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ViTAE</head><p>The original ViTAE <ref type="bibr">[14]</ref> follows the deep-narrow design of T2T-ViT <ref type="bibr" target="#b93">[63]</ref>, which found that simply decreasing channel dimensions and increasing layer depth can improve the feature richness of ViT, boosting the performance while reducing the model size and computational cost. Thus, the original ViTAE firstly downsamples the input image to 1/16 size by three reduction cells. Similar to ViT, a class token is concatenated with the output of the third reduction cell before adding an element-wise sinusoid position encoding. Then, multiple normal cells are stacked, and the feature size is always kept till the end. The class token feature of the last normal cell is used for classification through a linear layer.</p><p>Although the original ViTAE performs well on ImageNet classification, it is unsuitable for transferring to other tasks like segmentation, detection, pose estimation, and so on, since it cannot generate abundant intermediate features at different scales. Thus, the authors propose the ViTAEv2 variant <ref type="bibr">[29]</ref>, which adopts the classic stage-wise design of popular backbone networks such as ResNet and Swin. <ref type="figure">Figure 2</ref> shows the comparison between the original ViTAE and ViTAEv2. In ViTAEv2, the network is split into multiple stages, usually, the number is 4. In each stage, the first cell is a reduction cell for downsampling, which is followed by the stacked normal cells. An average pooling layer is used after the last normal cell to replace the class tokens. When finetuning on downstream tasks, this pooling layer is removed and the remained network is connected with corresponding task decoders.</p><p>In this paper, we employ the ViTAEv2 model for RSP. Concretely, inspired by Swin <ref type="bibr">[13]</ref>, some MHSAs in ViTAE are replaced by the WMHSA to reduce computational cost. Specifically, considering the feature size becomes smaller in later stages, it is unnecessary to partition the feature for WMHSA. Thus, only the MHSAs in the first two stages are substituted by WMHSA. It should be noticed that the adopted WMHSA does not need to be shifted as the original implementation, since the WMHSA is conducted on the merged multiscale feature from the pyramid reduction module (PRM), where different regions have communicated with each other through the overlapped receptive fields of the sliding dilated convolutions. Besides, it is also not necessary to use relative positional encoding since the convolutions already encode the positional information.</p><formula xml:id="formula_1">D-Conv D-Conv Concat GELU GELU LN MHSA LN FFN G-Conv BN SiLU G-Conv BN SiLU Img2Seq Img2Seq Seq2Img ? PRM PCM D-Conv D-Conv Concat GELU GELU LN WMHSA LN FFN Img2Seq Img2Seq Seq2Img ? PRM PCM G-Conv SiLU G-Conv BN SiLU G-Conv BN SiLU G-Conv LN MHSA LN FFN G-Conv BN SiLU Img2Seq Img2Seq Seq2Img PCM G-Conv SiLU G-Conv BN SiLU LN WMHSA LN FFN G-Conv BN SiLU Img2Seq Img2Seq Seq2Img PCM G-Conv G-Conv BN SiLU (a) (b) (c) (d)</formula><p>Additionally, the SiLU <ref type="bibr" target="#b94">[64]</ref> in the last convolutional layer of the parallel convolutional module (PCM) is also removed to reduce nonlinearity. The structures and comparisons of different cells in the original ViTAE and ViTAEv2 have been shown in <ref type="figure" target="#fig_1">Figure 3</ref>. For reduction cell, normal cell, PRM and PCM, readers can refer to <ref type="bibr">[14]</ref> and <ref type="bibr">[29]</ref> for more details.</p><p>In our implementation, we mainly evaluate the "small" version of the original ViTAE, named ViTAE-S. In addition, we also adopt the ViTAEv2-S model due to its excellent representation ability and transferability to downstream tasks. <ref type="table" target="#tab_1">Table I</ref> lists the details of ViTAE-S and ViTAEv2-S. Here, the length of the corresponded list equals the number of stages. "Embedding Dim" means the encoding dimension in PRM, while "Stage Dim" is the channel number of the feature through the corresponding stage, which is useful for aligning the related downstream task decoders. The "RC" and "NC" separately represent the reduction cell and normal cell, where "Head" is the head number in MHSA or WMHSA, "Group" represents the number of group convolutions in PCM, and "Type" is the specific attention types. The ViTAE-S adopts the performer of T2T-ViT <ref type="bibr" target="#b93">[63]</ref> in the first two reduction cells. "L" means this reduction cell does not use PCM and attention. "F" and "W" separately denote the MHSA and WMHSA in ViT and Swin. At last, the "Depth" is the number of stacked normal cells, which is also the N i in <ref type="figure">Figure 2</ref>.</p><p>C. Implementation 1) Determine the Pretraining Network: We first determine the type of deep models to be used for RSP. To this end, from the official training set, we construct a mini-training set and a mini-evaluation set, which have 9,775 and 225 images, respectively. Note the latter set is formed by randomly selecting 5 images from each category to balance the classes. For CNN, the classic ResNet-50 <ref type="bibr">[12]</ref> is employed. Since this research mainly explores the performance of vision transformer models with RSP, a series of typical vision transformer based networks including DeiT-S <ref type="bibr" target="#b75">[45]</ref>, PVT-S <ref type="bibr" target="#b76">[46]</ref>, and Swin-T <ref type="bibr">[13]</ref>, are also evaluated. The selection of a specific version is to guarantee these models have a similar amount of parameters compared with the ViTAE-S model. In addition, we also include ViT-B <ref type="bibr" target="#b74">[44]</ref> for reference since ViT is the most basic model of vision transformers.</p><p>All models are trained with 300 epochs and batch size 16. We adopt the AdamW optimizer where the momentum is set to 0.9 and the weight decay is set to 0.05. The initial learning rate is 1e-3, which is adjusted by the cosine scheduling policy: current lr = min lr + 1 2 (initial lr ? min lr)(1 + cos( iter max iter ?)), where min lr is 5e-6. In addition, we set the warming up epochs to 5, where the learning rate is set to 5e-7. Following the typical IMP, the input image is resized to 224 ? 224 by randomly cropping during training, while during testing, the image at the same size is obtained through "center crop". In addition, a series of data argumentations including AutoAugment <ref type="bibr" target="#b95">[65]</ref>, Random Erasing <ref type="bibr" target="#b96">[66]</ref>, Mixup <ref type="bibr" target="#b97">[67]</ref>, CutMix <ref type="bibr" target="#b98">[68]</ref>, and color jitter are applied to improve the training performance. The top-1 accuracy and top-5 accuracy are used as the evaluation metrics. In addition, all models are implemented on a single NVIDIA Tesla V100 GPU, and the results are shown in <ref type="table" target="#tab_1">Table II</ref>. It can be seen that despite the ViT-B has the most parameters, it performs not better than the classic ResNet-50. The DeiT-S performs the worst since we do not adopt the teacher model. Because our task is pretraining using RS images, obtaining the corresponding teacher model is our target instead of the prerequisite. By introducing the design paradigm of the feature pyramid, PVT-S improves the accuracy compared with ViT-B. On this foundation, the original ViTAE-S further considers the locality and scale-invariance modeling, which are the inductive biases of conventional CNN models. However, it cost much training time since the token number in the early RCs is large, requiring more computations. The Swin-T addresses this issue by restricting the MHSA in fixed windows and adopts the image shifting to implicitly promote the communications between windows. By taking the advantage of WMHSA, the ViTAEv2-S achieves the best performance and it exceeds the second place by 2.3% top-1 accuracy.</p><p>The procedure of model determination is shown as follows. For the ViTAE models, we choose the strongest one to expect good performance in downstream tasks such as the aerial scene recognition when adopting RSP, i.e., the ViTAEv2-S. For comparison, the ResNet-50 is selected as the representative network in conventional CNN, and the RS pretrained ResNet-50 can also provide a group of new CNN related baselines on a series of aerial datasets. The DeiT-S and ViT-B are eliminated because of the low accuracy and a large number of parameters, and they are difficult to be transferred into downstream tasks because of the design of stacking transformers. The Swin can be regarded as building on the foundation of PVT by substituting the global MHSA with the shiftable WMHSA. Since the top-1 accuracy of Swin is larger than PVT, and Swin-T requires less training time, we also choose Swin-T in the subsequent experiments.</p><p>2) Obtain the Suitable Weights: After determining the model candidates, we conduct the RSP to obtain the pretrained weights. Concretely, maintaining the category balance, we randomly choose 1,000 images in each category of the Million-AID dataset to form the validation set that has 51,000 images, achieving a similar volume compared with the ImageNet validation set, which contains 50,000 images. The remaining 949,848 images are used for training. Although the number of images and categories for RSP are less than those of the ImageNet training set, it still can perform to be competitive or even achieves SOTA results on aerial scene tasks, whose details will be presented later.</p><p>To obtain suitable pretraining weights, we separately train the ViTAEv2-S model under the configuration of different epochs. The basic learning rate is 5e-4, and the batch size  is set to 384. The remained settings are the same as the previous experiment. All experiments are conducted with 4 V100 GPUs, and the results are shown in <ref type="table" target="#tab_1">Table III</ref>. According to the results, it can be observed that the model starts saturation after about 40 epochs, since it only improves 0.64% top-1 accuracy compared with training 20 epochs, while the next 20 epochs only bring a gain of 0.23%. Thus, the network weights trained with 40 epochs are firstly chosen as the RSP parameters of ViTAEv2-S to be applied to the subsequent tasks. Intuitively, the model achieving good performance on the large-scale pretraining dataset will also perform well on the downstream tasks. Therefore, we also use the network weights trained with 100 epochs in the downstream tasks. These models are separately denoted with the suffix "E40" and "E100". For ResNet-50 and Swin-T, we follow <ref type="bibr">[13]</ref> to configure the training settings, where the networks are trained for 300 epochs. In the experiments, we observe that the top-1 accuracy of Swin-T-E120 on the validation set is roughly equivalent to ViTAEv2-S-E40. Thus, the training weights of Swin-T-E120 are selected. Similarly, we also choose the final network weights Swin-T-E300 as a comparison with ViTAEv2-S-E100. To make the experiments fair, the weights of ResNet-50 and Swin-T that are trained with 40 epochs are also considered, since they are trained using the same number of epochs with the ViTAEv2-S-E40.</p><p>The final pretraining models are listed in <ref type="table" target="#tab_1">Table IV</ref>. It can be seen that the validation accuracies are almost increasing with the increase of training epochs. However, the performance of Swin-T-E300 is not as well as Swin-T-E120. Nonetheless, we still keep it since it may have stronger generalization by seeing more diverse samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. FINETUNING ON DOWNSTREAM TASKS</head><p>In this section, the pretrained models are further finetuned on a series of downstream tasks, including recognition, semantic segmentation, object detection in aerial scenes as well as change detection. It should be clarified that models for scene recognition in this section are trained and evaluated on commonly used aerial scene datasets rather than the MillionAID engaging for RSP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Aerial Scene Recognition</head><p>We first introduce the used scene recognition datasets and the implementation details, then present the experimental results and analyses.</p><p>1) Dataset: The three most popular scene recognition datasets including the UC Merced Land Use (UCM) dataset <ref type="bibr" target="#b99">[69]</ref>, the Aerial Image Dataset (AID) <ref type="bibr" target="#b100">[70]</ref>, and the benchmark for RS Image Scene Classification that is created by Northwestern Polytechnical University (NWPU-RESISC) <ref type="bibr">[21]</ref>, are used to comprehensively evaluate the impact of RSP and the representation ability of the above adopted backbones.</p><p>? UCM: This is the most important dataset for scene recognition. It contains 2,100 images whose sizes are all 256 ? 256 and have a pixel resolution of 0.3m. The 2,100 images equally belong to 21 categories. Thus, each category has 100 images. All samples are manually extracted from the large images in the USGS National Map Urban Area Imagery Database collected from various urban areas around the country.</p><p>? AID: This is a challenging dataset, which is generated by collecting the images from multi-source sensors on GE. It has high intra-class diversities since the images are carefully chosen from different countries. And they are extracted at different times and seasons under different imaging conditions. It has 10,000 images at the size of 600 ? 600, belonging to 30 categories.  <ref type="bibr" target="#b64">[34]</ref>, five settings of these three datasets are adopted to comprehensively evaluate the RS pretrained models and make the experiments become convincible, including UCM (8:2), AID (2:8), AID (5:5), NWPU-RESISC (1:9), and NWPU-RESISC (2:8). Note the m : n means 10m% samples are used for training, while the others form the testing set. Similar to the previous section, the images in each category are proportionally divided into two groups that are separately used for training and evaluation, respectively. Besides the above three backbones we selected, the ImageNet pretrained ResNet-50 and the ResNet-50 pretrained by SeCo <ref type="bibr" target="#b88">[58]</ref> -an RS self-supervised method considering seasonal variation, are also adopted for a fair comparison. When implementing finetuning on each scene recognition task, only the neuron number of the last linear layer is changed to match the categories of the target dataset. The overall accuracy (OA), which is the most commonly used criterion in the aerial scene recognition community by counting the proportion of the correct classified images relative to all images in the testing set, is utilized in the experiments. The models are repeatedly trained and evaluated five times at each setting, and the average value ? and standard deviation ? of the results in different trials are recorded as ? ? ?.</p><p>3) Experimental Results: Quantitative Results and Analyses: <ref type="table" target="#tab_6">Table V</ref> presents the results of the above selected backbones pretrained using different methods and other SOTA methods. Since this research only focuses on the pretraining of deep networks, especially the vision transformers. We only lists the DL based aerial scene recognition methods. For convenience, the "IMP" and "RSP" are used to represent "ImageNet Pretraining" and "Remote Sensing Pretraining", respectively. It can be seen that the methods are split into five groups. The first group is the methods that adopt ResNet-50 as the backbone network, where the ResNet-50 is initialized by the ImageNet pretrained weights. This group can be used to compare with the third group. The second group includes the recent existing advanced methods whose backbone is other popular networks except for ResNet-50, such as the ImageNet pretrained VGG-16, ResNet-101, DenseNet-121, and so on. Then, the ResNet-50, Swin-T, and ViTAEv2-S networks, whose pretrained weights are obtained by IMP, RSP, or SeCo, form the last three groups, respectively. In addition, it should be noted that besides the network types, the weights pretrained for different epochs are also considered. The bold fonts in the last three groups mean the best results in each group, while "*" denotes the best among all models (same meanings in other tasks).</p><p>On the foundation of ImageNet pretrained ResNet-50, many methods are developed, which have been shown in the first group. Among these methods, many flexible and advanced modules have been explored. For example, the attention mechanisms (CBAM <ref type="bibr" target="#b65">[35]</ref>, EAM <ref type="bibr" target="#b101">[71]</ref>, MBLANet <ref type="bibr" target="#b64">[34]</ref>), where specific channels or spatial positions of the features are highlighted, and multiscale features (F 2 BRBM <ref type="bibr" target="#b102">[72]</ref> and GRMANet <ref type="bibr" target="#b103">[73]</ref>), where the intermediate features are also employed. In addition, the self-distillation technology combined with specially designed loss functions (ESD-MBENet <ref type="bibr">[25]</ref>) and the multibranch siamese networks (IDCCP <ref type="bibr" target="#b104">[74]</ref>) have also been applied. While in the second group, the more diverse frameworks with various backbones are presented. Besides In the implemented networks, the SeCo-ResNet-50 performs the worst compared with its counterparts, it may be because there still exists a gap between the Sentinel-2 multispectral images where the SeCo trained on with the RGB images for aerial scenes recognition. Compared with the ImageNet pretrained ResNet-50, our RS pretrained ResNet-50 improves the accuracy on all settings. These results imply that RSP brings a better starting point for the optimization of the subsequent finetuning process, attributing to the aerial images used for pretraining compared with the natural images in ImageNet. Similarly, the RSP-Swin-T outperforms IMP-Swin-T on three settings and achieves comparable results on the other two settings. In addition, the ResNet-50 and Swin-T can perform to be competitive compared to other complicated methods by only using the RSP weights without changing the network structures. Besides, when comparing the ImageNet pretrained ResNet-50 and Swin-T, we can find that the IMP-Swin-T performs better in all settings since the vision transformers have stronger context modeling capability. While being initialized by RSP weights, the ResNet becomes to be more competitive and surpasses the IMP-Swin-T on the AID (2:8), NWPU-RESISC (1:9), and NWPU-RESISC (2:8) settings, showing the benefit of RSP again. Owing to the excellent representation ability of ViTAEv2-S, which has both the locality modeling ability and long-range dependency modeling ability, it outperforms both ResNet-50 and Swin-T on almost all the settings, regardless of IMP and RSP. Moreover, the RSP-ViTAEv2-S achieves the best performance compared with all other methods on almost all settings except for the AID (5:5), though on which it also delivers comparable performance with the best one, i.e., RSP-Swin-T-E300.</p><p>In our experiments, RSP helps the networks obtain better performance on small datasets, it may be because the models are easier to converge when adopting the RS pretrained weights. While for the case where training samples are abundant, like AID (5:5), the representation ability of deeper models can be fully exploited. For example, the DenseNet-121 based ESD-MBENeT obtain the best accuracy. Nevertheless, it should be noted that only the feature output from the last layer of RSP-ResNet-50, RSP-Swin-T, or RSP-ViTAEv2-S is used for classification, and it is expected that their performance can be further improved when employing the multilayer intermediate features. In this sense, these RS pretrained models can serve as effective backbones for future research in the aerial recognition field. Furthermore, Table V also shows that the models pretraining with more epochs will probably have stronger representation abilities. Since RSP-ResNet-50-E40 and RSP-Swin-T-E40 fall behind their counterparts with more epochs, we only evaluate the "E120" and "E300" pretrained weights for these two types of networks in the rest experiments, while for ViTAEv2-S, both the "E40" and "E100" weights are still used.</p><p>Qualitative Results and Analyses: <ref type="figure" target="#fig_2">Figure 4</ref> shows the response maps of the above evaluated models using Grad-CAM++ <ref type="bibr" target="#b111">[80]</ref> on images from various scenes. The warmer the color is, the higher the response is. To better show the impact of RSP, we use the pretrained weights of "E300" for ResNet-50 and Swin-T, and the weights of "E100" for ViTAEv2-S. The first three rows are the natural landscapes, and the scenes in 4-8 rows mainly contain specific foreground objects, while the next six rows present some scenes with different artificial constructions. For example, the "Thermal power station" scene includes not only chimneys but also cooling towers.</p><p>Corresponding to the quantitative results in <ref type="table" target="#tab_6">Table V</ref>, the response maps of SeCo-ResNet-50 are scattered and they can not precisely capture the semantic-relevant areas, especially in the natural landscapes or complex scenes with artificial constructions. Compared with the IMP-ResNet-50, the RSP-ResNet-50 pays more attention to the important targets. It implies that RSP facilitates ResNet-50 to learn better semantic representations, probably by seeing semantic-similar images in the MillionAID dataset. Compared to the ResNet-50, the Swin-T has a better context modeling ability by attending faraway regions with the help of MHSA. Thus, their range of high response areas is wider. Surprisingly, the IMP-Swin-T mainly concentrates on background context, but the foreground responses have been enhanced when adopting the RSP. By combining the advantages of CNN and vision transformers, the ViTAEv2-S achieves a comprehensive perception of the whole scene. Especially, the RSP-ViTAEv2-S can better recognize the typical natural RS scenes, such as terrace, mountain and river. In the foreground object based scenes, compared with RSP-ResNet-50, the RSP-ViTAEv2-S not only focuses on the primary objects, but it also considers the related regions in the background. While on the objects, the RSP-ViTAEv2-S assigns higher attention, such as the airplane with a warmer color compared with IMP-ViTAEv2-S. In the residential areas with complex object distributions, the RSP-ViTAEv2-S can correctly capture the sparse buildings and connect these regions to form a holistic representation, effectively perceiving the overall information of the scene. In the first image of the school scene, the RSP-ViTAEv2-S simultaneously focuses on the playground and the surroundings, surpassing the SeCo-ResNet-50. As for the "school-2" image that is even difficult to be recognized by humans, these models show different recognition priorities. For example, the RSP-ViTAEv2-S not only considers the campus (it can be possibly distinguished by the irregular shape of buildings) like the IMP-ResNet-50, but also notices the surrounding roads. The results in <ref type="table" target="#tab_6">Table V</ref> and <ref type="figure" target="#fig_2">Figure 4</ref> validate the effectiveness of RSP and the superiority of vision transformers in the aerial scene recognition task. We also provide training loss curves and testing accuracy curves to investigate the performances of different pretraining methods during the training phase. Here, the setting of UCM (8:2) is chosen as our example. The corresponding results have been displayed in <ref type="figure" target="#fig_3">Figure 5</ref>, where the loss curves of three kinds of networks are separately plotted. It can be seen that SeCo-ResNet-50 performs the worst and it has the largest initial loss, further confirming our aforementioned hypothesis that there is a large gap between Sentinel-2 multispectral images and the used RGB aerial images, although they are both RS images. Compared with the IMP, it can be observed that the RS pretrained models have better starting points, proving our intuition that the RS pretrained weights are easier to transfer between aerial scenes. It is also noteworthy that the shapes of these curves are similar for the same network, implying the unique characteristics of different network structures. We can also find the advanced structures enable ViTAEv2-S to reduce the performance gap indicated by the different starting points between IMP and RSP, while other networks failed. In addition, we can also find that the RSP accelerates the learning of ResNet-50 and makes the corresponding accuracy curve similar to Swin-T compared with IMP-ResNet-50. For Swin-T, the RSP also helps it converge fast. When adopting RSP, among all models, the advanced transformer network ViTAEv2-S can simultaneously achieve the best accuracy and the fastest convergence speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Aerial Semantic Segmentation</head><p>Aerial semantic segmentation is also a classification task like aerial scene recognition but at the pixel-level rather than the scene-level. We then evaluate the above three models on the aerial semantic segmentation task, including the scene parsing and object segmentation subtasks, where the former focuses on labeling each pixel of the whole scene, while the latter emphasizes the segmentation of foreground objects. 1) Dataset: We use the ISPRS Potsdam dataset 1 and the large-scale segmentation benchmark -iSAID <ref type="bibr" target="#b112">[81]</ref>, to serve as the testbed of the corresponded subtasks, respectively.  2) Implementation Detail and Experimental Setting: We adopt different settings for the above backbone networks by following the common practice. Concretely, the ResNet based networks are trained with the mini-batch stochastic gradient descent with momentum (SGDM) strategy, where the initial learning rate, weight decay, and momentum are separately set to 0.01, 0.0005, and 0.9. The learning rate is optimized by the polynomial scheduler: current lr = min lr+initial lr?(1? iter max iter ) power , where min lr = 0.0001, power = 0.9. While the vision transformers, such as the Swin-T and ViTAEv2-S, are trained using the AdamW optimizer, whose learning rate and weight decay are 6e-5 and 0.01. The learning rate schedule adopts the polynomial decay policy with a power of 1.0 and min lr of 0. They also have a linear warming up stage at the first 1,500 iterations with the initial learning rate of 1e-6. For a fair comparison, all networks are training 80k iterations with a batch size of 8. Following <ref type="bibr">[13]</ref>, <ref type="bibr">[14]</ref>, we use the UperNet <ref type="bibr">[15]</ref> as the unified segmentation framework for all the pretrained backbones since the output stride equals 32. All methods are implemented based on mmsegmentation <ref type="bibr" target="#b113">[82]</ref>. For the convenience of training, the Potsdam and iSAID are separately sampled and cropped into patches with a size of 512 ? 512 and 896 ? 896 with a stride of 384 and 512, respectively. We use the random horizontal flipping data augmentation strategy. Following the evaluation protocol in the aerial segmentation community, for the Potsdam dataset, we report the OA, mean F1 score (mF1), and per class F1 score. Note that the clutter category is regarded as the background and ignored during computing loss and evaluation metrics. While for the iSAID dataset, the intersection over union (IoU) of foreground categories and the average IOU of all classes (including background) are calculated. All evaluations are conducted on a single scale for a fair comparison.</p><p>3) Experimental Results: Quantitative Results and Analyses: Table VI-VII present the segmentation results of our methods and other SOTA methods. It can be seen that when changing the backbone from ResNet-50 to Swin-T, and then to ViTAEv2-S, the performance is increased. The results are consistent with the aforementioned scene recognition results, showing the better representation ability of vision transformers. Although the ViTAEv2-S obtains the highest OA on the Potsdam dataset, its mF1 is not as well as LANet <ref type="bibr" target="#b123">[92]</ref>. From Table VII, we can find that the scores of the "Car" category of the selected models are worse than other methods. We suspect that it may be because of the encoder-decoder structure and the rough feature fusion strategy in the UperNet, where the high-resolution features have not encoded sufficient high-level semantics, while LANet <ref type="bibr" target="#b123">[92]</ref> not only simultaneously enhance the high and low-level features, it also enriches the semantics of the high-resolution features. Thus, the segmentation performance of the evaluated models based on UperNet on small objects, such as cars, needs to be improved. On the other hand, the IMP-Swin-T performs to be competitive and the IMP-ViTAEv2-S achieves the best performance on the iSAID dataset, outperforming the SOTA methods such as the HRNet <ref type="bibr" target="#b134">[103]</ref> and OCR <ref type="bibr" target="#b137">[106]</ref> as well as a series of methods that specially designed for aerial semantic segmentation, e.g., the FarSeg <ref type="bibr" target="#b139">[108]</ref> and FactSeg <ref type="bibr" target="#b140">[109]</ref>.</p><p>Table VII also shows the advantages of RSP models lying in the "Bridge" category, which conforms to the finding in the previous scene recognition task. Nevertheless, we can also see from <ref type="table" target="#tab_1">Table VI</ref>-VII that, on the segmentation task, the performances of RSP are not as good as the classical IMP. In our considerations, there may be two reasons. The first one is the difference between the pretraining dataset and the evaluation one. Besides the dataset volume (note that the training sample and category numbers of MillionAID are smaller than ImageNet-1k), the spectral disparities also have a side impact on the performance, especially on the Potsdam dataset, which adopts the IR-R-G channels instead of the ordinary RGB image (See <ref type="figure" target="#fig_4">Figure 6</ref>). Another reason we attribute to the difference between tasks. The representation used for scene recognition needs to have a global understanding of the whole scene as <ref type="figure" target="#fig_2">Figure 4</ref> shows, while the segmentation task requires the features to be more detailed while possessing high-level semantic information simultaneously since they separately conduct the scene-level or pixel-level classification. To prove this conjecture, we then evaluate these networks on the aerial</p><formula xml:id="formula_2">(a) (b) (c) (d) (e) (f) (g) (h)</formula><p>Imper. surf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Building</head><p>Low veg. object detection task in the next section. The granularity of the representation needed for detection probably lies between those for the segmentation and recognition tasks, since one of the aims in the detection task is the object-level classification.</p><p>Qualitative Results and Analyses: We present some visual segmentation results of the UperNet with different backbones on the Potsdam dataset in <ref type="figure" target="#fig_5">Figure 7</ref>. As can be seen, only the ViTAEv2-S successfully connects the long strip low vegetations (see the red boxes), while IMP-ViTAEv2-S performs slightly better than RSP-ViTAEv2-S, which is consistent with the quantitative results in <ref type="table" target="#tab_1">Table VI</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Aerial Object Detection</head><p>Since the aerial images are top-down photoed in the sky, the objects can be presented in any direction in the birdview. Thus, the aerial object detection is the oriented bounding box (OBB) detection, which is distinguished from the usual horizontal bounding box (HBB) task on natural images <ref type="bibr" target="#b142">[111]</ref>, <ref type="bibr" target="#b148">[117]</ref>, <ref type="bibr" target="#b165">[134]</ref>. In this paper, similar to segmentation, we also use different detection datasets in the experiments. Concretely, we evaluated on the multi-category RS objects detection and the single-category ship detection subtasks, respectively. 1) Dataset: Two datasets including the large-scale DOTA <ref type="bibr" target="#b166">[135]</ref> scenes and the commonly used HRSC2016 <ref type="bibr" target="#b167">[136]</ref> dataset are separately utilized for the above objectives.</p><p>? DOTA: This is the most famous large-scale dataset for OBB detection. It totally contains 2,806 images whose size ranges from 800 ? 800 to 4,000 ? 4,000, where 188,282 instances belonging to 15 categories are included. The training, validation, and testing set separately have 1,411/458/937 tiles. It should be noticed that the categories are completely the same with the iSAID dataset, since the two datasets share the same set of scenes. The difference lies in the annotations for different tasks.</p><p>? HRSC2016: This is a specialized ship detection dataset, where the bounding boxes are annotated in arbitrary orientations. 1,061 images with the size ranging from 300 ? 300 to 1,500 ? 900 are included. In the official division, 436/181/444 images are used for training, validation, and testing, respectively. The dataset only has one category, since there is no need to recognize the type of ships. 2) Implementation Detail and Experimental Setting: Similar to segmentation, the ResNet models are trained using the SGDM algorithm with a learning rate of 0.005, a momentum of 0.9, and a weight decay of 0.0001, while the vision transformers are trained with the AdamW optimizer, where the learning rate and weight decay are separately set to 0.0001 and 0.05. These models are trained for 12 and 36 epochs with a batch size of 2 on DOTA and HRSC2016 scenes, respectively. The learning rate is adjusted by a multi-step scheduler. On the DOTA dataset, the learning rate will be separately reduced by 10? after the 8th epoch and the 11th epoch, while on the HRSC2016 scene, the corresponded settings are epoch 24 and epoch 33. We use one of the SOTA OBB detection frameworks -ORCN <ref type="bibr" target="#b157">[126]</ref> to evaluate the performance of different pretrained backbones. We adopt the default hyper-parameters of ORCN, which is implemented in OBBDetection 2 . Following <ref type="bibr" target="#b157">[126]</ref>, the DOTA dataset is sampled and cropped to 1,024 ? 1,024 patches with a stride of 824, while the HRSC2016 images are scaled keeping the aspect ratio with the shorter side equals to 800, and the length of the longer side is less than or equal to 1333. Data augmentations during training include random horizontal and vertical flipping. For convenience, the original training and validation sets are merged for training,  On the challenging DOTA dataset, it can be seen that using the advanced ORCN framework, the models whose backbone is either ResNet-50 or Swin-T performs well, although the mAPs of Swin-T models are slightly lower than the ResNet models. The ViTAEv2-S, which is a kind of vision transformer network that is introduced the inductive biases including the locality and scale-invariance characteristics of CNN, obtains amazing performance that improves the ORCN baseline by nearly 2% mAP. Another point needed to be noticed is the performance of RSP weights on these three backbones all outperforms their ImageNet pretrained counterparts. These results support our previous claims that the granularity of the representation required for the detection task is closer to that for the scene recognition task compared with the segmentation task. Thus, the performance difference between RSP and IMP in the detection experiments aligns with the results in the scene recognition experiments.</p><p>In addition, we observe that compared with IMP-ViTAEv2-S, the APs of most categories obtained by RSP-ViTAEv2-S are smaller, implying the universality of IMP. Nevertheless, the mAP of RSP-ViTAEv2-S is higher than IMP-ViTAEv2-S, since RSP has significant advantages in the categories of "Bridge" and aerial vehicles including "Helicopter" or "Plane", echoing the previous finding in the segmentation experiments. While on the other categories, the gaps between these two models are not very large. Combining the above two points, RSP-ViTAEv2-S delivers better overall performance than IMP-ViTAEv2-S. On HRSC2016 dataset, the CHPDet <ref type="bibr" target="#b164">[133]</ref> that performs the best is a specifically designed detector by considering the ship characteristics. For ORCN <ref type="bibr" target="#b157">[126]</ref> related networks, the results of RSP and IMP are roughly the same, where there are wins or losses on both sides. Compared with CNN, the vision transformer models have not demonstrated the advantages. We think that on this relatively easy subtask, where only one category needed to be detected and the ship sizes in HRSC2016 are relatively larger than DOTA, the performance is probably saturated.</p><p>Qualitative Results and Analyses: We visualize some detection results of the ORCN model with the ViTAEv2-S backbones on the DOTA testing set in <ref type="figure" target="#fig_6">Figure 8</ref>. The red boxes show that, when objects are densely distributed, the RSP- ViTAEv2-S can still predict correct object categories, while the IMP-ViTAEv2-S is confused by the dense context and makes wrong predictions. For the "Bridge" category, the IMP-ViTAEv2-S produces missing detections (see yellow boxes), while the RSP-ViTAEv2-S model successfully detects the long bridge with a high confidence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Aerial Change Detection</head><p>We then apply the above models on a typical application in the RS field, i.e., change detection, which aims to find the changes between two aerial images of a same region captured at different times. It is formulated as a pixel-level binary classification task, where "1" indicates change.</p><p>1) Dataset: We adopt the commonly used CDD <ref type="bibr" target="#b168">[137]</ref> and LEVIR <ref type="bibr" target="#b169">[138]</ref> datasets to comprehensively evaluate the above models on this task since they separately involve the natural and artificial changes.  2) Implementation Detail and Experimental Setting: In this section, we adopt a SOTA framework -BIT <ref type="bibr" target="#b171">[140]</ref>, which uses the transformer to capture the contextual information between different temporal images for change detection. If BIT is equipped with the ResNet backbone, it is optimized by the SGDM optimizer, where the learning rate, momentum, and weight decay are 0.001, 0.99, and 0.0005. While the Swin or ViTAE based BIT models are trained with the AdamW optimizer with the learning rate of 6e-5 and weight decay of 0.01. These models are trained for 200 epochs with a batch size of 8, while the learning rate is linearly decayed until the end of training. Following <ref type="bibr" target="#b171">[140]</ref>, the LEVIR dataset is clipped to the patches at the size of 256 ? 256 with no overlaps. Thus, the sizes of the training, validation and testing set are 7,120/1,024/2,048. The final performance of different models is evaluated on the testing set, while the results on the validation set are only used to select the best model during training. We use the F1 score as the evaluation metric and the experiments are conducted on a single V100 GPU.</p><p>3) Experimental Results: Quantitative Results and Analyses: The quantitative results are summarized in <ref type="table" target="#tab_12">Table X</ref>. Without surprise, the self-supervised SeCo pretrained weights perform well on this task, e.g., the SeCo-ResNet-50 based BIT performs better than the IMP counterpart. Although the SeCo weights are trained to achieve seasonal invariance, the change features can be encoded via the multi-head sub-space embedding <ref type="bibr" target="#b88">[58]</ref>. Nevertheless, ViTAEv2-S pretrained either by IMP or RSP performs better than SeCo-ResNet-50, showing the benefit of using the advanced backbone.</p><p>Compared with other methods, it is no doubt that the ViTAEv2-S achieves the best performance, showing the potentiality of applying an advanced vision transformer on RS field. As before, we analyze the performance difference between the RSP with the IMP through the perspective of task  We hope this study can drive the works on aerial image field using vision transformers based on remote sensing pretraining.  characteristics. Given the analyses in previous sections, we can infer that the granularity of the required representation for the change detection lies in between those of the segmentation and detection, since it is also a segmentation task, although there only are two categories and there is no need to recognize specific object category. Qualitative Results and Analyses: We present some visual change detection results in <ref type="figure" target="#fig_7">Figure 9</ref>. As can be seen, ResNet-50 and Swin-T by IMP can not well detect the changes in roads inside the fields in the natural scene. This issue could be partly alleviated by adopting RSP. It is consistent with the results in <ref type="table" target="#tab_12">Table X</ref> that SeCo-ResNet-50 further improves the detection in the road areas. Compared with the above models, the ViTAEv2-S model effectively captures the road details. In the artificially changed scene, the ViTAEv2-S model greatly overcomes the problem of object adhesion in the results of all other models, demonstrating that the ViTAEv2-S features are more discriminative for distinguishing objects from the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Overall Comparison of Different Backbones on All Tasks</head><p>In this part, we comprehensively compare the performance of different backbones by RSP on all tasks. Specifically, the scores in <ref type="table" target="#tab_1">Table XI</ref> are calculated by averaging the scores across all datasets for each task. For example, we calculate the average score of the mean values on five settings in the scene recognition task, so the overall accuracy of RSP-ResNet-50-E120 is (99.52+96.60+97.78+93.76+94.97)/5 ? 96.53, while the overall score of RSP-Swin-T-E300 on the segmentation task is obtained by averaging the mF1 on Potsdam dataset and the mIOU on iSAID dataset: (90.03 + 64.10)/2 ? 77.07. From Table XI, we can find that the backbones pretraining with more epochs generally perform better on downstream tasks, since they obtain stronger representations, although there is an exception, i.e., the Swin-T on the object detection task, implying the task discrepancy also matters. In general, ViTAEv2-S takes advantage of CNN and transformers and delivers better performance when training with more epochs, and outperforms ResNet-50 and Swin-T on all the tasks.</p><p>V. CONCLUSION In this study, we investigate the remote sensing pretraining problem based on both CNN and vision transformers on the largest remote sensing dataset MillionAID, and comprehensively evaluate their performance on four related tasks, including scene recognition, semantic segmentation, object detection, and change detection, as well as compare them with the ImageNet pretraining and other SOTA methods. By synthetically analyzing the experiment results, we draw the following conclusions:</p><p>? Compared with the traditional CNN model, the vision transformers perform competitively on a series of remote sensing tasks, and they can obtain better performance on some more challenging datasets, such as the iSAID and DOTA. Particularly, the ViTAEv2-S, an advanced model introducing the inductive biases of CNN into the vision transformers, achieves the best performance on almost all settings of these tasks.</p><p>? Benefitting from the large capacity of ImageNet-1K dataset, the classical IMP enables deep models to learn more universal representations that generalize well to almost all categories in the downstream tasks. Thus, the IMP can produce competitive baseline results despite aerial scenes. RSP is comparable with IMP and performs extremely well on some specific categories, such as the "Bridge" and "Airplane", owing to mitigating the datalevel discrepancy between the upstream pretraining task and downstream task.</p><p>? The task-level discrepancy also has a side impact on the performance of RSP. If the granularity of the representation required for a specific downstream task is closer to that of the upstream pretraining task, i.e., scene recognition, RSP usually leads to better performances.</p><p>We hope this study can provide useful insights to the community about using advanced vision transformers and remote sensing pretraining. In future work, we will investigate the RSP on large-scale datasets for downstream tasks as well as the unsupervised pretraining considering the abundant unlabelled data in this area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The challenges of aerial scene recognition. (a) and (b) are the natural image and aerial image belonging to the "park" category. (c) and (d) are two aerial images from the "school" category. Despite the distinct view difference between (a) and (b), (b) contains the playground that is unusual in the park scenes but usually exists in the school scenes like (d). On the other hand, (c) and (d) show different colors as well as significantly different spatial distributions of land objects like the playground and swimming pool. Here, (a) is obtained from http://travel.qunar.com/p-oi24486013-townhill country park by searching "park" on internet, while (b), (c), and (d) are the aerial images from the AID dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The structures of different cells in ViTAE models. (a) and (c) are the reduction cell and normal cell in the original ViTAE, while (b) and (d) are the corresponded variants in the ViTAEv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Response maps of the evaluated models on different scenes. (a) Original image. (b) IMP-ResNet-50. (c) SeCo-ResNet-50. (d) RSP-ResNet-50. (e) IMP-Swin-T. (f) RSP-Swin-T. (g) IMP-ViTAEv2-S. (h) RSP-ViTAEv2-S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>(a), (b) and (c) are separately the training loss curves of ResNet-50, Swin-T and ViTAEv2-S, where the loss is recorded every 10 iterations. (d) is the testing accuracy curves of these models. All curves are obtained by training in the setting of UCM (8:2). The curves in (a), (b), and (c) have been smoothed by moving average.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Visual samples from different datasets: (a) MillionAID, (b) Potsdam, and (c) iSAID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Segmentation maps of the UperNet with different backbones on the Potsdam dataset. (a) Ground Truth. (b) IMP-ResNet-50. (c) SeCo-ResNet-50. (d) RSP-ResNet-50. (e) IMP-Swin-T. (f) RSP-Swin-T. (g) IMP-ViTAEv2-S. (h) RSP-ViTAEv2-S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Visual detection results of the ORCN model with the ViTAEv2-S backbones on the DOTA testing set. LV: large vehicle. SV: small vehicle. BR: Bridge. IMP: IMP-ViTAEv2-S. RSP: RSP-ViTAEv2-S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Visualization of the change detection maps. The first and second row separately show the change detection results of a sample image from CDD and LEVIR dataset. Here, (a)(k), (b)(l) are the first and second temporals of the same regions. (c)(m) are the corresponded change annotations. (d)(n) are generated by the IMP-ResNet-50 based BIT, while (e)(o), (f)(p), (f)(o), (g)(q), (h)(r), (i)(s), (g)(t) are separately the results from the SeCo-ResNet-50, RSP-ResNet-50, IMP-Swin-T, RSP-Swin-T, IMP-ViTAE-S-Stage-Win and RSP-ViTAE-S-Stage-Win backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Visual change detection results. The first and second row separately show the change detection results of a sample image from the CDD and LEVIR datasets. Here, (a) and (k), (b) and (l) are the first and second temporals of the same regions. (c) and (m) are ground truth change annotations. (d) and (n) are the results of the IMP-ResNet-50 based BIT, while (e) and (o), (f) and (p), (f) and (o), (g) and (q), (h) and (r), (i) and (s), (g) and (t) are the results from the SeCo-ResNet-50, RSP-ResNet-50, IMP-Swin-T, RSP-Swin-T, IMP-ViTAEv2-S, and RSP-ViTAEv2-S backbones, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I THE</head><label>I</label><figDesc>HYPERPARAMETER SETTINGS OF DIFFERENT "SMALL" VERSION VITAE MODELS. "P" DENOTES PERFORMER ATTENTION [63] WHILE "L" MEANS THE REDUCTION CELL HAS NO PCM AND ATTENTION. "F" DENOTES THE ORIGINAL MHSA WHILE "W" DENOTES THE WMHSA.</figDesc><table><row><cell>Network Stage Downsampling Ratio Embedding Dim Stage Dim RC Head Group Type NC Head Group Type Depth</cell><cell>ViTAE-S [14] 3 [4, 2, 2] [64, 64, 192] [96, 192, 384] [1, 1, 1] [1, 1, 1] [P, P, L] [1, 1, 6] [1, 1, 96] [F, F, F] [0, 0, 14]</cell><cell>ViTAEv2-S [29] 4 [4, 2, 2, 2] [64, 64, 128, 256] [64, 128, 256, 512] [1, 1, 2, 4] [1, 16, 32, 64] [W, W, F, F] [1, 2, 4, 8] [1, 32, 64, 128] [W, W, F, F] [2, 2, 8, 2]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II RESULTS</head><label>II</label><figDesc>OF DIFFERENT MODELS ON THE MINI-EVALUATION SET. THEY ARE TRAINED ON THE MINI-TRAINING SET FROM MILLIONAID.</figDesc><table><row><cell>Model ResNet-50 [12] DeiT-S [45] ViT-B [44] PVT-S [46] Swin-T [13] ViTAE-S [14] ViTAEv2-S [29]</cell><cell>Acc@1 80.8 72.9 78.4 80.0 84.7 85.9 88.2</cell><cell>Acc@5 96.1 92.9 92.1 94.1 93.7 96.9 96.1</cell><cell>Param(M) 23.6 21.7 114.4 23.9 27.6 22.8 18.8</cell><cell>Training Time (hh:mm:ss) 16:42:58 16:57:39 17:41:03 18:06:42 17:49:06 24:00:35 23:57:30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III RESULTS</head><label>III</label><figDesc>OF VITAEV2-S WITH DIFFERENT SETTINGS OF TRAINING EPOCH ON THE MILLIONAID VALIDATION SET.</figDesc><table><row><cell>Epoch 5 10 15 20 40 60 80 100</cell><cell>Acc@1 94.53 96.45 97.38 98.00 98.64 98.87 98.90 98.97</cell><cell>Acc@5 99.41 99.64 99.74 99.81 99.86 99.83 99.85 99.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV RESULTS</head><label>IV</label><figDesc>OF THE CANDIDATE MODELS FOR THE SUBSEQUENT FINETUNING EXPERIMENTS ON THE MILLIONAID VALIDATION SET.</figDesc><table><row><cell>Epoch</cell><cell>Acc@1</cell><cell>Acc@5</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell></row><row><cell>40 120 300</cell><cell>97.99 98.76 98.99</cell><cell>99.81 99.83 99.82</cell></row><row><cell>Swin-T</cell><cell></cell><cell></cell></row><row><cell>40 120 300</cell><cell>97.80 98.63 98.59</cell><cell>99.84 99.89 99.88</cell></row><row><cell>ViTAEv2-S</cell><cell></cell><cell></cell></row><row><cell>40 100</cell><cell>98.64 98.97</cell><cell>99.86 99.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>This dataset is characterized by a great number of samples. It contains 31,500 images and 45 categories in total, where each category has 700 samples. Each image has 256 ? 256 pixels. The spatial resolutions are varied from 0.2m to 30m. Some special landforms, such as islands, lakes, regular mountains, and snow mountains, maybe in lower resolutions. 2) Implementation Detail and Experimental Setting: The training settings are the same as previous experiments. The training epoch and batch size are set to 200 and 64, respectively. These experiments are conducted on a single V100 GPU. Following</figDesc><table /><note>? NWPU-RESISC:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V RESULTS</head><label>V</label><figDesc>OF THE SELECTED MODELS AND SOTA METHODS ON THE THREE SCENE RECOGNITION DATASETS UNDER DIFFERENT SETTINGS. THE BOLD FONTS IN THE LAST THREE GROUPS MEAN THE BEST RESULTS, WHILE "*" DENOTES THE BEST AMONG ALL MODELS. ? 0.23 94.66 ? 0.39 96.90 ? 0.04 92.10 ? 0.04 94.26 ? 0.12 EAM (IMP-ResNet-50) [71] GRSL2021 98.98 ? 0.37 93.64 ? 0.25 96.62 ? 0.13 90.87 ? 0.15 93.51 ? 0.12 F 2 BRBM (IMP-ResNet-50) [72] JSTARS2021 99.58 ? 0.23 96.05 ? 0.31 96.97 ? 0.22 92.74 ? 0.23 94.87 ? 0.15 MBLANet (IMP-ResNet-50) [34] TIP2021 99.64 ? 0.12 95.60 ? 0.17 97.14 ? 0.13 92.32 ? 0.15 94.66 ? 0.11</figDesc><table><row><cell>Model CBAM [35] 99.04 GRMANet (IMP-ResNet-50) [73] Publication UCM (8:2) ECCV2018 TGRS2021 99.19 ? 0.10 IDCCP (IMP-ResNet-50) [74] TGRS2021 99.05 ? 0.20 ESD-MBENet-v1 (IMP-ResNet-50) [25] TGRS2021 99.81 ? 0.10 ESD-MBENet-v2 (IMP-ResNet-50) [25] TGRS2021 99.86 ? 0.12 ARCNet (IMP-VGG-16) [75] TGRS2019 99.12 ? 0.40 SCCov (IMP-VGG-16) [76] TNNLS2019 99.05 ? 0.25 KFBNet (IMP-DenseNet-121) [77] TGRS2020 99.88 ? 0.12 GBNet (IMP-VGG-16) [24] TGRS2020 98.57 ? 0.48 MG-CAP (IMP-VGG-16) [78] TIP2020 99.00 ? 0.10 EAM (IMP-ResNet-101) [71] GRSL2021 99.21 ? 0.26 IMP-ViT-B [44] ICLR2021 99.28 ? 0.23 MSANet (IMP-ResNet-101) [79] JSTARS2021 98.96 ? 0.21 CTNet (IMP-MobileNet-V2+IMP-ViT-B) [52] GRSL2021 -LSENet (IMP-VGG-16) [32] TIP2021 99.78 ? 0.18 DFAGCN (IMP-VGG-16) [23] TNNLS2021 98.48 ? 0.42 MGML-FENet (IMP-DenseNet-121) [26] TNNLS2021 99.86 ? 0.12 ESD-MBENet-v1 (IMP-DenseNet-121) [25] TGRS2021 99.86 ? 0.12 ESD-MBENet-v2 (IMP-DenseNet-121) [25] TGRS2021 99.81 ? 0.10 IMP-ResNet-50 [12] CVPR2016 98.81 ? 0.23 SeCo-ResNet-50 [58] ICCV2021 97.86 ? 0.23 RSP-ResNet-50-E40 Ours 99.43 ? 0.24 RSP-ResNet-50-E120 Ours 99.52 ? 0.15 RSP-ResNet-50-E300 Ours 99.48 ? 0.10 IMP-Swin-T [13] ICCV2021 99.62 ? 0.19 RSP-Swin-T-E40 Ours 99.24 ? 0.18 RSP-Swin-T-E120 Ours 99.52 ? 0.00 RSP-Swin-T-E300 Ours 99.52 ? 0.00 IMP-ViTAEv2-S [29] arXiv2022 99.71 ? 0.10 RSP-ViTAEv2-S-E40 Ours 99.71 ? 0.10 RSP-ViTAEv2-S-E100 Ours 99.90 ? 0.13*</cell><cell>AID (2:8) 95.43 ? 0.32 94.80 ? 0.18 96.00 ? 0.15 95.81 ? 0.24 88.75 ? 0.40 93.12 ? 0.25 95.50 ? 0.27 92.20 ? 0.23 93.34 ? 0.18 94.26 ? 0.11 93.81 ? 0.21 93.53 ? 0.21 96.25 ? 0.10 94.41 ? 0.16 -96.45 ? 0.18 96.20 ? 0.15 96.39 ? 0.21 94.67 ? 0.15 93.47 ? 0.08 95.88 ? 0.07 96.60 ? 0.04 96.81 ? 0.03 96.55 ? 0.03 95.95 ? 0.06 96.73 ? 0.07 96.83 ? 0.08 96.61 ? 0.07 96.72 ? 0.06 96.91 ? 0.06*</cell><cell>AID (5:5) 97.39 ? 0.24 96.95 ? 0.13 98.54 ? 0.17 98.66 ? 0.20 93.10 ? 0.55 96.10 ? 0.16 97.40 ? 0.10 95.48 ? 0.12 96.12 ? 0.12 97.06 ? 0.19 96.08 ? 0.14 96.01 ? 0.43 97.70 ? 0.11 96.36 ? 0.19 94.88 ? 0.22 98.60 ? 0.04 98.85 ? 0.13* 98.40 ? 0.23 95.74 ? 0.10 95.99 ? 0.13 97.29 ? 0.07 97.78 ? 0.08 97.89 ? 0.08 98.10 ? 0.06 97.52 ? 0.04 98.20 ? 0.02 98.30 ? 0.04 98.08 ? 0.03 97.92 ? 0.06 98.22 ? 0.09</cell><cell>NWPU-RESISC (1:9) 93.19 ? 0.42 91.55 ? 0.16 92.50 ? 0.22 93.03 ? 0.11 -89.30 ? 0.35 93.08 ? 0.14 -90.83 ? 0.12 91.91 ? 0.22 90.96 ? 0.08 90.38 ? 0.17 93.90 ? 0.14 92.23 ? 0.14 -92.91 ? 0.22 93.24 ? 0.15 93.05 ? 0.18 90.09 ? 0.13 89.64 ? 0.17 92.86 ? 0.09 93.76 ? 0.03 93.93 ? 0.10 92.73 ? 0.09 91.22 ? 0.18 92.02 ? 0.14 93.02 ? 0.12 93.90 ? 0.07 94.12 ? 0.07 94.41 ? 0.11*</cell><cell>NWPU-RESISC (2:8) 94.72 ? 0.25 93.76 ? 0.12 95.58 ? 0.08 95.24 ? 0.23 -92.10 ? 0.25 95.11 ? 0.10 -92.95 ? 0.13 94.29 ? 0.09 93.96 ? 0.17 93.52 ? 0.21 95.40 ? 0.15 93.34 ? 0.15 89.29 ? 0.28 95.39 ? 0.08 95.50 ? 0.09 95.36 ? 0.14 94.10 ? 0.15 92.91 ? 0.13 94.40 ? 0.05 94.97 ? 0.07 95.02 ? 0.06 94.70 ? 0.10 93.30 ? 0.08 93.84 ? 0.07 94.51 ? 0.05 95.29 ? 0.12 95.35 ? 0.03 95.60 ? 0.06</cell></row></table><note>* the traditional CNN, the recent ViT has also been applied in some works. Compared with the IMP-ViT-B, the RSP-Swin- T-E300 performs better, although the former model has more trainable parameters. It can be observed that the backbones are changing over time. The VGG-16 used in the early years is gradually replaced by the deeper networks such as ResNet-101 or DenseNet-121 due to their better representation ability.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>:</head><label></label><figDesc>This dataset is released by ISPRS Commission WG II/4. It covers a large scene that is collected over 3.42 km 2 area of the Potsdam city. It contains 38 images, whose average size is 6,000 ? 6,000 pixels, and the resolution is 0.5m. Among them, the training and testing sets separately have 24 and 14 images. There are 6 categories included in these scenes, namely impervious surface, building, low vegetation, tree, car, and clutter.</figDesc><table><row><cell>? iSAID: This is a large-scale dataset that is mainly for in-stance segmentation. Also, it provides the semantic mask including 15 foregrounds and 1 background category over the aerial objects. It consists of 2,806 high-resolution images that range from 800 ? 800 to 4,000 ? 13,000</cell></row></table><note>? Potsdampixels. The training, validation, and test set separately have 1,411, 458, and 937 images. In this paper, only the validation set is used for evaluation since the testing set is unavailable.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI RESULTS</head><label>VI</label><figDesc>OF THE UPERNET SEGMENTATION MODEL WITH DIFFERENT BACKBONES AND SOTA METHODS ON THE TESTING SET OF THE POTSDAM DATASET.</figDesc><table><row><cell></cell><cell cols="2">Method FCN [83] S-RA-FCN [84] Multi-filter CNN [85] FCN [83] DANet [86] PSPNet [87] DeeplabV3+ [88] ResT [89] EaNet [90] BotNet [91] LANet [92] UperNet UperNet UperNet UperNet UperNet UperNet UperNet</cell><cell cols="2">Backbone IMP-VGG-16 IMP-VGG-16 IMP-VGG-16 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 SeCo-ResNet-50 RSP-ResNet-50 IMP-Swin-T RSP-Swin-T IMP-ViTAEv2-S RSP-ViTAEv2-S</cell><cell cols="2">OA 85.59 88.59 90.65 89.42 89.72 89.45 89.74 89.13 90.15 90.42 90.84 90.64 89.64 90.61 91.17 90.78 91.60* 91.21</cell><cell>mF1 87.61 90.17 85.23 88.66 89.14 90.51 90.94 90.89 91.73 91.77 91.95* 89.96 89.03 89.94 90.60 90.03 91.00 90.64</cell><cell cols="2">Imper. surf. 88.61 91.33 90.94 91.46 91.61 91.61 92.35 91.14 92.87 92.34 93.05 92.30 91.21 92.42 92.94 92.65 93.34* 93.05</cell><cell cols="2">F1 score per category Building Low veg. 93.29 83.29 94.70 86.81 96.98 76.32 96.63 85.99 96.44 86.11 96.30 86.41 96.77 85.22 95.11 86.30 96.30 86.16 96.30 87.32* 97.19* 87.30 96.14 85.93 94.92 85.12 96.15 85.75 96.66 86.54 96.35 86.02 96.84 87.28 96.62 86.62</cell><cell>Tree 79.83 83.47 73.37 86.94 88.04 86.84 86.79 87.27 87.99 88.74* 88.04 85.66 84.89 85.49 85.87 85.39 86.38 85.89</cell><cell>Car 93.02 94.52 88.55 82.28 83.54 91.38 93.58 94.63 95.30* 94.17 94.19 89.76 89.02 89.87 90.98 89.75 91.18 91.01</cell><cell></cell><cell></cell></row><row><cell cols="17">TABLE VII RESULTS OF THE UPERNET SEGMENTATION MODEL WITH DIFFERENT BACKBONES AND SOTA METHODS ON THE VALIDATION SET OF THE ISAID</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DATASET.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method FCN [83] UNet [93] DenseASPP [94] DenseUNet [95] Semantic FPN [96] RefineNet [97] PSPNet [87] DeeplabV3 [98] DeeplabV3+ [88] EMANet [99] ASP-OCNet [100] DANet [86] CCNet [101] EncNet [102] HRNet [103] RANet [104] AlignSeg [105] OCR [106] HMANet [107] FarSeg [108] FactSeg [109] UperNet UperNet UperNet UperNet UperNet UperNet UperNet</cell><cell>Backbone IMP-VGG-16 -IMP-DenseNet-121 IMP-DenseNet-121 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 IMP-HRNetW-18 IMP-ResNet-50 IMP-ResNet-50 IMP-HRNet-W48 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 IMP-ResNet-50 SeCo-ResNet-50 RSP-ResNet-50 IMP-Swin-T RSP-Swin-T IMP-ViTAEv2-S RSP-ViTAEv2-S</cell><cell>mIOU 41.7 39.2 56.8 58.7 59.3 60.2 60.3 59.0 60.8 55.4 40.2 57.5 58.3 58.9 61.5 62.1 62.1 62.6 62.6 63.7 64.8 61.9 57.2 61.6 64.6 64.1 65.3* 64.3</cell><cell>Ship 51.7 49.0 61.1 66.1 63.7 63.8 65.2 59.7 63.9 63.1 47.3 60.2 61.4 59.7 65.9 67.1 67.4 67.8 65.4 65.4 68.3 65.9 63.9 64.2 69.2 67.0 71.4* 71.3</cell><cell>ST 22.9 0 50.0 50.4 59.5 58.6 52.1 50.5 52.5 68.4 40.2 63.0 65.7 64.9 68.9 61.3 68.9 70.7 70.9 61.8 56.8 73.9 71.7 75.9 76.5 74.6 77.5* 74.3</cell><cell>BD 26.4 36.5 67.5 76.1 71.8 72.3 75.7 77.0 72.8 66.2 44.4 71.4 68.9 70.0 74.0 72.5 76.2 73.6 74.7 77.7 78.4* 68.1 66.9 68.8 74.1 73.7 68.2 72.2</cell><cell>TC 74.8 78.6 86.1 86.2 86.6 85.3 85.6 84.2 84.9 82.7 65.0 84.7 82.9 84.2 86.9 85.1 86.2 87.9 88.7 86.4 88.9* 70.7 69.9 69.9 69.9 70.7 71.0 70.4</cell><cell>BC 30.2 22.9 56.6 57.7 57.8 61.1 61.1 57.9 56.5 56.0 24.1 50.9 57.1 55.2 59.4 53.2 62.1 63.4 60.5 62.1 64.9* 57.3 54.5 58.5 56.3 59.0 60.8 57.4</cell><cell>GTF 27.9 5.5 52.3 49.5 51.6 52.8 60.2 59.6 58.9 18.8 29.9 52.5 56.8 46.3 61.5 47.1 52.0 47.7 54.6 56.7 54.6 52.5 45.9 54.4 60.1 60.1 61.9 63.0*</cell><cell cols="2">IOU per category 1 Bridge LV SV 8.2 49.3 37.0 7.5 49.9 35.6 29.6 57.1 38.4 33.9 54.7 46.2 34.0 59.2 45.1 32.6 58.2 42.4 32.5 58.0 43.0 32.9 54.8 33.7 32.2 59.1 42.9 42.1 58.2 41.0 27.1 46.3 13.6 28.6 57.5 42.1 34.0 57.6 38.3 36.8 57.2 38.7 33.8 62.1 46.9 45.3* 60.1 49.3 28.7 60.7 50.3 33.1 61.4 49.6 29.0 59.7 50.3 36.7 60.6 46.3 36.3 62.7 49.5 39.2 61.2 48.8 38.9 58.2 44.8 40.2 59.6 47.5 41.9 62.3 51.6 44.3 62.0 50.6 43.0 63.8* 53.6* 44.0 62.5 51.6</cell><cell>HC 0 0 0 0 0 23.0 10.9 31.3 31.4 33.4 10.3 30.4 31.6 34.8 14.9 38.1 31.2 30.4 32.6 35.8 42.7 34.3 33.2 32.1 44.7* 37.6 43.4 35.4</cell><cell>SP 30.7 38.0 43.3 45.1 46.4 43.4 46.8 44.7 46.1 38.9 34.6 46.1 36.5 42.4 44.2 41.8 45.7 48.4 51.4 51.2 51.5* 44.5 9.3 43.8 45.8 46.8 44.8 47.0</cell><cell>RA 51.9 46.5 64.8 65.9 68.7 65.6 68.6 66.0 67.7 46.9 37.9 40.6 57.2 59.8 52.9 70.5 56.2 59.5 62.9 71.4* 69.4 62.1 52.3 65.4 64.5 64.9 65.1 62.2</cell><cell>SBF 52.1 9.7 74.1 71.9 73.6 74.4 71.9 72.1 72.9 46.4 41.4 63.3 75.0 69.8 75.6 58.8 71.2 72.8 70.2 72.5 73.6 76.8 71.6 76.5 75.9 76.2 77.9* 77.7</cell><cell>Plane 62.9 74.7 78.1 82.2 80.8 79.9 79.5 75.8 79.8 78.5 68.1 80.9 75.8 76.1 81.7 83.1 82.9 83.3 83.8 82.0 84.1 83.8 83.3 82.8 85.7 85.2 86.4* 85.2</cell><cell>Harbor 42.0 45.6 51.1 54.6 51.3 51.1 54.3 45.7 52.6 47.5 38.0 48.8 45.9 48.0 52.2 55.6 54.8 53.3 51.9 53.9 55.7 52.2 51.4 51.5 56.7 53.8 57.7* 54.7</cell></row></table><note>1 ST: storage tank. BD: baseball diamond. TC: tennis court. BC: baseball court. GTF: ground track field. LV: large vehicle. SV: small vehicle. HC: helicopter.SP: swimming pool. RA: roundabout. SBF: soccer ball field.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII RESULTS</head><label>VIII</label><figDesc>OF THE ORCN DETECTION MODEL WITH DIFFERENT BACKBONES AND SOTA METHODS ON THE TESTING SET OF THE DOTA DATASET. ?: THE RESULT IS FROM AERIALDETECTON<ref type="bibr" target="#b141">[110]</ref>. ?: THE RESULT IS FROM THE ORIGINAL ORCN PAPER.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>mAP</cell><cell>Ship</cell><cell>ST</cell><cell>BD</cell><cell>TC</cell><cell>BC</cell><cell>GTF</cell><cell cols="3">AP per category 1 Bridge LV</cell><cell>SV</cell><cell>HC</cell><cell>SP</cell><cell>RA</cell><cell>SBF</cell><cell>Plane</cell><cell>Harbor</cell></row><row><cell>One-stage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RetinaNet [111]  ? DAL [112] RSDet [113] R3Det [114] R3Det [114]</cell><cell>IMP-ResNet-50-FPN IMP-ResNet-50-FPN IMP-ResNet-101-FPN IMP-ResNet-101-FPN IMP-ResNet-152-FPN</cell><cell>68.43 71.44 72.20 71.69 73.74</cell><cell>79.11 79.74 70.20 77.54 78.21</cell><cell>74.32 78.45 83.40 83.54 84.23</cell><cell>77.62 76.55 82.90 81.99 81.17</cell><cell>90.29 90.84 90.50 90.80 90.81</cell><cell>82.18 79.54 85.60 81.39 85.26</cell><cell>58.17 66.80 65.20 62.52 66.10</cell><cell>41.81 45.08 48.60 48.46 50.53</cell><cell>71.64 76.76 70.10 74.29 78.66</cell><cell cols="2">74.58 67.00 69.50 70.48 70.92</cell><cell>60.64 60.11 68.00* 60.05 67.17</cell><cell>69.67 73.14 67.20 67.46 69.83</cell><cell>60.60 62.27 63.90 59.82 63.77</cell><cell>54.75 57.71 62.50 61.97 61.81</cell><cell>88.67 88.68 89.80 89.54 89.49</cell><cell>62.57 69.05 65.60 65.44 68.16</cell></row><row><cell>S 2 ANet [115]</cell><cell>IMP-ResNet-50-FPN</cell><cell>74.12</cell><cell>87.25</cell><cell>85.64</cell><cell>82.84</cell><cell>90.83</cell><cell>84.90</cell><cell>71.11</cell><cell>48.37</cell><cell>78.39</cell><cell cols="2">78.11</cell><cell>57.94</cell><cell>69.13</cell><cell>62.60</cell><cell>60.36</cell><cell>89.11</cell><cell>65.26</cell></row><row><cell>S 2 ANet [115]</cell><cell>IMP-ResNet-101-FPN</cell><cell>76.11</cell><cell>88.04</cell><cell>86.22</cell><cell>81.41</cell><cell>90.69</cell><cell>84.75</cell><cell>69.75</cell><cell>54.28</cell><cell>80.54</cell><cell cols="2">78.04</cell><cell>58.86</cell><cell>73.37*</cell><cell>65.81</cell><cell>65.03</cell><cell>88.70</cell><cell>76.16</cell></row><row><cell>Two-stage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ICN [116] Faster R-CNN [117]  ? CAD-Net [118] ROI Transformer [119] SCRDet [120] ROI Transformer  ? [119] Gliding Vertex [121] FAOD [122] CenterMap-Net [123] FR-Est [124] Mask OBB [125] ORCN  ? [126] ORCN  ? [126] ORCN ORCN ORCN ORCN ORCN ORCN ORCN</cell><cell>IMP-ResNet-101-FPN IMP-ResNet-50-FPN IMP-ResNet-101-FPN IMP-ResNet-101-FPN IMP-ResNet-101-FPN IMP-ResNet-50-FPN IMP-ResNet-101-FPN IMP-ResNet-101-FPN IMP-ResNet-50-FPN IMP-ResNet-101-FPN IMP-ResNet-50-FPN IMP-ResNet-50-FPN IMP-ResNet-101-FPN IMP-ResNet-50-FPN SeCo-ResNet-50-FPN RSP-ResNet-50-FPN IMP-Swin-T-FPN RSP-Swin-T-FPN IMP-ViTAEv2-S-FPN RSP-ViTAEv2-S-FPN</cell><cell>68.16 69.05 69.90 69.56 72.61 74.61 75.02 73.28 71.74 74.20 74.86 75.87 76.28 76.14 70.02 76.50 76.07 76.12 77.38 77.72*</cell><cell>69.98 77.11 76.60 83.59 72.41 86.87 86.82 79.56 78.10 86.44 85.57 88.20* 87.52 88.16 86.33 88.17 88.02 87.83 88.14 88.04</cell><cell>78.20 83.90 73.30 81.46 86.86* 82.51 86.81 84.68 83.61 83.56 85.05 84.68 85.33 84.91 81.31 85.72 84.92 84.84 86.35 85.58</cell><cell>74.30 73.06 82.40 78.52 80.65 82.60 85.00 79.58 81.24 81.17 85.09* 82.12 83.48 81.35 73.32 81.88 82.23 79.74 83.50 83.04</cell><cell>90.76 90.84 90.90* 90.74 90.85 90.71 90.74 90.83 88.83 90.82 90.37 90.90* 90.90* 90.90* 90.88 90.84 90.90* 90.86 90.90* 90.90*</cell><cell>79.06 78.94 79.20 77.27 87.94 83.83 79.02 83.40 77.80 84.13 82.08 87.50 85.56 87.43 79.46 86.17 87.42 85.90 87.51 88.17*</cell><cell>70.32 59.09 73.50 75.92 68.36 70.87 77.34* 76.41 60.65 70.19 72.90 70.86 76.92 71.35 67.07 70.91 74.37 74.50 75.38 75.16</cell><cell>47.70 44.86 49.40 43.44 52.09 52.53 52.26 45.49 53.15 50.44 51.85 54.78 55.27 54.86 49.94 54.39 52.25 52.91 53.42 55.85*</cell><cell>67.82 71.49 63.50 73.68 60.32 76.67 73.14 68.27 66.55 77.98 73.23 83.00 82.10 83.03 76.48 83.01 83.55 84.02 85.15* 84.34</cell><cell cols="2">64.89 73.25 71.10 68.81 68.36 77.93 73.01 73.18 78.62 73.52 75.28 78.93 74.27 79.04 76.15 78.67 77.99 78.96 79.99* 79.95</cell><cell>50.23 56.18 62.20 47.67 65.21 61.03 57.32 64.86 58.70 60.55 66.33 52.28 57.28 58.14 49.71 62.22 63.07 57.36 66.03 67.89</cell><cell>64.17 64.91 67.00 58.93 68.24 68.75 70.86 69.69 72.36 66.72 69.87 68.84 70.15 69.05 65.32 72.21 69.30 70.61 66.12 67.15</cell><cell>62.90 62.95 60.90 53.54 66.68 67.61 70.91* 65.42 66.19 66.59 68.39 67.69 66.82 66.67 58.55 67.45 65.99 67.33 70.91* 70.60</cell><cell>53.64 48.59 48.40 58.39 65.02 53.95 59.55 53.40 49.36 60.64 55.73 63.97 65.51* 63.39 41.31 62.22 57.70 62.90 61.02 62.64</cell><cell>81.36 88.44 87.80 88.64 89.98 88.65 89.64 90.21* 88.88 89.63 89.61 89.46 88.86 89.58 88.64 89.78 89.48 89.54 89.27 89.66</cell><cell>67.02 62.18 62.00 62.83 66.25 74.67 72.94 74.17 72.10 70.59 71.61 74.94 74.36 74.19 65.90 73.99 73.88 74.45 76.95* 76.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX RESULTS</head><label>IX</label><figDesc>OF THE ORCN DETECTION MODEL WITH DIFFERENT BACKBONES AND SOTA METHODS ON THE TESTING SET OF THE HRSC2016 DATASET. ?: THE RESULT IS FROM THE ORIGINAL ORCN Table VIII-IX show the results of OBB detection experiments.</figDesc><table><row><cell></cell><cell>PAPER.</cell><cell></cell></row><row><cell>Method R2PN [127] RRD [128] FoRDet [129] R2CNN [130] Rotated RPN [131] ROI Transformer [119] Gliding Vertex [121] GRS-Det [132] GRS-Det [132] R3Det [114] DAL [112] AproNet [115]</cell><cell>Backbone IMP-VGG-16 IMP-VGG-16 IMP-VGG-16 IMP-ResNet-101 IMP-ResNet-101 IMP-ResNet-101-FPN IMP-ResNet-101-FPN IMP-ResNet-50-FPN IMP-ResNet-101-FPN IMP-ResNet-101-FPN IMP-ResNet-101-FPN IMP-ResNet-101-FPN</cell><cell>mAP 79.6 84.3 89.9 73.1 79.1 86.2 88.2 88.9 89.6 89.3 89.8 90.0</cell></row><row><cell>S 2 ANet [115] ORCN  ? [126] CHPDet [133] ORCN ORCN ORCN ORCN ORCN ORCN ORCN</cell><cell>IMP-ResNet-101-FPN IMP-ResNet-50-FPN Hourglass104 IMP-ResNet-50-FPN SeCo-ResNet-50-FPN RSP-ResNet-50-FPN IMP-Swin-T-FPN RSP-Swin-T-FPN IMP-ViTAEv2-S-FPN RSP-ViTAEv2-S-FPN</cell><cell>90.2 90.4 90.6* 90.4 88.9 90.3 89.7 90.0 90.4 90.4</cell></row><row><cell cols="3">while the original testing sets of DOTA and HRSC2016 are separately used for evaluation. We report the mean average precision (mAP) of all categories and the average precision (AP) of each class on the corresponding testing set. All models are trained on a single V100 GPU.</cell></row><row><cell cols="3">3) Experimental Results: Quantitative Results and Anal-</cell></row><row><cell>yses:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>:</head><label></label><figDesc>The original dataset contains 11 pairs of multisource real season-varying RS images collecting from GE, where 7 pairs of images are at the size of 4,725 ? 2,200 and 4 image pairs are at the size of 1,900 ? 1,000 pixels. The resolutions are ranging from 0.03m to 1m. Then,<ref type="bibr" target="#b170">[139]</ref> clipped the images to a series of 256 ? 256 patches and generated a dataset, where the sizes of the training, validation and testing set are 10,000/3,000/3,000, respectively.</figDesc><table /><note>? CDD? LEVIR: This dataset is collected using the GE API on 20 different regions in Texas, the USA, from 2002 to 2018. It contains 637 image pairs at the size of 1,024 ? 1,024 and with a high resolution of 0.5m, where most changes are from man-made structures, including 31,333 independent building change entities. The training, validation, and testing set contain 445/64/128 image pairs, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE X RESULTS</head><label>X</label><figDesc>OF THE BIT CHANGE DETECTION MODEL WITH DIFFERENT BACKBONES AND SOTA METHODS ON THE TESTING SET OF CDD AND LEVIR DATASETS. ?: THE RESULT IS FROM THE ORIGINAL BIT PAPER.</figDesc><table><row><cell>Method FC-EF [141] FC-Siam-conc [141] FC-Siam-diff [141] CLNet [142] SNUNet-c48 [143] IFN [144] DASNet [145] SRCDNet [146] STANet [138] BSFNet [147] DSAMNet [148] HRTNet [149] CDNet+IAug [150] BIT ? [140] ChangeFormer [151] CS-HSNet [152] LSS-Net [153] ChangeStar [154] BIT BIT BIT BIT BIT BIT BIT</cell><cell>Backbone -----IMP-VGG-16 IMP-VGG-16 IMP-ResNet-18 IMP-ResNet-18 IMP-ResNet-18 IMP-ResNet-18 IMP-HRNet-W18 IMP-ResNet-18 IMP-ResNet-18 IMP-MiT-B2 [47] IMP-ResNet-50 IMP-SE-ResNet-50 IMP-ResNext-101-32?4d IMP-ResNet-50 SeCo-ResNet-50 RSP-ResNet-50 IMP-Swin-T RSP-Swin-T IMP-ViTAEv2-S RSP-ViTAEv2-S</cell><cell>F1 score CDD LEVIR 77.11 62.32 82.50 68.21 83.73 63.09 92.10 90.00 96.20 -90.30 83.57 91.93 82.83 90.02 -90.75 87.34 91.90 88.00 93.69 -93.71 88.48 -89.00 -89.31 -90.40 94.95 90.79 96.30 --91.25 95.09 89.19 90.14 95.95 96.00 90.10 90.25 94.77 95.21 90.10 97.02* 91.26* 96.81 90.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XI OVERALL</head><label>XI</label><figDesc>COMPARISONS OF DIFFERENT BACKBONES TRAINED WITH DIFFERENT EPOCHS ON ALL DOWNSTREAM TASKS.</figDesc><table><row><cell>Backbone RSP-ResNet-50-E120 RSP-ResNet-50-E300 RSP-Swin-T-E120 RSP-Swin-T-E300 RSP-ViTAEv2-S-E40 RSP-ViTAEv2-S-E100</cell><cell>Scene Recognition 96.53 96.63 96.06 96.44 96.76 97.01</cell><cell>Semantic Segmentation 75.70 75.68 76.17 77.07 76.95 77.45</cell><cell>Object Detection 82.51 83.39 83.08 82.80 83.65 83.98</cell><cell>Change Detection 92.82 93.05 92.28 92.66 93.08 93.87</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-labelpotsdam.aspx</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/jbwang1997/OBBDetection</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">ST: storage tank. BD: baseball diamond. TC: tennis court. BC: baseball court. GTF: ground track field. LV: large vehicle. SV: small vehicle. HC: helicopter. SP: swimming pool. RA: roundabout. SBF: soccer ball field.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The authors would like to thank PhD candidate Yang Long and Prof. Gui-Song Xia for providing the MillionAID dataset and PhD candidate Qiming Zhang for offering the ViTAE series models. This work was done by Di Wang as the research intern in JD Explore Academy.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crop classification based on feature band set construction and object-oriented approach using hyperspectral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4117" to="4128" />
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Estimating soil salinity under various moisture conditions: An experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2525" to="2533" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00130487</idno>
		<ptr target="https://doi.org/10.1007/BF00130487" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Textural features for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1011139631724</idno>
		<ptr target="https://doi.org/10.1023/A:1011139631724" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Block-based semantic classification of high-resolution multispectral aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Avramovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Risojevi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Video Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="84" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Signal</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Do deep features generalize from everyday objects to remote sensing and aerial scenes domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning by probabilistic latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="196" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Category anchorguided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">DSP: Dual Soft-Paste for Unsupervised Domain Adaptive Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3474085.3475186</idno>
		<ptr target="https://doi.org/10.1145/3474085.3475186" />
		<imprint>
			<date type="published" when="2021" />
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page">28252833</biblScope>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive spectralspatial multiscale contextual feature extraction for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2461" to="2477" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stagewise unsupervised domain adaptation with adversarial self-training for road segmentation of remote-sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully contextual network for hyperspectral scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Learning-Based Classification of Hyperspectral Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classification of hyperspectral image based on deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="5132" to="5136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep feature aggregation framework driven by graph convolutional network for scene classification in remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Remote sensing scene classification by gated bidirectional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="96" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Embedded self-distillation in compact multibranch ensemble network for remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mgml: Multigranularity multilevel feature ensemble network for remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rotation-invariant deep embedding for remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fernandez-Beltran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On creating benchmark dataset for aerial image interpretation: Reviews, guidances and million-aid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="4205" to="4230" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Aerial scene parsing: From tile-level scene classification to pixel-wise semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Crop classification based on feature band set construction and object-oriented approach using hyperspectral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4117" to="4128" />
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Empowering things with intelligence: a survey of the progress, challenges, and opportunities in artificial intelligence of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="7789" to="7817" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Estimating soil salinity under various moisture conditions: An experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2525" to="2533" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Textural features for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Do deep features generalize from everyday objects to remote sensing and aerial scenes domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised learning by probabilistic latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="196" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Vitae: Vision transformer advanced by exploring intrinsic inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="418" to="434" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Category anchor-guided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dsp: Dual soft-paste for unsupervised domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia, 2021</title>
		<imprint>
			<biblScope unit="page" from="2825" to="2833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adaptive spectral-spatial multiscale contextual feature extraction for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2461" to="2477" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stagewise unsupervised domain adaptation with adversarial self-training for road segmentation of remote-sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fully contextual network for hyperspectral scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep feature aggregation framework driven by graph convolutional network for scene classification in remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Remote sensing scene classification by gated bidirectional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="96" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Embedded self-distillation in compact multibranch ensemble network for remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mgml: Multigranularity multilevel feature ensemble network for remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep Unsupervised Embedding for Remotely Sensed Images Based on Spatially Augmented Momentum Contrast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fernandez-Beltran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2598" to="2610" />
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On creating benchmark dataset for aerial image interpretation: Reviews, guidances and million-aid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="4205" to="4230" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10108</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Enhanced feature pyramid network with deep semantic embedding for remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="7918" to="7932" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Local semantic enhanced convnet for aerial scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6498" to="6511" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Auto learning attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1488" to="1500" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Remote sensing scene classification via multi-branch local attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y.</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="99" to="109" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A multipleinstance densely-connected convnet for aerial scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4911" to="4926" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification meets deep learning: Challenges, methods, benchmarks, and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="3735" to="3756" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing applications: A meta-analysis and review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS-J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="166" to="177" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Object detection in optical remote sensing images: A survey and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS-J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="296" to="307" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Change detection based on artificial intelligence: State-of-the-art and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1688</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A survey of the usages of deep learning for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Kalita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="604" to="624" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A survey on vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR, 2021</title>
		<imprint>
			<biblScope unit="page" from="10" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Exploring sequence feature alignment for domain adaptive detection transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia, 2021</title>
		<imprint>
			<biblScope unit="page" from="1730" to="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">FP-DETR: Detection transformer advanced by fully pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Towards dataefficient detection transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09507</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Recurrent glimpse-based decoder for detection with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04632</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">When cnns meet vision transformer: A joint framework for remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Geographical knowledgedriven representation learning for remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Regioncl: Can simple region swapping contribute to contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12309</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Seasonal contrast: Unsupervised pre-training from uncurated remote sensing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ma?as</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9414" to="9423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Geography-aware self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tanmay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lobell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">190</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">The color out of space: learning self-supervised representations for earth observation imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vincenzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buzzega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cipriano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fronte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cuccu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ippoliti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3034" to="3041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Functional map of the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Christie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fendley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="6172" to="6180" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Bigearthnet: A large-scale benchmark archive for remote sensing image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sumbul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charfuelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Markl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IGARSS</title>
		<imprint>
			<biblScope unit="page" from="5901" to="5904" />
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words and spatial extensions for land-use classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GEOProcessing</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Aid: A benchmark data set for performance evaluation of aerial scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3965" to="3981" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Remote Sensing Image Scene Classification Based on an Enhanced Attention Module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1926" to="1930" />
			<date type="published" when="2021-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Best Representation Branch Model for Remote Sensing Image Scene Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="9768" to="9780" />
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Gated Recurrent Multiattention Network for VHR Remote Sensing Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">3093914</biblScope>
			<date type="published" when="2022-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Invariant Deep Compressible Covariance Pooling for Aerial Scene Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="6549" to="6561" />
			<date type="published" when="2021-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Scene Classification With Recurrent Attention of VHR Remote Sensing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1155" to="1167" />
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Skip-connected covariance network for remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1461" to="1474" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">High-Resolution Remote Sensing Image Scene Classification via Key Filter Bank Based on Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="8077" to="8092" />
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Multi-Granularity Canonical Appearance Pooling for Remote Sensing Scene Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5396" to="5407" />
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">A Multiscale Attention Network for Remote Sensing Scene Images Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Yk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="9530" to="9545" />
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chattopadhay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WACV. IEEE</publisher>
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">isaid: A large-scale dataset for instance segmentation in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="28" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Relation matters: Relational contextaware fully convolutional network for semantic segmentation of highresolution aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="7557" to="7569" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Developing a multi-filter convolutional neural network for semantic segmentation using highresolution aerial imagery and LiDAR data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3141" to="3149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="801" to="818" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Rest: An efficient transformer for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Parsing very high resolution urban scene images by learning deep ConvNets with edgeaware loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="15" to="28" />
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">LANet: Local Attention Embedding to Improve the Semantic Segmentation of Remote Sensing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="426" to="435" />
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Denseu-net-based semantic segmentation of small objects in urban remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="65" to="347" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6392" to="6401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Expectationmaximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9166" to="9175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7151" to="7160" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">A relation-augmented fully convolutional network for semantic segmentation in aerial scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Alignseg: Feature-aligned segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="550" to="557" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Hybrid multiple attention network for semantic segmentation in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Foreground-aware relation network for geospatial object segmentation in high spatial resolution remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Factseg: Foreground activation-driven small object semantic segmentation in large-scale remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Aerialdetection</title>
		<ptr target="https://github.com/dingjiansw101/AerialDetection" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Dynamic anchor learning for arbitrary-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2355" to="2363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Learning modulated loss for rotated object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2458" to="2466" />
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">R3det: Refined single-stage detector with feature refinement for rotating object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3163" to="3171" />
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Align Deep Features for Oriented Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">3062048</biblScope>
			<date type="published" when="2022-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">CAD-Net: A Context-Aware Detection Network for Objects in Remote Sensing Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno>pp. 10 015-10 024</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2844" to="2853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8231" to="8240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1452" to="1459" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Featureattentioned object detection in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="3886" to="3890" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9756" to="9765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Point-based estimator for arbitrary-oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4370" to="4387" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Mask OBB: A Semantic Attention-Based Mask Oriented Bounding Box Representation for Multi-Category Object Detection in Aerial Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">2930</biblScope>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Oriented r-cnn for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="3520" to="3529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Toward Arbitrary-Oriented Ship Detection With Rotated Region Proposal and Discrimination Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1745" to="1749" />
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Foreground refinement network for rotated object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">R2cnn: Rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Grs-det: An anchor-free rotation ship detector based on gaussian-mask in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3518" to="3531" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Arbitraryoriented ship detection through center-head point extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Recursive context routing for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="160" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">A high resolution optical satellite image dataset for ship recognition and some new baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPRAM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="324" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Change Detection in Remote Sensing Images Using Conditional Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">V</forename><surname>Vizilter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">V</forename><surname>Vygolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Knyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Rubis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogrammetry, Remote Sens. Spatial Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">422</biblScope>
			<biblScope unit="page" from="565" to="571" />
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">A spatial-temporal attention-based method and a new dataset for remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Remote Sensing Image Change Detection With Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">3095166</biblScope>
			<date type="published" when="2022-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Fully convolutional siamese networks for change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICIP</title>
		<imprint>
			<biblScope unit="page" from="4063" to="4067" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">CLNet: Cross-layer convolutional neural network for change detection in optical remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="247" to="267" />
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">SNUNet-CD: A Densely Connected Siamese Network for Change Detection of VHR Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">3056416</biblScope>
			<date type="published" when="2022-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">A deeply supervised image fusion network for change detection in high resolution bi-temporal remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tapete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="183" to="200" />
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">DASNet: Dual Attentive Fully Convolutional Siamese Networks for Change Detection in High-Resolution Satellite Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1194" to="1206" />
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Super-Resolution-Based Change Detection Network With Stacked Attention Module for Images With Different Resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marinoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">3091758</biblScope>
			<date type="published" when="2022-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Bilateral Semantic Fusion Siamese Network for Change Detection From Multitemporal Optical Remote Sensing Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">3082630</biblScope>
			<date type="published" when="2022-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">A deeply supervised attention metric-based network and an open aerial image dataset for remote sensing change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">High-resolution triplet network with dynamic multiscale feature for change detection on satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="103" to="115" />
			<date type="published" when="2021-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Adversarial Instance Augmentation for Building Change Detection in Remote Sensing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">3066802</biblScope>
			<date type="published" when="2022-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<monogr>
		<title level="m" type="main">A transformer-based siamese network for change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G C</forename><surname>Bandara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.01293</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">CS-HSNet: A Cross-Siamese Change Detection Network Based on Hierarchical-Split Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Local Similarity Siamese Network for Urban Land Change Detection on Remote Sensing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="4139" to="4149" />
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Change is everywhere: Single-temporal supervised object change detection in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15" to="173" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
