<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly-Supervised 3D Human Pose Learning via Multi-view Images in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
							<email>uiqbal@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
							<email>pmolchanov@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
							<email>jkautz@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvidia</forename></persName>
						</author>
						<title level="a" type="main">Weakly-Supervised 3D Human Pose Learning via Multi-view Images in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One major challenge for monocular 3D human pose estimation in-the-wild is the acquisition of training data that contains unconstrained images annotated with accurate 3D poses. In this paper, we address this challenge by proposing a weakly-supervised approach that does not require 3D annotations and learns to estimate 3D poses from unlabeled multi-view data, which can be acquired easily in in-the-wild environments. We propose a novel end-to-end learning framework that enables weakly-supervised training using multi-view consistency. Since multi-view consistency is prone to degenerated solutions, we adopt a 2.5D pose representation and propose a novel objective function that can only be minimized when the predictions of the trained model are consistent and plausible across all camera views. We evaluate our proposed approach on two large scale datasets (Human3.6M and MPII-INF-3DHP) where it achieves state-of-the-art performance among semi-/weaklysupervised methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning to estimate 3D body pose from a single RGB image is of great interest for many practical applications. The state-of-the-art methods <ref type="bibr">[6, 16, 17, 28, 32, 39-41, 52, 53]</ref> in this area use images annotated with 3D poses and train deep neural networks to directly regress 3D pose from images. While the performance of these methods has improved significantly, their applicability in in-the-wild environments has been limited due to the lack of training data with ample diversity. The commonly used training datasets such as Human3.6M <ref type="bibr" target="#b9">[10]</ref>, and MPII-INF-3DHP <ref type="bibr" target="#b21">[22]</ref> are collected in controlled indoor settings using sophisticated multi-camera motion capture systems. While scaling such systems to unconstrained outdoor environments is impractical, manual annotations are difficult to obtain and prone to errors. Therefore, current methods resort to existing training data and try to improve the generalizabilty of trained models by incorporating additional weak supervision in form of various 2D annotations for in-the-wild images <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b51">52]</ref>. While 2D annotations can be obtained eas-ily, they do not provide sufficient information about the 3D body pose, especially when the body joints are foreshortened or occluded. Therefore, these methods rely heavily on the ground-truth 3D annotations, in particular, for depth predictions.</p><p>Instead of using 3D annotations, in this work, we propose to use unlabeled multi-view data for training. We assume this data to be without extrinsic camera calibration. Hence, it can be collected very easily in any in-the-wild setting. In contrast to 2D annotations, using multi-view data for training has several obvious advantages e.g., ambiguities arising due to body joint occlusions as well as foreshortening or motion blur can be resolved by utilizing information from other views. There have been only few works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> that utilize multi-view data to learn monocular 3D pose estimation models. While the approaches <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33]</ref> need extrinsic camera calibration, <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> require at least some part of their training data to be labelled with ground-truth 3D poses. Both of these requirements are, however, very hard to acquire for unconstrained data, hence, limit the applicability of these methods to controlled indoor settings. In <ref type="bibr" target="#b13">[14]</ref>, 2D poses obtained from multiple camera views are used to generate pseudo ground-truths for training. However, this method uses a pre-trained pose estimation model which remains fixed during training, meaning 2D pose errors remain unaddressed and can propagate to the generated pseudo ground-truths.</p><p>In this work, we present a weakly-supervised approach for monocular 3D pose estimation that does not require any 3D pose annotations at all. For training, we only use a collection of unlabeled multi-view data and an independent collection of images annotated with 2D poses. An overview of the approach can be seen in <ref type="figure">Fig. 1</ref>. Given an RGB image as input, we train the network to predict a 2.5D pose representation <ref type="bibr" target="#b11">[12]</ref> from which the 3D pose can be reconstructed in a fully-differentiable way. Given unlabeled multi-view data, we use a multi-view consistency loss which enforces the 3D poses estimated from different views to be consistent up to a rigid transformation. However, naively enforcing multi-view consistency can lead to degenerated solutions. We, therefore, propose a novel objective function which is constrained such that it can only be minimized when the 3D poses are predicted correctly from all camera views. The proposed approach can be trained in a fully end-to-end manner, it does not require extrinsic camera calibration and is robust to body part occlusions and truncations in the unlabeled multi-view data. Furthermore, it can also improve the 2D pose predictions by exploiting multi-view consistency during training.</p><p>We evaluate our approach on two large scale datasets where it outperforms existing methods for semi-/weaklysupervised methods by a large margin. We also show that the MannequinChallenge dataset <ref type="bibr" target="#b17">[18]</ref>, which provides inthe-wild videos of people in static poses, can be effectively exploited by our proposed method to improve the generalizability of trained models, in particular, when their is a significant domain gap between the training and testing environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We discuss existing methods for monocular 3D human pose estimation with varying degree of supervision.</p><p>Fully-supervised methods aim to learn a mapping from 2D information to 3D given pairs of 2D-3D correspondences as supervision. The recent methods in this direction adopt deep neural networks to directly predict 3D poses from images <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">53]</ref>. Training the data hungry neural networks, however, requires large amounts of training images with accurate 3D pose annotations which are very hard to acquire, in particular, in unconstrained scenarios. To this end, the approaches in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45]</ref> try to augment the training data using synthetic images, however, still need real data to obtain good performance. More recent methods try to improve the performance by incorporating additional data with weak supervision i.e., 2D pose annotations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b51">52]</ref>, boolean geometric relationship between body parts <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref>, action labels <ref type="bibr" target="#b19">[20]</ref>, and temporal consistency <ref type="bibr" target="#b1">[2]</ref>. Adverserial losses during training <ref type="bibr" target="#b49">[50]</ref> or testing <ref type="bibr" target="#b43">[44]</ref> have also been used to improve the performance of models trained on fully-supervised data.</p><p>Other methods alleviate the need of 3D image annotations by directly lifting 2D poses to 3D without using any image information e.g., by learning a regression network from 2D joints to 3D <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> or by searching nearest 3D poses in large databases using 2D projections as the query <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31]</ref>. Since these methods do not use image information for 3D pose estimation, they are prone to reprojection ambiguities and can also have discrepancies between the 2D and 3D poses.</p><p>In contrast, in this work, we present a method that combines the benefits of both paradigms i.e., it estimates 3D pose from an image input, hence, can handle the reprojection ambiguities, but does not require any images with 3D pose annotations.</p><p>Semi-supervised methods require only a small subset of training data with 3D annotations and assume no or weak supervision for the rest. The approaches <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b50">51]</ref> assume that multiple views of the same 2D pose are available and use multi-view constraints for supervision. Closest to our approach in this category is <ref type="bibr" target="#b33">[34]</ref> in that it also uses multiview consistency to supervise the pose estimation model. However, their method is prone to degenerated solutions and its solution space cannot be constrained easily. Consequently, the requirement of images with 3D annotations is inevitable for their approach. In contrast, our method is weakly-supervised. We constrain the solution space of our method such that the 3D poses can be learned without any 3D annotations. In contrast to <ref type="bibr" target="#b33">[34]</ref>, our approach can easily be applied to in-the-wild scenarios as we will show in our experiments. The approaches <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b47">48]</ref> use 2D pose annotations and re-projection losses to improve the performance of models pre-trained using synthetic data. In <ref type="bibr" target="#b18">[19]</ref>, a pretrained model is iteratively improved by refining its predictions using temporal information and then using them as supervision for next steps. The approach in <ref type="bibr" target="#b29">[30]</ref> estimates the 3D poses using a sequence of 2D poses as input and uses a re-projection loss accumulated over the entire sequence for supervision. While all of these methods demonstrate impressive results, their main limiting factor is the need of ground-truth 3D data.</p><p>Weakly-supervised methods do not require paired 2D-3D data and only use weak supervision in form of motioncapture data <ref type="bibr" target="#b41">[42]</ref>, images/videos with 2D annotations <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b46">47]</ref>, collection of 2D poses <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b45">46]</ref>, or multi-view images <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref>. Our approach also lies in this paradigm and learns to estimate 3D poses from unlabeled multi-view data. In <ref type="bibr" target="#b41">[42]</ref>, a probabilistic 3D pose model learned using motion-capture data is integrated into a multi-staged 2D pose estimation model to iteratively refine 2D and 3D pose predictions. The approach <ref type="bibr" target="#b24">[25]</ref> uses a re-projection loss to train the pose estimation model using images with only 2D pose annotations. Since re-projection loss alone is insufficient for training, they factorize the problem into the estimation of view-point and shape parameters and provide inductive bias via a canonicalization loss. Similar in spirit, the approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b45">46]</ref> use collection of 2D poses with reprojection loss for training and use adversarial losses to distinguish between plausible and in-plausible poses. In <ref type="bibr" target="#b46">[47]</ref>, non-rigid structure from motion is used to learn a 3D pose estimator from videos with 2D pose annotations. The closest to our work are the approaches of <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref> in that they also use unlabeled multi-view data for training. The approach of <ref type="bibr" target="#b28">[29]</ref>, however, requires calibrated camera views that are very hard to acquire in unconstrained environments. The approach <ref type="bibr" target="#b13">[14]</ref> estimates 2D poses from multi-view images and reconstructs corresponding 3D pose using Epipolar geometry. The reconstructed poses are then used for  <ref type="figure">Figure 1</ref>. An end-to-end approach for learning 3D pose estimation model without 3D annotations. For training, we only use unlabeled multi-view data along with an independent collection of images with 2D pose annotations. Given an RGB image, the model is trained to generate 2D heatmaps H 2D and latent depth-maps H z -shown only for I2 for simplicity. The 2D heatmaps are converted to 2D pose coordinates using soft-argmax. The relative-depth values? r are obtained by taking channel-wise summation of the multiplication of normalized heatmapsH 2D and latent depth-maps H z . The 3D pose is reconstructed in a fully differentiable manner by exploiting the scale normalization constraint (Sec. 3.1). The images with 2D pose annotation are used for heatmap loss LH. The 3D supervision is provided via a multi-view consistency loss LMC that enforces that the 3D poses generated from different views should be identical up to a rigid transform. Given 2D pose estimates from different views and camera intrinsics, the objective is designed such that the only way for the network to minimize it is to produce correct relative depth values? r (Sec. 3.3). We also enforce a bone-length loss LB on each predicted 3D pose to further constrain the search space.</p><p>training in a fully-supervised way. The main drawback of this method is that the 3D poses remain fixed throughout the training, and the errors in 3D reconstruction directly propagate to the trained models. This is, particularly, problematic if the multi-view data is captured in challenging outdoor environments where 2D pose estimation may fail easily. In contrast, in this work, we propose an end-to-end learning framework which is robust to challenges posed by the data captured in in-the-wild scenarios. It is trained using a novel objective function which can simultaneously optimize for 2D and 3D poses. In contrast to <ref type="bibr" target="#b13">[14]</ref>, our approach can also improve 2D predictions using unlabeled multi-view data. We evaluate our approach on two challenging datasets where it outperforms existing methods for semi-/weaklysupervised learning by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our goal is to train a convolutional neural network F(I, ?) parameterized by weights ? that, given an RGB image I as input, estimates the 3D body pose P = {p j } j?J consisting of 3D locations p j = (x j , y j , z j ) ? R 3 of J body joints with respect to the camera.</p><p>We do not assume any training data with paired 2D-3D annotations and learn the parameters ? of the network in a weakly-supervised way using unlabeled multi-view images and an independent collection of images with 2D pose annotations. To this end, we build on the 2.5D pose repre-sentation of <ref type="bibr" target="#b11">[12]</ref> for hand pose estimation and extend it to human body. This 2.5D pose representation has several key features that allow us to exploit multi-view information and devise loss functions for weakly-supervised training.</p><p>In the following, we first recap the 2.5D pose representation (Sec. 3.1) and the approach to reconstruct absolute 3D pose from it (Sec. 3.1.1). We then describe a fullysupervised approach to regress the 2.5D pose using a convolutional neural network (Sec.3.2) followed by our proposed method for weakly-supervised training in Sec. 3.3. An overview of the proposed approach can be seen in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">2.5D Pose Representation</head><p>Many existing methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref> for 3D body pose estimation adopt a 2.5D pose representation</p><formula xml:id="formula_0">P 2.5D = {p 2.5D j = (u j , v j , z r j )} j?J</formula><p>where u j and v j are the 2D projection of the body joint j on a camera plane and z r j = z root ?z j represents its metric depth with respect to the root joint. This decomposition of 3D joint locations into their 2D projection and relative depth has the advantage that additional supervision from in-the-wild images with only 2D pose annotations can be used for better generalization of the trained models. However, this representation does not account for scale ambiguity present in the image which might lead to ambiguities in predictions.</p><p>The 2.5D representation of <ref type="bibr" target="#b11">[12]</ref>, however, differs from the rest in terms of scale normalization of 3D poses. Specifically, they scale normalize the 3D pose P such that a spe-cific pair of body joints has a unit distance:</p><formula xml:id="formula_1">P = P s ,<label>(1)</label></formula><p>where s = p k ? p l 2 is estimated independently for each pose. The pair (k, l) corresponds to the indices of the joints used for scale normalization. The resulting scale normalized 2.5D pose representationp 2.5D j = (u j , v j ,? r j ) is agnostic to the scale of the person. This not only makes it easier to be estimated from cropped RGB images, but also allows to reconstruct the absolute 3D pose of the person up to a scaling factor in a fully differentiable manner as described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Differentiable 3D Reconstruction</head><p>Given the 2.5D poseP 2.5D , we need to find the depth? root of the root joint to reconstruct the scale normalized 3D lo-cationsP of body joints using perspective projection:</p><formula xml:id="formula_2">p j =? j K ?1 ? ? u j v j 1 ? ? = (? root +? r j )K ?1 ? ? u j v j 1 ? ? . (2)</formula><p>The value of? root can be calculated via the scale normalization constraint:</p><formula xml:id="formula_3">(x k ?x l ) 2 + (? k ?? l ) 2 + (? k ?? l ) 2 = 1,<label>(3)</label></formula><p>which leads to an analytical solution as derived in <ref type="bibr" target="#b11">[12]</ref>. Since all operations for 3D reconstruction are differentiable, we can devise loss functions that directly operate on the reconstructed 3D poses.</p><p>In the rest of this paper, we will use the scale normalized 2.5D pose representation. We use the distance between the neck and pelvis joints to calculate the scaling factor s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">2.5D Pose Regression</head><p>Since the 3D pose can be reconstructed analytically from 2.5D pose, we train the network to predict 2.5D pose and implement 3D reconstruction as an additional parameterfree layer. To this end, we adopt the 2.5D heatmap regression approach of <ref type="bibr" target="#b11">[12]</ref>. Specifically, given an RGB image as input, the network produces 2J channels as output with J channels for 2D heatmaps (H 2D ) while the remaining J channels are regarded as latent depth maps H z . The 2D heatmaps are converted to 2D pose coordinates (u j , v j ) by first normalizing them using spatial softmax, i.e.,H 2D j = softmax(H 2D j , ?), and then using the soft-argmax operation:</p><formula xml:id="formula_4">uj = u,v?U u ?H 2D j (u, v); vj = u,v?U v ?H 2D j (u, v),<label>(4)</label></formula><p>where U is a 2D grid sampled according to the effective stride size of the network, and ? is a constant that controls the temperature of the normalized heatmaps.</p><p>The relative scale normalized depth value? r j for each joint can then be obtained as the summation of the elementwise multiplication ofH 2D j and latent depth maps H z j :</p><formula xml:id="formula_5">z r j = u,vH 2D j H z j .<label>(5)</label></formula><p>Given the 2D pose coordinates {(u j , v j )} j?J , relative depths? r = {? r j } j?J and intrinsic camera parameters K, the 3D pose can be reconstructed as explained in Sec. 3.1.1.</p><p>In the fully-supervised (FS) setting, the network can be trained using the following loss function:</p><formula xml:id="formula_6">L FS = L H (H 2D , H 2D gt ) + ?L z (? r ,? r gt ),<label>(6)</label></formula><p>where H 2D gt and? r gt are the ground-truth 2D heatmaps and ground-truth scale-normalized relative depth values, respectively. We use mean squared error as the loss functions L H (?) and L z (?).</p><p>We make one modification to the original loss to better learn the confidence scores of predictions. Specifically, in contrast to <ref type="bibr" target="#b11">[12]</ref>, we do not learn 2D heatmaps in a latent way. Instead, we chose to explicitly supervise the 2D heatmap predictions via ground-truth heatmaps with Gaussian distributions at the true joint locations. We will rely on the confidence scores to devise a weakly-supervised loss that is robust to uncertainties in 2D pose estimates, as described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Weakly-Supervised Training</head><p>We describe our proposed approach for training the regression network in a weakly-supervised way without any 3D annotations. For training, we assume a set M = {{I n c } c?Cn } n?N of N samples, with the n th sample consisting of C n camera views of a person in same body pose. The multi-view images can be taken at the same time using multiple cameras, or using a single camera assuming a static body pose over time. We do not assume knowledge of extrinsic camera parameters. Additionally, we use an independent set of images annotated only with 2D poses which is available abundantly or can be annotated by people even for in-the-wild data. For training, we optimize the following weakly-supervised (WS) loss function:</p><formula xml:id="formula_7">L WS = L H (H 2D , H 2D gt ) + ?L MC (M) + ?L B (L,? L ),<label>(7)</label></formula><p>where L H is the 2D heatmap loss, L MC is the multi-view consistency loss, and L B is the limb length loss.</p><p>Recall that, given an RGB image, our goal is to estimate the scale normalized 2.5D poseP 2.5D = {p 2.5D j = (u j , v j ,? r j )} j?J from which we can reconstruct the scale normalized 3D poseP as explained in Sec. 3.1.1. While L H provides supervision for 2D pose estimation, the loss L MC supervises the relative depth component (? r ). The limb length loss L B further ensures that the reconstructed 3D poseP has plausible limb lengths. In the following, we explain these loss functions in more detail.</p><p>Heatmap Loss (L H ) measures the difference between the predicted 2D heatmaps H 2D and ground-truth heatmaps H 2D gt with Gaussian distribution at the true joint location. It operates only on images annotated with 2D poses and is assumed to be zero for all other images.</p><p>Multi-View Consistency Loss (L MC ) enforces that the 3D pose estimates obtained from different views should be identical up to a rigid transform. Formally, given a multiview training sample M = {I c } c?C with C camera views, we define the multi-view consistency loss as the weighted sum of the difference between the 3D joint locations across different views after the rigid alignment:</p><formula xml:id="formula_8">L MC = c,c ?C c =c j?J ? j,c ? j,c ? d(p j,c , R c cpj,c ), (8) where ? j,c = H 2D j,c (u j,c , v j,c ) and ? j,c = H 2D j,c (u j,c , v j,c )</formula><p>are the confidence scores of the j th joint in camera viewpoint I c and I c , respectively. Thep j,c andp j,c are the scale normalized 3D coordinates of the j th joint estimated from viewpoint I c and I c , respectively. R c c ? R 3?4 is a rigid transformation matrix that best aligns the two 3D poses, and d is the distance metric used to measure the difference between the aligned poses. In this work, we use L 1 -norm as the distance metric d. In order to understand the contribution of L MC more clearly, we can rewrite the distance term in <ref type="bibr" target="#b7">(8)</ref> in terms of the 2.5D pose representation using (2), i.e.:</p><formula xml:id="formula_9">d(p j,c , R c cpj,c ) = d((? root,c +? r j,c )K ?1 c ? ? u j,c v j,c 1 ? ? , R c c (? root,c +? r j,c )K ?1 c ? ? u j,c v j,c 1 ? ? ).<label>(9)</label></formula><p>Let us assume that the 2D coordinates (u j,c , v j,c ) and (u j,c , v j,c ) are predicted accurately due to the loss L H and the camera intrinsics K c and K c are known. For simplicity, let us also assume the ground-truth transformation R c c between the two views is known. Then, the only way for the network to minimize the difference d(., .) is to predict the correct values for relative depths? r j,c and? r j,c . Hence, the joint optimization of the losses L H and L MC allows us to learn correct 3D poses using only weak supervision in form of multi-view images and 2D pose annotations. Without the loss L H the model can lead to degenerated solutions.</p><p>While in many practical scenarios, the transformation matrix R c c can be known a priori via extrinsic calibration, we, however, assume it is not available and estimate it using predicted 3D poses and Procrustes analysis as follows:</p><formula xml:id="formula_10">R c c = argmin R j?J ? j,c ? j,c p j,c ? Rp j,c 2 2 .<label>(10)</label></formula><p>During training, we follow <ref type="bibr" target="#b33">[34]</ref> and do not back-propagate through the optimization of transformation matrix (10), since it leads to numerical instabilities arising due to singular value decomposition. Note that the gradients from L MC not only influence the depth estimates, but also affect heatmap predictions due to the calculation of? root in (3). Therefore, L MC can also fix the errors in 2D pose estimates as we will show in our experiments. Limb Length Loss (L B ) measures the deviation of the limb lengths of predicted 3D pose from the mean bone lengths:</p><formula xml:id="formula_11">L B = j,j ?E ? j ? j ( p j ?p j ?? L j,j ) 2 ,<label>(11)</label></formula><p>where E corresponds to the used kinematic structure of the human body and? L j,j is the scale normalized mean limb length for joint pair (j, j ). Since the limb lengths of all people will be roughly the same after scale normalization (1), this loss ensures that the predicted poses have plausible limb lengths. During training, we found that having a limb length loss leads to faster convergence.</p><p>Additional Regularization We found that if a large number of samples in multi-view data have a constant background, the network learns to recognize these images and starts predicting same 2D pose and relative depth values for such images. Interestingly, it predicts correct values for other samples. In order to prevent this, we incorporate an additional regularization loss for such samples. Specifically, we run a pre-trained 2D pose estimation model and generate pseudo ground-truths by selecting joint estimates with confidence score greater than a threshold ? = 0.5. These pseudo ground-truths are then used to enforce the 2D heatmap loss L H , which prevents the model from predicting degenerated solutions. We generate the pseudo ground-truths once at the beginning of the training and keep them fixed throughout. Specifically, we use the regularization loss for images from Human3.6M <ref type="bibr" target="#b9">[10]</ref> and MPII-INF-3DHP <ref type="bibr" target="#b21">[22]</ref> that are both recorded in controlled indoor settings. While the regularization may reduce the impact of L MC on 2D poses, the gradients from L MC will still influence the heatmap predictions of body joints that were not detected with high confidence (see <ref type="figure">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our proposed approach for weaklysupervised 3D body pose learning and compare it with the state-of-the-art methods. Additional training and implementation details can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We use two large-scale datasets, Human3.6M <ref type="bibr" target="#b9">[10]</ref> and MPII-INF-3DHP <ref type="bibr" target="#b21">[22]</ref> for evaluation. For weaklysupervised training, we also use the MannequinChallenge dataset <ref type="bibr" target="#b17">[18]</ref> and MPII Human Pose dataset <ref type="bibr" target="#b0">[1]</ref>. The details of each dataset are as follows. Human3.6M (H36M) <ref type="bibr" target="#b9">[10]</ref> provides images of actors performing a variety of actions from four views. We follow the standard protocol and use five subjects (S1, S5, S6, S7, S8) for training and test on two subjects (S9 and S11). MPII-INF-3DH (3DHP) <ref type="bibr" target="#b21">[22]</ref> provides ground-truth 3D poses obtained using markerless motion-capture system. Following the standard protocol <ref type="bibr" target="#b21">[22]</ref>, we use five chest height cameras for training. The test-set consists of six sequences with actors performing a variety activities. MannequinChallenge Dataset (MQC) <ref type="bibr" target="#b17">[18]</ref> provides inthe-wild videos of people in static poses while a hand-held camera pans around the scene. The videos do not come with any ground-truth annotations, however, the data is very adequate for our proposed weakly-supervised approach using multi-view consistency. The dataset consists of three splits for training, validation and testing. In this paper, we use ?3300 videos from training and validation set as proposed by <ref type="bibr" target="#b17">[18]</ref>, but in practice one could download an immense amount of such videos from YouTube (#Mannequin-Challenge). We will show in our experiments that using these in-the-wild videos during training yields better generalization, in particular, when there is a significant domain gap between the training and testing set. Since the videos can have multiple people inside each frame, they have to be associated across frames to obtain the required multiview data. To this end, we adopt the pose based tracking approach of <ref type="bibr" target="#b48">[49]</ref> and generate person tracklets from each video. For pose estimation, we use a HRNet-w32 <ref type="bibr" target="#b37">[38]</ref> model pretrained on MPII Pose dataset <ref type="bibr" target="#b0">[1]</ref>. In order to avoid training on noisy data, we discard significantly occluded or truncated people. We do this by discarding all poses that have more than half of the estimated body joints with confidence score lower than a threshold ? =0.5. We also discard poses in which neck or pelvis joints have confidence lower than ? =0.5 since both joints are important for? root reconstruction using <ref type="bibr" target="#b2">(3)</ref>. Finally, we discard all tracklets with the length lower than 5 frames. This gives us 11,413 multiview tracklets with 241k images in total. The minimum and maximum length of the tracklets is 5 and 140 frames, respectively. MPII Pose Dataset (MPII) <ref type="bibr" target="#b0">[1]</ref> provides 2D pose annotations for 28k in-the-wild images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>For evaluation on H36M, we follow the standard protocols and use MPJPE (Mean Per Joint Position Error), N-MPJPE (Normalized-MPJPE) and P-MPJPE (Procrustes- aligned MPJPE) for evaluations. MPJPE measures the mean euclidean error between the ground-truth and estimated location of 3D joints after root alignment. While NMPJPE <ref type="bibr" target="#b33">[34]</ref> also aligns the scale of the predictions with ground-truths, PMPJPE aligns both the scale and rotations using Procrustes analysis. For evaluation on 3DHP dataset, we follow <ref type="bibr" target="#b21">[22]</ref> and also report PCK (Percentage of Correct Keypoints) and Normalized-PCK as defined in <ref type="bibr" target="#b33">[34]</ref>. PCK measures the percentage of predicted body joints that lie within the radius of 150mm from their ground-truths. 3DHP evaluation protocol uses 14 joints for evaluation excluding the pelvis joint which is used for alignment of the poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>Tab. 1 evaluates the impact of different levels of supervision for training with the proposed approach. We use H36M for evaluation. We start with a fully-supervised setting (FS) which uses 2D supervision from H36M and MPII (2D=H+M) datasets and 3D pose supervision from H36M (3D=H). No multi-view (MV) data is used in this case. The fully-supervised model yields a MPJPE of 5.9px and 55.5mm for 2D and 3D pose estimation, respectively. We then remove the 3D supervision and instead train the network using the proposed approach for weakly-supervised learning (WS+R). The MV data is taken from H36M (MV=H). For this experiment, we assume that the 2D pose annotations for MV data are available (2D=H+M) and the camera extrinsics R are known. This setting is essentially similar to fully-supervised case since the 2D poses from different views can be triangulated using the known R. Training the network under this setting, however, serves as a sanity check that the proposed weakly-supervised approach works as intended which can be confirmed by the obtained 3D pose error of 57.2mm. If R is unknown (WS) and is <ref type="bibr">Figure 2</ref>. Impact of using MQC dataset. We run the trained models on the tracks taken from MQC dataset and align the estimated 3D poses using <ref type="bibr" target="#b9">(10)</ref>. Since people in MQC dataset do not move, the aligned poses should be very similar. Adding MQC dataset for training (right) yields more consistent 3D pose estimates as compared to when only H36M is used (left) for multi-view consistency loss. Note that our proposed approach can also fix the errors in 2D pose estimates in the unlabeled multi-view data. obtained from estimated 3D poses using <ref type="bibr" target="#b9">(10)</ref>, the error increases slightly to 59.3mm.</p><p>All of the aforementioned settings assume that the MV data is annotated with 2D poses which is infeasible to collect in large numbers. Therefore, we have designed the proposed method to work with MV data without even 2D annotations. Next, we remove the 2D supervision from MV data and only use MPII dataset for 2D supervision (2D=M). For reference, we also report the error of a 2D-only model trained on MPII dataset which yields a 2D pose error of 8.9px. Training without 2D pose annotations for MV data with and without ground-truth R yields errors of 62.3mm (WS+R) and 69.1mm (WS), respectively, as compared to 57.2mm and 59.3mm when the 2D pose annotations are available. While using ground-truth R always yields better performance, for the sake of easier applicability, in the rest of this paper, we assume it to be unknown unless specified otherwise. It is also interesting to note that the 2D pose error decreases from 8.9px to 8.3px when the multi-view consistency loss (8) is used. Some qualitative examples of improvements in 2D poses can be seen in <ref type="figure">Fig. 2</ref>.</p><p>We also evaluate the case when the training data is recorded in different settings than testing data. For this, we use 3DHP for training (MV=I) and test on H36M. Since the images of 3DHP are very different from H36M, it leads to a very high error of 106.2mm. Adding the generated training data from MQC dataset (MV=I+Q) significantly reduces the error to 93.6mm which demonstrates the effectiveness of in-the-wild data from MQC. Combining all three datasets (MV=H+I+Q) reduces the error further to 67.4mm as compared to 69.1mm when only H36M dataset was used for training. We also provide the results when groundtruth R is known (WS+R) for H36M and 3DHP datasets (MV=H+I+Q) which shows a similar behaviour and decreases the error from 62.3mm to 60.3mm.</p><p>In our experiments, we found that training only on MQC dataset is not sufficient for convergence and it has to be combined with another dataset which provides multi-view data from more distant viewing angles. This is likely because most videos in MQC dataset do not capture same person from very different viewing angles, whereas datasets such as H36M and 3DHP provide images from cameras with sufficiently large baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with the State-of-the-Art</head><p>Tab. 2 compares the performance of our proposed method with the state-of-the-art on H36M dataset. We group all approaches in three categories; fully-supervised, semi-supervised, and weakly-supervised, and compare the performance of our method under each category. While fully-supervised methods use complete training set of H36M for 3D supervision, semi-supervised methods use 3D supervision from only one subject (S1) and use other subjects (S5, S6, S7, S9) for weak supervision. Weaklysupervised methods do not use any 3D supervision. Some methods also use ground-truth information during inference <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. For a fair comparison with those, we also report our performance under the same settings. It is important to note that, many approaches such as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">50]</ref>   on the other hand, estimates absolute 3D poses. While our fully-supervised baseline (Ours-H-baseline) performs better or on-par with the state-of-the-art fully-supervised methods, our proposed approach for weakly-supervised learning significantly outperforms other methods under both semi-and weakly-supervised categories. For a fair comparison with other methods, we report results of our method under two settings: i) using H36M and MPII dataset for training (Ours-H), and ii) with multiview data from 3DHP and MQC as additional weak supervision (Ours-H+I+Q). In the fully-supervised case, using additional weak supervision slightly worsens the performance (55.5mm vs 56.1mm) which is not surprising on a dataset like H36M which is heavily biased to indoor data and have training and testing images recorded with a same background. Whereas, our approach, in particular the data from MQC, is devised for in-the wild generalization. The importance of additional multi-view data, however, can be seen evidently in the semi-/weakly-supervised settings where it decreases the error from 62.8mm to 59.7mm and from 69.1mm to 67.4mm, respectively.</p><p>Compared to the state-of-the-art method <ref type="bibr" target="#b13">[14]</ref> that also uses multi-view information for weak supervision, our  <ref type="table">Table 3</ref>. Comparison with the state-of-the-art on 3DHP dataset. *use ground-truth 3D location of the root joint during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPJPE? NMPJPE? PCK? NPCK?</head><p>method performs significantly better even though the fullysupervised baselines of both approaches perform similar. This demonstrates the effectiveness of our end-to-end training approach and proposed loss functions that are robust to errors in 2D poses. While our weakly-supervised approach does not outperform fully-supervised methods, it performs on-par with many recent fully-supervised approaches. Tab. 3 compares the performance of our proposed approach with the state-of-the-art on 3DHP dataset. We use our models trained with Ours-H+I+Q setting, as described above. We do not use any 3D pose supervision from 3DHP dataset and instead use the same models used for evaluation on H36M dataset. Our proposed approach outperforms all existing methods with large margins under all three categories which also demonstrates the cross dataset generalization of our proposed method.</p><p>Some qualitative results of the proposed approach can be seen in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a weakly-supervised approach for 3D human pose estimation in the wild. Our proposed approach does not require any 3D annotations and can learn to estimate 3D poses from unlabeled multi-view data. This is made possible by a novel end-to-end learning framework and a novel objective function which is optimized to predict consistent 3D poses across different camera views. The proposed approach is very practical since the required training data can be collected very easily in in-the-wild scenarios. We demonstrated state-of-the-art performance on two challenging datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>estimate root-relative 3D pose. Our approach,</figDesc><table><row><cell>Methods</cell><cell>MPJPE ?</cell><cell>NMPJPE ?</cell><cell>PMPJPE ?</cell></row><row><cell cols="3">Fully-Supervised Methods</cell><cell></cell></row><row><cell>Rogez et al. [36] (CVPR'17)</cell><cell>87.7</cell><cell>-</cell><cell>71.6</cell></row><row><cell>Habibie et al. [8] (ICCV'19)</cell><cell>-</cell><cell>65.7</cell><cell>-</cell></row><row><cell>Rhodin et al. [34] (CVPR'18)</cell><cell>66.8</cell><cell>63.3</cell><cell>51.6</cell></row><row><cell>Zhou et al. [52] (ICCV'17)</cell><cell>64.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Martinez et al. [21] (ICCV'17)</cell><cell>62.9</cell><cell>-</cell><cell>47.7</cell></row><row><cell>Sun et al. [39] (ICCV'17) *</cell><cell>59.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Yang et al. [50] (CVPR'18)</cell><cell>58.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Pavlakos et al. [27] (CVPR'18)</cell><cell>56.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Sun et al. [40] (ECCV'18)*</cell><cell>49.6</cell><cell>-</cell><cell>40.6</cell></row><row><cell>Kocabas et al. [14] (CVPR'19)*</cell><cell>51.8</cell><cell>51.6</cell><cell>45.0</cell></row><row><cell>Ours -H -baseline</cell><cell>55.5</cell><cell>51.4</cell><cell>41.5</cell></row><row><cell>Ours* -H -baseline</cell><cell>50.2</cell><cell>49.9</cell><cell>36.9</cell></row><row><cell>Ours -H+I+Q</cell><cell>56.1</cell><cell>52.7</cell><cell>45.9</cell></row><row><cell cols="4">Semi-Supervised Methods -only Subject-1 is used for training</cell></row><row><cell>Rohdin et al. [33] (ECCV'18)</cell><cell>131.7</cell><cell>122.6</cell><cell>98.2</cell></row><row><cell>Pavlakos et al. [26] (ICCV'19)</cell><cell>110.7</cell><cell>97.6</cell><cell>74.5</cell></row><row><cell>Li et al. [19] (ICCV'19)</cell><cell>88.8</cell><cell>80.1</cell><cell>66.5</cell></row><row><cell>Rhodin et al. [34] (CVPR'18)</cell><cell>n/a</cell><cell>80.1</cell><cell>65.1</cell></row><row><cell>Kocabas et al. [14] (CVPR'19)</cell><cell>n/a</cell><cell>67.0</cell><cell>60.2</cell></row><row><cell>Ours -H</cell><cell>62.8</cell><cell>59.6</cell><cell>51.4</cell></row><row><cell>Ours -H+I+Q</cell><cell>59.7</cell><cell>56.2</cell><cell>50.6</cell></row><row><cell cols="3">Weakly-Supervised Methods -no 3D supervision</cell><cell></cell></row><row><cell>Pavlakos et al. [29] (CVPR'17)</cell><cell>118.4</cell><cell>-</cell><cell>-</cell></row><row><cell>Kanzawa et al. [13] (CVPR'18)</cell><cell>106.8</cell><cell>-</cell><cell>67.5</cell></row><row><cell>Wandt et al. [46] (CVPR'19)</cell><cell>89.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Tome et al. [42] (CVPR'17)</cell><cell>88.4</cell><cell>-</cell><cell>-</cell></row><row><cell>Kocabas et al. [14] (CVPR'19)</cell><cell>n/a</cell><cell>77.75</cell><cell>70.67</cell></row><row><cell>Chen et al. [4] (CVPR'19)</cell><cell>-</cell><cell>-</cell><cell>68.0</cell></row><row><cell>Drover et al. [7] (ECCV-W'18)</cell><cell>-</cell><cell>-</cell><cell>64.6</cell></row><row><cell>Kolotouros et al. [15] (ICCV'19)</cell><cell>-</cell><cell>-</cell><cell>62.0</cell></row><row><cell>Wang et al. [47] (ICCV'19)</cell><cell>83.0</cell><cell>-</cell><cell>57.5</cell></row><row><cell>Ours -H</cell><cell>69.1</cell><cell>66.3</cell><cell>55.9</cell></row><row><cell>Ours -H+I+Q</cell><cell>67.4</cell><cell>64.5</cell><cell>54.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with the state-of-the-art on H36M dataset.</figDesc><table /><note>*use ground-truth depth of the root keypoint during inference.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>We are thankful to Kihwan Kim and Adrian Spurr for helpful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We provide implementation details to reproduce the results in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>We adopt HRNet-w32 <ref type="bibr" target="#b37">[38]</ref> as the back-bone of our network architecture. We pre-train the model for 2D pose estimation before introducing weakly-supervised losses. This ensures that the 2D pose estimates are sufficiently good to enforce multi-view consistency L MC . We use MPII dataset for pre-training. The additional weights for latent depthmaps are not pre-trained. We use a maximum of four camera views C n =4 to calculate L MC . If a sample contains more than four views, we randomly sample four views from it in each epoch. We train the model with a batch size of 256, where each batch consists of 128 images with 2D pose annotations and 32 unlabeled multi-view samples (32?4=128 images). For pre-processing, we use a person bounding-box to crop the person into a 256 ? 256 image such that the person is centered and covers roughly 75% of the image. The training data is augmented by random scaling (?20%) and rotation (?30% degrees). We found that the training converges after 60k iterations. The learning rate is set to 5e?4, which drops to 5e?5 at 50k iterations following the Adam optimization algorithm. We use ?=50. Since the training objectives (6) and <ref type="formula">(7)</ref> consist of multiple loss terms, we balance their contributions by empirically choosing ?=5, ?=10, and ?=100. Since our pose estimation model estimates absolute 3D pose up to a scaling factor, during inference, we approximate the scale using mean bone-lengths from the training data:</p><p>where is ? L j,j is the mean length of the limb formed by joint pair (j, j ). In all of our experiments, we use mean lengths from the training set of H36M dataset.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D human pose estimation = 2D pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric selfsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can 3d pose be learned from 2d projections alone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Phuoc</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3D pose estimation in single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashim</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Kr?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hand pose estimation via 2.5D latent heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Maximummargin structured learning with deep networks for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning the depths of moving people by watching frozen people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On boosting single-frame 3d human pose estimation via monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">and Hedi Tabia. 2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Diogo C Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">VNect: Real-time 3D human pose estimation with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3D human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">C3dpo: Canonical 3d pose networks for non-rigid structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Texturepose: Supervising human mesh estimation with texture consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Posebits for monocular human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep multitask architecture for integrated 2D and 3D human sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation learning for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mathieu Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrg</forename><surname>Sprri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frdric</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MoCap-guided data augmentation for 3D pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">LCR-Net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">It&apos;s all relative: Monocular 3d human pose estimation from weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Matteo Ruggero Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structured prediction of 3D human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3D pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu Fish</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Distill knowledge from nrsfm for weakly supervised 3d pose learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Monet: Multiview semi-supervised keypoint detection via epipolar divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasamin</forename><surname>Jafarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
