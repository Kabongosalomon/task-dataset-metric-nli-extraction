<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechen</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ZongMu Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhang</forename><surname>Wu</surname></persName>
							<email>zizhang.wu@zongmutech.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ZongMu Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>T?th</surname></persName>
							<email>r.toth@tue.nl</email>
							<affiliation key="aff1">
								<orgName type="department">TU/e</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating 3D orientation and translation of objects is essential for infrastructure-less autonomous navigation and driving. In case of monocular vision, successful methods have been mainly based on two ingredients: (i) a network generating 2D region proposals, (ii) a R-CNN structure predicting 3D object pose by utilizing the acquired regions of interest. We argue that the 2D detection network is redundant and introduces non-negligible noise for 3D detection. Hence, we propose a novel 3D object detection method, named SMOKE, in this paper that predicts a 3D bounding box for each detected object by combining a single keypoint estimate with regressed 3D variables. As a second contribution, we propose a multi-step disentangling approach for constructing the 3D bounding box, which significantly improves both training convergence and detection accuracy. In contrast to previous 3D detection techniques, our method does not require complicated pre/post-processing, extra data, and a refinement stage. Despite of its structural simplicity, our proposed SMOKE network outperforms all existing monocular 3D detection methods on the KITTI dataset, giving the best state-of-the-art result on both 3D object detection and Bird's eye view evaluation. The code will be made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision-based object detection is an essential ingredient of autonomous vehicle perception and infrastructure less robot navigation in general. This type of detection methods are used to perceive the surrounding environment by detecting and classifying object instances into categories and identifying their locations and orientations. Recent developments in 2D object detection <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b42">42]</ref> have achieved promising performance on both detection accuracy and speed. In contrast, 3D object detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b43">43]</ref> has proven to be a more challenging task as it aims to estimate pose and location for each object simultaneously.</p><p>Currently, the most successful 3D object detection methods heavily depend on LiDAR point cloud <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b40">40]</ref> SMOKE <ref type="figure">Figure 1</ref>. SMOKE directly predicts the 3D projected keypoint and 3D regression parameters on a single image. The whole network is trained end-to-end in a single stage.</p><p>or LiDAR-Image fusion information <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b4">5]</ref> (features learned from the point cloud are key components of the detection network). However, LiDAR sensors are extremely expensive, have a short service life time and too heavy for autonomous robots. Hence, LiDARs are currently not considered to be economical to support autonomous vehicle operations. Alternatively, cameras are cost-effective, easily mountable and light-weight solutions for 3D object detection with long expected service time. Unlike LiDAR sensors, a single camera in itself can not obtain sufficient spatial information for the whole environment as single RGB images can not supply object location information or dimensional contour in the real world. While binocular vision restores the missing spatial information, in many robotic applications, especially Unmanned Aerial Vehicles (UAVs), it is difficult to realize binocular vision. Hence, it is desirable to perform 3D detection on a monocular image even if it is a more difficult and challenging task.</p><p>Previous state-of-the-art monocular 3D object detection algorithms <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">21]</ref> heavily depend on region-based convolutional neural networks (R-CNN) or region proposal network (RPN) structures <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7]</ref>. Based on the learned high number of 2D proposals, these approaches attach an additional network branch to either learn 3D information or to generate a pseudo point cloud and feed it into pointcloud-detection network. The resulting multi-stage complex process introduces persistent noise from 2D detection, which significantly increases the difficulty for the network to learn 3D geometry. To enhance performance, geometry reasoning <ref type="bibr" target="#b25">[25]</ref>, synthetic data <ref type="bibr" target="#b22">[22]</ref> and post 3D-2D processing <ref type="bibr" target="#b0">[1]</ref> have also been used to improve 3D object detection on single image. By the knowledge of the authors, no reliable monocular 3D detection method has been introduced  <ref type="figure">Figure 2</ref>. Network Structure of SMOKE. We leverage DLA-34 <ref type="bibr" target="#b41">[41]</ref> to extract features from images. The size of the feature map is 1:4 due to downsampling by 4 of the original image. Two separate branches are attached to the feature map to perform keypoint classification (pink) and 3D box regression (green) jointly. The 3D bounding box is obtained by combining information from two branches. so far to learn 3D information directly from the image plane avoiding the performance decrease that is inevitable with multi-stage methods.</p><p>In this paper, we propose an innovative single-stage 3D object detection method that pairs each object with a single keypoint. We argue and later show that a 2D detection , which introduces nonnegligible noise in 3D parameter estimation, is redundant to perform 3D object detection. Furthermore, 2D information can be naturally obtained if the 3D variables and camera intrinsic matrix are already known. Consequently, our designed network eliminates the 2D detection branch and estimates the projected 3D points on the image plane instead. A 3D parameter regression branch is added in parallel. This design results in a simple network structure with two estimation threads. Rather than regressing variables in a separate method by using multiple loss functions, we transform these variables together with projected keypoint to 8 corner representation of 3D boxes and regress them with a unified loss function. As in most single-stage 2D object detection algorithms, our 3D detection approach only contains one classification and regression branch. Benefiting from the simple structure, the network exhibits improved accuracy in learning 3D variables, has better convergence and less overall computational needs.</p><p>Second contribution of our work is a multi-step disentanglement approach for 3D bounding box regression. Since all the geometry information is grouped into one parameter, it is difficult for the network to learn each variable accurately in a unified way. Our proposed method isolates the contribution of each parameter in both the 3D bounding box encoding phase and the regression loss function, which significantly helps to train the whole network effectively.</p><p>Our contribution is summarized as follows:</p><p>? We propose a one-stage monocular 3D object detection with a simple architecture that can precisely learn 3D geometry in an end-to-end fashion.</p><p>? We provide a multi-step disentanglement approach to improve the convergence of 3D parameters and detection accuracy.</p><p>? The resulting method outperforms all existing state-ofthe-art monocular 3D object detection algorithms on the challenging KITTI dataset at the submission date November 12, 2019.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we provide an in-depth overview of the state-of-the-art of 3D object detection based on the used sensor inputs. We first discuss LiDAR based and LiDARimage fusion methods. After that, stereo image based methods are overviewed. Finally, we summarize approaches that only depend on single RGB images. LiDAR/Fusion based methods: LiDAR-based 3D object detection methods achieve high detection precision by processing sparse point clouds into various representations. Some existing methods, e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">39]</ref>, project point clouds into 2D Bird's eye view and equip standard 2D detection networks to perform object classification and 3D box regression. Others methods, like <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">38]</ref>, represent point clouds in voxel grid and then leverage 2D/3D CNNs to generate proposals. LiDAR-image fusion methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b4">5]</ref> learn relevant features from both the point clouds and the images together. These features are then combined and fed into a joint network trained for detection and classification.</p><p>Stereo images based methods: The early work 3DOP <ref type="bibr" target="#b3">[4]</ref> generates 3D proposals by exploring many handcrafted fea-tures such as stereo reconstruction, depth features, and object size priors. TLNet <ref type="bibr" target="#b26">[26]</ref> introduces a triangulation based learning network to pair detected regions of interests between left and right images. Stereo R-CNN <ref type="bibr" target="#b15">[16]</ref> creates 2D proposals simultaneously on stereo images. Then, the methods utilize keypoint prediction to generate a coarse 3D bounding box per region. A 3D box alignment w.r.t. stereo images is finally used on the object instance to improve the detection accuracy. Pseudo-LiDAR methods, e.g., <ref type="bibr" target="#b32">[32]</ref>, generate a "fake" point cloud and then feed these features into a point cloud based 3D detection network.</p><p>Monocular image based methods: 3D object detection based on a single perspective image has been extensively studied and it is considered to be a challenging task. A common approach is to apply an additional 3D network branch to regress orientation and translation of object instances, see <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b31">31]</ref>. Mono3D <ref type="bibr" target="#b2">[3]</ref> generates 3D anchors by using massive amount of features via semantic segmentation, object contour, and location priors. These features are then evaluated via an energy function to accommodate learning of relative information. Deep3DBox <ref type="bibr" target="#b23">[23]</ref> introduces bins based discretization for the estimation of local orientation for each object and 2D-3D bounding box constrain relationships to obtain the full 3D pose. Mono-GRNet <ref type="bibr" target="#b25">[25]</ref> subdivides the 3D object localization task into four tasks that estimate instance depth, 3D location of objects, and local corners respectively. These components are then stacked together to refine the 3D box in a global context. The network is trained in a stage-wise fashion and then trained end-to-end to obtain the final result. Some methods, like <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10]</ref>, rely on features detected in a 2D object box and leverage external data to pair information from 2D to 3D. DeepMANTA <ref type="bibr" target="#b1">[2]</ref> proposes a coarse-to-fine process to generate accurate 2D object proposals, which proposals are then used to match a 3D CAD model from an external annotated dataset. 3D-RCNN <ref type="bibr" target="#b9">[10]</ref> also uses 3D models to pair the outputs from a 2D detection network. They then recover the 3D instance shape and pose by deploying a render-andcompare loss. Other approaches, like <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b8">9]</ref>, generate hand-crafted features by transforming region of interest on images to other representations. AM3D transforms 2D imagery to a 3D point cloud plane by combining it with a depth map. A PointNet <ref type="bibr" target="#b24">[24]</ref> is then used to estimate 3D dimensions, locations and orientations. The only one-stage method M3D-RPN <ref type="bibr" target="#b0">[1]</ref> proposes a standalone network to generate 2D and 3D object proposals simultaneously. They further leverage a depth-aware network and post 3D-2D optimization technique to improve precision. OFTNet <ref type="bibr" target="#b29">[29]</ref> maps the 2D feature map to bird-eye view by leveraging orthographic feature transform and regress each 3D variable independently. Consequently, none of the above methods can estimate 3D information accurately without generating 2D proposals. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Detection Problem</head><p>We formulate the monocular 3D object detection problem as follows: given a single RGB image I ? R W ?H?3 , with W the width and H the height of the image, find for each present object its category label C and its 3D bounding box B, where the latter is parameterized by 7 variables (h, w, l, x, y, z, ?). Here, (h, w, l) represent the height, weight, and length of each object in meters, and (x, y, z) is the coordinates (in meters) of the object center in the camera coordinate frame. Variable ? is the yaw orientation of the corresponding cubic box. The roll and pitch angles are set to zero by following the KITTI <ref type="bibr" target="#b5">[6]</ref> annotation. Additionally, we take the mild assumption that the camera intrinsic matrix K is known for both training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SMOKE Approach</head><p>In this section, we describe the SMOKE network that directly estimates 3D bounding boxes for detected object instances from monocular imagery. In contrast to previous techniques that leverage 2D proposals to predict a 3D bounding box, our method can detect 3D information with a simple single stage. The proposed method can be divided into three parts: (i) backbone, (ii) 3D detection, (iii) loss function. First, we briefly discuss the backbone for feature extraction, followed by the introduction of the 3D detection network consisting of two separated branches. Finally, we discuss the loss function design and the multi-step disentanglement to compute the regression loss. The overview of the network structure is depicted in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Backbone</head><p>We use a hierarchical layer fusion network DLA-34 <ref type="bibr" target="#b41">[41]</ref> as the backbone to extract features since it can aggregate information across different layers. Following the same structure as in <ref type="bibr" target="#b42">[42]</ref>, all the hierarchical aggregation connections are replaced by a Deformable Convolution Network (DCN) <ref type="bibr" target="#b44">[44]</ref>. The output feature map is downsampled 4 times with respect to the original image. Compared with the original implementation, we replace all BatchNorm (BN) <ref type="bibr" target="#b7">[8]</ref> operation with GroupNorm (GN) <ref type="bibr" target="#b35">[35]</ref> since it has been proven to be less sensitive to batch size and more robust to training noise. We also use this technique in the two prediction branches, which will be discussed in Sec. 4.2. This adjustment not only improves detection accuracy, but it also reduces considerably the training time. In Sec. 5.2, we provide performance comparison of BN and GN to demonstrate these properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">3D Detection Network</head><p>Keypoint Branch: We define the keypoint estimation network similar to <ref type="bibr" target="#b42">[42]</ref> such that each object is represented by one specific keypoint. Instead of identifying the center of a 2D bounding box, the key point is defined as the projected 3D center of the object on the image plane. The comparison between 2D center points and 3D projected points is visualized in <ref type="figure" target="#fig_0">Fig. 3</ref>. The projected keypoints allow to fully recover 3D location for each object with camera parameters. Let x y z represent the 3D center of each object in the camera frame. The projection of 3D points to points x c y c on the image plane can be obtained with the camera intrinsic matrix K in a homogeneous form:</p><formula xml:id="formula_0">? ? z ? x c z ? y c z ? ? = K 3?3 ? ? x y z ? ? .<label>(1)</label></formula><p>For each ground truth keypoint, its corresponding downsampled location on the feature map is computed and distributed using a Gaussian Kernel following <ref type="bibr" target="#b42">[42]</ref>. The standard deviation is allocated based on the 3D bounding boxes of the ground truth projected to the image plane. Each 3D box on the image is represented by 8 2D points } that encircles the 3D box. Regression Branch: Our regression head predicts the essential variables to construct 3D bounding box for each keypoint on the heatmap. Similar to other monocular 3D de-tection framework <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b31">31]</ref>, the 3D information is encoded as an 8-tuple ? = ? z ? xc ? yc ? h ? w ? l sin ? cos ? . Here ? z denotes the depth offset, ? xc , ? yc is the discretization offset due to downsampling, ? h , ? w , ? l denotes the residual dimensions, sin(?), cos(?) is the vectorial representation of the rotational angle ?. We encode all variables to be learnt in residual representation to reduce the learning interval and ease the training task. The size of feature map for regression results in S r ? R H R ? W R ?8 . Inspired by the lifting transformation described in <ref type="bibr" target="#b22">[22]</ref>, we introduce a similar operation F that converts projected 3D points to a 3D bounding box B = F(? ) ? R 3?8 . For each object, its depth z can be recovered by pre-defined scale and shift parameters ? z and ? z as</p><formula xml:id="formula_1">z = ? z + ? z ? z .<label>(2)</label></formula><p>Given the object depth z, the location for each object in the camera frame can be recovered by using its discretized projected centroid x c y c on the image plane and the down-</p><formula xml:id="formula_2">sampling offset ? xc ? yc : ? ? x y z ? ? = K ?1 3?3 ? ? z ? (x c + ? xc ) z ? (y c + ? yc ) z ? ? .<label>(3)</label></formula><p>This operation is the inverse of Eq. (1). In order to retrieve object dimensions h w l , we use a pre-calculated category-wise average dimension hwl computed over the whole dataset. Each object dimension can be recovered by using the residual dimension offset ? h ? w ? l :</p><formula xml:id="formula_3">? ? h w l ? ? = ? ?h ? e ? h w ? e ?w l ? e ? l ? ? .<label>(4)</label></formula><p>Inspired by <ref type="bibr" target="#b23">[23]</ref>, we choose to regress the observation angle ? instead of the yaw rotation ? for each object. We further change the observation angle with respect to the object head ? x , instead of the commonly used observation angle value ? z , by simply adding ? 2 . The difference between these two angles is shown in <ref type="figure" target="#fig_1">Fig. 4</ref>. Moreover, each ? is encoded as the vector sin(?) cos(?) . The yaw angle ? can be obtained by utilizing ? z and the object location:</p><formula xml:id="formula_4">? = ? z + arctan x z .<label>(5)</label></formula><p>Finally, we can construct the 8 corners of the 3D bounding box in the camera frame by using the yaw rotation matrix R ? , object dimensions h w l and location x y z :</p><formula xml:id="formula_5">B = R ? ? ? ?h/2 ?w/2 ?l/2 ? ? + ? ? x y z ? ? .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Loss Function</head><p>Keypoint Classification Loss: We employ the penaltyreduced focal loss <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">42]</ref> in a point-wise manner on the downsampled heatmap. Let s i,j be the predicted score at the heatmap location (i, j) and y i,j be the ground-truth value of each point assigned by Gaussian Kernel. Definey i,j and s i,j as:</p><formula xml:id="formula_6">y i,j = 0 if y i,j = 1 y i,j otherwise ,s i,j = s i,j if y i,j = 1 1 ? s i,j otherwise ,</formula><p>For simplicity, we only consider a single object class here. Then, the classification loss function is constructed as</p><formula xml:id="formula_7">L cls = ? 1 N h,w i,j=1 (1 ?y i,j ) ? (1 ?s i,j ) ? log(s i,j ),<label>(7)</label></formula><p>where ? and ? are tunable hyper-parameters and N is the number of keypoints per image. The term (1 ? y i,j ) corresponds to penalty reduction for points around the groundtruth location. Regression Loss: We regress the 8D tuple ? to construct the 3D bounding box for each object. We also add channelwise activation to the regressed parameters of dimension and orientation at each feature map location to preserve consistency. The activation functions for the dimension and the orientation are chosen to be the sigmoid function ? and the 2 norm, respectively:</p><formula xml:id="formula_8">? ? ? h ? w ? l ? ? = ? ? ? ? ? o h o w o l ? ? ? ? ? 1 2 , sin ? cos ? = o sin / o 2 sin + o 2 cos o cos / o 2 sin + o 2 cos ,</formula><p>Here o stands for the specific output of network. By adopting the keypoint lifting transformation introduced in Sec. 4.2, we define the 3D bounding box regression loss as the 1 distance between the predicted transformB and the groundtruth B:</p><formula xml:id="formula_9">L reg = ? N B ? B 1 ,<label>(8)</label></formula><p>where ? is a scaling factor. This is used to ensure that neither the classification, nor the regression dominates the other. The disentangling transformation of loss has been proven to be an effective dynamic method to optimize 3D regression loss functions in <ref type="bibr" target="#b31">[31]</ref>. Following this design, we extend the concept of loss disentanglement into a multi-step form. In Eq. (3), we use the projected 3D groundtruth points on the image plane x c y c with the network predicted discretization offset ? xc?yc and depth? to retrieve the location x?? of each object. In Eq. (5), we use the groundtruth location x y z and the predicted observation angle? z to construct the estimated yaw orientation?. The 8 corners representation of the 3D bounding box is also isolated into three different groups following the concept of disentanglement, namely orientation, dimension and location. The final loss function can be represented by:</p><formula xml:id="formula_10">L = L cls + 3 i=1 L reg (B i ),<label>(9)</label></formula><p>where i represents the number of groups we define in the 3D regression branch. The multi-step disentangling transformation divides the contribution of each parameter group to the final loss. In Sec. 5.2, we show that this method significantly improves detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation</head><p>In this section, we discuss the implementation of our proposed methodology in detail together with selection of the hyperparemeters.</p><p>Preprocessing: We avoid applying any complicated preprocessing method on the dataset. Instead, we only eliminate objects whose 3D projected center point on the image plane is out of the image range. Note that the total number of projected center points outside the image boundary for the car instance is 1582. This accounts for only the 5.5% of the entire set of 28742 labeled cars Data Augmentation: Data augmentation techniques we used are random horizontal flip, random scale and shift. The scale ratio is set to 9 steps from 0.6 to 1.4, and the shift ratio is set to 5 steps from -0.2 to 0.2. Note that the scale and shift augmentation methods are only used for heatmap classification since the 3D information becomes inconsistent with data augmentation.</p><p>Hyperparameter Choice: In the backbone, the group number for GroupNorm is set to 32. For channels less than 32, it is set to be 16. For Eq. <ref type="formula" target="#formula_7">(7)</ref>, we set ? = 2 and ? = 4 in all experiments. Based on <ref type="bibr" target="#b31">[31]</ref>, the reference car size and depth statistics we use are hwl = [1.63 1. Training: Our optimization schedule is easy and straightforward. We use the original image resolution and pad it to 1280 ? 384. We train the network with a batch size of 32 on 4 Geforce TITAN X GPUs for 60 epochs. The learning rate is set at 2.5 ? 10 ?4 and drops at 25 and 40 epochs by a factor of 10. During testing, we use the top 100 detected 3D projected points and filter it with a threshold of 0.25. No data augmentation method and NMS are used in the test procedure. Our implementation platform is Pytorch 1.1, CUDA 10.0, and CUDNN 7.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Performance Evaluation</head><p>We evaluate the performance of our proposed framework on the challenging KITTI dataset. The KITTI dataset is a broadly used open-source dataset to evaluate visual algorithms on a driving scene considered representative for autonomous driving. It contains 7481 images for training and 7518 images for testing. The test metric is divided into easy, moderate and hard cases based on the height of the 2D bounding box of object instances, occlusion and truncation level. Frequently, the training set is split into 3712 training examples and 3769 validation examples as mentioned in <ref type="bibr" target="#b2">[3]</ref>. For the 3D detection task of our proposed method, the 3D Object Detection and Bird's Eye View benchmarks are available for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Detection on KITTI</head><p>3D Object Detection Performance: The 3D detection results of our proposed method on the split sets test and val are compared with the state-of-the-art single image-based methods in Tabs. 1 and 2. We principally focus on the car class since it has been at the focus of previous cooperative studies. For both tasks, the average precision (AP) with Intersection over Union (IoU) larger than 0.7 is used as the metric for evaluation. Note that as pointed out by <ref type="bibr" target="#b31">[31]</ref>, the official KITTI evaluation has been using 40 recall points instead of 11 recall points to measure the AP value since October 8, 2019. However, previous methods only report accuracy at 11 points on the val set. For fair comparison, we report the average precision on 40 points AP| R40 on the test set and AP| R11 on the val set. Results on the test split, shown in Tab. 1, show that SMOKE outperforms all existing monocular methods on both 3D object detection and Bird's eye view evaluation metrics. We achieve improvement in the moderate and hard sets and comparable results on the easy set in the 3D object detection task. For Bird's eye view detection, we also achieve notable improvement on the moderate and hard sets. Compared with other methods that increase image size for better performance, our approach uses relatively lowresolution input and still achieves competitive results on the hard set in 3D detection. Next to these, SMOKE shows a significant improvement on detection speed. Without the time-consuming region proposal process and by the benefits of single-stage structure, our proposed method only needs 30ms to run on a TITAN XP. Note that we only compare our method with methods that directly learn features from images. Approaches based on hand-crafted features <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b21">21]</ref> are not listed in the table. However, with respect to the val set of KITTI, the performance degrades as reported in Tab. 2. We argue that this is due to a lack of training objects. A similar problem has been reported in <ref type="bibr" target="#b42">[42]</ref>. Estimation of object location in a monocular image is difficult since the incompleteness of spatial information. We evaluate the depth estimation of SMOKE using two different distance measures. In <ref type="figure" target="#fig_4">Fig. 5</ref>, the achieved depth error is displayed in intervals of 10 meters. The error is computed if the 2D bounding box of a detection with any of the ground truth objects has an IoU larger than 0.7. As shown in the figure, the depth estimation error increases as the distance grows. This phenomenon has been observed in many monocular image-based detection algorithms since small objects have large distance distribution. We compare our method with two other methods Mono3D <ref type="bibr" target="#b2">[3]</ref> and 3DOP <ref type="bibr" target="#b3">[4]</ref> on the same val set. The curve indicates that our proposed SMOKE method outperforms both methods largely on depth error. Especially at distances larger than 40m, our method achieves more robust and accurate depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Object Detection:</head><p>The 2D detection performance on the official KITTI test set is depicted in Tab. 3. Although the 2D bounding box is not directly regressed in the SMOKE network, we observe that our method achieves comparative results on the 2D object detection task. The 2D detection box is obtained as the smallest rectangle that encircles the projected 3D bounding box on the image plane. Unlike other approaches following a 2D?3D structure, our proposed method reverse this process in a 3D?2D fashion and outperforms many of the existing methods. This clearly shows that 3D object detection provides more abundant information than 2D detection, hence 2D proposals are redundant and not needed for 3D detection. Furthermore, our proposed method does not use extra data, complicated networks and high-resolution input compared to other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>In this section, we show the results of experiments we conducted to compare different normalization choices, loss function, and rotation angle parameterizations. All exper-   iments are performed on the train/val split on the KITTI dataset. Moreover, we use car class to evaluate our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalization Strategy:</head><p>We chose GN as the normalization strategy since it is less sensitive to batch size and cross-GPU training issues. We compare the performance difference in the 3D detection task of BN and GN used in the backbone network. As illustrated in Tab. 4, GN achieves significant improvement over BN on the val set. In addition, we notice that GN can save considerable time in training. For each epoch, GN consumes around 5 minutes while BN needs 8 minutes which takes 60% more time compared to GN.</p><p>Regression Loss: As shown in Tab. 5, we compare different regression loss functions for 3D bounding box estimation performance. We observe that 1 loss performs better than Smooth 1 loss. Same phenomenon is also found in the keypoint estimation problem <ref type="bibr" target="#b42">[42]</ref> where 1 loss yields better performance than 2 loss. Moreover, applying disentanglement to 3D bounding box regression achieves significantly better performance on both 3D object detection and Birds' eye view evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotation Parametrization:</head><p>We compare the performance of SMOKE with respect to different representations of rotation. Following prior work <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b31">31]</ref>, the orientation can be encoded as a 4D quaternion to formulate 3D bounding box. The result with this representation is illustrated in Tab. 6. We observe that our simple vectorial representation yields slightly better result than the quaternion representation on both 3D detection and Bird's eye view evaluation. <ref type="figure">Figure 6</ref>. Qualitative examples from the validation (left) and test (right) sets in KITTI. The non-transparent side of the bounding box represents the front part of each car. Bird's eye view is also provided to show that SMOKE can recover object distances accurately. Note that all these images are not included in the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Qualitative Results</head><p>Qualitative results on both the test and val sets are displayed in <ref type="figure">Fig. 6</ref>. For better visualization and comparison, we also plot the object localization in Bird's eye view. The results clearly demonstrate that SMOKE can recover object distances accurately</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this paper, we presented a novel single-stage monocular 3D object detection method based on projected 3D points on the image plane. Unlike previous methods, which depend on 2D proposals to estimate 3D information, our approach regresses 3D bounding boxes directly. This leads to a simple and efficient architecture. To further improve the convergence of regression loss, we proposed a multi-step disentanglement method to isolate the contribution of vari-ous parameter groups. In addition, our model does not need synthetic data, complicated pre/post-processing, and multistage training. In overall, we largely improve both the detection accuracy and speed on KITTI 3D object detection and Bird's eye view tasks.</p><p>Our proposed SMOKE 3D detection framework achieves promising accuracy and efficiency, which can be further extended and used on autonomous vehicles and in robotic navigation. In the future, we aim at extending our method to stereo images and further improving the estimation of projected 3D keypoints and their depth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of difference between 2D center points (red) and 3D projected points (orange). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Relation of the observation angle ?x and ?z. ?x is provided in KITTI, while ?z is the value we choose to regress.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>x b,1?8 y b,1?8 and the standard deviation is computed by the smallest 2D box with {x min b , y min b , x max b , y max b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>53 3.88] and ? z ? z = [28.01 16.32] (measured in meters).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Average depth estimation error visualized in intervals of 10 meters. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Test set performance. 3D object detection and Birds' eye view performance w.r.t. the car class on the official KITTI data set using the test split. Both metrics are evaluated by AP|R 40 at 0.7 IoU threshold. Validation set performance. 3D object detection and Birds' eye view performance w.r.t. the car class on the official KITTI data set using the val split. Both metrics are evaluated by AP|R 11 at 0.7 IoU threshold.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Backbone</cell><cell cols="2">Runtime(s)</cell><cell cols="3">3D Object Detection Easy Moderate Hard</cell><cell>Birds' Eye View Easy Moderate Hard</cell></row><row><cell cols="2">OFTNet[29]</cell><cell cols="2">ResNet-18</cell><cell>0.50</cell><cell></cell><cell>1.32</cell><cell>1.61</cell><cell>1.00</cell><cell>7.16</cell><cell>5.69</cell><cell>4.61</cell></row><row><cell cols="2">GS3D[14]</cell><cell cols="2">VGG-16</cell><cell>2.00</cell><cell></cell><cell>4.47</cell><cell>2.90</cell><cell>2.47</cell><cell>8.47</cell><cell>6.08</cell><cell>4.94</cell></row><row><cell cols="2">MonoGR[25]</cell><cell cols="2">VGG-16</cell><cell>0.06</cell><cell></cell><cell>9.61</cell><cell>5.74</cell><cell>4.25</cell><cell>18.19</cell><cell>11.17</cell><cell>8.73</cell></row><row><cell cols="2">ROI-10D[22]</cell><cell cols="2">ResNet-34</cell><cell>0.20</cell><cell></cell><cell>4.32</cell><cell>2.02</cell><cell>1.46</cell><cell>9.78</cell><cell>4.91</cell><cell>3.74</cell></row><row><cell cols="2">MonoDIS[31]</cell><cell cols="2">ResNet-34</cell><cell>0.10</cell><cell></cell><cell>10.37</cell><cell>7.94</cell><cell>6.40</cell><cell>17.23</cell><cell>13.19</cell><cell>11.12</cell></row><row><cell cols="4">M3D-RPN[1] DenseNet-121</cell><cell>0.16</cell><cell></cell><cell>14.76</cell><cell>9.71</cell><cell>7.42</cell><cell>21.02</cell><cell>13.67</cell><cell>10.23</cell></row><row><cell>Ours</cell><cell></cell><cell cols="2">DLA-34</cell><cell>0.03</cell><cell></cell><cell>14.03</cell><cell>9.76</cell><cell>7.84</cell><cell>20.83</cell><cell>14.49</cell><cell>12.75</cell></row><row><cell>Method</cell><cell></cell><cell cols="5">3D Object Detection / Birds' Eye View Easy Moderate Hard</cell><cell></cell><cell></cell></row><row><cell>CenterNet[42]</cell><cell cols="2">0.86 / 3.91</cell><cell cols="2">1.06 / 4.46</cell><cell cols="2">0.66 / 3.53</cell><cell></cell><cell></cell></row><row><cell>Mono3D[3]</cell><cell cols="2">2.53 / 5.22</cell><cell cols="2">2.31 / 5.19</cell><cell cols="2">2.31 / 4.13</cell><cell></cell><cell></cell></row><row><cell>OFTNet[29]</cell><cell cols="2">4.07 / 11.06</cell><cell cols="2">3.27 / 8.79</cell><cell cols="2">3.29 / 8.91</cell><cell></cell><cell></cell></row><row><cell>GS3D[14]</cell><cell cols="2">11.63 / -</cell><cell cols="2">10.51 / -</cell><cell cols="2">10.51 / -</cell><cell></cell><cell></cell></row><row><cell>MonoGR[25]</cell><cell cols="2">13.88 / -</cell><cell cols="2">10.19 / -</cell><cell cols="2">7.62 / -</cell><cell></cell><cell></cell></row><row><cell>ROI-10D[22]</cell><cell cols="2">9.61 / 14.50</cell><cell cols="2">6.63 / 9.91</cell><cell cols="2">6.29 / 8.73</cell><cell></cell><cell></cell></row><row><cell cols="7">MonoDIS[31] 18.05 / 24.26 14.98 / 18.43 13.42 / 16.95</cell><cell></cell><cell></cell></row><row><cell>M3D-RPN[1]</cell><cell cols="6">20.40 / 26.86 16.48 / 21.15 13.34 / 17.14</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell cols="6">14.76 / 19.99 12.85 / 15.61 11.50 / 15.28</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>2D detection. AP|R 40 performance w.r.t. the car class on the official KITTI data set using the test split.</figDesc><table><row><cell>Method</cell><cell cols="3">2D Object Detection Easy Moderate Hard</cell></row><row><cell>Mono3D[3]</cell><cell>94.52</cell><cell>89.37</cell><cell>79.15</cell></row><row><cell>OFTNet[29]</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GS3D[14]</cell><cell>86.23</cell><cell>76.35</cell><cell>62.67</cell></row><row><cell>MonoGR[25]</cell><cell>88.65</cell><cell>77.94</cell><cell>63.31</cell></row><row><cell>ROI-10D[22]</cell><cell>76.56</cell><cell>70.16</cell><cell>61.15</cell></row><row><cell cols="2">MonoDIS[31] 94.61</cell><cell>89.15</cell><cell>78.37</cell></row><row><cell cols="2">M3D-RPN[1] 89.04</cell><cell>85.08</cell><cell>69.26</cell></row><row><cell>Ours</cell><cell>92.88</cell><cell>86.95</cell><cell>77.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>/ 17.85 8.27 / 15.46 6.50 / 15.21 GN 10.60 / 18.06 8.33 / 16.07 6.98 / 15.39</figDesc><table><row><cell>Option</cell><cell>3D Object Detection / Birds' Eye View Easy Moderate Hard</cell></row><row><cell>BN</cell><cell>8.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Normalization Strategy. GN perfoms better than BN on all difficulty sets and in both evaluation metrics. 11.03 / 20.90 10.53 / 15.95 9.14 / 15.57 Dis. 1 14.76 / 19.99 12.85 / 16.07 11.50 / 15.39 Regression Loss. 1 loss gains better performance than Smooth 1 loss. The disentanglement form further improves detection result. 36 / 17.81 12.52 / 15.16 11.31 / 15.00 Vectorial 14.76 / 19.99 12.85 / 16.07 11.50 / 15.39</figDesc><table><row><cell>Option</cell><cell cols="2">3D Object Detection / Birds' Eye View Easy Moderate Hard</cell></row><row><cell cols="2">Smooth 1 10.60 / 18.06 8.33 / 16.07</cell><cell>6.98 / 15.39</cell></row><row><cell>Option</cell><cell cols="2">3D Object Detection / Birds' Eye View Easy Moderate Hard</cell></row><row><cell cols="2">Quaternion 13.</cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Rotation Parametrization. Vectorial representation of angles yileds better result than the quaternion representation.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">M3D-RPN: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep MANTA: A coarseto-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celine</forename><surname>Teuliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3D object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Piotr Dollar, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3D-RCNN: Instance-level 3d object reconstruction via renderand-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PointPillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d fully convolutional network for vehicle detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GS3D: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vehicle detection from 3d lidar using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stereo R-CNN based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rui Hu, and Raquel Urtasun. Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep fitting degree scoring network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<pubPlace>Scott Reed; Alexander C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Pengbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ROI-10D: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3D bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MonoGRNet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Triangulation learning network: from monocular to stereo 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PointR-CNN: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>L?pez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bharath Hariharan, Mark Campbell, and Kilian Weinberger. Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1903.09847</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Subcategory-aware convolutional neural networks for object proposals and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3D object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensors</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pixor: Realtime 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">STD: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">VoxelNet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
