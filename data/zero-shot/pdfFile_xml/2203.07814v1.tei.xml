<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Competition-Level Code Generation with AlphaCode</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Choi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Leblond</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Eccles</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Keeling</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agustin</forename><surname>Dal Lago</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Choy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyprien</forename><surname>De Masson D&amp;apos;autume</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Gowal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Cherepanov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Molloy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Mankowitz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esme</forename><forename type="middle">Sutherland</forename><surname>Robson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
						</author>
						<title level="a" type="main">Competition-Level Code Generation with AlphaCode</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Joint first authors</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce Alpha-Code, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance:</p><p>(1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.</p><p>1 The dataset is located at https://github.com/deepmind/code_contests. 2 https://codeforces.com/ 3 The rating system is similar to the classic Elo score and is primarily explained in three blog posts: 1, 2, and 3 4 AlphaCode's overall rating percentile is better than its per-contest percentile. We hypothesise that higher rated competitors compete more regularly than lower rated competitors, and therefore the group ranking above AlphaCode in contests is relatively more stable than the group ranking below. worked on development of the dataset, evaluation, and general infrastructure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cyprien de Masson d'Autume worked on model development and analysis.</head><p>Daniel J. Mankowitz worked on clustering.</p><p>David Choi was the technical lead, developed initial prototypes for solving competitive programming problems, and contributed to aspects including general infrastructure, metrics, large-scale training, model development, sampling, evaluation, datasets, training, and paper writing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Esme Sutherland Robson worked on project management.</head><p>Felix Gimeno worked on model development, metrics, datasets (notably the APPS benchmark), and clustering.</p><p>Igor Babuschkin 13 worked on initial prototypes of code generation models and contributed to infrastructure tools. James Keeling worked on code execution and evaluation, sampling infrastructure and scaling, clustering, and paper writing.</p><p>James Molloy worked on improving the efficiency of our models on accelerators.</p><p>Julian Schrittwieser worked on datasets, evaluation, model development and training losses, tokenization, visualisations, and paper writing.</p><p>Junyoung Chung worked on initial prototypes of code generation models, model development and tuning, training losses, training pipeline, model performance, datasets, sampling, large-scale models, running most final experiments (notably the main experiments), and paper writing. Nate Kushman worked on initial sample scaling efforts, metrics, evaluation, model development and tuning, training losses, datasets (notably the HumanEval benchmark), large-scale models, analysing scaling behavior, running final experiments, and paper writing.</p><p>Oriol Vinyals was an early advocate for code generation models, supported and advised the project throughout, and was involved in project management and paper writing.</p><p>Peter Choy worked on model development, sampling, and copying analysis.</p><p>R?mi Leblond worked on model development and tuning, optimisation, improved training losses, model analysis, sampling efficiency and scaling, datasets, and paper writing.</p><p>Thomas Hubert worked on model development, infrastructure, and additional training objectives.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Computer programming has emerged as a general-purpose problem-solving tool throughout science, industry, and daily life. As part of this growth, there has been continuously increasing demand for tools that can make programmers more productive <ref type="bibr" target="#b49">(Matsakis and Klock, 2014)</ref>, or make programming and programming education more accessible <ref type="bibr" target="#b61">(Resnick et al., 2009)</ref>. Developing AI systems that can effectively model and understand code can transform these tools and the way we interact with them. Systems that can generate code are not only useful, but also stepping stones that can lead to greater understanding of AI and how it relates to programming.</p><p>Generating code that solves a specified task requires searching in the huge structured space of possible programs, with a very sparse reward signal. Single character edits can completely change program behaviour even if they don't cause crashes, solutions can look dramatically different even for the same problem, and judging if a partial or incorrect program is useful is a difficult challenge. Therefore, most prior work has been limited to either restricted domain-specific programming languages <ref type="bibr" target="#b29">(Gulwani, 2011)</ref> or short code snippets <ref type="bibr" target="#b9">(Bruch et al., 2009;</ref><ref type="bibr" target="#b59">Raychev et al., 2014)</ref>.</p><p>Recent large-scale transformer-based <ref type="bibr" target="#b70">(Vaswani et al., 2017)</ref> language models, used to achieve impressive performance generating text , have successfully generated code that solves simple programming problems in Python <ref type="bibr" target="#b2">(Austin et al., 2021;</ref>. A stripped-down version of our model, without the modifications described in Section 4, performs similarly to Codex <ref type="table">(Table A3)</ref>. However, problems used in the Codex paper and similar work consist of mostly simple task descriptions with short solutions -far from the full complexity of real-world programming. Generating an entire program in a general-purpose programming language such as C ++ or Python, starting from a long natural language task description, has remained an open problem. The difference in difficulty between generating short code snippets and entire programs can be analogous to that of imperative versus declarative problem solving. Generating short code snippets typically amounts to translating the task specification directly into code, and sometimes reduces to invoking the correct API calls. In contrast, generating entire programs often relies on understanding the task and figuring out how to accomplish it, which requires deeper algorithmic reasoning.</p><p>Competitive programming problems represent a significant step forward in all these aspects. Solving such problems requires understanding complex natural language descriptions, reasoning about previously unseen problems, mastering a wide range of algorithms and data structures, and precisely implementing solutions that can span hundreds of lines. Solutions are evaluated by executing them on an exhaustive suite of unknown tests, checking for correct behaviour on edge cases as well as execution speed. The fact that the test cases used for evaluation are hidden is an important part of the challenge. These complex problems are newly created for each competition, with the understanding that competitors can draw on solutions to previous contests (either implicitly, by remembering old problems, or explicitly, by searching for them). Moreover, competitive programming is very popular; events like the International Collegiate Programming Competition (ICPC, 2021) and the International Olympiad in Informatics <ref type="bibr">(IOI, 2021)</ref> are widely recognized as some of the most prestigious competitions in computer science, drawing hundreds of thousands of participants from around the world. Using problems that humans find challenging from such battle-tested competitions ensures robustness against shortcuts and provides a meaningful benchmark for many aspects of intelligence.</p><p>Early work using program synthesis for competitive programming has shown that large transformer models can achieve low single-digit solve rates <ref type="bibr" target="#b32">Hendrycks et al., 2021)</ref>, but could not yet reliably generate solutions for the vast majority of problems. Furthermore, as we show in Section 3.2.1, the lack of sufficient test cases in existing competitive programming datasets makes shows the estimated rating of AlphaCode among users who have participated in at least 1 contest in the last 6 months. AlphaCode's estimated rating of 1238 is greater than 72% of these users.</p><p>the metrics defined on them prone to high false positive rates (with 30% or more programs which pass all tests but are not actually correct), and therefore unreliable for measuring research progress.</p><p>In this paper we present AlphaCode, a code generation system applied to solving competitive programming problems. We use large transformer language models to generate code, pre-training them on selected GitHub code and fine-tuning on our curated set of competitive programming problems.</p><p>For each unseen problem we generate a large set of program samples, filter them based on execution results on example tests from the problem description, then cluster the remaining samples to obtain a small set of candidates to be submitted for evaluation. We describe AlphaCode in detail in Section 4.</p><p>A core part of developing our system was ensuring that submissions are rigorously evaluated and that evaluation problems are truly unseen during training, so difficult problems cannot be solved by copying from the training set. Towards this goal, we release a new training and evaluation competitive programming dataset, CodeContests 1 (Section 3). This dataset combines data from various sources, splits temporally so all training data predates all evaluation problems, adds additional generated tests to ensure correctness, and evaluates submissions in a setting that mirrors that of competitive programming. In our evaluation (Section 3.2.1), CodeContests reduces the false positive rate from 30-60% in existing datasets to just 4%. Our best model solves 34.2% of held-out competitive programming problems in this dataset, using at most 10 submissions per problem (comparable to humans), as opposed to previously reported solve rates of around 1-5% on existing datasets (see Section 5.4).</p><p>To further validate our results, we evaluated AlphaCode on simulated programming competitions hosted on the popular Codeforces platform 2 (Section 5.1). In the evaluation of 10 recent contests with over 5,000 participants each, AlphaCode achieved an average ranking within the top 54.3%. Based on these results, we estimate that our system has achieved a Codeforces rating 3 of 1238 which is within the top 28% 4 of users who have participated in a contest in the last 6 months ( <ref type="figure" target="#fig_0">Figure 1)</ref> Backspace You are given two strings and , both consisting of lowercase English letters. You are going to type the string character by character, from the first character to the last one.</p><p>When typing a character, instead of pressing the button corresponding to it, you can press the "Backspace" button. It deletes the last character you have typed among those that aren't deleted yet (or does nothing if there are no characters in the current string). For example, if is "abcbd" and you press Backspace instead of typing the first and the fourth characters, you will get the string "bd" (the first press of Backspace deletes no character, and the second press deletes the character 'c'). Another example, if is "abcaa" and you press Backspace instead of the last two letters, then the resulting text is "a".</p><p>Your task is to determine whether you can obtain the string , if you type the string and press "Backspace" instead of typing several (maybe zero) characters of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The first line contains a single integer (1 ? ? 10 5 ) the number of test cases. The first line of each test case contains the string (1 ? | | ? 10 5 ). Each character of is a lowercase English letter. The second line of each test case contains the string (1 ? | | ? 10 5 ). Each character of is a lowercase English letter. It is guaranteed that the total number of characters in the strings over all test cases does not exceed 2 ? 10 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>For each test case, print "YES" if you can obtain the string by typing the string and replacing some characters with presses of "Backspace" button, or "NO" if you cannot. You may print each letter in any case (YES, yes, Yes will all be recognized as positive answer, NO, no and nO will all be recognized as negative answer). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explanation</head><p>In order to obtain "ba" from "ababa", you may press Backspace instead of typing the first and the fourth characters.</p><p>There's no way to obtain "bb" while typing "ababa".</p><p>There's no way to obtain "aaaa" while typing "aaa".</p><p>In order to obtain "ababa" while typing "aababa", you have to press Backspace instead of typing the first character, then type all the remaining characters.  <ref type="bibr">(Mirzayanov, 2020)</ref>. This is a problem of medium difficulty, with a rating of 1500. The right side shows the public example test case included in the problem description. Hidden tests used to evaluate submissions are shown in <ref type="figure" target="#fig_0">Figure A1</ref>. A solution produced by AlphaCode is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The entire statement is given to AlphaCode, and examples of the exact formatting of problem descriptions seen by the model are provided in Appendix F. <ref type="bibr">(Ebtekar, 2021)</ref>. These evaluations only include users who have tried such competitions, which is a self-selected subset of all programmers. This is the first time that a computer system has achieved such a competitive level in programming competitions.</p><p>We also performed a detailed analysis of our system (Section 6), showing that AlphaCode does not duplicate sections of code from the training dataset to solve problems, but instead relies heavily on the natural language problem descriptions to create original solutions. We further examine the types of problems the model can and cannot solve, and discuss how the validation loss is a poor proxy for the solve rate.  <ref type="formula">(0)</ref> The model successfully extracted the information necessary to solve the problem from the natural language description:</p><p>1. The problem is to figure out if it is possible to convert one phrase to another by pressing backspace instead of typing some letters. So first we read the two phrases (lines 3-4). 2. If the letters at the end of both phrases don't match, the last letter must be deleted. If they do match we can move onto the second last letter and repeat (11-18). 3. Backspace deletes two letters. The letter you press backspace instead of, and the letter before it <ref type="bibr">(19)</ref><ref type="bibr">(20)</ref>. 4. If we matched every letter, it is possible to obtain string from (23-26).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem setup 2.1. Competitive programming</head><p>Programming competitions first began in the 1970s and have since grown in popularity to include hundreds of thousands of participants worldwide. The annual International Collegiate Programming Contest attracts almost 60,000 students from over 3,000 universities <ref type="bibr" target="#b38">(ICPC Factsheet, 2020)</ref>, and companies including Google (Google Code Jam, 2021) and Facebook <ref type="bibr" target="#b21">(Facebook Hacker Cup, 2021)</ref> hold regular competitions. The popular Codeforces platform, used throughout this paper, has more than 500,000 active users and holds weekly competitions with tens of thousands of participants <ref type="bibr">(Mirzayanov, 2020)</ref>.</p><p>The exact format of a programming competition varies between contests, but in general individuals or teams of competitors are given between 5 and 10 problem descriptions ( <ref type="figure" target="#fig_1">Figure 2)</ref>, and approximately 3 hours to write programs ( <ref type="figure" target="#fig_2">Figure 3</ref>) to correctly solve as many problems as possible. The program submissions are sent to a server which automatically evaluates them on an exhaustive set of hidden tests ( <ref type="figure" target="#fig_0">Figure A1</ref>). Competitors are told whether or not their submission passed all tests, though not necessarily the exact cause of a failure. There are penalties based on the number of incorrect submissions per problem and the amount of time it took to solve each problem <ref type="bibr">(ICPC Rules, 2021)</ref>. Submissions can be written in a variety of programming languages, among which C ++ and Python are currently the most popular. Problems are often given ratings to indicate difficulty, and more difficult problems are worth more points.</p><p>There are three steps involved in solving a problem. First, participants must read and understand a natural language description spanning multiple paragraphs that contains: narrative background typically unrelated to the problem, a description of the desired solution that the competitors need to understand and parse carefully, a specification of the input and output format, and one or more example input/output pairs (that we call "example tests").</p><p>The next step is to create an efficient algorithm that solves the problem. Going from "what the problem is" to "how to solve the problem" is a great leap that requires understanding and reasoning about the problem, as well as a deep comprehension of a wide range of algorithms and data structures. This leap is a significant difference from previous works, which tend to explicitly specify what to implement. The algorithm must also be efficient enough to execute in time for the input sizes and time limits specified by the problem, 5 which often eliminates easier, naive attempts.</p><p>Finally, the algorithm must be implemented. Implementation efficiency matters given execution time constraints (harder problems can sometimes only be solved in faster languages such as C ++ ), subtle edge cases can be difficult to account for, and the solution itself can be over a hundred lines of precise code. Participants are given small example test cases to run against, and often debug, fix, and rerun their candidate submission many times before attempting an official submission against the hidden tests cases. An example correct solution generated by AlphaCode for the problem in <ref type="figure" target="#fig_1">Figure 2</ref> is given in <ref type="figure" target="#fig_2">Figure 3</ref>, and extensive results and analysis can be found in Section 5 and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Evaluation</head><p>Though running a system against a live programming competition is an unbiased evaluation, it adds a large degree of complexity and is not a stable benchmark. To alleviate this issue, we developed a proxy measure suitable for research iteration similar to the development sets present in most supervised learning datasets. Our measure mirrors the fundamental structure of competitions while simplifying incidental details. The metric we use is "percentage of problems solved using submissions from samples per problem", denoted as @ .</p><p>This metric indicates the percentage of problems a model can solve if for each problem it is allowed first to create samples, and then to evaluate ? of these samples against the hidden tests. The problem is considered solved if any of these evaluations passes all tests. The filtering method is up to the system itself, but should only be based on information available to competitors (e.g. the example tests given as part of the problem description, but not the hidden tests). To decrease variance between runs, assuming both and are finite, the metrics we report are expectations computed using bootstrapping on a set of samples typically much larger than (Appendix A.3). Decreasing variance through expectations makes comparisons of improvements more meaningful, as our validation and test sets are relatively small, and there is significant variance when sampling from a single model.</p><p>Limiting the amount of submissions to emulates the penalty for incorrect submissions and prevents systems from exploiting the evaluation metric by evaluating against the hidden tests an unreasonable number of times. Fixing is important for comparing different evaluations, as we found that performance increases with the number of samples (Section 5). Our use of bootstrapping ensures that we can still benefit from the variance reduction obtained from generating a much larger set of samples to estimate the @ metric.</p><p>The setting we use to model programming competitions is 10@ -10 submissions per problem from samples. We also use @ (solve rate with samples), to be consistent with , which assumes all samples can be submitted for evaluation. @ = @ , and is an upper bound metric for using samples. We show solve rate with respect to different values as good results at low sample budgets do not necessarily correlate with good performance at high sample budgets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets</head><p>All our models were first pre-trained on a collection of open-source code from GitHub, and subsequently fine-tuned on a dataset we created (CodeContests, released here) of programming competition data.</p><p>The pre-training stage helps the model learn good representations of code and generate code fluently, while the fine-tuning stage helps the model adapt to the target competitive programming domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pre-training dataset</head><p>Our pre-training dataset is based on a snapshot of selected public GitHub repositories taken on 2021/07/14. We included all code files from several popular languages: C ++ , C#, Go, Java, JavaScript, Lua, PHP, Python, Ruby, Rust, Scala, and TypeScript. Following previous work , we filtered out all files larger than 1MB or with lines longer than 1000 characters, to exclude automatically generated code. We also removed duplicates of the same file, ignoring whitespace in comparisons. After filtering, our final pre-training dataset contains a total of 715.1 GB of code. The dataset composition across languages can be found in the appendix <ref type="table" target="#tab_2">(Table A1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CodeContests fine-tuning dataset</head><p>Models pre-trained on GitHub can generate good code and solve simple programming problems, but as shown in Appendix B.3 they can solve very few competitive programming problems. Fine-tuning the model on a dedicated competitive programming dataset is critical for performance.</p><p>To facilitate fine-tuning and evaluation, we curated a new dataset of competitive programming problems, named CodeContests. <ref type="bibr">6</ref> The dataset includes problems, solutions and test cases we scraped from the Codeforces platform, along with existing public competitive programming datasets mixed into our training set. More concretely, the training dataset combines newly scraped data from Codeforces <ref type="bibr">(Mirzayanov, 2020)</ref> with existing data from Description2Code <ref type="bibr" target="#b10">(Caballero et al., 2016)</ref>, and CodeNet <ref type="bibr" target="#b56">(Puri et al., 2021)</ref>. The validation and test splits of the dataset consist entirely of newly scraped Codeforces problems. To guard against data leakage, we adopted a strict temporal split: all pre-training and fine-tuning training data appeared online before any validation problems, and all validation problems before test ones. Following our GitHub pre-training dataset snapshot date, all training data in CodeContests was publicly released on or before 2021/07/14. Validation problems appeared between 2021/07/15 and 2021/09/20, and the test set contains problems published after 2021/09/21. This temporal split means that only information humans could have seen is available for training the model (see Appendix B.3 for more details and analysis). Some basic statistics of this dataset are shown in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Our scraped data from Codeforces includes full problem descriptions like that shown in <ref type="figure" target="#fig_1">Figure 2</ref>, along with metadata for each problem. The metadata includes difficulty ratings and tags that indicate which approaches might be required to solve the problem (e.g. "greedy" or "dp"). Neither the difficulty rating nor the tags are visible at competition time (and so should not be used at test time). Our dataset also contains both correct and incorrect human submissions written in the most popular submission languages: C ++ , Python, and Java. Each problem includes all the test cases that are accessible from the platform: example tests in the problem statements and hidden test cases that are made available at the evaluation result pages once a contest is finished. To improve data quality and consistency, and to avoid duplication issues involved in merging datasets, we cleaned this data using the procedure outlined in Appendix B.2.</p><p>The correctness of a program is checked by executing it on the test cases and comparing the program output with the expected correct output. More details about this correctness checking process are documented in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tests per problem</head><p>Solutions per problem (% correct) Split Problems Example Hidden Generated C ++ Python Java  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">False positives and additional generated tests</head><p>We want the test cases to be as exhaustive as possible, so that submissions cannot be marked as correct by exploiting a lack of test coverage. Unfortunately, high-quality test cases are not readily available. For example, the Codeforces platform does not display full test cases when they are longer than approximately 400 characters. Lack of test coverage leads to "false positives" where incorrect submissions are marked as correct, and "slow positives" where correct but algorithmically inefficient solutions that do not fulfill time and memory constraints are marked correct (e.g. a solution that is of the wrong complexity class). These false positives do not effect the evaluation on Codeforces described Section 5.1.</p><p>Notably, both issues are common in prior datasets and the program synthesis literature, as input/output examples are an under-specification of program behavior <ref type="bibr" target="#b30">(Gulwani et al., 2017)</ref>. <ref type="table" target="#tab_3">Table 2</ref> shows the estimated false positive rate of our dataset compared to APPS <ref type="bibr" target="#b32">(Hendrycks et al., 2021)</ref> and HumanEval , which both have many false positives. A high average number of tests per problem does not necessarily indicate exhaustive tests, because some problems may have far fewer tests per problem than average, and some tests may examine similar cases.</p><p>We reduced the false positive rates of our dataset by generating additional test cases, created by mutating existing test inputs. Possible mutations are applying bit flips to binary inputs, randomly incrementing or decrementing integers, and swapping and changing characters in strings. Mutated inputs are verified by running 30 correct solutions on them, and checking that all solutions produce the same output. This process was run on each problem for a maximum of 10 CPU hours or 200 generated tests. Because of complex input formats, we failed to generate the full set of 200 tests for 6.3% of problems. Lastly, we filtered out problems in the validation and test splits with insufficient test coverage, keeping only problems with at least 5 hidden or generated test cases that result in at least 2 different outputs. This ensures a model cannot trivially solve problems by always outputting a constant, such as YES or NO. As seen in <ref type="table" target="#tab_3">Table 2</ref>, generated tests and filtering reduced our false positive rates from 62% to 4%. CodeContests has significantly better false positive rates than prior work even though we drew fewer samples for both APPS and HumanEval, and the problems in those datasets are relatively less complex (both of which tend to lower the false positive rates). However, there is still a significant number of problems where slow but semantically correct solutions are accepted by the tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head><p>Generating code that solves a specific task requires searching in a huge structured space of programs with a very sparse reward signal. To make matters worse, for many domains including competitive programming, there is a limited number of examples of such tasks and solutions to learn from. Finally, as we restrict the amount of submissions per problem our model can do, each submission must be used wisely.</p><p>Our system, AlphaCode, is meant to address all these challenges. A high-level view of our approach can be seen in <ref type="figure" target="#fig_3">Figure 4</ref>. The main process is to:</p><p>1. Pre-train a transformer-based language model on GitHub code with standard language modelling objectives. This model can reasonably represent the space of human coding, which greatly reduces the problem search space. 2. Fine-tune the model on our dataset of competitive programming data, using GOLD <ref type="bibr" target="#b54">(Pang and He, 2020)</ref> with tempering <ref type="bibr" target="#b15">(Dabre and Fujita, 2020)</ref> as the training objective. This further reduces the search space, and compensates for the small amount of competitive programming data by leveraging pre-training. 3. Generate a very large number of samples from our models for each problem. 4. Filter the samples to obtain a small set of candidate submissions (at most 10), to be evaluated on the hidden test cases, by using the example tests and clustering to pick samples based on program behaviour.</p><p>Among these, the large-scale sampling followed by filtering is unique to our setup, and we found that this process greatly improves problem solve rate. Therefore many of our design decisions were made to facilitate efficient and effective sampling.  <ref type="table">Table 3</ref> | Architecture configuration of our models at different parameter scales. This table lists the total number of parameters in the model , the hidden dimension of the transformer blocks , the number of query and key-value heads, the number of transformer blocks in the encoder and decoder, the training batch size, the number of gradient update steps, and the number of total training tokens. The head size is always 128, with a feed-forward fan-out ratio of 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model architecture</head><p>The competitive programming code generation problem can be viewed as a sequence-to-sequence <ref type="bibr" target="#b65">(Sutskever et al., 2014)</ref> translation task: given a problem description in natural language (e.g. <ref type="figure" target="#fig_1">Figure  2</ref>), produce a corresponding solution in a programming language (e.g. <ref type="figure" target="#fig_2">Figure 3</ref>). This naturally motivates the choice of an encoder-decoder transformer architecture <ref type="bibr" target="#b70">(Vaswani et al., 2017)</ref> for AlphaCode, which models ( | ). The architecture takes as input to the encoder the problem description as a flat sequence of characters (including metadata, tokenized), and samples autoregressively from the decoder one token at a time until an end of code token is produced, at which point the code can be compiled and run (see Appendix F for example , pairs, and https://alphacode.deepmind.com/ for an interactive model visualisation).</p><p>Compared to decoder-only architectures commonly used for language modeling and generation, an encoder-decoder architecture allows a bidirectional description representation (tokens at the beginning of the description can attend to tokens at the end) and the extra flexibility to untie the encoder structure from the decoder. Because problem descriptions are on average twice as long as their corresponding human solutions, we use an asymmetric architecture with 1536 tokens for the encoder but only 768 tokens for the decoder. We further found that using a shallow encoder and a deep decoder significantly improves the efficiency of training without hurting problem solve rate. The exact architectures for our models are listed in <ref type="table">Table 3</ref>. The 9B and 41B models were trained using model parallelism, with 1 key and value head per shard. We built our model using JAX <ref type="bibr" target="#b7">(Bradbury et al., 2018)</ref> and Haiku <ref type="bibr" target="#b33">(Hennigan et al., 2020)</ref>, and trained them on TPUv4 accelerators using bfloat16 precision.</p><p>To reduce the cost of sampling from our models, we take advantage of multi-query attention <ref type="bibr" target="#b63">(Shazeer, 2019)</ref>. Using a full set of query heads but sharing key and value heads per attention block significantly reduces memory usage and cache update costs, which are the main bottleneck during sampling. This memory reduction also allows larger batch sizes for sampling, further increasing efficiency.</p><p>For tokenization we used a SentencePiece tokenizer <ref type="bibr" target="#b44">(Kudo and Richardson, 2018)</ref> with a vocabulary size of 8,000 tokens trained on a mix of GitHub and CodeContests data. The training mix ensures it can effectively tokenize programs from a range of languages, as well as the natural language descriptions of problems. The encoder and decoder in our models use the same tokenizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pre-training</head><p>We pre-trained our models on the GitHub dataset described in Section 3, with a standard cross-entropy next-token prediction loss for the decoder and a masked language modeling loss <ref type="bibr" target="#b17">(Devlin et al., 2018)</ref> for the encoder. The masked language modeling loss was essential for improving the representation learning of the encoder. We split GitHub files by uniformly sampling pivot locations, using content before the pivot as input to the encoder, and content after for the decoder.</p><p>Our base 1B parameter model was trained for 10 6 steps with a batch size of 256. Following , we adjusted the amount of training for other model sizes such that larger models are trained more and smaller models are trained less to optimize the use of compute. However, due to resource limitations and to make optimal use of compute, the training of our largest 41B model was stopped early, and therefore this model was relatively undertrained compared to models at other scales ( <ref type="table">Table 3</ref>).</p><p>We trained all models using the AdamW variant <ref type="bibr" target="#b47">(Loshchilov and Hutter, 2017)</ref> of the Adam optimiser <ref type="bibr" target="#b43">(Kingma and Ba, 2014</ref>) with 1 = 0.9, 2 = 0.999 for {300M, 1B, 3B} models, and 2 = 0.95 for {9B, 41B} models. We used a weight decay of 0.1 to enhance regularization. We trained the models with an initial learning rate of 10 ?4 , which was then cosine decayed to 10 ?5 at the end of pre-training. We linearly warmed-up the learning rate from 10 ?9 to 10 ?4 over the first 1, 000 training steps, and clipped the global gradient norm to stay below 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fine-tuning</head><p>We fine-tuned our model on our CodeContests dataset. During fine-tuning, we used the natural language problem description for the encoder and the program solution for the decoder. Similar to pre-training, we used both the standard next-token prediction and masked language modeling losses. We also adopted additional conditioning and modifications that we found improved the overall solve rate: tempering, value conditioning and prediction, and GOLD described below, as well as metadata conditioning described in Appendix C.2. We set the initial learning rate as 10 ?5 , and cosine decayed it to 10 ?6 at the end of fine-tuning. We used the same linear warm-up stage for the learning rate over the first 1, 000 training steps.</p><p>Tempering. Tempering, introduced by <ref type="bibr" target="#b15">Dabre and Fujita (2020)</ref>, is a regularization technique that makes the token probability distribution artificially smoother or sharper at training time by dividing the output logits of a model by a scalar temperature before the softmax layer. We observed that when using = 0.2 &lt; 1, tempering helps avoid overfitting to our fine-tuning dataset by making the training distribution sharper, and consequently the inference distribution smoother. Notably, this is the opposite of the suggestion of <ref type="bibr" target="#b15">Dabre and Fujita (2020)</ref> to use &gt; 1 to make a sharper inference distribution. At sampling time, we divided the logits by another temperature tuned on the validation set ( = 0.12 for models trained with tempering only; = 0.25 for models trained with tempering and GOLD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Value conditioning &amp; prediction.</head><p>CodeContests contains both correct and incorrect problem submissions. We used value conditioning and prediction to discriminate between these two types of submissions, providing an additional training signal and allowing use of data which could otherwise mislead the model. Similar approaches were used in, e.g., <ref type="bibr" target="#b71">Vinyals et al. (2019)</ref>. In value conditioning, we inserted whether or not a submission was correct into the problem description so that the model can condition on this information, as shown in <ref type="figure">Figure 5</ref>. At sampling time, the model was always conditioned on the sample being correct. In value prediction, we added an auxiliary value prediction task during training such that the last layer token representations before projecting to logits are also used in a small Transformer to classify whether the submission is correct. Value prediction was not RATING : 1200 TAGS : dp , im pleme ntat ion LANGUAGE IS python3 CORRECT SOLUTION Polycarp must pay exactly n burles at the checkout ... ( rest of the description ) <ref type="figure">Figure 5</ref> | Example format of the additional metadata information. This is added to the top of problem descriptions. Metadata and problem descriptions are handled identically. See Appendix F for a full example of what is used in the decoder. The problem in this example can be found here.</p><p>used during sampling.</p><p>GOLD (Pang and He, 2020). Solving competitive programming problems from descriptions is inherently a one-of-many task <ref type="bibr" target="#b53">(Nandwani et al., 2021)</ref>: each unique problem allows many distinct solutions that depend on algorithm choice, implementation, etc. CodeContests contains several orders of magnitude more solutions than descriptions ( <ref type="table" target="#tab_2">Table 1</ref>). Standard maximum likelihood objectives minimise loss by putting some weight on each solution in the training set (like recall), whereas our metric measures whether a model can find a single correct solution in the submission attempt budget (like precision). To resolve this discrepancy, we adopted a variation of GOLD <ref type="bibr" target="#b54">(Pang and He, 2020)</ref>, an offline RL algorithm which allows the model to both learn from tokens it already assigns high likelihood to, and to ignore tokens that are not in its distribution (allowing it to concentrate on precision). To combine GOLD and tempering, we introduce a short training phase between pretraining and finetuning. Full details of GOLD and this combination are in Appendix C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Large scale sampling</head><p>Sampling from transformer models can be easily parallelized, which allowed us to scale to millions of samples per problem -a critical driving force for performance improvement. To ensure sufficient diversity in such a large number of samples, we take a single trained model and: (i) generate half of the samples in Python and half in C ++ , (ii) randomize the problem tags and ratings in the natural language prompt (see <ref type="figure">Figure 5</ref> for an example and Appendix C.2 for more details), and (iii) use a relatively high sampling temperature. The single model, via the additional metadata we condition upon, can generate solutions with different languages, tags, and ratings. To make the most effective use of our samples we then apply filtering (Section 4.5) and clustering (Section 4.6) to obtain a small number of candidate submissions.</p><p>For problem tags and ratings conditioning, we picked random tags from the most popular 50 for the model to condition on, and sampled ratings uniformly in the range of 800 to 3500 as these metadata are not visible for new unseen problems in a competition. We found that conditioning on random tags and ratings can improve performance, potentially by increasing diversity of the samples.</p><p>The optimal sampling temperature depends on the total number of samples (in general the more samples, the higher the optimal temperature). However different temperatures in a wide range do not significantly change the solve rates ( <ref type="figure">Figure A5</ref>). We therefore use a fixed sampling temperature = 0.25 in all experiments that use tempering and GOLD, = 0.12 when using tempering only, and tune the sampling temperature separately otherwise.</p><p>We also experimented with top- <ref type="bibr">(Fan et al., 2018)</ref> and nucleus sampling <ref type="bibr" target="#b35">(Holtzman et al., 2019)</ref>. As seen in <ref type="figure">Figure A5</ref>, despite running exhaustive hyperparameter sweeps we did not observe significant performance improvements with these methods. We therefore use regular sampling with temperature in our experiments. A few complete examples of model prompts and samples are provided in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Filtering</head><p>To accurately represent competitive programming contests and penalties, our formulation limits us to just 10 submissions per problem no matter how many samples we draw. One powerful tool for selecting these submissions is filtering samples to only those that pass the example tests given in the problem statement. Filtering removes approximately 99% of model samples, although the exact amount depends on the problem and model, and filtering can still leave tens of thousands of candidate samples for many problems. Finding solutions that pass example tests is itself a difficult problem, and on approximately 10% of problems our models cannot find a single such program. Indeed this easier version of our setting is a classic program synthesis formulation, where the task is specified by a list of given input/output pairs <ref type="bibr" target="#b30">(Gulwani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Clustering</head><p>Filtering using example tests can still leave thousands of candidate programs per problem. Randomly picking from this pool wastes the limited submission budget on programs that are syntactically different but semantically equivalent. Semantically equivalent programs could be detected if we had additional test inputs, by executing all remaining programs on these inputs and grouping programs that produce the same outputs together into clusters. We could then avoid repeatedly picking from the same clusters.</p><p>We trained a separate test input generation model, using the same architecture as our main models, and initialised from the same GitHub pre-trained checkpoint. This model was trained to predict test inputs from problem descriptions, using example, hidden, and generated test inputs as training data. After training, this model was used to create new test inputs for unseen problems. Note that although these created test inputs are not guaranteed to be valid, especially when problems have complex constraints, imperfect and even invalid test inputs can still be useful for grouping sampled programs.</p><p>This learned test input generation model is different from the mutation-based test generation process used in Section 3.2.1 to augment our dataset. The latter requires correct solutions (which are not available at test time) to filter out bad test cases.</p><p>After clustering on program behaviour we found that selecting one solution from each cluster from largest to smallest performed best, perhaps because there are many ways solutions can be incorrect while correct solutions tend to behave the same and therefore are grouped into larger clusters. If the candidate solutions for a problem form less than 10 clusters (or more in the case of more than 10 submissions), after reaching the smallest cluster, we repeat from the first cluster skipping samples that have already been submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section we present experimental results that give insights into our model performance, and evidence that guided our design decisions. We highlight the results obtained by evaluating on the Codeforces platform (Section 5.1) and on CodeContests (Section 5.2), present a detailed study of model performance on our dataset in Section 5.3, and conclude by comparing to published models in the literature on the public APPS <ref type="bibr" target="#b32">(Hendrycks et al., 2021)</ref> benchmark of programming problems in Section 5.4. To ensure that our baseline models are comparable to past work we also compare our decoder-only baseline directly to    <ref type="table">Table 4</ref> | Estimated percent ranking of our system in 10 Codeforces competitions (lower is better). For each contest, we show ranking using simulated time and incorrect submission penalties (Estimated), as well as the best and worst possible rankings using minimum and maximum possible time penalties as estimates, averaged over 3 evaluations. Percents are how many users performed better than AlphaCode. Our system achieved an overall ranking of top 54.3% averaged across the 10 contests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Codeforces competitions evaluation</head><p>Evaluating on programming competitions checks program correctness more thoroughly, compared to evaluating on our dataset which has known weaknesses including false positives, accepting algorithmically inefficient solutions, and handling problems with multiple acceptable outputs. Additionally, evaluating in the real setting allows us to benchmark against the best performers on this task: human competitors.</p><p>We evaluated our best system on all Codeforces competitions from 2021/12/01 to 2021/12/28 with more than 5,000 participants per contest, a total of 10 competitions. The system was an ensemble of 41B and 9B models with clustering, which performed best on our validation set but turned out to be slightly worse than using the 41B model alone with clustering (see Appendix C.1 for more on ensembling). For each contest, we simulated running AlphaCode live, generating samples for each problem, filtering with example tests, 7 and then clustering to get candidate submissions. We submitted these selected candidates to the Codeforces platform, 8 and computed AlphaCode's placement in each contest. After the first run, we repeated this procedure two more times to measure variance and performance with more than 10 submissions. Sources of variance include problem distribution, model training, sampling, filtering, and clustering. See Appendix D for the exact evaluation procedure, and <ref type="table" target="#tab_6">Table A5</ref> and <ref type="table">Table A6</ref> for full results. <ref type="table">Table 4</ref> shows evaluation results across the 10 competitions. For each competition, we show the estimated percentile ranking using a simulated penalty, and upper and lower bounds assuming zero and maximum submission time penalties. The bounds represent how ranking depends on the number of accelerators used to draw samples during competition. For the second and third runs, <ref type="table">Table A6</ref> shows the estimated percentile when not limiting to 10 submissions per problem (still taking into account penalties for incorrect submission), which although not human-like does follow competition rules. We found that the model still continued to solve problems when given more attempts, though at a decreased rate. The model tends to solve the easier problems in competitions, but it does manage to solve harder problems including one rated 1800.</p><p>Overall our system achieved an average ranking of top 54.3% limiting to 10 submissions per problem, with an actual average of 2.4 submissions for each problem solved. 9 When allowed more than 10 submissions per problem (the second and third evaluation), AlphaCode achieved a ranking of top 48.8%, with an actual average of 28.8 submissions for each problem solved. Our 10 submissions per problem result corresponds to an estimated Codeforces rating of 1238, which is within the top 28% of users who have participated in a contest in the last 6 months (a small and selected subset of all programmers). To the best of our knowledge, this is the first time that a computer system has been competitive with human participants in programming competitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">CodeContests evaluation</head><p>As well as the Codeforces evaluation, we evaluated our model on the validation and test sets of CodeContests. The test set is a superset of the competitions used in Section 5.1. <ref type="bibr">10</ref> The metrics on our dataset are lower variance and easier to measure, since they do not involve submitting to an external site. For CodeContests (both here and in Section 5.3), we focus on the two main metrics discussed in Section 2.2:</p><p>? pass@k: The percentage of problems solved when we take samples from the model for each problem and submit all of them for evaluation on the hidden tests. If any solution in the specified sample budget solves a problem, the problem is counted as solved. Therefore this metric measures mostly the search aspect of the sampling process, and is used in Section 5.3. ? 10@k: The percentage of problems solved when we take samples from the model for each problem but can only submit 10 of them for evaluation on the hidden tests. This measures factors including the filtering process and how models behave at a very large number of samples.</p><p>The results are shown in <ref type="table" target="#tab_6">Table 5</ref>. With up to a million samples per problem, we can solve 34.2% of problems in our validation set; and with one hundred thousand samples, we solve 31.8% of problems in our validation set, and 29.6% of problems in our test set. Because of the temporal split, no problem in either set was seen by our model during training. Given the difficulty of these problems (since they are problems given to the self-selected group of those who try competitive programming), this is a substantial proportion of the dataset.</p><p>Differences in solve rates between the validation and test sets are caused by variation in problem distributions (as the test set and validation set were collected in temporally disjoint periods), as well as some overfitting. However, the difference in performance between the two sets remains limited. The 41B consistently outperforms the 9B model, and clustering consistently provides an improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">CodeContests ablations &amp; results</head><p>This section contains results that support our design decisions described in Section 4. All results are on the CodeContests validation set, with models fine-tuned on the CodeContests training set and not using clustering unless otherwise noted. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Solve rates scale with respect to parameter count, compute, number of samples, and dataset size</head><p>As would be expected, scaling up the number of model parameters or the size of the dataset greatly improves model performance (see <ref type="figure" target="#fig_4">Figure A6</ref> for scaling with dataset size). However, even when only 10 samples can be submitted, scaling up the total number of samples leads to massive improvements in model solve rate. <ref type="figure" target="#fig_4">Figure 6</ref> shows how the model performance scales on the 10@ and @ metrics with more samples, i.e. as we increase . The difference between the two metrics highlights the importance of selecting which samples to submit. <ref type="figure">Figure 7</ref> shows how performance scales with the amount of compute used for training and for sampling. These scaling curves highlight a few interesting facts about this problem domain and our models:</p><p>Solve rates scale log-linearly with more samples. Both the 10@k and pass@k solve rates scale approximately log-linearly with , with the 10@k curve bending down slightly at high sample budgets. The fact that sampling significantly more than 10 still improves the 10@k solve rate shows how important it is to sufficiently explore the search space before committing to the final 10 submissions per problem. However, improving solve rate requires exponentially increasing amounts of samples and the costs quickly become prohibitive.</p><p>Better models have higher slopes in the scaling curve. Another observation from <ref type="figure" target="#fig_4">Figure 6</ref> is that larger models tend to have better model quality, reflected as better solve rate with the same number of samples and higher slope in this log-linear scaling curve. Because of log-linear scaling, a better model with a higher slope can reach the same solve rate with exponentially fewer samples than worse models. This points to improving model quality as an effective way to counter the exponential explosion of sample budget required to reach a higher solve rate.</p><p>Solve rates scale log-linearly with more compute. As shown in <ref type="figure">Figure 7</ref>(a), the solve rate also scales approximately log-linearly with more training compute. Each point on the curves corresponds to one model size. <ref type="figure">Figure 7</ref>(b) shows how solve rate scales with sampling compute, and highlights that larger models take more compute to draw each sample, but they eventually outperform smaller   <ref type="table">Table 6</ref> | Architecture comparison. Architecture changes increase sampling speed without significantly impacting the solve rate.</p><p>models even with the same sampling compute as the better quality of samples from the larger models become the dominant factor for performance. These results present an interesting trade-off between how much of the available compute should be used to train a model compared to sampling from it. Both ways of leveraging more compute demonstrate log-linear scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Architecture changes to improve sampling speed</head><p>Because drawing more samples is important for improving performance, architecture changes that increase sampling speed would also increase the overall solve rate within a certain compute budget. Therefore, we made two architecture decisions: using <ref type="formula" target="#formula_2">(1)</ref> an encoder-decoder architecture with asymmetric encoder and decoder structures and <ref type="formula">(2)</ref> the multi-query attention setup from Shazeer (2019) which uses one shared attention head for keys and one for values each block.</p><p>To investigate the effects of these decisions, we compared our base 1B parameter model against the two alternatives that remove each of the changes. We pre-trained and fine-tuned the standard multi-head attention model in exactly the same way as our base 1B model. The decoder-only model was trained with the same amount of compute. However, due to the significantly longer decoder sequence length (2304 tokens), with the same amount of training compute it consumes 50% more loss tokens than training the encoder-decoder models. <ref type="table">Table 6</ref> shows that our encoder-decoder model with multi-query attention significantly improves the sampling speed while keeping the sample quality at the same level as the more expensive alternatives. <ref type="table">Table 7</ref> compares our base 1B model trained on our full GitHub dataset with equivalent models that are pretrained on <ref type="formula" target="#formula_2">(1)</ref> the Python-only portion of GitHub, (2) the MassiveText generic text dataset  which also includes a portion of GitHub or (3) not pre-trained at all. The pre-trained models are then fine-tuned and sampled in exactly the same way, except that the model pre-trained on Python-only data is also fine-tuned on Python-only data and only samples Python solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Choice of the pre-training dataset</head><p>As <ref type="table">Table 7</ref> shows, pre-training on the full GitHub dataset with all languages leads to significantly better results than pre-training either on Python alone, or on the MassiveText dataset that mostly consists of natural language text. Any pre-training significantly improves the results over training from scratch on CodeContests. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4.">Model enhancements</head><p>As discussed in Section 4, we adopted training and model enhancements which significantly improved the solve rate relative to the standard encoder-decoder transformer setup. <ref type="table" target="#tab_10">Table 8</ref> presents the results of a build-up ablation of the enhancements we added to AlphaCode, starting from the base setting with no enhancements (beyond the multi-query attention change discussed in Section 5.3.2). We added one new setting at a time, with the final setting that corresponds to AlphaCode reported at the bottom of the   <ref type="table">Table 9</ref> | Example test statistics. Example tests help us filter out more than 99% of model samples, and as models get better with larger scales, they are more likely to find samples that pass example tests for more problems. One million samples were drawn per problem from each model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5.">Filtering &amp; clustering</head><p>To solve problems within a realistic evaluation budget, we rely on filtering and clustering to select a small number of samples to evaluate from the large amount of model samples we generate.</p><p>Filtering using example tests. <ref type="table">Table 9</ref> shows the percentage of model samples that pass example tests and the percentage of problems where at least one sample passes example tests. Note that these percentages are calculated based on the full set of samples, without first filtering out programs that have syntax errors (see Section 6.2 for more on syntactic correctness of the samples). Overall less than 1% of samples from our models pass example tests, though the percentage varies greatly across problems, which means that filtering using example tests removes more than 99% of the model samples. On problems where our models do find a correct solution, the fraction of samples that pass example tests roughly doubles but still remains at a low level. The non-uniform distribution of pass example tests across problems is highlighted more in Appendix C.4. Another observation from <ref type="table">Table 9</ref> is that larger and better quality models produce samples more likely to pass example tests, and pass example tests for significantly more problems. With 10 6 samples, our largest 41B models can generate solutions that pass example tests for over 90% of problems, a remarkable success as finding programs that satisfy I/O example constraints remains a very challenging problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering.</head><p>A solution has to pass hidden tests in addition to example tests, so we must further select correct samples from those that pass all public tests. Filtering 99% of a million samples still leaves thousands of samples per problem to select from. We cluster the remaining samples based on their behaviour on generated test inputs, to make the most of the evaluation budget. <ref type="figure" target="#fig_6">Figure 8</ref> shows a comparison between (i) randomly picking model samples without filtering, (ii) filtering and then randomly selecting from the filtered samples, (iii) filtering and then using clustering to select samples, and (iv) allowing unlimited evaluation attempts, which gives us the upper bound performance attainable with a perfect sample selection method. Filtering and clustering clearly enable scaling, as otherwise the solve rate remains flat. However there is still a large gap between them and the theoretical upper bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results on APPS</head><p>In addition to evaluating on Codeforces competitions and CodeContests, we performed evaluations on the previously published APPS benchmark to directly compare to previous work. The APPS dataset <ref type="bibr" target="#b32">(Hendrycks et al., 2021)</ref> contains a total of 10,000 programming problems divided equally between training and test sets. Because of missing information in the dataset, we could not apply our full We show random selection ("10@k no filtering"), filtering using example tests ("10@k with filtering"), clustering after filtering ("10@k with filtering + clustering"), and perfect sample selection ("pass@k"). method. We therefore followed the settings we used for pre-training on GitHub, and fine-tuned our pre-trained models on the APPS training set without using clustering, tags, ratings, value conditioning, or prediction, and with sampling temperature 0.25 and nucleus sampling. Other settings were the same as our main models. <ref type="table" target="#tab_2">Table 10</ref> compares our model with existing large language models fine-tuned on this dataset as reported by <ref type="bibr" target="#b32">Hendrycks et al. (2021)</ref>, as well as the 1-shot performance of the Codex model reported by . A small 1B parameter model already outperforms the GPT-NEO baseline on all difficulty levels, and outperforms Codex 12B on the interview and competition difficulty levels. We highlight that AlphaCode still improves when increasing the number of samples per problem, showing support for our claim of the importance of large scale sampling. Differences in performance between APPS results and CodeContests could be attributed to dataset quality (e.g. the high APPS false positive rate shown in Section 3.2.1), dataset size, missing components of AlphaCode, and tuning for the problem distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">AlphaCode's capabilities &amp; limitations</head><p>We performed a detailed analysis of the capabilities and limitations of our models. In particular, we find that our models are not simply copying from the training set (Section 6.1) and our models are sensitive to various changes in the problem descriptions and metadata used for conditioning (Section 6.3 and 6.4), both of which indicate that we are not solving problems by exploiting obvious weaknesses in the task structure.</p><p>We also analyze the characteristics of the solutions the model finds, for syntactic correctness, dead code, and the types of problems it can solve (Section 6.2). We further show that using validation loss as a proxy for model performance has several issues (Section 6.5). More analysis of our model and approach are included in Appendix E, and an attention visualization as well as example problems and solutions generated by the model can be found at https://alphacode.deepmind.com/. All analysis results are reported without clustering unless otherwise noted.  <ref type="bibr">(2021)</ref>. We used a time limit of 3 seconds per test to match Codex 12B, and report average numbers over 3 different fine-tuning runs for AlphaCode. Note that this does not include all components described in Section 4, and does not use the CodeContests dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Copying from training data</head><p>A commonly raised concern for large language models trained on large amounts of data is that they may solve downstream problems by simply memorising the training set (e.g. Albert Ziegler (2021); Carlini et al. <ref type="formula" target="#formula_2">(2021)</ref>). For competitive programming to be a good test of problem-solving ability we expect that models need to come up with novel solutions to solve new problems.</p><p>Based on the results in Appendix B.3, simply copying full solutions from the training set is not sufficient to solve any problems in the unseen validation set. However, it might be possible to solve problems by duplicating large or critical parts of previous solutions, if problems are sufficiently similar to previous ones. To investigate this, we found the longest common substrings between correct validation problem solutions generated by the model and the entire training dataset (GitHub + CodeContests, ignoring whitespace), and compared the distribution of the lengths of these matches to human solutions. <ref type="figure">Figure 9</ref> contains these results, using 50 C ++ and 50 Python solutions from a selection of 16 validation problems that had that many solutions.</p><p>The figure shows that model and human solutions share substrings with the training data at similar rates, although the average longest common substring is slightly higher for model solutions, and human solutions have a heavier tail. The common substrings between model solutions and training data mostly contained boilerplate code for reading and parsing the input data format, rather than key logic for solving problems (for example, a FastIO Python class has length 805 and appears in 1.66% of all human Python solutions). AlphaCode thus does not seem to solve problems by copying long blocks of code.</p><p>To investigate beyond just the longest common substrings between the solution and the training data, we performed a more detailed qualitative analysis of 50 model-generated solutions. We took each solution and iteratively removed the longest common substring from it, creating a partitioning of each solution consisting of substrings from the finetuning training data. <ref type="figure" target="#fig_0">Figure 10</ref> shows an example. We found no evidence that our model copies core logic from the training data. Further examples are  <ref type="table" target="#tab_2">Table 11</ref> | Solve rate (10@10k) for the 10 most common problem types at different model sizes.</p><p>The tags are ordered by popularity, with "Greedy" as the most popular.</p><p>provided in Appendix F.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Model solution characteristics</head><p>We measured the proportion of samples from the model that are syntactically correct (i.e. compile for C ++ , and do not generate a SyntaxError for Python) for each language and model size. As shown in <ref type="table">Table A7</ref>, our models tend to produce mostly syntactically correct programs for Python, and C ++ syntax is harder to master than Python.</p><p>We further analysed the amount of dead code (i.e. lines of code that have no impact on program behaviour) in our solutions. Such code is present in our training data; for example competitors will sometimes copy-paste unused imports or functions. High amounts of dead code could indicate the model has a poor understanding of what it is generating. <ref type="figure" target="#fig_0">Figure 11</ref> shows the results of applying a standard code formatter and Python's Abstract Syntax Tree (ast) module on correct Python human and model solutions to remove unused imports, functions and classes. AlphaCode generates approximately the same amount of dead code as humans. <ref type="table" target="#tab_2">Table 11</ref> shows our models' solve rates across different problem types specified using tags for each problem. Notably the solve rates across all tags overall go up as models improve with larger scales. Our models are relatively better at problems that deal with bitmasks, sorting, maths, and greedy algorithms, but notably worse at dynamic programming (DP) and constructive algorithms.</p><p>Model-generated solution Source document of LCS  Note that our samples contain a maximum of 768 tokens, and standard formatting can sometimes make the code longer.</p><p>Appendix E.2 contains the solve rate of our models in different problem difficulty rating buckets. Unsurprisingly our models have significantly higher overall solve rates for easier problems, and lower solve rates for harder problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Sensitivity to problem descriptions</head><p>We performed a detailed analysis of our models' sensitivity to the problem description in order to measure the importance of the description to the model performance. Overall, AlphaCode seems to respond appropriately to important changes in the problem description and makes use of extra information available in it. This indicates that, for example, the system does not ignore most of the description and brute force every possible solution that fits the category of problem (e.g. algorithms related to prime numbers if the word "prime" is mentioned in the description).</p><p>Full details of this study are in Appendix E.3, and a summary of results is shown in <ref type="figure" target="#fig_0">Table 12. (a)</ref> shows that when given a simplified description of the problem (not available in real evaluations), the model solves it at a much higher rate. (b) shows that the solve rate goes down dramatically when given related but different problems, and is not very affected by different ways of describing the same the <ref type="figure">problem. (c, d, e, g, h)</ref> show that the model is largely unaffected by changes that do not seem significant (like replacing words with synonyms or removing some type details), but responds more to larger changes (like deleting words or making the problem ill-posed). Notably, in Appendix E.3.3, we see that the model deteriorates relatively more in response to lower quality descriptions as model quality increases, indicating that better models are more capable of paying attention to subtle but important description changes.</p><p>Finally, (f ) shows that the model relies on different sections of the description, and particularly on the specification. This makes sense because the specification describes how to read the input, and otherwise the model would have to guess the input format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Sensitivity to provided metadata</head><p>As described in Section 4.4, at sampling time we provide randomised metadata to AlphaCode to increase sample diversity. This includes tags (e.g. whether the problem is of type "binary search" or "brute force"), ratings (how difficult the problem is), programming language, and whether or not the solution being generated is correct.</p><p>There are two natural questions to explore: does the model respond appropriately to variations in % correct (a) Description simplification   We examined the model's tag conditioning behaviour on an example problem in our validation set: Codeforces problem 1549A, Gregor and Cryptography ( <ref type="figure" target="#fig_6">Figure A8</ref>). In this problem, we are given a prime number, , and need to find two integers, and , such that mod = mod and 2 ? &lt; ? . It's tempting to solve this via brute-force, but there is a simple number theory solution: must be odd, so mod 2 and mod ( ? 1) both equal 1.</p><p>We sampled from our model, first with the tag "brute force", and then with the tag "number theory". These tags changed the sample distribution as demonstrated by the first successful samples in the two sampling runs, shown in <ref type="figure" target="#fig_0">Figure 12</ref>. The "brute force" approach is just that -although in reality it is guaranteed to break out of its loop on the first iteration -whereas the "number theory" approach Code generated with tag "brute force":</p><formula xml:id="formula_0">t = int ( input () )</formula><p>for _ in range ( t ) : p = int ( input () ) for a in range <ref type="bibr">(2 , p )</ref> </p><formula xml:id="formula_1">: b = p -a + 1 if p % print (a , b ) break</formula><p>Code generated with tag "number theory": t = int ( input () ) while t : p = int ( input () ) print ( '2 % t -=1 <ref type="figure" target="#fig_0">Figure 12</ref> | Sensitivity to tags. The first successful samples when attempting Gregor and Cryptography with different tags provided as part of the prompt. simply outputs the answer with no loop structure. This pattern continued in the following 2048 samples. The model solved the problem three times more often with the "number theory" tag (29 instead of 9 solutions), and output a perfect loop-free solution (other than reading the input) four times more often (12 instead of 3 solutions).</p><p>There are two possible ways that tag conditioning can improve the model solve rate: random tags may increases sample diversity or sampling correct tags may provides a useful hint. To distinguish between them, we compared solve rates when sampling a set of random tags for each sample (default), when providing the true problem tags, and when sampling a set of random tags for each problem (and reusing it for all samples for the same problem). The results are shown in <ref type="table" target="#tab_2">Table 13</ref>. Providing the true tags for a problem is impossible at test time, but is better than providing a fixed set of random tags. However, the best results come from sampling with random tags per sample, showing that the extra diversity of samples is important to increasing the solve rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Loss is a poor proxy for solve rate</head><p>When finetuning AlphaCode models we observed that the validation language modelling loss starts increasing after around 50k steps for an early training run of the 1B model, while the training loss still decreases. This normally indicates overfitting. However, contrary to the validation loss, our target metric solve rate continues to improve well past 50k steps as shown in <ref type="figure" target="#fig_0">Figure 13</ref>.</p><p>As discussed in Section 4.3, solving a problem is a one-of-many task, i.e. as long as one of many samples solves a problem, the problem is considered solved. Our finetuning dataset CodeContests contains many solutions per problem. Our main solve rate metric, 10@k, also uses samples rather than a single sample. We hypothesize that the model reallocates probability mass from some atypical solutions towards more typical solutions, leading to a worse validation loss overall, but a higher probability of producing more typical solutions and therefore a better solve rate.</p><p>Although solve rate is the ultimate metric, its high variance and computational cost make it difficult to use for decisions like the number of training steps. Improving validation loss to better correlate with performance could guide the decision making process better. We leave a full investigation of the relationship between validation loss and solve rate to future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Program synthesis</head><p>Program synthesis consists of automatically generating a program that satisfies a task specification. Possible ways of expressing the task include natural language descriptions, a set of input/output examples, or a series of constraints. As a research topic, program synthesis has a long history. Most classic approaches formulate the problem as searching for programs in a search space defined by the underlying programming language, where the programs must satisfy all the constraints which define the task. A notable example is the deductive synthesis approach <ref type="bibr" target="#b28">(Green, 1969;</ref><ref type="bibr" target="#b48">Manna and Waldinger, 1971</ref>), that transforms the task specification into constraints, uses a theorem prover to find a proof that satisfies all the constraints, and extracts the program from the proof. Later on, input/output-based task specifications became more popular, with notable examples like FlashFill <ref type="bibr" target="#b29">(Gulwani, 2011)</ref>. Finally, sketch-based approaches (Solar-Lezama, 2008) that synthesize programs from a provided skeleton of the target program greatly reduce the search space. <ref type="bibr" target="#b30">Gulwani et al. (2017)</ref> provides an excellent survey of these program synthesis approaches.</p><p>In recent years, deep learning has emerged as a useful tool for program synthesis. <ref type="bibr" target="#b73">Yin and Neubig (2017)</ref> used recurrent networks with attention to map text to abstract syntax trees and then code. <ref type="bibr" target="#b46">Ling et al. (2016)</ref> used similar models, with pointer networks, to generate complex class structures from mixed natural language and structured specifications of Hearthstone cards. Learned models can now be used to guide program search <ref type="bibr" target="#b4">(Balog et al., 2016)</ref>, generate program sketches <ref type="bibr" target="#b52">Murali et al., 2017)</ref>, convert pseudocode to code <ref type="bibr" target="#b45">(Kulal et al., 2019)</ref>, directly generate a target program <ref type="bibr" target="#b16">(Devlin et al., 2017)</ref>, or even generate programmatic policies in reinforcement learning settings <ref type="bibr" target="#b69">(Trivedi et al., 2021)</ref>.</p><p>Automatic code completion is also relevant to our work and has become an integral part of most code editors and integrated development environments (IDEs). While typing, these tools suggest possible continuations, greatly improving programming productivity. The earliest code completion systems were purely syntax-based. <ref type="bibr" target="#b34">Hindle et al. (2012)</ref> provided empirical evidence that code can be modeled by statistical -gram language models, and capitalised on this to develop a simple code completion engine for Java. More recent intelligent code completion systems can learn from history <ref type="bibr" target="#b62">(Robbes and Lanza, 2008)</ref> and large amounts of existing code data <ref type="bibr" target="#b3">(Aye et al., 2021;</ref>.</p><p>However, until very recently most code completion systems only generated suggestions for at most a single line. Similar trends are present throughout program synthesis: either restricting to short programs in narrowly defined domain-specific languages, or short code snippets of general-purpose programming languages. Scaling up, which increases both the depth and width of the search problem, has proven to be a difficult challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Transformers for program synthesis</head><p>Recently, the successes of large transformers in natural language modelling  have created a surge of interest in using transformer models for code retrieval, translation and generation <ref type="bibr" target="#b13">Clement et al., 2020;</ref><ref type="bibr" target="#b24">Feng et al., 2020)</ref>, making significant progress on program synthesis challenges. Trained on huge datasets covering a wide spectrum of text on the Internet, these models are capable of generating text with unprecedented fluency. The most relevant work to ours is the recent Codex system , a GPT language model <ref type="bibr" target="#b57">(Radford et al., 2019)</ref> trained on public code from GitHub. This model demonstrated impressive performance, achieving a high success rate at correctly completing hand-specified Python functions given the function signature and docstring, especially after fine-tuning on a similar dataset. Codex was used to build interactive program synthesis systems that are capable of solving university-level linear algebra and probability and statistics questions in <ref type="bibr" target="#b68">Tang et al., 2021)</ref>, and further used to create an advanced autocomplete system in GitHub Copilot. A similar model to Codex was trained by <ref type="bibr" target="#b2">Austin et al. (2021)</ref>, who also show that fine-tuning on a portion of a programming task dataset can improve the success rate on similar tasks. However, the programming tasks these works address are simple compared to the full scope of competitive programming problems, where both the task specification and the solutions are more involved. For example, in our dataset the median problem description length is 1,628 characters and the median solution length is 606 characters, while the HumanEval benchmark  has a median description length of 396 characters and solution length of 148.5 characters, about 4 times shorter for both. HumanEval problems also tend to include instructions about exactly what to implement, as opposed to competitive programming problems which pose a problem with no suggested implementation. Finally, it is not clear how unseen tasks are. Though they are hand-written instead of copied from an existing source, tasks like sorting arrays or checking if a number is prime have solutions that can be copied from the training dataset. Our work uses transformers but pushes model performance a significant step forward, from generating function completions to creating full solutions to held-out competitive programming problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Scaling sampling</head><p>Similar to our sampling and filtering, though on a much smaller scale, , <ref type="bibr" target="#b2">Austin et al. (2021)</ref>, and <ref type="bibr" target="#b14">Cobbe et al. (2021)</ref> found that repeated sampling on the same problem significantly increases the probability of finding a correct solution. <ref type="bibr" target="#b14">Cobbe et al. (2021)</ref> further introduced a way of selecting a small number of final submissions from multiple samples by majority voting. They also demonstrate verifiers (value functions) used to judge the correctness of model samples as a way of reranking samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Evaluation metrics</head><p>Evaluation metrics have evolved as models themselves have improved. Early work evaluated performance by measuring how well the generated code matched the ground truth reference code at the token level, syntax tree level, or full program level <ref type="bibr" target="#b60">(Ren et al., 2020)</ref>. These metrics determine whether code matches rather than whether a program is correct (syntactically different programs can be functionally identical), so as models have improved to be capable of fully solving programming problems, evaluation processes that execute programs and measure functional correctness <ref type="bibr" target="#b32">Hendrycks et al., 2021;</ref><ref type="bibr" target="#b45">Kulal et al., 2019)</ref> have become more popular. However both  and <ref type="bibr" target="#b32">Hendrycks et al. (2021)</ref> are somewhat limited as benchmarks because we have found that it is common for incorrect programs to be marked correct due to limited test coverage ( <ref type="table" target="#tab_3">Table 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Competitive programming</head><p>Lastly, progress in developing models that can solve competitive programming problems would not be possible without competitive programming datasets for training and evaluation. <ref type="bibr" target="#b10">Caballero et al. (2016)</ref> released a dataset of a few thousand competitive programming problems, and corresponding Python and C ++ solutions gathered from popular competitive programming platforms. <ref type="bibr" target="#b74">Zavershynskyi et al. (2018)</ref> introduced another dataset of competitive programming problems and solutions, though they converted solutions into an intermediate programming language which makes using pre-trained models difficult. <ref type="bibr" target="#b56">Puri et al. (2021)</ref> released a dataset of a large number of solutions in a wide variety of programming languages, with correct and incorrect solutions, and rich meta-data. Finally <ref type="bibr" target="#b32">Hendrycks et al. (2021)</ref> introduced the APPS dataset, a collection of 10,000 coding competition problems, and were the first to evaluate large transformer language models on competitive programming. The authors found that the overall solve rate on interview or competition level problems using large language models remained close to 0%. However, their evaluation format is not representative of competitive programming and, as noted above, this solve rate is an upper bound because of false positives due to a lack of test coverage ( <ref type="table" target="#tab_3">Table 2</ref>). Our dataset is built upon <ref type="bibr" target="#b10">Caballero et al. (2016)</ref> and <ref type="bibr" target="#b56">Puri et al. (2021)</ref>, with our own additional scraping from the Codeforces platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Broader impact</head><p>Good code generation models have the potential to have a positive, transformative impact on society, with a wide range of applications including computer science education, developer tooling, and making programming more accessible. However, like most technologies, these models might enable applications with societal harms which we need to guard against, and desire to have a positive impact is not itself a mitigation against harm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Applications</head><p>Although there are few direct applications of this work outside of competitive programming, improving human readable code generation opens the door to many future applications with large real-world impact. All these applications require varying amounts of future work.</p><p>Automating code generation could make existing programmers more productive, with potential applications ranging from suggesting extended code completions to optimizing blocks of code. Further in the future, advanced code generation models could let developers operate at a higher level of abstraction that elides details, much in the same way that modern software engineers typically no longer write in assembly.</p><p>Derived tools could make programming more accessible or help educate new programmers. Models could suggest alternative, more efficient or idiomatic, ways of implementing programs, which would allow one to improve their coding style. A code-to-documentation tool <ref type="bibr" target="#b24">(Feng et al., 2020)</ref>, would make it easier to understand what a complex section of code does. More extreme systems that operate entirely in natural language, like that proposed in Codex , could make it so that no knowledge of coding is required to create software.</p><p>However, code generation tools could also be used by bad actors. Better tools may make it easier to create new versions of malware, thus helping them avoid detection by security software (which often rely on databases of file fingerprints). Tools that improve the productivity of developers would also improve the productivity of developers writing malicious code <ref type="bibr" target="#b72">Weidinger et al., 2021)</ref>. Alternatively, competitive programming code generation models in particular could give users unfair advantages in programming competitions or technical interviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Potential risks and benefits</head><p>Interpretability. One major advantage of code generation models is that code itself is relatively interpretable. Understanding the behavior of neural networks is challenging, but the code that code generation models output is human-readable and can be analysed by traditional methods (and is therefore easier to trust). Proving a sorting algorithm is correct is usually easier than proving a network will sort numbers correctly in all cases.</p><p>Interpretability makes code generation safer for real-world environments and for fairer machine learning. We can examine code written by a human-readable code generation system for bias, and understand the decisions it makes.</p><p>Generalisation. Code generated by code generation models tends to generalise; passing a sufficient number of tests makes code more likely to also pass even out-of-distribution tests. This level of generalisation over the full domain of inputs and outputs is often hard to obtain with neural networks, and could make code generation models more reliable in real-world out-of-distribution applications.</p><p>Bias, fairness, and representation. Similar to natural language models , code generation models are prone to reproducing the deficiencies and biases of their training data. When trained on diverse corpora of human data, these models can reinforce and perpetuate societal stereotypes, leading to disproportionate impact on marginalized communities. For example, programs can contain culture and location-specific assumptions about names (McKenzie, 2010), addresses <ref type="bibr" target="#b67">(Tandy, 2013)</ref>, or time (Sussman, 2017), excluding underrepresented users.</p><p>Bias can also lead to low quality code that perpetuates bugs or the use of outdated APIs, 11 resulting in performance and security issues. This could decrease uptake of new libraries or programming languages.</p><p>Security. As mentioned above, code generation can have security risks and benefits. Models can generate code with exploitable weaknesses, either unintentional vulnerabilities from outdated code or intentional ones injected by malicious actors into the training set <ref type="bibr" target="#b55">(Pearce et al., 2021)</ref>. Further, code generation could enable both threat actors and threat defenders, increasing productivity and enabling new techniques. For example, polymorphic malware changes its implementation to hide from detection .</p><p>Environmental impact. Like large-scale language models, training transformer-based code generation models takes a significant amount of compute. Further, because we found that large-scale sampling is critical to improving performance, relatively more compute is spent executing our model compared to traditional language models. Both sampling and training from our model required hundreds of petaFLOPS days. 12 However, one comparative advantage of code generation models is that once a program is synthesized it can generally be executed cheaply by any computer, unlike neural network models that typically need to be run on accelerators. Therefore code generation models can potentially be scaled to many applications more easily.</p><p>Intellectual property. There are intellectual property concerns with large training corpora used to train code generation models. Whether training on publicly available data is fair use is an open question for some systems <ref type="bibr" target="#b26">(Gershgorn, 2021)</ref>, although this is less relevant to AlphaCode which filters its dataset based on licenses. There still remains the decision of how to credit and use people's code, even with permissive licenses.</p><p>Automation. As programming becomes more accessible and productive, and code generation can automate some simple tasks, it's possible that there could be increased supply and decreased demand for programmers. This is partially mitigated because writing code is only one portion of the job, and previous instances of partially automating programming (e.g. compilers and IDEs) have only moved programmers to higher levels of abstraction and opened up the field to more people.</p><p>Advanced AI risks. Longer term, code generation could lead to advanced AI risks. Coding capabilities could lead to systems that can recursively write and improve themselves, rapidly leading to more and more advanced systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>In this work, we present AlphaCode, a system applied to code generation for competitive programming that can generate novel solutions to unseen programming problems. Evaluated on Codeforces, AlphaCode performs roughly at the level of the median competitor. We find that massively scaling up sampling and then filtering and clustering samples to a small set, together with new samplingefficient transformer architectures to support large-scale sampling, are essential to achieving good performance. Our clean dataset and robust evaluation procedure also contributed significantly to guiding our research progress. We also show through detailed analysis that there is no evidence that AlphaCode copies important parts of previous solutions or exploits weaknesses in the problem structure. This indicates that our model indeed is able to solve problems it has never seen before, even though those problems require significant reasoning. Finally, we present the results of various model probings, and discuss broader impact and hazards of such code generation models.</p><p>Stanway for logistically making the project possible; Sumanth Dathathri for analysing our model; Ethan Caballero for giving us permission to use Description2Code data; Rosemary Ke for helping connect us with Ethan; Pablo Heiber for helping connect us with Codeforces; Petr Mitrichev for helping connect us with Codeforces, and lending competitive programming expertise when writing the paper; Mike Mirzayanov for allowing us to evaluate on Codeforces; and everyone at DeepMind for their insight and support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Xinyun Chen 14 worked on model development.</head><p>Yujia Li was the project lead, developed initial prototypes for solving competitive programming problems, and contributed to aspects including the development of datasets, metrics, evaluation, models, training, clustering, infrastructure tools, and paper writing.</p><p>Alexey Cherepanov, Johannes Welbl, Po-Sen Huang, and Sven Gowal worked on model analysis.</p><p>Nando de Freitas, Koray Kavukcuoglu, and Pushmeet Kohli advised the project, and Nando de Freitas further contributed to paper writing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data availability</head><p>The datasets used in the experiments have been made available for download on GitHub. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Hidden tests</head><p>Competitive programming problems typically contains example tests in the problem statement, and also hidden tests not visible to the competitors that are used for evaluation. <ref type="figure" target="#fig_0">Figure A1</ref> contains a hidden test case for the example problem in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Program judging</head><p>When submitting to Codeforces, as in Section 5.1, we can use the same judging system used in competitions. We try to emulate this system within CodeContests: we check the correctness of a program by executing it on the test cases and comparing the program outputs with the expected correct outputs. However, judging whether the program outputs are correct can be challenging, and Appendix <ref type="figure" target="#fig_0">Figure A1</ref> | Hidden test cases used to verify correctness of solutions to the problem from <ref type="figure" target="#fig_1">Figure 2</ref>. Compared to the example tests seen by participants (and used within AlphaCode), the held-out hidden test cases used to evaluate correctness are substantially longer and more demanding. Hidden test case sourced from Codeforces. involves more than checking for an exact match against the correct outputs. Each problem can have specific rules including case sensitivity, whitespace, format, and floating point precision. Further, problems may have multiple correct outputs (e.g. permitting any sequence that follows a constraint), or multiple possible inputs (e.g. an interactive problem where the input depends on what the program outputs). The judging process described below takes place both for final submissions, and for filtering based on example tests.</p><p>Because these constraints are difficult to extract from the problem, our judging code takes a permissive view of formatting. Floating point numbers are considered equivalent if their difference is less than 10 ?5 , string comparison is case insensitive, and whitespace differences are ignored. This does not exactly match formats given by problems, but we found these issues are not too significant. When verifying dataset false positive rates, we did not find any problems that were incorrectly marked correct because of this issue.</p><p>We determined which problems have multiple correct outputs heuristically using human solutions; if any test case had at least 5 distinct outputs from human-written correct solutions, or 2 outputs produced by multiple human solutions, we assumed it had multiple outputs. About 1/4 of our validation set problems are multiple output problems by this criteria. These problems are judged using the same permissive formatting and against a single correct output, where the correct output is chosen to be what the majority of human solutions output. Because we assume a single correct output, our judging can underestimate the actual model performance. An alternative is to accept any output that a human solution outputs, which decreases the false negative rate in judging, but we found that this leads to significantly increased false positives.</p><p>Interactive problems are substantially rarer than multiple output problems, and we do not explicitly handle them, which could lead to both false negatives and false positives. We did not find any interactive problem false positives.</p><p>Competitive programming problems also often include time and memory limits, and we use these limits when executing submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Evaluation metrics</head><p>As described in Section 2.2, we use the @ solve rate to evaluate model performance, which measures the fraction of problems a model can solve when allowed to generate samples but only submit of them for evaluation.</p><p>There are two sources of variance in the computation of this metric:</p><p>? If we train the same model architecture with the same data but different random seeds, we will end up with different models which will produce different samples. ? If we take samples from the same trained model multiple times, we will end up with a different set of samples each time.</p><p>We use the technique discussed below to reduce the second source of variance in all of our reported results except for clustering results (which are discussed below). Reducing the first source of variance is more challenging, however, and given the computational cost of training our models it is not practical to pre-train and fine-tune multiple models for all results in the paper. Nonetheless, given the importance of the ablation results in <ref type="table" target="#tab_10">Table 8</ref>, for only this table, we fine-tuned at least 3 models for each setting from the same pre-trained checkpoint, and reported average @ results over these models.</p><p>To compute @ from a single model we:</p><p>? Draw a set of ? samples from the model. ? Draw sub-samples of size from the full set of samples without replacement.</p><p>? Calculate solve rates with submissions for each sub-sample.</p><p>? Report the average solve rate across all of these sub-samples.</p><p>This estimation process is outlined in Algorithm 1. To create a scaling curve, we use this procedure to calculate @ for different values of , using the same set of samples.</p><p>To generate the confidence intervals for the ablation results in <ref type="table" target="#tab_10">Table 8</ref>, we use bootstrap re-sampling. Specifically, we:</p><p>? Re-sample with replacement models from the models we have trained. ? For each model, re-sample with replacement samples from the we have taken from that model. ? Compute @ with the re-sampled models and samples using the process described above.</p><p>We perform this re-sampling and estimation process many times, and report the 95% confidence interval as the 2.5th percentile and 97.5th percentile from the resulting set of estimates.</p><p>It's difficult to use the sub-sampling process described above for results which include clustering, because computing clusterings adds additional computational complexity. Thus unless otherwise noted, we compute all clustering results as follows. For each size and each model trained for the base setting used for clustering, we run clustering on five different subsamples. The reported means are Algorithm 1 Algorithm for computing n@k with filtering using example tests. Input = the number of allowed submissions in @ Input = the number of allowed samples in @ Input = the number of samples which pass the example tests for each problem Input = the number of samples which solve the problem (pass all tests) for each problem Input = the number of samples actually taken per problem Hyperparameter = the number of subsamples to use for calculation <ref type="bibr">1:</ref> for each problem in the problem set do <ref type="bibr">2:</ref> for each of the subsamples do <ref type="bibr">3:</ref> Sample ?Hypergeometric( , ? , ) ? # samples out of which pass examples tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>? min( , ) ? Only submit samples that pass the example tests. Compute @ for this problem as the average of all solved . 9: end for <ref type="bibr">10:</ref> return the average @ across all problems. the average of these 5 data points across all trained models for each size . The confidence intervals are computed using bootstrap re-sampling similar to the process above, where we first re-sample models from the we have available, and then re-sample five clustering runs from the five we have available for each model and each size . The clustering results in <ref type="table" target="#tab_10">Table 8</ref> and <ref type="figure" target="#fig_6">Figure 8</ref> were based on 5 models fine-tuned in the "+ GOLD" setting. The clustering results for the 9B and 41B models are based on single models due to the cost of training multiple models at these sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. GitHub dataset composition</head><p>Our GitHub pre-training dataset contains a total of 715GB of data across a range of different programming languages. The exact composition is listed in <ref type="table" target="#tab_2">Table A1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Dataset cleaning</head><p>To avoid data quality and duplication issues involved in combining datasets from different sources, and to make our scraped code more consistent, we performed the following data-cleaning steps:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language</head><p>Files <ref type="formula">(</ref> (a) all submissions that pass no tests, (b) all tests that less than 10% of the remaining submissions produce non-empty outputs on, (c) all submissions that pass less than 10% of the remaining tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Data leakage and temporal split</head><p>Transformer language models trained on large datasets of text from the Internet can generate text with impressive fluency. However, due to the scale of the training corpus many downstream evaluation tasks derived from data available on the Internet are subject to training / evaluation data leakage. This constitutes an obvious issue, especially since these models sometimes copy verbatim from the training set . While such copying can be beneficial <ref type="bibr" target="#b58">(Borgeaud et al., 2021)</ref>, referring to information that was not available at the time of the competition would constitute cheating.</p><p>In code, data leakage and duplication across training and evaluation sets are particularly common <ref type="bibr" target="#b1">(Allamanis, 2019)</ref>, and many participants publish their solutions online after competitions. The strict temporal split of our datasets can guard against this type of leakage, as it ensures that our training data only includes information that would be available to a typical human participant in a competition.</p><p>We verified the temporal split for both pre-training and fine-tuning by examining the solve rate on CodeContests validation problems. For the fine-tuning dataset, a baseline consisting of evaluating one solution from each training problem on the validation set reached a solve rate of 4.1% with a random split, but 0% with a temporal split. When using a random validation split a 1B parameter model pretrained on GitHub had a solve rate of 0.8% with 13k samples per problem, while the temporal split solve rate remained 0%. 13k was chosen to match the number of problems (and therefore solutions) used in the baseline. The 0% solve rate with our temporal split means that models must go beyond simply remembering the training set, and instead they need to create novel solutions to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Approach and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Ensembling</head><p>Ensembling is another approach we tried to more effectively search in the program space. To ensemble a set of models, we pool their samples together before running filtering and clustering. Since different models can have different strengths and weaknesses, ensembling them can increase the diversity of the samples. However, it is also possible to hurt the performance of good models by ensembling them with significantly worse ones, because this wastes the sample budget. Finding the right composition for the ensembling therefore is important for performance improvement.</p><p>We found that ensembling can indeed increase or decrease performance relative to the individual components in the ensemble. In particular, when the performance difference of individual runs is large, the ensemble tends to be dominated by the better run and is typically a bit worse than it. When the performance difference of individual runs is smaller, the ensemble is more likely to be better, because it covers the search space more effectively than any single component. Two examples of ensembling are shown in <ref type="figure" target="#fig_1">Figure A2</ref>.</p><p>The ensemble of our best models at 41B and 9B scales, using equal amounts of samples from each model, outperforms the individual models by a consistent yet small margin. With 1 million samples per problem, the 41B + 9B ensemble reached a solve rate of 32% on our validation set with 10 submissions, and 35.5% with clustering. We therefore used the ensemble of 41B and 9B models for our evaluation on Codeforces described in Section 5.1.</p><p>However, this ensemble turns out to be slightly worse, or at least not better, than using our 41B model alone on the test set. Given the small margin of improvement from ensembling, this performance regression is not entirely unexpected. Appendix <ref type="table" target="#tab_3">Table A2</ref> | Comparison of ensemble performance on validation set vs. test set. The ensemble of our best 41B and 9B models performed better than individual runs by a small margin on the validation set, but not better than 41B on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Metadata conditioning</head><p>Codeforces problems in our CodeContests dataset contain rich metadata, notably tags and difficulty ratings. Problems have zero or more tags that suggest what kind of algorithms may be useful for approaching the problem, for example "divide and conquer", "dynamic programming", and "data structures". Difficulty ratings are values in the range [800, 3500], where higher ratings correspond to more difficult problems. Tags and ratings are only available after a programming contest has ended. Solutions also contain information about what programming language the solution is.</p><p>At sampling time, we do not access the actual ratings and tags as they are not available during competitions. We found however that solve rates improved by conditioning (i) on ratings sampled uniformly from 800 to 3500 in increments of 100, (ii) on sets of tags sampled uniformly at random from the 50 most popular combinations, and (iii) on language sampled uniformly between C ++ and Python. We believe that sampling metadata leads to a more diverse set of model samples, a strategy similar to that used by <ref type="bibr" target="#b71">Vinyals et al. (2019)</ref>, and allows our model to take advantage of the relative strengths across the metadata distribution. Because we take a large number of samples, exploring different approaches is more important than maximizing per-sample reward.</p><p>When conditioning on metadata, the chosen metadata values are added as a prefix to the natural language prompt, as shown in <ref type="figure">Figure 5</ref> and <ref type="figure">Figure F</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. GOLD</head><p>We want to train a model that is capable of finding any correct solution to a problem (like precision), rather than one that over-focuses on capturing the entire training distribution (like recall). To avoid the tendency of maximum-likelihood objectives to put some weight on every solution, we used a variation of the -reward version of GOLD <ref type="bibr" target="#b54">(Pang and He, 2020)</ref>, an offline RL algorithm which adds an off-policy importance weight to the standard maximum likelihood objective gradient:</p><formula xml:id="formula_2">?L GOLD ( ) = ? ?? ?Solution tokens ( )? log ( ),<label>(1)</label></formula><p>where are the model parameters, and log ( ) is the standard log-likelihood objective for predicting the next token . The additional ( ) multiplicative importance weight allows the model to both learn from tokens it already assigns high likelihood to, and to ignore tokens that are not in its distribution. This way, the model can concentrate on precision rather than recall, and increase its chance of getting at least one correct sample. To mitigate instabilities during training, we replace ( ) in the importance weight with max( ( ) , ), = 1 2 , = 0.05.</p><p>Combining GOLD with tempering presents a difficulty of which distribution should be used for the reweighting term. The non-tempered distribution becomes smooth as fine-tuning progresses, which means losing the selection benefits of GOLD. The tempered distribution stays relatively sharp during fine-tuning, but is too sharp at the beginning of fine-tuning (as our pre-trained model is not trained with tempering), leading to overly strong GOLD selection.</p><p>To resolve this issue, we add a short training phase between pre-training and fine-tuning, during which we apply tempering but crucially not GOLD. This allows the initial pre-trained distribution to transition to a smoother distribution. During fine-tuning, we can then use the tempered distribution by dividing the logits by the temperature before computing the loss, so both the log-loss term and the importance weight use the tempered distribution. Appendix <ref type="figure" target="#fig_2">Figure A3</ref> | Fraction of samples that pass example tests. pass example test for each problem in the validation set for each model size. The problems are sorted by the 41B model's pass example test . <ref type="figure" target="#fig_2">Figure A3</ref> shows the distribution of pass example test across problems in our validation set for each model. Notably, this distribution is far from uniform across problems; just over 1/3 of the problems (around the number of problems we solve) have a pass example test significantly higher than zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probability of samples passing example tests varies significantly across problems.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings for clustering.</head><p>Since the test input generation model we trained is not perfect, it can generate test inputs that are invalid or cannot sufficiently distinguish correct solutions from incorrect samples. These test inputs can still be used to cluster model samples, but this process may give us ambiguous clusters containing both correct and incorrect samples. Additionally, even correct solutions may be put into multiple clusters, as their behaviour on invalid test inputs may still differ. We tuned two hyperparameters for clustering on the evalidation set: the number of test inputs to use in clustering as well as the maximum number of model samples to consider for clustering. We used 50 test inputs with 8192 model samples for all the clustering results reported in this paper. Performance did not continue to increase with higher numbers of either hyperparameter. Appendix <ref type="table">Table A3</ref> | Decoder-only Results on HumanEval. GPT-Neo, TabNine, and Codex numbers from . Our decoder-only models are pre-trained on the Python-only subset of GitHub and evaluated without any fine-tuning. The pass@k rates are computed using the algorithm from  with 1000 samples. For each row, column pair we only report the best nucleus sampling result from the temperatures 0.0, 0.2, 0.4, 0.6 and 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. HumanEval comparison</head><p>To ensure that our baseline decoder-only models are as comparable as possible with Codex, we evaluated our models on the HumanEval benchmark from . From <ref type="table">Table A3</ref> we can see that our pretrained decoder-only baseline models obtain HumanEval solve rates which are within about 1-3% of the comparable Codex model for most settings, and significantly better than GPT-Neo and GPT-J at all comparable settings. The HumanEval results for all of our encoder-decoder models (including the final AlphaCode model) are significantly worse than the decoder-only models, so we do not report them here. We believe this performance difference stems from the fact that encoder-decoder models are well aligned with the competition programming setting where we have a dataset with clear inputs (programming contest problem descriptions) and outputs (solution code), as well as example tests for effective filtering. Therefore the encoder-decoder models can both learn effectively and sample efficiently. However, encoder-decoders are not well aligned with the HumanEval setting where the only training data is GitHub code which cannot easily be split into meaningful inputs and outputs. As a result, the fact that decoder-only models compute loss on all tokens (rather than only one-third of tokens) enables them to learn more efficiently in the HumanEval setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6. APPS dataset settings</head><p>The CodeContests training set has a non-empty intersection with the APPS test set, and therefore CodeContests cannot be used during training when evaluating on the APPS benchmark. Appendix <ref type="figure" target="#fig_3">Figure A4</ref> | Code used to extract the number of example tests from the descriptions in APPS problems for the filtering system used within AlphaCode.</p><p>The example tests from problem descriptions were not parsed in the APPS dataset, and therefore we parsed them using <ref type="figure" target="#fig_3">Figure A4</ref>. Our code fails to produce example tests for less than 2% of APPS test problems, including problems in Russian and a problem without a description (APPS test 4109).</p><p>The Codex authors write "In coding competitions and in the APPS datasets, tasks are provided with 3 input/output examples included in the task description" , but not all problems in APPS have 3 example input/output pairs. Some have 0 (like APPS test 4109 and APPS test 4671), 1 (APPS test 4659) and 2 (APPS test 2844). We assumed that they only pass 3 example input/output pairs if available, and otherwise parse fewer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7. Best settings for sampling</head><p>As we generate a large amount (? 1M) of samples for each problem, the exact settings to use for sampling can potentially have a large impact on the model performance.</p><p>In <ref type="figure">Figure A5(a-b)</ref>, we show that sampling temperature does have an impact on the solve rate, but the temperature of = 0.25 works well across a wide range of sample budgets. <ref type="figure">Figure A5</ref>(c-e) shows the results for top-k sampling and nucleus sampling. Both of them turn out to be not better than the simple sampling with temperature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.8. Scaling with dataset size</head><p>Besides scaling with model size, compute, and number of samples presented in Section 5.3.1, we also observe that, as expected, the model performance scales with the dataset size. <ref type="figure" target="#fig_4">Figure A6</ref> shows how the model performance scales with larger datasets. Increasing the number of problems has a more significant positive impact than increasing the number of solutions for each problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Codeforces contest evaluation</head><p>In this section we describe the procedure and detailed settings we used for the evaluation on Codeforces contests, as well as detailed results of that evaluation. Summarized evaluation results are presented in Section 5.1. (e-f) Nucleus sampling does not significantly improve the sampling baseline. Performance increases with the nucleus size, with an optimal size close to 1.0 (i.e. plain sampling). Appendix <ref type="figure" target="#fig_4">Figure A6</ref> | Dataset scaling. Increasing either the number of problems (a) or solutions (b) in the finetuning dataset improves solve rate of an early 1B parameter model, although increasing the number of solutions has a smaller effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of submissions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fraction of time left in contest</head><p>Relative number of samples that pass example tests, compared to the clustering default 1 0.1 0.005 5 0.5 0.05 Remaining 0.9 1.0</p><p>Appendix <ref type="table">Table A4</ref> | Codeforces submission points. Each row specifies the conditions for when submissions happen, and the number of submissions at that point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Simulation</head><p>Contest scoring is based not only on whether or not a problem is solved, but also on when in the contest it is solved and how many incorrect submissions there were before a correct one. Our procedure takes all three into account.</p><p>For each contest, we simulated running live with 3750 TPUv4 and 3750 TPUv4i <ref type="bibr" target="#b41">(Jouppi et al., 2021)</ref> chips for the duration of the contest. These devices continuously draw samples, using a pool of workers to evaluate samples against example tests. Because clustering runs best when there are enough samples that pass example tests, we cannot continuously submit submissions. Instead, for each problem in the contest we clustered and submitted up to a total of 10 samples at three points <ref type="table">(Table A4)</ref>, where the point was either based on the fraction of the time remaining in the competition or based on the relative number of samples that pass example tests available compared to the default number. When computing these points, all contests were assumed to be two hours for simplicity even though some were slightly longer. Clustering and a specified number of submissions were done when either of these conditions were reached. The time of correct submission was then the time in the contest that corresponds to when the condition for each row was reached, plus 120 seconds for clustering. Because submissions were submitted in order, we also know the number of incorrect submissions. Additionally, we did not submit solutions to Codeforces that were obviously incorrect (by quick manual inspection) and instead automatically counted them as incorrect, though note that this can only decrease model performance.</p><p>In practice, however, we only simulated running live and instead submitted our solutions after the contest already ended. One consequence of this is that we do not fully consider the "hacking" phase of the competition, where competitors can get points for finding vulnerabilities in the code of others. This is only a factor for some competitions, hacking is not generally a major concern, and since the model itself does not hack others the only consequence is that the system receives more timely feedback about correctness on edge cases, so we do not believe this omission is significant.</p><p>A more significant consequence is that, as described in Appendix A.2, we filter multiple output problems based on the consensus example output produced by human solutions, rather than the example output itself (which may be intentionally designed to cover multiple possible ways of solving the problem, and therefore not be output by any particular human solution). This example output change affected approximately five problems AlphaCode solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Multiple evaluations</head><p>After the first evaluation, we decided to run the evaluation procedure multiple times to measure variance. Variance in this evaluation process can come from the (1) trained model, <ref type="formula">(2)</ref> set of samples,</p><p>ordering of samples, and (4) clustering process. Due to compute limitations, we did not retrain or resample the models for each evaluation, and instead tried to measure variance from <ref type="formula" target="#formula_3">(3)</ref> and <ref type="formula">(4)</ref>. We used the samples from the first evaluation, but reshuffled them to simulate drawing them in a different order. We also used a different set of inputs generated by the input generation model for clustering, and selected samples from clusters with a different random seed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Results</head><p>After submissions we computed our score on each contest (including penalties) using the contests' scoring system, and found where the model would have placed on the contests' official scoreboards. Per-problem contest results can be found in <ref type="table" target="#tab_6">Table A5</ref>. Overall contest results can be found in <ref type="table">Table  A6</ref>. In the second and third evaluations, we submitted more than 10 submissions per problem. We found that there were some problems we only solved with many samples.</p><p>We also computed our estimated Codeforces Elo score by tracking what our Elo would have been if we started with the first contest, and competed in each contest in the order they were released, placing according to our calculated placement in <ref type="table">Table A6</ref>. This was done separately for all three evaluations, and then averaged.</p><p>Our Elo estimation is based on our reproduction of the Codeforces Elo method, as we didn't compete live. We checked correctness by reproducing other participants' Elo scores. Our approach largely matches the Codeforces Elo (differing by &lt; 15 points), but our Elo score is still only an estimation. <ref type="table">Table A7</ref> shows the percentage of syntactically correct samples from the models. That is, samples that compile for C ++ , and samples that do not generate a SyntaxError for Python.  <ref type="bibr">3,8,4 18,110,19 link</ref> Appendix <ref type="table" target="#tab_6">Table A5</ref> | Codeforces per-problem results. Problems on Codeforces are grouped into contests, and uniquely identified by their contest and problem letter. For each solved problem, we specify whether they were solved (1 for solved), how many incorrect submissions there were before a correct one, and the simulated submission time in minutes. Unsolved problems are not listed. Results corresponding to the three evaluations are separated by commas. "-" indicates that the problem in the evaluation was unsolved. Submitted programs can be found on our 3 accounts on Codeforces: SelectorUnlimited, WaggleCollide, and AngularNumeric. Note that for the first evaluation, we only submitted at most 10 times per problem. Unsolved problems and submissions beyond 10 (for the second and third evaluations) are marked in red to indicate that they are not included when measuring performance in the setting with a maximum of 10 submissions per problem. Appendix <ref type="table">Table A7</ref> | Percentage of syntactically correct model samples. We report results on the validation set, for each language and model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional analysis of AlphaCode's capabilities and limitations E.1. Model sample statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Solve rate for different problem difficulty ratings</head><p>respectively, and the number of problems in each bucket can be quite small. Therefore, the numbers in this table may have large variance. Also note that due to the lack of long test cases, our evaluation is susceptible to accepting correct but slow solutions (see Section 3.2.1). The high difficulty rating problems are particularly affected by this.</p><p>As expected, we see that overall our models perform significantly better on problems with low difficulty ratings, and the performance quickly degrades when the problem difficulty increases. High solve rates at high difficulty buckets are caused by our slow positive rate being quite high (46% as reported in Section 3.2.1), and by the buckets being quite small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Sensitivity to the problem descriptions</head><p>We measured model performance in response to assorted changes in the problem description, to see if the model responds appropriately and makes use of the description. Because of compute limitations, we were unable to retrain models on these modifications, but instead sampled already trained models. The main metric used for most analyses was the 10@1024 solve rate, that is, solve rate using 10 submissions from 1024 samples. We found overall that the problem description is critical, and AlphaCode does not simply brute force possible solutions. Appendix <ref type="table" target="#tab_10">Table A8</ref> | 10@100k Solve rates of our models in different difficulty buckets. Also shown is the number of problems in each difficulty bucket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.1. Simplification of problem descriptions</head><p>Understanding what to implement is a key component of competitive programming problems. It involves parsing the problem statement (which is often phrased as a story), and coming up with the insights needed to solve it. If our model makes use of this statement, a simplified statement should improve its solve rate as this helps with problem understanding.</p><p>For example, consider Codeforces problem 1559A, Mocha and Math, shown in full in <ref type="figure">Figure A7</ref>. The key observation to solve this problem is to note that the optimal solution is achieved when every element is bitwise ANDed with every other element. We can therefore simplify this problem by changing the description part of the problem statement to a single sentence:</p><p>Given a sequence of integers, compute the bitwise AND of all of its elements. Appendix <ref type="table" target="#tab_14">Table A9</ref> | Performance on original vs. simplified problems. The percentage of correct samples from a total of 100k samples, for original problem wordings and rewordings which make the required algorithm more explicit. This result was obtained using a 1B parameter model.</p><p>We find that as expected, simplifying problem descriptions this way significantly improves our model's performance <ref type="table" target="#tab_14">(Table A9</ref>). The solve rate for our base 1B parameter model on this particular problem increased from 12% to 55%. We performed this analysis for five problems: three from our validation set and two hand-written ourselves. The full wordings of these problems are included in Appendix F.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.2. Incorrect or irrelevant rewordings</head><p>To investigate what parts of the problem description the model pays attention to and how strongly it conditions on the description, we investigated how the solve rate for a problem changes when we add irrelevant information to the description, or reword it to be under-specified or incorrect. We</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mocha and Match</head><p>Mocha is a young girl from high school. She has learned so much interesting knowledge from her teachers, especially her math teacher. Recently, Mocha is learning about binary system and very interested in bitwise operation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>For each test case, print one integer -the minimal value of the maximum value in the sequence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explanation</head><p>In the first test case, Mocha can choose the interval <ref type="bibr" target="#b75">[1,</ref><ref type="bibr" target="#b76">2]</ref>, then the sequence becomes [0, 0], where the first element is 1&amp;2, and the second element is 2&amp;1.</p><p>In the second test case, Mocha can choose the interval <ref type="bibr" target="#b75">[1,</ref><ref type="bibr" target="#b77">3]</ref>, then the sequence becomes <ref type="bibr" target="#b75">[1,</ref><ref type="bibr" target="#b75">1,</ref><ref type="bibr" target="#b75">1]</ref>, where the first element is 1&amp;3, the second element is 1&amp;1, and the third element is 3&amp;1.</p><p>Appendix <ref type="figure">Figure A7</ref> | Example problem statement. Problem statement of Mocha and Math, a Codeforces problem <ref type="bibr">(Mirzayanov, 2020)</ref>. This is an easy problem, with a rating of 900. performed this analysis on the simplified Cherry problem, measuring solve rate of different rewordings. Full problem descriptions can be found in Appendix F.2.2.</p><p>The results in <ref type="table" target="#tab_2">Table A10</ref> continue to suggest that the model is strongly conditioning on the description (rather than, for example, brute forcing all possible solutions related to the problem domain). The model is also able to parse algorithm descriptions from either symbols or from natural language, and can ignore irrelevant natural language explanations; indeed, the model actually does better with more language-heavy descriptions, perhaps because of the verbose nature of the training distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.3. Capturing variables and their relations</head><p>Problem descriptions include multiple variable names to denote objects and quantities of interest, and to specify relationships between them. For example, a problem might feature an array, , of length , or two integers called and with &lt; . Understanding these variables and their relations is necessary for solving problems.</p><p>We investigated two changes: random variable name substitutions either consistently applied throughout a problem, or applied at random for each instance such that consistency is not guaranteed (which renders the problem ill-posed). Perturbations were done on up to 6 different variables, maintaining consistent character case, and variables were replaced by other existing variable names from the same problem (see <ref type="figure" target="#fig_6">Figure A8</ref> for a concrete example). <ref type="figure">Figure A9</ref> shows the results of this evaluation. The small (300M) model is largely unaffected by both consistent and inconsistent renaming, suggesting that it does not model the different variables well. As model size increases however, the relative performance drop observed with ill-formed inputs becomes more and more pronounced, while sensitivity to consistent variable renaming decreases. This suggests that as models get larger, they are increasingly able to capture relevant relationships between variables described in the problem formulation. The non-trivial solve rate for ill-posed problems, on the other hand, suggests that other parts in the problem description provide important cues for a solution, which models can learn to leverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.4. Sensitivity to word-level changes</head><p>Typing. We analysed whether the model is sensitive to the implicit type information contained in problem descriptions. In particular, we replaced the words integer, array, and string with the more generic terms number, sequence, and sequence of characters, respectively. This process allows us to determine whether the model pays undue attention to specific output and input types to generate viable solutions. Overall, we observe no significant differences in solve rates with and without type information <ref type="table" target="#tab_2">(Table A11</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>300M 1B 3B</head><p>With type information 8.23% 13.54% 14.89% Without type information 8.27% 13.30% 14.89%</p><p>Appendix <ref type="table" target="#tab_2">Table A11</ref> | Solve rate sensitivity to type information. 10@1024 solve rates with different model sizes, with and without type information.</p><p>Typos. Typographical errors are another type of word changes. To measure the model performance in this setting, we introduced a number of typos to the description, where each typo is a swap of adjacent characters of a randomly chosen English word (to avoid modifying nonsensical strings that are relevant to the solution). For example, the word "middle" could become "midlde". The solve rate deteriorates roughly linearly with the number of introduced typos, as shown in <ref type="figure" target="#fig_0">Figure A10 (a)</ref>.</p><p>Synonym substitution. We also analysed sensitivity to synonym substitutions in the problem description and specification, using synonym pairs in <ref type="bibr" target="#b37">Huang et al. (2019)</ref> relying on the PPDB database <ref type="bibr" target="#b25">(Ganitkevitch et al., 2013)</ref>. <ref type="figure" target="#fig_0">Figure A10</ref> (b) shows model performance with different numbers of synonym changes. Overall, we observe very little degradation in solve rate as we increase the number of substitutions.</p><p>Word-level perturbations. We studied the impact of word-level perturbations as done in Edunov et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gregor and Cryptography</head><p>Gregor is learning about RSA cryptography, and although he doesn't understand how RSA works, he is now fascinated with prime numbers and factoring them. <ref type="bibr">Gregor</ref> Appendix <ref type="figure" target="#fig_6">Figure A8</ref>  (2018), specifically swapping words and deleting words in the problem description and specification. Words were swapped by randomly permuting words no more than positions apart ( <ref type="figure" target="#fig_0">Figure A10 (c)</ref>), and words were deleted with probability ( <ref type="figure" target="#fig_0">Figure A10 (d)</ref>). With both permutations and deletions, we observe stronger word level noise has a negative impact on the model performance. However, the model is relatively robust for low levels of words deletion ( = 0.05) and swapping ( = 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.5. Description section ablations</head><p>We performed an ablation study on the importance of the different parts of the problem description: task description, specification, and input/output examples. In <ref type="table" target="#tab_2">Table A12</ref>, we see that removing any of the three sections impacts performance. Removing the IO examples has the least impact, followed by the description, and then the specification. This is as we might expect, as without the specification it is difficult to know how to parse the problem input. We also studied the impact of reordering sections, however different permutations have only a small impact on the solve rates of the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. Sensitivity to problem metadata</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4.1. Problem ratings</head><p>Problems are often giving ratings that indicate how difficult they are, though ratings are typically not available during the competition. We investigated how the ratings provided to the model change the samples it produces. <ref type="figure" target="#fig_0">Figure A11</ref> plots the solve rate when conditioning on various ratings. Specifying an easier rating is generally better than a harder one (although there are concerns that this could lead to more algorithmically inefficient solutions), and specifying a uniform random rating is competitive with any fixed rating.</p><p>Next, we might expect that conditioning on a rating close to the true one could increase model solve rate. <ref type="figure" target="#fig_0">Figure A12</ref> shows the solve rate for the four quartiles of problem difficulty in the validation set, conditioning on different ratings. This indicates that for more difficult problems it is relatively better to condition the model on a harder rating, while for the easier problems there is a larger negative impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4.2. Solution correctness</head><p>Value conditioning conditions the model with either a CORRECT SOLUTION or an INCORRECT SOLUTION tag at training time depending on the solution, and always with a CORRECT SOLUTION tag during sampling. <ref type="table" target="#tab_10">Table 8</ref> shows that it results in a performance improvement. Here, we investigate whether our models are strongly conditioned on this tag by supplying the INCORRECT SOLUTION tag or even no tag at all, instead of the CORRECT SOLUTION. The comparison is illustrated in <ref type="figure" target="#fig_0">Figure  A13</ref>. Conditioning on the INCORRECT SOLUTION tag hurts in the 10@ metric, although not the @ metric, indicating that the model may produce more solutions that pass example tests but not hidden tests. Removing conditioning hurts in both metrics, although not significantly. Appendix <ref type="figure" target="#fig_0">Figure A12</ref> | 10@1024 solve rates by problem difficulty for samples conditioned on different ratings. Dotted lines shows solve rates when conditioning on a uniformly random rating. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Solution duplication</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.1. Solution decompositions</head><p>Human solution Model solution</p><p>Appendix <ref type="figure" target="#fig_0">Figure A16</ref> | Example decomposition of human and model solutions to the 'Digits Sum' problem into substrings from the finetuning dataset. Each color identifies one substring, but repetition of any color is not meaningful, nor is there a relationship between substrings in the human and model solutions with the same color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.2. Very long common subsequences between human solutions and finetuning data</head><p>Appendix <ref type="figure" target="#fig_1">Figure A20</ref> | A human validation solution to the 'The Miracle and the Sleeper' problem with a very long LCS with the finetuning dataset (length 914). The remaining part of the solution is composed of much smaller substrings. Each color identifies one substring, but repetition of any color is not meaningful.</p><p>Appendix <ref type="figure" target="#fig_0">Figure A21</ref> | A human validation solution to the 'Integers Have Friends' problem with a very long LCS with the finetuning dataset (length 666). The remaining part of the solution is composed of much smaller substrings. Each color identifies one substring, but repetition of any color is not meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Problem description rewordings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.1. Simplified rewordings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nim -Original</head><p>Nim is a game in which 2 players take turns removing objects from heaps of different sizes . On each turn , a player must remove at least one object , and may remove any number of objects provided they all come from the same heap . The player to remove the last object is the winner .</p><p>Formally there are n heaps , with integer values a_1 , ... , a_n . A turn consists of reducing the value of some a_i to a value between zero and a_i -1.</p><p>Given the list of heap sizes you need to figure out which player wins if both play optimally .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The first line contains a single integer t (1 &lt;= t &lt;= 10 000) -the number of test cases .</p><p>The first line of each test case contains a single integer n (2 &lt;= n &lt;= 10^5) .</p><p>The second line of each test case contains n integers a_1 , a_2 , ... , a_n (1 &lt;= a_i &lt;= 10^6) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>If the first player wins print "1" , otherwise print "2" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nim -Simplified</head><p>Given an array a , of length n , with values a_1 , ... , a_n , compute the xor of all of the a_i .</p><p>If the xor is zero , output "2" , else output "1".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The first line contains a single integer t (1 $ \ leq$ t &lt;= 10 000) -the number of test cases .</p><p>The first line of each test case contains a single integer n (2 &lt;= n &lt;= 10^5) .</p><p>The second line of each test case contains n integers a_1 , a_2 , ... , a_n (1 &lt;= a_i &lt;= 10^6) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>If the xor of all the a_i is zero , print "2" , otherwise print "1" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No Consecutive Zeros -Original</head><p>Find the number of binary strings of length n that have no two consecutive zeros .</p><p>Consider all possible binary strings of length n . Many of these have two consecutive zeros , such as 101001. But some , such as 11010 , do not . Find the number which do not have two consecutive zeros .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The first line contains a single integer t (1 &lt;= t &lt;= 10 000) -the number of test cases .</p><p>The first line of each test case contains a single integer n (2 &lt;= n &lt;= 10^5) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>For each test case , print a single integer -the number of binary strings of length n which do not contain two consecutive zeros . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No Consecutive Zeros -Simplified</head><p>Given an integer n , find the ( n +2) th fibonacci number .</p><p>Consider the 0 th fibonacci number to be 0 , and the 1 st fibonacci number to be 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The first line contains a single integer t (1 &lt;= t &lt;= 10 000) -the number of test cases .</p><p>The first line of each test case contains a single integer n (2 &lt;= n &lt;= 10^5) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>For each test case , print a single integer -the ( n +2) th fibonacci number . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1554A Cherry -Original</head><p>You are given n integers a_1 , a_2 , ... , a_n . Find the maximum value of max ( a_l , a_ { l + 1} , ... , a_r ) . min ( a_l , a_ { l + 1} , ... , a_r ) over all pairs (l , r ) of integers for which 1 &lt;= l &lt; r &lt;= n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The first line contains a single integer t (1 &lt;= t &lt;= 10 000) -the number of test cases .</p><p>The first line of each test case contains a single integer n (2 &lt;= n &lt;= 10^5) .</p><p>The second line of each test case contains n integers a_1 , a_2 , ... , a_n (1 &lt;= a_i &lt;= 10^6) .</p><p>It is guaranteed that the sum of n over all test cases doesn ' t exceed 3 . 10^5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>For each test case , print a single integer -the maximum possible value of the product from the statement .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Input <ref type="formula">4</ref>  In the first test case , * f (1 , 2) = max ( a_1 , a_2 ) . min ( a_1 , a_2 ) = max <ref type="bibr">(2 , 4)</ref> . min (2 , 4) = 4 . 2 = 8. * f (1 , 3) = max ( a_1 , a_2 , a_3 ) . min ( a_1 , a_2 , a_3 ) = max <ref type="bibr">(2 , 4 , 3)</ref> . min <ref type="bibr">(2 , 4 , 3)</ref> = 4 . 2 = 8. * f (2 , 3) = max ( a_2 , a_3 ) . min ( a_2 , a_3 ) = max (4 , 3) . min (4 , 3) = 4 . 3 = 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1554A Cherry -Simplified</head><p>You are given n integers a_1 , a_2 , ... , a_n . Find the maximum value of a_l times a_ { l + 1} for an integer l for which 1 &lt;= l &lt; n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The first line contains a single integer t (1 &lt;= t &lt;= 10 000) -the number of test cases .</p><p>The first line of each test case contains a single integer n (2 &lt;= n &lt;= 10^5) .</p><p>The second line of each test case contains n integers a_1 , a_2 , ... , a_n (1 &lt;= a_i &lt;= 10^6) .</p><p>It is guaranteed that the sum of n over all test cases doesn ' t exceed 3 . 10^5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>For each test case , print a single integer -the maximum possible value of the product from the statement .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Input <ref type="formula">4</ref>  You are given a string s , consisting of n letters , each letter is either 'a ' or 'b '. The letters in the string are numbered from 1 to n . s [ l ; r ] is a continuous substring of letters from index l to r of the string inclusive .</p><p>A string is called balanced if the number of letters 'a ' in it is equal to the number of letters 'b '. For example , strings " baba " and " aabbab " are balanced and strings " aaab " and " b " are not .</p><p>Find any non -empty balanced substring s [ l ; r ] of string s . Print its l and r (1 &lt;= l &lt;= r &lt;= n ) . If there is no such substring , then print -1 -1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The first line contains a single integer t (1 &lt;= t &lt;= 1000) -the number of testcases .</p><p>Then the descriptions of t testcases follow .</p><p>The first line of the testcase contains a single integer n (1 &lt;= n &lt;= 50) -the length of the string .</p><p>The second line of the testcase contains a string s , consisting of n letters , each letter is either 'a ' or 'b '.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>For each testcase print two integers . If there exists a non -empty balanced substring s [ l ; r ] , then print l r (1 &lt;= l &lt;= r &lt;= n ) . Otherwise , print -1 -1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Input 4 1 a 6 abbaba 6 abbaba 9 babbabbaa Output -1 -1 1 6 3 6 2 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note</head><p>In the first testcase there are no non -empty balanced subtrings .</p><p>In the second and third testcases there are multiple balanced substrings , including the entire string " abbaba " and substring " baba ".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1569A Balanced Substring -Simplified</head><p>You are given a string s , consisting of n letters , each letter is either 'a ' or 'b '. The letters in the string are numbered from 1 to n .</p><p>Find two adjacent letters which are not equal and print their indexes . If there is no such pair , print -1 , -1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The first line contains a single integer t (1 &lt;= t &lt;= 1000) -the number of testcases .</p><p>Then the descriptions of t testcases follow .</p><p>The first line of the testcase contains a single integer n (1 &lt;= n &lt;= 50) -the length of the string .</p><p>The second line of the testcase contains a string s , consisting of n letters , each letter is either 'a ' or 'b '.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>For each testcase print two integers . If there is an adjacent pair of non -identical letters at indexes l and r , print l , r . Otherwise , print -1 -1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Input 4 1 a 6 abbaba 6 abbaba 9 babbabbaa Output -1 -1 1 6 3 6 2 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note</head><p>In the first testcase there are no non -identical pairs .</p><p>In the second and third testcases there are nonidentical pairs .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.2. Incorrect and verbose rewordings</head><p>Original You are given n integers a_1 , a_2 , ... , a_n . Find the maximum value of a_l times a_ { l + 1} for an integer l for which 1 &lt;= l &lt; n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The first line contains a single integer t (1 &lt;= t &lt;= 10 000) --the number of test cases .</p><p>The first line of each test case contains a single integer n (2 &lt;= n &lt;= 10^5) .</p><p>The second line of each test case contains n integers a_1 , a_2 , ... , a_n (1 &lt;= a_i &lt;= 10^6) .</p><p>It is guaranteed that the sum of n over all test cases doesn ' t exceed 3 . 10^5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>For each test case , print a single integer --the maximum possible value of the product from the statement .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Input <ref type="formula">4</ref>  So the maximum is f (2 , 3) = 12.</p><p>In the second test case , the maximum is f (1) = f (2) = 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Opposite</head><p>You are given n integers a_1 , a_2 , ... , a_n . Find the minimum value of a_l times a_ { l + 1} for an integer l for which 1 &lt;= l &lt; n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The first line contains a single integer t (1 &lt;= t &lt;= 10 000) --the number of test cases .</p><p>The first line of each test case contains a single integer n (2 &lt;= n &lt;= 10^5) .</p><p>The second line of each test case contains n integers a_1 , a_2 , ... , a_n (1 &lt;= a_i &lt;= 10^6) .</p><p>It is guaranteed that the sum of n over all test cases doesn ' t exceed 3 . 10^5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>For each test case , print a single integer --the minimum possible value of the product from the statement .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Input <ref type="formula">4</ref>  So the minimum is f (2 , 3) = 8.</p><p>In the second test case , the minimum is f (3) = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related</head><p>You are given n integers a_1 , a_2 , ... , a_n . Find the maximum value of ( a_l . a_r ) over all pairs (l , r ) of integers for which 1 &lt;= l &lt; r &lt;= n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The first line contains a single integer t (1 &lt;= t &lt;= 10 000) --the number of test cases .</p><p>The first line of each test case contains a single integer n (2 &lt;= n &lt;= 10^5) .</p><p>The second line of each test case contains n integers a_1 , a_2 , ... , a_n (1 &lt;= a_i &lt;= 10^6) .</p><p>It is guaranteed that the sum of n over all test cases doesn ' t exceed 3 . 10^5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>For each test case , print a single integer --the maximum possible value of the product from the statement .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Input <ref type="formula">4</ref>  So the maximum is f (2 , 3) = 12.</p><p>In the second test case , the maximum is f (1 , 3) = 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Underspecified</head><p>You are given n integers a_1 , a_2 , ... , a_n . Find the maximum value of f ( a_l , a_r ) over all pairs (l , r ) of integers for which 1 &lt;= l &lt; r &lt;= n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The first line contains a single integer t (1 &lt;= t &lt;= 10 000) --the number of test cases .</p><p>The first line of each test case contains a single integer n (2 &lt;= n &lt;= 10^5) .</p><p>The second line of each test case contains n integers a_1 , a_2 , ... , a_n (1 &lt;= a_i &lt;= 10^6) .</p><p>It is guaranteed that the sum of n over all test cases doesn ' t exceed 3 . 10^5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>For each test case , print a single integer --the maximum value of the function from the statement .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Input <ref type="formula">4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Verbose</head><p>William has been given an array for his birthday , which consists of n integers a_1 , a_2 , ... , a_n . He is very proud of his array , but naturally his friend Mary is curious about it . Mary would like to know a certain function of consecutive elements of the array . Concretely , Mary would like to know the maximum value of a_l times a_ { l + 1} for an integer l for which 1 &lt;= l &lt; n . Can you help William by calculating this value for him ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The first line contains a single integer t (1 &lt;= t &lt;= 10 000) --the number of test cases .</p><p>The first line of each test case contains a single integer n (2 &lt;= n &lt;= 10^5) .</p><p>The second line of each test case contains n integers a_1 , a_2 , ... , a_n (1 &lt;= a_i &lt;= 10^6) .</p><p>It is guaranteed that the sum of n over all test cases doesn ' t exceed 3 . 10^5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>For each test case , print a single integer --the maximum possible value of the product from the statement .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Input <ref type="formula">4</ref>  So the maximum is f (2 , 3) = 12.</p><p>In the second test case , the maximum is f (1) = f (2) = 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm described in words</head><p>You are given n integers a_1 , a_2 , ... , a_n . Find the maximum value of the product of two consecutive members of the array .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The first line contains a single integer t (1 &lt;= t &lt;= 10 000) --the number of test cases .</p><p>The first line of each test case contains a single integer n (2 &lt;= n &lt;= 10^5) .</p><p>The second line of each test case contains n integers a_1 , a_2 , ... , a_n (1 &lt;= a_i &lt;= 10^6) .</p><p>It is guaranteed that the sum of n over all test cases doesn ' t exceed 3 . 10^5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>For each test case , print a single integer --the maximum possible value of the product from the statement .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Input <ref type="formula">4</ref>  So the maximum is f (2 , 3) = 12.</p><p>In the second test case , the maximum is f (1) = f (2) = 6.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 |</head><label>1</label><figDesc>AlphaCode's ranking on 10 simulated Codeforces contests and estimated rating (right is better). AlphaCode ranked in the top 54.3% among contest participants averaged over 10 contests, and achieved an estimated average rating of 1238. (a) shows the rating of participants (y-axis) and their rankings in each contest (x-axis), as well as AlphaCode's ranking for each of the 10 contests. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 |</head><label>2</label><figDesc>Competitive programming problem statement. Problem statement of Backspace, a Codeforces problem</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 |</head><label>3</label><figDesc>) a.pop(0) elif a[0]!=b[0] and len(a)!=1: a.pop(0) a.pop(0) elif a[0]!=b[0] and len(a)==1: a.pop(0) if len(b)==0: Solution to Figure 2 generated by Al-phaCode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 |</head><label>4</label><figDesc>Overview of AlphaCode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 |</head><label>6</label><figDesc>Solve rate scaling vs. number of samples. The solve rate scales approximately loglinearly with the number of samples, although this tapers off slightly in the 10@k setting. The better, larger-parameter models have higher scaling slopes in this log-linear plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 |</head><label>8</label><figDesc>Comparison of different sample selection methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 |</head><label>11</label><figDesc>Sample and human solution lengths in tokens before and after dead code removal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 |</head><label>13</label><figDesc>Validation loss and solve rate (10@1024) by finetuning steps. The validation loss starts to increase early in finetuning, indicating overfitting, while the solve rate keeps improving.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1 F</head><label>1</label><figDesc>Hidden tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.2 Program judging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.3 Evaluation metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 B Datasets 41 B.1 GitHub dataset composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 B.2 Dataset cleaning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 B.3 Data leakage and temporal split . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 C Approach and Results 43 C.1 Ensembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 C.2 Metadata conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 C.3 GOLD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 C.4 Additional results for filtering and clustering . . . . . . . . . . . . . . . . . . . . . . . 45 C.5 HumanEval comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 C.6 APPS dataset settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 C.7 Best settings for sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 C.8 Scaling with dataset size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 D Codeforces contest evaluation 47 D.1 Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 D.2 Multiple evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 D.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 E Additional analysis of AlphaCode's capabilities and limitations 50 E.1 Model sample statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 E.2 Solve rate for different problem difficulty ratings . . . . . . . . . . . . . . . . . . . . 50 E.3 Sensitivity to the problem descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . 52 E.4 Sensitivity to problem metadata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 Complete prompt and model examples 60 F.1 Solution duplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 F.2 Problem description rewordings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>q j c b c w v i q k o j n z y r u w y g t b v w w s bofzr by over a hundred more extensive tests that probe various cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Ensemble better than all components (b) Strong components better than ensemble Appendix Figure A2 | Ensemble performance. Good ensembles can be better than individual components, but bad ensembles can be worse than an individual component. unseen problems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>x t r a c t _ n u m _ p u b l i c _ t e s t c a s e s _ f r o m _ d e s c r i p t i o n ( desc : str ) -&gt; int : """ Parses the description to extract the number of public testcases . """ if ' ---Sample Input ' in desc : return desc . count ( ' ---Sample Input ') elif ' ---Example Input ' in desc : return desc . count ( ' ---Example Input ') else : for section in desc . split ( '\n -----') : if section . startswith ( ' Example ') : n_tests = max ( section . count ( '\ nInput ') , section . count ( '\ nSample Input ') ) if n_tests : return n_tests return 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>|</head><label></label><figDesc>Sampling with temperature, top-k sampling and nucleus sampling. (a-b) Lower temperatures are better for small sample budgets, and higher temperatures are better for large sample budgets. However, the model is relatively tolerant to a wide range of temperatures. (c-d) Top-k sampling does not outperform the sampling with temperature baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Varying numbers of problems. (b) Varying numbers of solutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>|</head><label></label><figDesc>Consistent / inconsistent variable renaming example. This example shows the original description that has variable names , , and , along with versions with consistent and inconsistent replacement. The original description's variables are the blue variables in each triplet [A][B][C], and similarly the consistent and inconsistent replacements are green ( replaced with everywhere) and red ( replaced with a random name independently for each appearance) respectively. Consistent replacement does not change the problem, but random replacement introduces noise and renders the problem ill-posed. Model deterioration with such modifications allows analysis of how well a model captures the given variable structure. Problem sourced from Codeforces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>10 attempts per problem (b) Unlimited attempts per problem Appendix Figure A13 | Conditioning on the CORRECT SOLUTION, the INCORRECT SOLUTION, or no tag.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>l , r ) = max ( a_l , a_ { l + 1} , ... , a_r ) . min ( a_l , a_ { l + 1} , ... , a_r ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>l ) = a_l . a_ { l +1} In the first test case , * f (1) = a_1 . a_2 = 2 . 4 = 8. * f (2) = a_2 . a_3 = 4 . 3 = 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>l ) = a_l . a_ { l +1} In the first test case , * f (1) = a_1 . a_2 = 2 . 4 = 8. * f (2) = a_2 . a_3 = 4 . 3 = 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head></head><label></label><figDesc>l ) = a_l . a_ { l +1} In the first test case , * f (1) = a_1 . a_2 = 2 . 4 = 8. * f (2) = a_2 . a_3 = 4 . 3 = 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 |</head><label>1</label><figDesc>Statistics of our CodeContests dataset. The number of problems in each split, and the per-problem averages for the number of test cases, number of solutions, and percentage of solutions which are correct.</figDesc><table><row><cell>Train</cell><cell>13328</cell><cell>2.0</cell><cell>14.8</cell><cell cols="2">79.1 493.4 (27%) 281.1 (47%) 147.9 (46%)</cell></row><row><cell>Valid</cell><cell>117</cell><cell>1.5</cell><cell>12.9</cell><cell cols="2">190.0 231.6 (47%) 137.2 (55%) 131.1 (54%)</cell></row><row><cell>Test</cell><cell>165</cell><cell>1.7</cell><cell>9.4</cell><cell>192.7 196.0 (45%)</cell><cell>97.3 (54%) 105.2 (51%)</cell></row><row><cell></cell><cell>Dataset</cell><cell></cell><cell cols="3">Tests / problem False Positive (FP) Rate FP or Slow Rate</cell></row><row><cell></cell><cell>APPS</cell><cell></cell><cell>20.99</cell><cell>60%</cell><cell>70%</cell></row><row><cell></cell><cell>HumanEval</cell><cell></cell><cell>7.77</cell><cell>30%</cell><cell>N/A</cell></row><row><cell></cell><cell cols="2">CodeContests raw</cell><cell>12.4</cell><cell>62%</cell><cell>88%</cell></row><row><cell></cell><cell>CodeContests</cell><cell></cell><cell>203.7</cell><cell>4%</cell><cell>46%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 |</head><label>2</label><figDesc></figDesc><table /><note>Dataset false positive rates. The bottom row is the dataset we used, while "CodeContests raw" does not use generated tests and does not filter out problems with insufficient tests. Validation splits were used for CodeContests and APPS. We randomly selected 50 problems our 1B parameter model solved (from 10,000 samples per problem for APPS, 200 for HumanEval, and 1,000,000 for CodeContests), and manually examined one solution for each problem to check whether they are false positives or slow solutions. HumanEval does not have timing constraints for most problems, so there is no slow rate.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>on the HumanEval benchmark in Appendix C.5. Contest ID 1591 1608 1613 1615 1617 1618 1619 1620 1622 1623 Average Best 43.5% 43.6% 59.8% 60.5% 65.1% 32.2% 47.1% 54.0% 57.5% 20.6% 48.4% Estimated 44.3% 46.3% 66.1% 62.4% 73.9% 52.2% 47.3% 63.3% 66.2% 20.9% 54.3% Worst 74.5% 95.7% 75.0% 90.4% 82.3% 53.5% 88.1% 75.1% 81.6% 55.3% 77.2%</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>Approach</cell><cell cols="4">Validation Set 10@1k 10@10k 10@100k 10@1M 10@1k 10@10k 10@100k Test Set</cell></row><row><cell>9B</cell><cell>16.9% 22.6%</cell><cell>27.1%</cell><cell>30.1% 14.3% 21.5%</cell><cell>25.8%</cell></row><row><cell>41B</cell><cell>16.9% 23.9%</cell><cell>28.2%</cell><cell>31.8% 15.6% 23.2%</cell><cell>27.7%</cell></row><row><cell cols="2">41B + clustering 21.0% 26.2%</cell><cell>31.8%</cell><cell>34.2% 16.4% 25.4%</cell><cell>29.6%</cell></row></table><note>| Solve rates of our best systems on the validation set and test set .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 |</head><label>8</label><figDesc>Build-up ablation for model enhancements. Effect of each additional model enhancement building up from No enhancements which is a plain fine-tuned 1B encoder-decoder model trained with the standard next token prediction loss. Numbers in parentheses represent 95% confidence intervals. For each setting we fine-tuned and sampled from at least 3 different models from the same pre-trained checkpoint, and computed means and confidence intervals using a combination of subsampling and bootstrapping as discussed in Appendix A.3.% Problems with ? 1Average pass example tests Average pass example tests Model samples pass example tests</figDesc><table><row><cell>on all problems</cell><cell>on solved problems</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>The human solution distribution is in blue and the model is in orange, and the results are compared against the GitHub and CodeContests datasets. Model and human solutions have similar distributions. On CodeContests, approximately 3% of human solutions and less than 1% of model solutions had a common substring of length greater than 600.</figDesc><table><row><cell>Model Greedy Math DP</cell><cell cols="5">Constructive Brute Algorithms Force Structures entation Data Implem-Graphs Bitmasks Sortings</cell></row><row><cell>300M 13.1% 19.3% 4.5%</cell><cell>7.5%</cell><cell>9.8%</cell><cell>8.8%</cell><cell>5.0%</cell><cell>0.2% 22.2% 16.9%</cell></row><row><cell>1B 19.7% 22.7% 4.5%</cell><cell>9.1%</cell><cell cols="2">12.0% 10.5%</cell><cell cols="2">14.1% 5.9% 26.8% 21.5%</cell></row><row><cell>3B 19.9% 22.7% 4.9%</cell><cell>11.2%</cell><cell cols="2">13.2% 11.9%</cell><cell cols="2">13.4% 8.8% 25.4% 23.8%</cell></row><row><cell>9B 23.7% 29.4% 7.1%</cell><cell>13.8%</cell><cell cols="2">19.5% 16.9%</cell><cell cols="2">16.4% 16.6% 27.4% 27.8%</cell></row><row><cell>41B 25.0% 28.2% 8.8%</cell><cell>14.9%</cell><cell cols="2">20.4% 15.7%</cell><cell cols="2">16.5% 13.6% 33.8% 25.5%</cell></row></table><note>Figure 9 | Length of longest common substrings between solutions and training data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Figure 10 |</head><label>10</label><figDesc>Left: Example decomposition of model solution into substrings from the fine-tuning dataset. Each new color denotes a new substring. Right: Document that shares the longest substring (183 chars). This solution doesn't duplicate core logic. However, it does share snippets that parse input and construct graph adjacency lists. Document sourced from Codeforces.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table A9</head><label>A9</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Original</cell><cell>3.0%</cell></row><row><cell></cell><cell></cell><cell>Simplified</cell><cell>15.7%</cell></row><row><cell></cell><cell></cell><cell>Original</cell><cell>17.1%</cell></row><row><cell></cell><cell></cell><cell>Opposite</cell><cell>0.1%</cell></row><row><cell>(b) Description rewording</cell><cell>Table A10</cell><cell>Related Underspecified</cell><cell>3.2% 0.03%</cell></row><row><cell></cell><cell></cell><cell>Verbose</cell><cell>19.4%</cell></row><row><cell></cell><cell></cell><cell>Algorithm described in words only</cell><cell>19.7%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10@1024</cell></row><row><cell></cell><cell></cell><cell>Original</cell><cell>13.5%</cell></row><row><cell>(c) Variable renaming</cell><cell>Figure A9</cell><cell cols="2">? 6 variables consistently renamed ? 6 variables inconsistently renamed 10.1% 12.1%</cell></row><row><cell>(d) Type information</cell><cell cols="2">Table A11 No type information</cell><cell>13.3%</cell></row><row><cell>(e) Typos</cell><cell cols="2">Figure A10 30 typos</cell><cell>11.3%</cell></row><row><cell></cell><cell></cell><cell>Description + Specification</cell><cell>10.4%</cell></row><row><cell>(f ) Missing description sections</cell><cell>Table A12</cell><cell>Description + IO</cell><cell>4.8%</cell></row><row><cell></cell><cell></cell><cell>Specification + IO</cell><cell>6.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 |</head><label>12</label><figDesc></figDesc><table /><note>Summary of solve rates in response to description changes. Solve rate deteriorates consistently when removing or obscuring information in the description, demonstrating that the model relies on this information. All solve rates are for the 1B model; the model was not retrained for description changes. Some of the studies were done on the full validation set, while others were done on selected subsets. (a) and (b) use the percentage of correct samples on selected problems, and (c-h) use 10@1024 solve rates.this conditioning metadata, and what are the best settings to generate this metadata with at test time when they are not available. We find that the model is indeed conditioned on this metadata; providing different tags changes what algorithms the model generates. We also find that we should sample randomly for tags and ratings, and condition on the solution being CORRECT. Here we present an analysis of tag conditioning, and additional results on ratings and correctness conditioning are included in Appendix E.4.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Python 2 and 3 are distinguished by whether the code can be successfully parsed using Python 3's parser.step did not work for a portion of our dataset; when the cleaning failed we simply keep the solution as is. 4. Executed all Python and C ++ solutions on all the test cases, and removed in order:</figDesc><table><row><cell></cell><cell cols="3">Millions) Bytes (GB) Bytes percentage</cell></row><row><cell>C ++</cell><cell>21.50</cell><cell>290.5</cell><cell>40.62%</cell></row><row><cell>C#</cell><cell>6.73</cell><cell>38.4</cell><cell>5.37%</cell></row><row><cell>Go</cell><cell>2.19</cell><cell>19.8</cell><cell>2.77%</cell></row><row><cell>Java</cell><cell>19.35</cell><cell>113.8</cell><cell>15.91%</cell></row><row><cell>JavaScript</cell><cell>10.55</cell><cell>88.0</cell><cell>12.31%</cell></row><row><cell>Lua</cell><cell>0.57</cell><cell>2.9</cell><cell>0.41%</cell></row><row><cell>PHP</cell><cell>11.03</cell><cell>64.0</cell><cell>8.95%</cell></row><row><cell>Python 2</cell><cell>1.00</cell><cell>10.7</cell><cell>1.50%</cell></row><row><cell>Python 3</cell><cell>6.09</cell><cell>43.6</cell><cell>6.10%</cell></row><row><cell>Ruby</cell><cell>4.45</cell><cell>11.6</cell><cell>1.62%</cell></row><row><cell>Rust</cell><cell>0.32</cell><cell>2.8</cell><cell>0.39%</cell></row><row><cell>Scala</cell><cell>0.83</cell><cell>4.1</cell><cell>0.57%</cell></row><row><cell>TypeScript</cell><cell>1.69</cell><cell>24.9</cell><cell>3.48%</cell></row><row><cell>Total</cell><cell>86.31</cell><cell>715.1</cell><cell>100.00%</cell></row></table><note>Appendix Table A1 | Composition of our GitHub pre-training dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table A8</head><label>A8</label><figDesc>shows the 10@100k solve rates for our 9B and 41B models on the validation and test sets in different problem difficulty ratings. The validation and test sets contain 117 and 165 problems</figDesc><table><row><cell cols="5">Contest Problem Problem Solved Incorrect</cell><cell>Submission</cell><cell>Problem</cell></row><row><cell></cell><cell>Rating</cell><cell>Letter</cell><cell></cell><cell cols="2">Submissions Time (minutes)</cell><cell>Link</cell></row><row><cell>1623</cell><cell>800</cell><cell>A</cell><cell>1,1,1</cell><cell>0,0,0</cell><cell>14,14,14</cell><cell>link</cell></row><row><cell>1623</cell><cell>1100</cell><cell>B</cell><cell>1,1,1</cell><cell>0,0,0</cell><cell>14,14,14</cell><cell>link</cell></row><row><cell>1622</cell><cell>800</cell><cell>A</cell><cell>0,1,1</cell><cell>-,345,7</cell><cell>-,110,110</cell><cell>link</cell></row><row><cell>1622</cell><cell>1000</cell><cell>B</cell><cell>1,1,1</cell><cell>0,0,1</cell><cell>14,14,62</cell><cell>link</cell></row><row><cell>1615</cell><cell>800</cell><cell>A</cell><cell>1,1,1</cell><cell>4,0,0</cell><cell>10,2,3</cell><cell>link</cell></row><row><cell>1615</cell><cell>1300</cell><cell>B</cell><cell>0,0,1</cell><cell>-,-,129</cell><cell>-,-,110</cell><cell>link</cell></row><row><cell>1615</cell><cell>1600</cell><cell>C</cell><cell>0,1,0</cell><cell>-,170,-</cell><cell>-,110,-</cell><cell>link</cell></row><row><cell>1619</cell><cell>800</cell><cell>A</cell><cell>1,1,1</cell><cell>0,0,0</cell><cell>2,2,2</cell><cell>link</cell></row><row><cell>1619</cell><cell>800</cell><cell>B</cell><cell>1,1,1</cell><cell>0,0,0</cell><cell>3,3,3</cell><cell>link</cell></row><row><cell>1619</cell><cell>1800</cell><cell>D</cell><cell>0,0,1</cell><cell>-,-,91</cell><cell>-,-,110</cell><cell>link</cell></row><row><cell>1620</cell><cell>800</cell><cell>A</cell><cell>0,1,1</cell><cell>-,7,10</cell><cell>-,110,110</cell><cell>link</cell></row><row><cell>1620</cell><cell>1000</cell><cell>B</cell><cell>1,1,1</cell><cell>0,0,0</cell><cell>14,14,14</cell><cell>link</cell></row><row><cell>1617</cell><cell>900</cell><cell>B</cell><cell>1,1,1</cell><cell>2,3,2</cell><cell>62,62,62</cell><cell>link</cell></row><row><cell>1618</cell><cell>800</cell><cell>A</cell><cell>1,1,1</cell><cell>0,0,0</cell><cell>4,4,3</cell><cell>link</cell></row><row><cell>1618</cell><cell>800</cell><cell>B</cell><cell>0,1,1</cell><cell>-,5,14</cell><cell>-,110,110</cell><cell>link</cell></row><row><cell>1618</cell><cell>1300</cell><cell>D</cell><cell>1,1,1</cell><cell>8,4,8</cell><cell>110,110,110</cell><cell>link</cell></row><row><cell>1618</cell><cell>1700</cell><cell>E</cell><cell>1,1,1</cell><cell>1,0,1</cell><cell>62,110,62</cell><cell>link</cell></row><row><cell>1591</cell><cell>800</cell><cell>A</cell><cell>1,1,1</cell><cell>3,0,0</cell><cell>9,3,2</cell><cell>link</cell></row><row><cell>1591</cell><cell>900</cell><cell>B</cell><cell>1,1,1</cell><cell>0,0,0</cell><cell>2,3,2</cell><cell>link</cell></row><row><cell>1608</cell><cell>800</cell><cell>A</cell><cell>1,1,1</cell><cell>0,0,0</cell><cell>2,2,2</cell><cell>link</cell></row><row><cell>1613</cell><cell>900</cell><cell>A</cell><cell>0,0,1</cell><cell>-,-,359</cell><cell>-,-,110</cell><cell>link</cell></row><row><cell>1613</cell><cell>1000</cell><cell>B</cell><cell>0,1,1</cell><cell>-,0,0</cell><cell>-,19,14</cell><cell>link</cell></row><row><cell>1613</cell><cell>1200</cell><cell>C</cell><cell>1,1,1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>9%,20.9%,20.9% -,20.9%,20.9% 20.6%,20.6%,20.6% 55.3%,55.3%,55.3%  1622   68.6%,68.6%,61.5% -,61.5%,61.5% 61.5%,61.5%,49.5% 91.2%,91.6%,61.Results of running AlphaCode on all Codeforces competitions. Top XX% percentile ranks are given, where the number indicates the percentage of competitors who performed better than our system. Results for the three evaluations are separated by commas. We did not track unlimited attempts numbers for the first evaluation.</figDesc><table><row><cell>Contest</cell><cell>10 Submissions</cell><cell>Unlimited</cell><cell cols="3">10 Submissions</cell><cell>10 Submissions</cell></row><row><cell></cell><cell></cell><cell>Submissions</cell><cell cols="3">Min Penalty Time</cell><cell>Max Penalty Time</cell></row><row><cell>1623</cell><cell cols="5">20.5%</cell></row><row><cell>1615</cell><cell cols="5">91.2%,47.2%,48.9% -,41.5%,44.9% 91.2%,45.1%,45.2% 92.6%,89.2%,89.2%</cell></row><row><cell>1619</cell><cell cols="5">47.2%,47.3%,47.2% -,37.3%,47.1% 47.1%,47.1%,45.2% 88.1%,88.1%,88.1%</cell></row><row><cell>1620</cell><cell cols="5">64.4%,61.1%,64.4% -,61.1%,62.9% 63.7%,65.6%,63.7% 88.8%,82.9%,80.8%</cell></row><row><cell>1617</cell><cell cols="5">72.9%,75.9%,72.9% -,75.9%,72.9% 64.9%,65.6%,64.9% 82.0%,82.9%,82.0%</cell></row><row><cell>1618</cell><cell cols="5">62.3%,32.1%,62.3% -,32.1%,35.1% 43.0%,10.6%,43.0% 62.7%,35.1%,62.7%</cell></row><row><cell>1591</cell><cell cols="5">53.6%,39.7%,39.7% -,39.7%,39.7% 51.1%,39.7%,39.7% 74.9%,74.4%,74.4%</cell></row><row><cell>1608</cell><cell cols="5">46.3%,46.3%,46.3% -,46.3%,46.3% 43.6%,43.6%,43.6% 95.7%,95.7%,95.7%</cell></row><row><cell>1613</cell><cell cols="5">75.6%,68.2%,54.5% -,68.2%,50.1% 72.9%,55.3%,51.1% 84.6%,70.2%,70.1%</cell></row><row><cell>Average</cell><cell cols="5">60.3%,50.8%,51.9% -,49.5%,48.2% 55.9%,42.4%,46.8% 84.6%,70.2%,70.1%</cell></row><row><cell>Average all</cell><cell>54.3%</cell><cell>48.8%</cell><cell></cell><cell>48.4%</cell><cell>77.2%</cell></row><row><cell cols="3">Appendix Table A6 | Codeforces per-contest results. Language 300M 1B</cell><cell>3B</cell><cell>9B</cell><cell>41B</cell></row><row><cell></cell><cell>C ++</cell><cell cols="4">67.5% 63.7% 64.1% 61.6% 66.8%</cell></row><row><cell></cell><cell>Python</cell><cell cols="4">90.4% 88.6% 90.1% 89.5% 88.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>any number of times.</head><label></label><figDesc>This day, Mocha got a sequence of length . In each operation, she can select an arbitrary interval [ , ] and for all values (0 ? ? ? ), replace + with + &amp; ? at the same time, where &amp; denotes the bitwise AND operation. This operation can be performed For example, if = 5, the array is [ 1 , 2 , 3 , 4 , 5 ], and Mocha selects the interval [2, 5], then the new array is [ 1 , 2 &amp; 5 , 3 &amp; 4 , 4 &amp; 3 , 5 &amp; 2 ].</figDesc><table><row><cell cols="2">Now Mocha wants to minimize the maximum value in the sequence.</cell></row><row><cell>As her best friend, can you help her to get the answer?</cell><cell></cell></row><row><cell>Input</cell><cell></cell></row><row><cell>Each test contains multiple test cases.</cell><cell></cell></row><row><cell cols="2">The first line contains a single integer (1 ? ? 100) -the number of test</cell></row><row><cell>cases. Each test case consists of two lines.</cell><cell></cell></row><row><cell cols="2">The first line of each test case contains a single integer (1 ? ? 100) -the</cell></row><row><cell>length of the sequence.</cell><cell></cell></row><row><cell>The second line of each test case contains integers 1 , 2 , ..., (0 ? 10 9 ).</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head></head><label></label><figDesc>Rewording the Cherry problem. The percentage of solutions in 50000 samples from the 1B parameter model when attempting the simplified version of the Cherry problem with different rewordings.</figDesc><table><row><cell>Rewording</cell><cell>Solve rate</cell></row><row><cell>Original (maximum product of two consecutive array elements)</cell><cell>17.1%</cell></row><row><cell>Opposite (minimum product of two consecutive array elements)</cell><cell>0.1%</cell></row><row><cell>Related (maximum product of two any array elements)</cell><cell>3.2%</cell></row><row><cell>Underspecified (maximum function of two consecutive array elements)</cell><cell>0.03%</cell></row><row><cell>Verbose</cell><cell>19.4%</cell></row><row><cell>Algorithm described in words</cell><cell>19.7%</cell></row><row><cell>Appendix Table A10 |</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head></head><label></label><figDesc>'s favorite prime number is [P][H][b]. Gregor wants to find two bases of [P][H][y]. Formally, Gregor is looking for two integers a and b which satisfy both of the following properties.</figDesc><table><row><cell>[P][H][b] mod a = [P][H][b] mod b, where x mod [y][f ][b] denotes the remainder when x is divided by [y][f ][x], and</cell></row><row><cell>* 2 ? a &lt; b ? [P][H][t].</cell></row><row><cell>Help Gregor find two bases of his favorite prime number!</cell></row><row><cell>Input</cell></row><row><cell>Each test contains multiple test cases. The first line contains the number of test cases [t][m][b] (1 ? [t][m][b] ?</cell></row><row><cell>1000). Each subsequent line contains the integer [P][H][b] (5 ? [P][H][x] ? 10 9 ), with [P][H][b] guaranteed to be prime.</cell></row><row><cell>Output</cell></row><row><cell>Your output should consist of [t][m][t] lines. Each line should consist of two integers a and b (2 ? a &lt; b ? [P][H][b]). If</cell></row><row><cell>there are multiple possible solutions, print any.</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Sensitivity to variable renaming.</head><label></label><figDesc>The 10@1024 solve rate for consistent and inconsistent (i.e. ill-posed) variable renaming, for different model sizes. Larger models are robust to invariant variable renaming, but deteriorate with greater amounts of inconsistent renaming.</figDesc><table><row><cell></cell><cell>0.18</cell><cell></cell></row><row><cell>10@1024 Solve rate</cell><cell>0.10 0.12 0.14 0.16</cell><cell></cell><cell>300M, Consistent 300M, Inconsistent 1B, Consistent 1B, Inconsistent 3B, Consistent 3B, Inconsistent 9B, Consistent 9B, Inconsistent</cell></row><row><cell></cell><cell>0.08</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1 Maximum number of renamed variables 2 3 4 5</cell><cell>6</cell></row><row><cell cols="3">Appendix Figure A9 |</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Solve rates under different word-level change regimes.10@1024 solve rates for samples conditioned on different ratings.</head><label></label><figDesc>We report 10@1024 solve rates for different model sizes and different description ablations. 'Description + Specification + IO' is the original prompt, the middle rows are different orderings, and the bottom rows remove sections. Dotted lines shows solve rates when conditioning on a uniform random rating.</figDesc><table><row><cell>(a) Transposing characters 0 10 20 Number of typos 30 0.06 0.08 0.10 0.12 0.14 10@1024 Solve rate 1000 1500 0.04 0.06 0.08 0.10 0.12 0.14 Solve Rate with 10 attempts from 1024 samples</cell><cell cols="4">(b) Replacing words with synonyms 0 2 4 Number of synonyms 0.06 0.08 0.10 0.12 0.14 10@1024 Solve rate 2500 3000 Conditioning rating 40 50 2000 3500 300M 1B 3B</cell><cell>6</cell></row><row><cell cols="5">(c) Permuting words 0 2 Number of permuted words 4 6 0.06 0.08 0.10 0.12 0.14 10@1024 Solve rate 300M 1B 3B 1500 2000 2500 3000 3500 Conditioning rating 0.00 0.25 300M 1000 1500 2000 Conditioning rating (d) Deleting words probabilistically 0.0 0.1 0.2 0.3 Deletion Rate 0.06 0.08 0.10 0.12 0.14 10@1024 Solve rate 2500 3000 3500 0.05 0.35 0.40 1B 1000 1500 2000 2500 3000 3500 Conditioning rating 0.05 0.35 0.40 3B Appendix Figure A11 | 1000 0.05 0.10 0.15 0.20 Solve Rate with 0.10 0.15 0.30 0.10 0.15 0.30 10 attempts from 1024 samples 0.20 0.25 0.20 0.25</cell><cell>0.4 1200 to 1700 800 to 1100 1800 to 2300 2500 to 3400</cell></row><row><cell>Appendix Figure A10 | Prompt</cell><cell></cell><cell>300M</cell><cell>1B</cell><cell>3B</cell></row><row><cell cols="5">Description + Specification + IO 8.33% 13.75% 14.90%</cell></row><row><cell cols="5">Description + IO + Specification 8.20% 14.35% 14.22%</cell></row><row><cell cols="5">Specification + Description + IO 7.74% 12.88% 14.62%</cell></row><row><cell cols="5">Specification + IO + Description 8.06% 13.08% 12.45%</cell></row><row><cell cols="5">IO + Description + Specification 7.51% 12.94% 13.76%</cell></row><row><cell cols="5">IO + Specification + Description 7.48% 13.41% 13.08%</cell></row><row><cell cols="2">Description + Specification</cell><cell cols="3">5.92% 10.42% 11.75%</cell></row><row><cell>Description + IO</cell><cell></cell><cell>1.70%</cell><cell>4.81%</cell><cell>4.98%</cell></row><row><cell>Specification + IO</cell><cell></cell><cell>5.43%</cell><cell>6.87%</cell><cell>7.95%</cell></row></table><note>Appendix Table A12 | Description section ablations.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The time limit for the problem inFigure 2is 2 seconds, using at most 256 MB of memory.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The dataset can be found on GitHub.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">For problems permitting multiple correct outputs, we change the example test outputs to be the most canonical, which gives our approach a slight advantage in the evaluation. See Appendix D for more details.8 Submitted programs can be found on our 3 accounts on Codeforces: SelectorUnlimited, WaggleCollide, and Angular-Numeric. Attention visualizations for these problems can be found here.9  Our estimated performance is closer to its upper bound than its lower bound, because human solutions (and our solutions) are typically submitted early in the contest, especially for easier problems.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Except one problem that does not have 5 test cases and is therefore not included in our test set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">This issue also exists for human programmers who copy old code.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Experiments were run in Google datacenters, which purchase renewable energy equal to the amount consumed(H?lzle,  2018).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">Work conducted during a DeepMind internship, UC Berkeley affiliation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">In the second test case , Mocha can choose the interval[1 ,3]  , then the sequence becomes[ 1 ,1 ,1]  , where the first element is 1 \&amp; 3 , the second element is 1 \&amp; 1 , and the third element is 3 \&amp; 1.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Trevor Cai, Jack Rae, Sebastian Borgeaud, Mia Glaese, Roman Ring, Laurent Sifre, Jordan Hoffman, John Aslanides, Jean-Baptiste Lespiau, Arthur Mensch, Erich Elsen, George van den Driessche, and Geoffrey Irving for developing tools we use to train large language models, and for lending their expertise in model training; Kirsty Anderson, Claudia Pope, and Rachel Foley for project management in early stages; Yee Whye Teh, Chris Dyer, David Silver, Amin Barekatain, Anton Zhernov, Matt Overlan, and Petar Veli?kovi? for research advice and assistance; Karen Simonyan, Chris Dyer, and Dani Yogatama for reviewing the paper; Lorrayne Bennett, Kareem Ayoub, and Jeff</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Complete prompt and model examples</head><p>All problems and human solutions in this section are sourced from Codeforces.</p><p>Encoder Input : // RATING : 1200 // TAGS : math // LANGUAGE IS cpp // CORRECT SOLUTION // n towns are arranged in a circle sequentially . The towns are numbered from 1 // to n in clockwise order . In the i -th town , there lives a singer with a // repertoire of a_i minutes for each i ? <ref type="bibr">[1 , n ]</ref>. // // Each singer visited all n towns in clockwise order , starting with the town he // lives in , and gave exactly one concert in each town . In addition , in each // town , the i -th singer got inspired and came up with a song that lasts a_i // minutes . The song was added to his repertoire so that he could perform it in // the rest of the cities . // // Hence , for the i -th singer , the concert in the i -th town will last a_i // minutes , in the ( i + 1) -th town the concert will last 2 ? a_i minutes , ... , // in the (( i + k ) mod n + 1) -th town the duration of the concert will be ( k + // 2) ? a_i , ... , in the town (( i + n -2) mod n + 1) -n ? a_i minutes . // // You are given an array of b integer numbers , where b_i is the total duration // of concerts in the i -th town . Reconstruct any correct sequence of positive // integers a or say that it is impossible . // // Input // // The first line contains one integer t (1 ? t ? 10^3) -the number of test // cases . Then the test cases follow . // // Each test case consists of two lines . The first line contains a single // integer n (1 ? n ? 4 ? 10^4) -the number of cities . The second line contains // n integers b_1 , b_2 , ... , b_n (1 ? b_i ? 10^{9}) -the total duration of // concerts in i -th city . // // The sum of n over all test cases does not exceed 2 ? 10^5. // // Output // // For each test case , print the answer as follows : // // If there is no suitable sequence a , print NO . Otherwise , on the first line // print YES , on the next line print the sequence a_1 , a_2 , ... , a_n of n // integers , where a_i (1 ? a_i ? 10^{9}) is the initial duration of repertoire // of the i -th singer . If there are multiple answers , print any of them . // // Example // // Input // // // 4 // 3 // 12 16 14 // 1 // 1 // 3 // 1 2 3 // 6 // 81 75 75 93 93 87 // // // Output // // // YES // 3 1 3 // YES // 1 // NO // YES // 5 5 4 1 4 5 // // Note // // Let 's consider the 1 -st test case of the example : // // 1. the 1 -st singer in the 1 -st city will give a concert for 3 minutes , in // the 2 -nd -for 6 minutes , in the 3 -rd -for 9 minutes ; // 2. the 2 -nd singer in the 1 -st city will give a concert for 3 minutes , in // the 2 -nd -for 1 minute , in the 3 -rd -for 2 minutes ; // 3. the 3 -rd singer in the 1 -st city will give a concert for 6 minutes , in // the 2 -nd -for 9 minutes , in the 3 -rd -for 3 minutes .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder Output :</head><p># include &lt; bits / stdc ++. h &gt; using namespace std ; void solve () { long long int n , i , j ; cin &gt;&gt; n ; vector &lt; long long int &gt; a ( n ) ; for ( i = 0; i &lt; n ; i ++) cin &gt;&gt; a [ i ]; long long int sum = 0; for ( i = 0; i &lt; n ; i ++) sum += a [ i ]; if ( sum % cout &lt;&lt; " NO " &lt;&lt; '\ n '; return ; } long long int k = sum / ( n * ( n + 1) / 2) ; vector &lt; long long int &gt; ans ( n ) ; for ( i = 0; i &lt; n ; i ++ Appendix <ref type="figure">Figure A17</ref> | Example decomposition of human and model solutions to the 'Pizzaforces' problem into substrings from the finetuning dataset. Each color identifies one substring, but repetition of any color is not meaningful, nor is there a relationship between substrings in the human and model solutions with the same color.</p><p>Appendix <ref type="figure">Figure A19</ref> | Example decomposition of human and model solutions to the 'Pizzaforces' problem into substrings from the finetuning dataset. Each color identifies one substring, but repetition of any color is not meaningful, nor is there a relationship between substrings in the human and model solutions with the same color.</p><p>Mocha is a young girl from high school . She has learned so much interesting knowledge from her teachers , especially her math teacher . Recently , Mocha is learning about binary system and very interested in bitwise operation . This day , Mocha got a sequence a of length n . In each operation , she can select an arbitrary interval [l , r ] and for all values i (0 &lt;= i &lt;= r -l ) , replace a_ { l + i } with a_ { l + i } \&amp; a_ {r -i } at the same time , where \&amp; denotes the [ bitwise AND operation ]( https :// en . wikipedia . org / wiki / B i t w i s e _ o p e r a t i o n # AND ) . This operation can be performed any number of times .</p><p>For example , if n =5 , the array is <ref type="bibr">[ a_1 , a_2 , a_3 , a_4 , a_5 ]</ref> , and Mocha selects the interval <ref type="bibr">[2 ,5]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note</head><p>In the first test case , Mocha can choose the interval <ref type="bibr">[1 ,2]</ref> , then the sequence becomes [ 0 , 0] , where the first element is 1 \&amp; 2 , and the second element is 2 \&amp; 1.</p><p>In the second test case , Mocha can choose the interval <ref type="bibr">[1 ,3]</ref> , then the sequence becomes <ref type="bibr">[ 1 ,1 ,1]</ref> , where the first element is 1 \&amp; 3 , the second element is 1 \&amp; 1 , and the third element is 3 \&amp; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1559A Mocha and Math -Simplified</head><p>Given a sequence of integers , compute the bitwise AND of all of its elements .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Each test contains multiple test cases .</p><p>The first line contains a single integer t (1 &lt;= t &lt;= 100) -the number of test cases . Each test case consists of two lines .</p><p>The first line of each test case contains a single integer n (1 &lt;= n &lt;= 100) -the length of the sequence .</p><p>The second line of each test case contains n integers a_1 , a_2 , ... , a_n (0 &lt;= a_i &lt;= 10^9) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>For each test case , print one integer -the bitwise AND of all elements of a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Input 4 2 1 2 3 1 1 3 4 3 11 3 7 5 11 7 15 3 7 Output 0 1 3 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note</head><p>In the first test case , Mocha can choose the interval <ref type="bibr">[1 ,2]</ref> , then the sequence becomes [ 0 , 0] , where the first element is 1 \&amp; 2 , and the second element is 2 \&amp; 1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Research recitation: A first look at rote learning in GitHub Copilot suggestions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Ziegler</surname></persName>
		</author>
		<ptr target="https://docs.github.com/en/github/copilot/research-recitation" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The adverse effects of code duplication in machine learning models of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software</title>
		<meeting>the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="143" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07732</idno>
		<title level="m">Program synthesis with large language models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning autocompletion from real-world datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Aye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="131" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepcoder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01989</idno>
		<title level="m">Learning to write programs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sifre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Improving language models by retrieving from trillions of tokens</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van-Derplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://github.com/google/jax" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning from examples to improve code completion systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monperrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mezini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on the foundations of software engineering</title>
		<meeting>the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on the foundations of software engineering</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="213" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Description2Code Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Openai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://github.com/ethancaballero/description2code" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extracting training data from large language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Erlingsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th USENIX Security Symposium (USENIX Security 21)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2633" to="2650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P D O</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">PyMT5: multi-mode translation of natural language and Python code with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Timcheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Softmax tempering for training neural machine translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujita</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09372</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv prepring</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">RobustFill: Neural program learning under noisy I/O</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Solving linear algebra by program synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Verma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">How to interpret contest ratings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ebtekar</surname></persName>
		</author>
		<ptr target="https://codeforces.com/blog/entry/68288,2021" />
		<imprint>
			<biblScope unit="page" from="2021" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding back-translation at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1045</idno>
		<ptr target="https://aclanthology.org/D18-1045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Hacker Cup</surname></persName>
		</author>
		<ptr target="https://www.facebook.com/codingcompetitions/hacker-cup" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2021" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical neural story generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th</title>
		<meeting>the 56th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CodeBERT: a pre-trained model for programming and natural languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1536" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PPDB: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N13-1092" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">GitHub&apos;s automatic coding tool rests on untested legal ground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gershgorn</surname></persName>
		</author>
		<ptr target="https://www.theverge.com/2021/7/7/22561180/github-copilot-legal-copyright-fair-use-public-code" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jam</forename><surname>Google Code</surname></persName>
		</author>
		<ptr target="https://codingcompetitions.withgoogle.com/codejam,2021" />
		<title level="m">Google Code Jam</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Application of theorem proving to problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automating string processing in spreadsheets using input-output examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="317" to="330" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Program synthesis. Foundations and Trends? in Programming Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning to generate code sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10158</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Puranik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09938</idno>
		<title level="m">Measuring coding challenge competence with APPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Haiku: Sonnet for JAX</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<ptr target="http://github.com/deepmind/dm-haiku" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the naturalness of software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Software Engineering</title>
		<meeting>the 34th International Conference on Software Engineering</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="837" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7 ? International Conference on Learning Representations (ICLR)</title>
		<meeting>the 7 ? International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Meeting our match: Buying 100 percent renewable energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>H?lzle</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Achieving verified robustness to symbol substitutions via interval bound propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4081" to="4091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Icpc Factsheet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Icpc</surname></persName>
		</author>
		<ptr target="https://icpc.global/worldfinals/pdf/Factsheet.pdf" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2021" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<ptr target="https://icpc.global/worldfinals/rules,2021" />
		<title level="m">ICPC Rules. ICPC rules</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<ptr target="https://ioinformatics.org/,2021" />
		<title level="m">IOI. International olympiad in informatics</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ten lessons from three generations shaped Google&apos;s TPUv4i</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ashcraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gottscho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Jablin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spoc: Search-based pseudocode to code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Padon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Latent predictor networks for code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="16" to="1057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Toward automatic program synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Manna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Waldinger</surname></persName>
		</author>
		<idno type="DOI">10.1145/362566.362568</idno>
		<ptr target="https://doi.org/10.1145/362566.362568" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="165" />
			<date type="published" when="1971-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The Rust language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Matsakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Klock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGAda Ada Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="103" to="104" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Falsehoods programmers believe about names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mckenzie</surname></persName>
		</author>
		<ptr target="https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/" />
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Codeforces: Results of 2020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirzayanov</surname></persName>
		</author>
		<ptr target="https://codeforces.com/blog/entry/89502,2020" />
		<imprint>
			<biblScope unit="page" from="2021" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Neural sketch learning for conditional program generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jermaine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05698</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Neural learning of one-of-many solutions for combinatorial problems in structured output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nandwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07839</idno>
		<title level="m">Text generation by learning from demonstrations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">An empirical cybersecurity evaluation of GitHub Copilot&apos;s code contributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Karri</surname></persName>
		</author>
		<idno>abs/2108.09293</idno>
		<ptr target="https://arxiv.org/abs/2108.09293" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zolotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Decker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.12655</idno>
		<title level="m">Project CodeNet: A large-scale AI for code dataset for learning a diversity of coding tasks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Scaling language models: Methods, analysis &amp; insights from training Gopher</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Code completion with statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vechev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="419" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">CodeBLEU: a method for automatic evaluation of code synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Scratch: programming for all</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maloney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Monroy-Hern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rusk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eastmond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Millner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="60" to="67" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">How program history can improve code completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Robbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd IEEE/ACM International Conference on Automated Software Engineering</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Fast transformer decoding: One write-head is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02150</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Program synthesis by sketching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<ptr target="https://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time" />
	</analytic>
	<monogr>
		<title level="m">Sussman. Falsehoods programmers believe about time</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">IntelliCode compose: Code generation using transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
		<meeting>the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1433" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Falsehoods programmers believe about addresses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tandy</surname></persName>
		</author>
		<ptr target="https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/" />
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Solving probability and statistics problems by program synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Drori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08267</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning to synthesize programs as interpretable and generalizable policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Grandmaster level in StarCraft II using multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">575</biblScope>
			<biblScope unit="issue">7782</biblScope>
			<biblScope unit="page" from="350" to="354" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Ethical and social risks of harm from language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birhane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legassick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gabriel</surname></persName>
		</author>
		<idno>abs/2112.04359</idno>
		<ptr target="https://arxiv.org/abs/2112.04359" />
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zavershynskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Skidanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03168</idno>
		<title level="m">NAPS: Natural program synthesis dataset</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Removed problems that are duplicates of each other, ignoring whitespace. Submissions for duplicate problems were merged</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Removed submissions that are duplicates of others</title>
		<imprint/>
	</monogr>
	<note>ignoring whitespace</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Cleaned C ++ submissions to compile with our compiler and sandboxes, for example by adding int in front of main() where it was missing. We further formatted C ++ code using clang-format, replaced all the includes with #include &lt;bits/stdc++.h&gt;, and expanded all other preprocessor directives and typedefs, 15 which also made the code shorter. Note that this cleaning 15 #defines and typedefs are widely used by competitive programmers to introduce idiosyncratic abbreviations for common constructs, e.g. #define rep(i,n) for</title>
		<imprint/>
	</monogr>
	<note>int i=0;i&lt;n;i++) or typedef vector&lt;pair&lt;ll,ll? vpll</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Appendix Figure A18 | Example decomposition of human and model solutions to the &apos;Digits Sum&apos; problem into substrings from the finetuning dataset. Each color identifies one substring, but repetition of any color is not meaningful, nor is there a relationship between substrings in the human and model solutions with the same color</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
