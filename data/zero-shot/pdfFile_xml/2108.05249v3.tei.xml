<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fog Simulation on Real LiDAR Point Clouds for 3D Object Detection in Adverse Weather</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hahner</surname></persName>
							<email>mhahner@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
							<email>csakarid@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
							<email>ddai@mpi-inf.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">MPI for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
							<email>luc.vangool@kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<settlement>Leuven</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fog Simulation on Real LiDAR Point Clouds for 3D Object Detection in Adverse Weather</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work addresses the challenging task of LiDARbased 3D object detection in foggy weather. Collecting and annotating data in such a scenario is very time, labor and cost intensive. In this paper, we tackle this problem by simulating physically accurate fog into clear-weather scenes, so that the abundant existing real datasets captured in clear weather can be repurposed for our task. Our contributions are twofold: 1) We develop a physically valid fog simulation method that is applicable to any LiDAR dataset. This unleashes the acquisition of large-scale foggy training data at no extra cost. These partially synthetic data can be used to improve the robustness of several perception methods, such as 3D object detection and tracking or simultaneous localization and mapping, on real foggy data. 2) Through extensive experiments with several state-of-theart detection approaches, we show that our fog simulation can be leveraged to significantly improve the performance for 3D object detection in the presence of fog. Thus, we are the first to provide strong 3D object detection baselines on the Seeing Through Fog dataset. Our code is available at www.trace.ethz.ch/lidar fog simulation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Light detection and ranging (LiDAR) is crucial for the implementation of safe autonomous cars, because LiDAR measures the precise distance of objects from the sensor, which cameras cannot measure directly. Thus, LiDAR has found its way into many applications, including detection <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref>, tracking <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b49">50]</ref>, localization <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b7">8]</ref>, and mapping <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b14">15]</ref>. Despite the benefit of measuring exact depth information, LiDAR has a significant drawback. The light pulses that LiDAR sensors emit in the invisible near infrared (NIR) spectrum (typically at 850 and 903 to 905 nm wavelength <ref type="bibr" target="#b3">[4]</ref>) do not penetrate water particles, as opposed to automotive radars. This means as soon as there are water particles in the form of fog in the air, light pulses emitted by the sensor will undergo backscattering and attenuation. Attenuation reduces the received signal power that corresponds to the range of the solid object in the line of sight which should be measured, while backscattering creates a spurious peak in the received signal power at an incorrect range. As a result, the acquired LiDAR point cloud will contain some spurious returns whenever there is fog present at the time of capture. This poses a big challenge for most outdoor applications, as they typically require robust performance under all weather conditions.</p><p>In recent years, several LiDAR datasets for 3D object detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref> have been presented. Although many of them contain diverse driving scenarios, none of them allows an evaluation on different kinds of adverse weather. Only recently, the Canadian Adverse Driving Conditions (CADC) Dataset <ref type="bibr" target="#b27">[28]</ref> and the Seeing Through Fog (STF) Dataset <ref type="bibr" target="#b1">[2]</ref> address the need for such an evaluation. While CADC focuses on snowfall, STF is targeted towards evaluation under fog, rain and snow. Consequently, there is still not a large quantity of LiDAR foggy data available that could be used to train deep neural networks.</p><p>The reason for this is obvious: collecting and annotating large-scale datasets per se is time, labor and cost intensive, let alone when done for adverse weather conditions. This is exactly the shortfall that our work addresses. In Sec. 3, we propose a physically-based fog simulation that converts real clear-weather LiDAR point clouds into foggy counterparts. In particular, we use the standard linear system <ref type="bibr" target="#b29">[30]</ref> that models the transmission of LiDAR pulses. We distinguish between the cases of clear weather and fog with respect to the impulse response of this system and establish a formal connection between the received response under fog and the respective response in clear weather. This connection enables a straightforward transformation of the range and intensity of each original clear-weather point, so that the new range and intensity correspond to the measurement that would have been made if fog was present in the scene. We then show in Sec. 4 that several state-of-the-art 3D object detection pipelines can be trained on our partially synthetic data to get improved robustness on real foggy data. This scheme has already been applied on images for semantic segmentation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b12">13]</ref> and we show that it is also successful for LiDAR data and 3D object detection.</p><p>For our experiments, we simulate fog on the clearweather training set of STF <ref type="bibr" target="#b1">[2]</ref> and evaluate on their real foggy test set. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an example scene from the STF dense fog test set, where the noise introduced by fog is clearly visible in the LiDAR data. The authors of STF <ref type="bibr" target="#b1">[2]</ref> used a Velodyne HDL-64E as their main LiDAR sensor. This sensor comes with 64 channels and a so-called dual mode. In this mode, it can measure not only the strongest, but also the last return received for each individual emitted light pulse. Even though the last signal contains less severe noise, fog still causes a significant amount of spurious returns. Therefore, even in this dual mode, the sensor cannot fully "see through fog". <ref type="figure" target="#fig_2">Fig. 2</ref> shows an interesting characteristic of the noise introduced by fog, namely that it is not uniformly distributed around the sensor. On the contrary, the presence of noise depends on whether there is a target in the line of sight below a certain range from the sensor. If there is a solid object at a moderate range, there are few, if any, spurious returns from the respective pulses. On the other hand, if there is no target in the line of sight below a certain range, there are a lot of spurious returns that are caused by fog. This becomes apparent in the example of <ref type="figure" target="#fig_2">Fig. 2</ref>, where on the left side of the road there is a hill and on the right side there is open space behind the guardrail. Only in the latter case does the noise caused by fog appear in the measurement. This behavior is explained with our theoretical formulation in Sec. 3.</p><p>As a side note, similar sensor noise can also be caused by exhaust smoke, but if the future of transportation goes electric, at least this problem may vanish into thin air.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Effects of Adverse Weather on LiDAR</head><p>Some of the early works include Isaac et al. <ref type="bibr" target="#b19">[20]</ref>. In 2001, they investigate the influences of fog and haze on optical wireless communications in the NIR spectrum. Then, in 2011, Rasshofer et al. <ref type="bibr" target="#b29">[30]</ref> investigate the influences of weather phenomena on automotive LiDAR systems. In recent years, adverse weather conditions got a lot more attention and there are many other works worth mentioning that look into the degradation of LiDAR data in different adverse weather conditions <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22]</ref>. Very recently, in 2020, the authors of LIBRE <ref type="bibr" target="#b3">[4]</ref> test several LiDAR sensors in a weather chamber under rain and fog. Thereby they provide great and valuable insights on the robustness of individual sensors of this time on challenging weather conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Adverse Weather and LiDAR Simulation</head><p>In the automotive context, artificial fog simulation is so far mostly limited to image based methods. Sakaridis et al. <ref type="bibr" target="#b31">[32]</ref> e.g. create a foggy version of Cityscapes <ref type="bibr" target="#b5">[6]</ref>, a dataset for Semantic Segmentation, and Hahner et al. <ref type="bibr" target="#b12">[13]</ref> a foggy version of the purely synthetic dataset Synscapes <ref type="bibr" target="#b45">[46]</ref> by leveraging the depth information given in the original datasets. Sakaridis et al. also released ACDC <ref type="bibr" target="#b32">[33]</ref>, a dataset providing semantic pixel-level annotations for 19 Cityscapes classes in adverse conditions. Only recently, Bijelic et al. <ref type="bibr" target="#b1">[2]</ref> propose a first order approximation to simulate fog in an automotive LiDAR setting. However, their simulation only aims at reproducing measurements they carried out in a 30m long fog chamber.</p><p>Goodin et al. <ref type="bibr" target="#b11">[12]</ref> develop a model to quantify the performance degradation of LIDAR in rain and incorporate their model into a simulation which they use to test an advanced driver assist system (ADAS). Michaud et al. <ref type="bibr" target="#b25">[26]</ref> and Tremblay et al. <ref type="bibr" target="#b40">[41]</ref> propose a method to render rain on images to evaluate and improve the robustness on rainy images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">3D Object Detection</head><p>After the release of many LiDAR datasets <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b1">2]</ref> over the past few years, 3D object detection is receiving increasing attention in the race towards autonomous driving. While there exist camera based methods such as Simonelli et al. <ref type="bibr" target="#b37">[38]</ref> and gated camera methods such as Gated3D <ref type="bibr" target="#b17">[18]</ref>, the top ranks across all dataset leaderboards are typically sorted out among LiDAR based methods.</p><p>Seminal work in LiDAR based 3D object detection methods include PointNet <ref type="bibr" target="#b28">[29]</ref> and VoxelNet <ref type="bibr" target="#b48">[49]</ref>. PointNet <ref type="bibr" target="#b28">[29]</ref> is a neural network that directly processes point clouds without quantizing the 3D space into voxels beforehand. Most notably, the network architecture is robust to input perturbation, so the order in which the points get fed into the network do not effect its performance. VoxelNet <ref type="bibr" target="#b48">[49]</ref> builds on the idea to quantize the 3D space into equally sized voxels and then leverages PointNet-like layers to process each and every voxel. Due to it's computational intensive 3D convolutions, it is however a rather heavy architecture.</p><p>That's where PointPillars <ref type="bibr" target="#b22">[23]</ref> comes in: it gets rid of the quantization in the height domain and processes the point cloud instead in a 3D pillar grid. PointPillars <ref type="bibr" target="#b22">[23]</ref> is based on the SECOND <ref type="bibr" target="#b46">[47]</ref> codebase, but due to the novel pillar idea, it can fall back to much faster 2D convolutions and achieves very competitive results at a much faster speed. Its successor PointPainting <ref type="bibr" target="#b42">[43]</ref> further leverages image segmentation results and "paints" the points with a pseudo class label before processing them with the PointPillars <ref type="bibr" target="#b22">[23]</ref> architecture.</p><p>Shi et al. achieved several recent milestones in 3D object detection. PointRCNN <ref type="bibr" target="#b35">[36]</ref> is a two-stage architecture, where the first stage generates 3D bounding box proposals from a point cloud in a bottom-up manner and the second stage refines these 3D bounding box proposals in a canonical fashion. Part-A 2 <ref type="bibr" target="#b36">[37]</ref> is part-aware in a sense that the network takes into account which part of the object a point belongs to. It leverages these intra-object part locations and can thereby achieve higher results. PV-RCNN <ref type="bibr" target="#b33">[34]</ref> and it's successor PV-RCNN++ <ref type="bibr" target="#b34">[35]</ref> are the latest of their works that simultaneously process (coarse) voxels and the raw points of the point cloud at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fog Simulation on Real LiDAR Point Clouds</head><p>To simulate the effect of fog on real-world LiDAR point clouds that have been recorded in clear weather, we need to resort to the optical system model that underlies the function of the transmitter and receiver of the LiDAR sensor. In particular, we examine a single measurement/point, model the full signal of received power as a function of the range and recover its exact form corresponding to the original clearweather measurement. This allows us to operate in the signal domain and implement the transformation from clear weather to fog simply by modifying the part of the impulse response that pertains to the optical channel (i.e. the atmosphere). In the remainder of this section, we first provide the background on the LiDAR sensor's optical system and then present our fog simulation algorithm based on this system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background on the LiDAR Optical Model</head><p>Rasshofer et al. <ref type="bibr" target="#b29">[30]</ref> introduced a simple linear system model to describe the received signal power at a LiDAR's receiver, which is valid for non-elastic scattering. In particular, the range-dependent received signal power P R is modeled as the time-wise convolution between the time-dependent transmitted signal power P T and the timedependent impulse response H of the environment:</p><formula xml:id="formula_0">P R (R) = C A 2R/c 0 P T (t) H R ? ct 2 dt.<label>(1)</label></formula><p>c is the speed of light and C A is a system constant which is independent of time and range. For our fog simulation, as we explain in Sec. 3.2, C A can be factored out.</p><p>We proceed with the description and modeling of the remaining terms in <ref type="bibr" target="#b0">(1)</ref>. The time signature of the transmit pulse can be modeled for automotive LiDAR sensors <ref type="bibr" target="#b29">[30]</ref> by a sin 2 function:</p><formula xml:id="formula_1">P T (t) = P 0 sin 2 ? 2 ? H t , 0 ? t ? 2 ? H 0 otherwise.<label>(2)</label></formula><p>Where P 0 denotes the pulse's peak power and ? H the halfpower pulse width. Typical values for ? H lie between 10 and 20 ns <ref type="bibr" target="#b29">[30]</ref>. In (2), the time origin is set to the start of the pulse, so in case a LiDAR sensor does not report the range associated with a rising edge, but the maximum of the corresponding peak in the return signal, we can perform the required correction later in our pipeline. Since it is common to report the rising edge in embedded signal processing, we keep this convention throughout all of our equations and show where one can perform such a correction later on. The spatial impulse response function H of the environment can be modeled as the product of the individual impulse responses of the optical channel, H C , and the target,</p><formula xml:id="formula_2">H T : H(R) = H C (R) H T (R).<label>(3)</label></formula><p>The impulse response of the optical channel H C is</p><formula xml:id="formula_3">H C (R) = T 2 (R) R 2 ?(R),<label>(4)</label></formula><p>where T (R) stands for the total one-way transmission loss and ?(R) denotes the crossover function defining the ratio of the area illuminated by the transmitter and the part of it observed by the receiver, as illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>. Because generally the full details of the optical configuration in commercial LiDAR sensors are not publicly available (i.e. the precise values of R 1 and R 2 are unknown), ?(R) in our case is a piece-wise linear approximation defined as</p><formula xml:id="formula_4">?(R) = ? ? ? 0, R ? R 1 R R2?R1 ? R1 R2?R1 , R 1 &lt; R &lt; R 2 1, R 2 ? R.<label>(5)</label></formula><p>The total one-way transmission loss T (R) is defined as</p><formula xml:id="formula_5">T (R) = exp ? R r=0 ?(r)dr ,<label>(6)</label></formula><p>where ?(r) denotes the spatially varying attenuation coefficient. In our simulation, we make the assumption of a homogeneous optical medium, i.e. ?(r) = ?. As a result,</p><formula xml:id="formula_6">(6) yields T (R) = exp (??R) .<label>(7)</label></formula><p>The attenuation coefficient ? depends on the weather at the time of measurement and increases as visibility range decreases. Therefore, for the same 3D scene, the impulse response of the optical channel H C varies with visibility. The last term of the optical system (1) that remains to be modeled is the impulse response of the target, H T . However, we need to distinguish cases for H T according to the weather condition, as the composition of the target for the same 3D scene is different in fog than in clear weather. We make the contribution of constructing a direct relation between the response P R in clear weather and in fog for the same 3D scene and this relation enables us to simulate fog on real clear-weather LiDAR measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fog Simulation for LiDAR</head><p>We now particularize the optical model of Sec. 3.1 for the individual cases of clear weather and fog in terms of the impulse response terms, H C and H T .</p><p>In clear weather, the attenuation coefficient ? is 0, so</p><formula xml:id="formula_7">H C (R) = ?(R) R 2 .<label>(8)</label></formula><p>Moreover, the target in clear weather is comprised only of the solid object on which the LiDAR pulse is reflected. This type of target is called a hard target <ref type="bibr" target="#b29">[30]</ref>.  <ref type="figure">Figure adjusted</ref> from <ref type="bibr" target="#b29">[30]</ref>.</p><p>The impulse response H T of a hard target at range R 0 is a Dirac delta function of the form</p><formula xml:id="formula_8">H T (R) = H hard T (R) = ? 0 ?(R ? R 0 ),<label>(9)</label></formula><p>where ? 0 denotes the differential reflectivity of the target. If we only consider diffuse reflections (Lambertian surfaces), ? 0 is given by</p><formula xml:id="formula_9">? 0 = ? ? , 0 &lt; ? ? 1.<label>(10)</label></formula><p>Consequently, plugging in <ref type="formula" target="#formula_7">(8)</ref> and <ref type="formula" target="#formula_8">(9)</ref> into <ref type="formula" target="#formula_2">(3)</ref>, in clear weather the total impulse response function H(R) can be expressed as</p><formula xml:id="formula_10">H(R) = ?(R 0 ) R 0 2 ? 0 ?(R ? R 0 ),<label>(11)</label></formula><p>where we have used the property f (x)?(x ? x 0 ) = f (x 0 )?(x ? x 0 ). Since in practice R 2 is less than two meters <ref type="bibr" target="#b41">[42]</ref>, we can safely assume R 2 ? R 0 , so ?(R 0 ) = 1. Thus, starting from (1) and given the range measurement R 0 for the original clear-weather LiDAR point, we compute the received signal power in closed form as</p><formula xml:id="formula_11">P R,clear (R) = C A 2? H 0 P0 sin 2 ? 2? H t 1 R0 2 ?0?(R ? ct 2 ? R0)dt = C A P 0 ? 0 R 0 2 sin 2 ?(R?R 0 ) c? H , R0 ? R ? R0 + c? H 0 otherwise.<label>(12)</label></formula><p>The received signal power attains its maximum value at R 0 + c? H 2 . So as we mentioned in Sec. 3.1, here one can simply shift the response P R,clear (R) by ? c? H 2 if necessary. We now establish the transformation of P R,clear (R) to P R,fog (R) under fog. While the same hard target still exists since the 3D scene is the same, there is now an additional contribution from fog-which constitutes a soft target <ref type="bibr" target="#b29">[30]</ref> that provides distributed scattering-to the impulse response H T .</p><p>The impulse response of this soft fog target, H soft T , is a Heaviside function of the form</p><formula xml:id="formula_12">H soft T (R) = ?U (R 0 ? R),<label>(13)</label></formula><p>where ? denotes the backscattering coefficient, which is constant under our homogeneity assumption, and U is the Heaviside function. The co-existence of a hard target and a soft target can be modeled by taking the superposition of the respective impulse responses:</p><formula xml:id="formula_13">H T (R) = H soft T (R) + H hard T (R) = ?U (R 0 ? R) + ? 0 ?(R ? R 0 ).<label>(14)</label></formula><p>Consequently, plugging in <ref type="formula" target="#formula_6">(7)</ref> into (4) and then <ref type="formula" target="#formula_3">(4)</ref> and <ref type="formula" target="#formula_0">(14)</ref> into <ref type="formula" target="#formula_2">(3)</ref>, in fog the total impulse response function H(R) can be expressed as</p><formula xml:id="formula_14">H(R) = exp(?2?R)?(R) R 2 ? (?U (R 0 ? R) + ? 0 ?(R ? R 0 )) .<label>(15)</label></formula><p>We observe that compared to clear weather, the spatial impulse response in fog is more involved, but it can still be decomposed into two terms, corresponding to the hard and the soft target respectively, leading to a respective decomposition of the received response as P R,fog (R) = P hard R,fog (R) + P soft R,fog (R).</p><p>Focusing on the hard target term, using (1) to calculate the corresponding term of the received response P hard R,fog and leveraging again the assumption that R 2 ? R 0 , we obtain</p><formula xml:id="formula_16">P hard R,fog (R) = = C A exp(?2?R0) R0 2 2? H 0 P0 sin 2 ? 2? H t ?0?(R ? ct 2 ? R0)dt = C A P 0 ? 0 exp(?2?R 0 ) R 0 2 sin 2 ?(R?R 0 ) c? H , R0 ? R ? R0 + c? H 0 otherwise. = exp(?2?R0)P R,clear (R).<label>(17)</label></formula><p>In other words, the hard target term of the response in fog is an attenuated version of the original clear-weather response P R,clear . On the other hand, the soft target term is</p><formula xml:id="formula_17">P soft R,fog (R) = C A P0? 2? H 0 sin 2 ? 2? H t ? ? exp ? 2? R ? ct 2 R ? ct 2 2 ?(R ? ct 2 )U (R0 ? R + ct 2 )dt<label>(18)</label></formula><p>and does not possess a closed-form expression, as the respective integral I(R, R 0 , ?, ? H ) on the right-hand side of (18) cannot be calculated analytically.  <ref type="figure">Figure 4</ref>: The two terms of the received signal power P R,fog from a single LiDAR pulse, associated to the solid object that reflects the pulse (P hard R,fog ) and the soft fog target (P soft R,fog ), plotted across the range domain. While in (a) the fog is not thick enough to yield a return, in (b) it is thick enough to yield a return that overshadows the solid object at R 0 = 30m.</p><p>However, for given ? H and ?, I(R, R 0 , ?, ? H ) can be computed numerically for fixed values of R. We use Simpson's 1 ?3 rule for numerical integration and provide indicative examples of the profile of P soft R,fog (R) in <ref type="figure">Fig. 4</ref>. Depending on the distance of the hard target from the sensor, the soft target term of the response may exhibit a larger maximum value than the hard target term, which implies that the measured range changes due to the presence of fog and becomes equal to the point of maximum of the soft-target term.</p><p>The formulation that we have developed affords a simple algorithm for fog simulation on clear-weather point clouds. The input parameters to the algorithm are ?, ?, ? 0 and ? H . The main input of the algorithm is a clear-weather point cloud, where each point p ? R 3 has a measured intensity i. We make the assumption that the intensity readings of the sensor are a linear function of the maxima of the received signal power P R,clear corresponding to each measurement. The procedure for each point p is given in Algorithm 1. Note, that we add some noise to the distance of P soft R,fog (line <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>, otherwise all points introduced by P soft R,fog would lie precisely on a circle around the LiDAR sensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Fog Simulation</head><p>A qualitative comparison between our fog simulation and the fog simulation in <ref type="bibr" target="#b1">[2]</ref> can be found in <ref type="figure" target="#fig_6">Fig. 5</ref>. We can see that in contrast to the fog simulation in <ref type="bibr" target="#b1">[2]</ref> where the response of soft target is only modeled heuristically, our fog simulation models P soft R,fog in a physically sound way. To highlight this difference, we specifically picked a clear weather scene with a similar layout to the real foggy scene depicted in <ref type="figure" target="#fig_2">Fig. 2</ref>. Only in our fog simulation (best visible in the bottom right visualization of <ref type="figure" target="#fig_6">Fig. 5</ref>), a similar half circle of fog noise gets simulated. In the Supplementary Materials we show a comparison with additional ? values. R 0 ? ?p? <ref type="bibr">3:</ref> x, y, z ? p ? i = P R,clear 4:</p><formula xml:id="formula_18">C A P 0 ? i R0 2 ?0</formula><p>? follows from Eq. <ref type="bibr" target="#b11">(12)</ref> 5:</p><formula xml:id="formula_19">i hard ? i ? exp(?2?R 0 ) ? see Eq. (17) 6:</formula><p>for R in <ref type="figure" target="#fig_0">(0, 0.1, .</ref>.., R 0 ) do ? 10cm accuracy 7:  In the left column, the point cloud is color coded by the intensity and in the right column it is color coded by the height (z value). The top row shows the original point cloud.</p><formula xml:id="formula_20">I R ? SIMPSON(I (R, R 0 , ?, ? H )) ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">3D Object Detection in Fog</head><p>Our experimental setup codebase is forked from Open-PCDet <ref type="bibr" target="#b39">[40]</ref>. It comes with implementations of the 3D Object Detection methods PV-RCNN <ref type="bibr" target="#b33">[34]</ref>, PointRCNN <ref type="bibr" target="#b35">[36]</ref>, SECOND <ref type="bibr" target="#b46">[47]</ref>, Part-A 2 <ref type="bibr" target="#b36">[37]</ref> and PointPillars <ref type="bibr" target="#b22">[23]</ref>. For our experiments we train all of these methods from scratch for 80 epochs with their provided standard training policies on the STF <ref type="bibr" target="#b1">[2]</ref> dataset. We also tried to fine-tune from KITTI <ref type="bibr" target="#b9">[10]</ref> weights (which uses the same LiDAR sensor), but besides the networks converging faster, we did not see any benefit, so all the numbers you see in section 4.2 are trained from scratch on the STF <ref type="bibr" target="#b1">[2]</ref> clear weather training set that consist of 3469 scenes. The STF <ref type="bibr" target="#b1">[2]</ref> clear weather validation and testing set consists of 781 and 1847 scenes respectively. However, the main benefit of using STF <ref type="bibr" target="#b1">[2]</ref> for our experiments is because it comes with test sets for different adverse weather conditions. In particular, it comes with a light fog test set of 946 scenes and a dense fog test set with 786 scenes. This allows us to test the effectiveness of our fog simulation pipeline on real foggy data.</p><p>Regarding our fog simulation, we assumed the halfpower pulse width ? H of the Velodyne HDL-64E sensor to be 20ns and set ? to 0.046 MOR as in Rasshofer et al. <ref type="bibr" target="#b29">[30]</ref>. We empirically set ? 0 to 1?10 ?6 ? for all points to get a similar intensity distribution as we can observe in the real foggy point clouds of STF <ref type="bibr" target="#b1">[2]</ref>. Since the Velodyne HDL-64E uses some unknown internal dynamic gain mechanism, it delivers at each and every time step intensity values in the full value range [0, 255]. To mimic this behaviour and also cover the full value range again we linearly scale up the intensity values after they have been modified by Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Quantitative Results</head><p>For the numbers we report, we select the snapshot with the best performance on the clear weather validation set and test it on the aforementioned test splits. In <ref type="table">Table 1</ref> we report the 3D average precision (AP) on the STF <ref type="bibr" target="#b1">[2]</ref> dense fog test split for the classes Car, Cyclist and Pedestrian as well as the 3D mean average precision (mAP) over those three classes. Note, it is always one model that predicts all three classes and not one model per class. All AP and mAP numbers reported in this paper are being calculated using 40 recall positions as suggested in <ref type="bibr" target="#b37">[38]</ref>. We can see that in mAP over all classes and on the major Car class, the training runs of all methods using our fog simulation outperforms the clear weather baseline and the training runs using the fog simulation in <ref type="bibr" target="#b1">[2]</ref>.</p><p>As a second baseline, we evaluate the clear weather model after applying an additional preprocessing step at test time, where we only feed those points to the network that are present in both, the strongest and last measurement of the same scene. We dub this filter "strongest ? last filter".  The idea for this filter stems from the fact, that all the points that get discarded by this filter, must be noise (most likely introduced by fog in the scene) and can not be from a physical object of interest. We can see that this filter most of the time boosts the performance of the clear weather model but also does not surpass any fog simulation runs for the majority of cases. One might also notice, that the performance on the Cyclist class is generally lower than on the other two classes. We attribute this to the fact, that the Cyclist class is fairly underrepresented compared to the other two classes in the STF <ref type="bibr" target="#b1">[2]</ref> dataset (e.g. 28 cyclists vs. 490 pedestrians and 1186 cars in the dense fog test split). For the Pedestrian (and Cyclist) class we still achieve three out of five times state-of-the-art performance. For the training runs using either our fog simulation or the fog simulation in <ref type="bibr" target="#b1">[2]</ref>, we uniformly sample for each training example ? from [0, 0.005, 0.01, 0.02, 0.03, 0.06] which corresponds to a MOR of approximately [?, 600, 300, 150, 100, 50]m respectively. Trying out more sophisticated techniques like curriculum learning <ref type="bibr" target="#b0">[1]</ref> is kept for future work.</p><p>In <ref type="table" target="#tab_2">Table 2</ref> we present the 3D AP of the major Car class on the dense fog, light fog and clear test set as well as the mAP over those three weather conditions. We can see that in dense fog the training runs of all methods using our fog simulation outperforms all other training runs, which is exactly what we aimed for with our physically accurate fog simulation. We can further see that mixing in our fog simulation in training does not hurt the performance in clear weather too much, hence we also achieve state-of-the-art for most cases in mAP over all three weather conditions.</p><p>In the Supplementary Materials, we discuss why we chose to focus on relaxed intersection over union (IoU) thresholds and present results using the official KITTI <ref type="bibr" target="#b9">[10]</ref> evaluation strictness. Additionally, we present 2D and Birds Eye View (BEV) results, and further details on the STF <ref type="bibr" target="#b1">[2]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Qualitative Results</head><p>In <ref type="figure">Fig. 6</ref> we showcase three examples where we clearly outperform the clear weather baseline. We can examine that the model that sees our simulated fog in training, has less false positives (left), more true positives (middle) and overall more accurate predictions (right), each time applying the same confidence threshold for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we introduce a physically accurate way to convert real-world clear weather point clouds into foggy point clouds. In this process we have full control over all parameters involved in the physical equations. This not only allows us to realistically simulate any density of fog, but also allows us to simulate the influence of fog on basically any LiDAR sensor currently available on the market.</p><p>We show that by using this physically accurate fog simulation, we can improve the performance of several state of the art 3D object detection methods on point clouds that have been collected in real-world dense fog. We expect that our fog simulation can lead to even greater performance boosts if the LiDAR data is annotated in 360?and not just in the field of view of a single forward facing camera, but no such dataset is publicly available yet to test this hypothesis.</p><p>We believe that our physically accurate fog simulation is not just applicable to the task of 3D object detection. So we hope that our fog simulation also finds its way into many other tasks and works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>LiDAR returns caused by fog in the (top) scene. (a) shows the strongest returns and (b) the last returns, color coded by the LiDAR channel. The returns of the ground are removed for better visibility of the points introduced by fog. Best viewed in color (red = low, cyan = high, 3D bounding box annotation in green, ego vehicle dimensions in gray).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>LiDAR returns caused by fog in the (top) scene. Color coded by the LiDAR channel in (a) and by the intensity in (b). The returns of the ground are removed for better visibility of the points introduced by fog. Best viewed in color, same color coding as inFig. 1 applies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Sketch of a LiDAR sensor where the transmitter Tx and the receiver Rx do not have coaxial optics, but have parallel axes. This is called a bistatic beam configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) P soft R,fog (R) &lt; P hard R,fog (b) P soft R,fog &gt; P hard R,fog</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1</head><label>1</label><figDesc>LiDAR fog simulation 1: procedure FOGGIFY(p, i, ?, ?, ? 0 , ? H ) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of our fog simulation (bottom) to the fog simulation in<ref type="bibr" target="#b1">[2]</ref> (middle) with ? set to 0.06, which corresponds to a meteorological optical range (MOR) ? 50m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>RCNN [34] ? 0 45.03 46.00 45.08 24.33 24.63 24.63 43.96 41.92 40.09 37.77 37.51 36.60 PV-RCNN [34] ? 0 45.24 46.18 45.25 24.38 24.67 24.67 44.81 43.09 40.98 38.15 37.98 36.97 fog simulation in [2] * 45.60 46.60 45.60 26.42 26.93 27.80 42.95 40.89 39.09 38.32 38.14 37.50 our fog simulation * 46.69 47.38 46.51 27.89 27.89 29.29 42.38 40.65 39.20 38.99 38.64 38.33 PointRCNN [36] ? 0 44.00 45.03 43.73 22.99 22.99 24.23 41.73 38.38 35.71 36.24 35.47 34.56 PointRCNN [36] ? 0 44.40 45.25 44.15 23.52 23.52 25.62 43.23 40.16 37.05 37.05 36.31 35.61 fog simulation in [2] * 46.08 47.02 45.85 20.36 20.36 20.36 43.03 41.60 39.95 36.49 36.33 35.39 our fog simulation * 47.81 47.99 46.68 22.88 22.88 25.18 45.79 43.47 41.33 38.83 38.11 37.73 SECOND [47] ? 0 42.36 42.99 41.99 24.03 25.21 25.21 36.72 35.37 33.84 34.37 34.52 33.68 SECOND [47] ? 0 42.78 43.47 42.42 22.32 23.69 23.69 37.06 36.14 34.14 34.05 34.43 33.42 fog simulation in [2] * 42.67 43.58 42.77 26.28 27.11 27.11 37.89 36.54 35.38 35.61 35.74 35.09 our fog simulation * 43.47 44.01 43.20 26.85 27.21 27.46 38.41 37.06 35.87 36.24 36.09 35.51 Part-A? [37] ? 0 37.60 38.15 37.76 24.51 25.59 25.59 41.03 39.29 37.59 34.38 34.34 33.65 Part-A? [37] ? 0 38.04 38.73 38.30 24.37 25.45 25.45 40.36 38.55 36.65 34.26 34.25 33.47 fog simulation in [2] * 41.07 41.63 40.81 21.12 21.12 21.12 38.83 37.57 34.94 33.67 33.44 32.29 our fog simulation * 42.16 42.75 41.70 25.13 25.72 26.22 39.19 38.29 36.29 35.49 35.59 34.74 PointPillars [23] ? 0 34.30 35.23 35.00 23.05 23.26 25.50 26.43 25.35 24.17 27.93 27.95 28.22 PointPillars [23] ? 0 34.89 35.84 35.47 24.14 24.36 25.38 27.17 26.04 24.85 28.74 28.75 28.57 fog simulation in [2] * 37.02 38.16 37.88 21.68 21.68 23.33 28.84 28.25 26.95 29.18 29.37 29.39 our fog simulation * 38.31 39.14 38.91 23.40 23.40 25.37 30.50 29.51 27.91 30.73 30.68 30.73 Table 1: 3D average precision (AP) results on the STF [2] dense fog test split. ? clear weather baseline ? clear weather baseline (same model as ?) with strongest ? last filter applied at test time * fog simulation gets applied to every training example with ? uniformly sampled from [0, 0.005, 0.01, 0.02, 0.03, 0.06] ? 0 45.24 46.18 45.25 69.64 70.30 68.42 79.80 77.16 71.08 64.89 64.55 61.58 fog simulation in [2] * 45.60 46.60 45.60 70.02 70.56 69.37 79.63 77.48 72.48 65.08 64.88 62.48 our fog simulation * 46.69 47.38 46.51 71.42 70.96 69.03 79.27 76.75 71.80 65.79 65.03 62.45 PointRCNN [36] ? 0 44.00 45.03 43.73 71.30 71.48 68.31 80.05 76.52 70.80 65.12 64.34 60.95 PointRCNN [36] ? 0 44.40 45.25 44.15 71.36 70.45 68.28 79.96 76.37 70.59 65.24 64.02 61.01 fog simulation in [2] * 46.08 47.02 45.85 70.80 70.27 67.66 79.90 76.16 69.18 65.59 64.48 60.90 our fog simulation * 47.81 47.99 46.68 70.74 70.84 67.65 80.41 76.58 69.68 66.32 65.14 61.34 SECOND [47] ? 0 42.36 42.99 41.99 70.51 70.07 68.60 78.67 75.20 70.67 63.85 62.75 60.42 SECOND [47] ? 0 42.78 43.47 42.42 70.50 70.08 68.63 78.53 75.08 69.91 63.93 62.87 60.32 fog simulation in [2] * 42.67 43.58 42.77 69.67 69.89 68.45 79.23 76.61 71.89 63.85 63.36 61.04 our fog simulation * 43.47 44.01 43.20 69.55 69.63 68.49 79.44 75.95 71.94 64.15 63.20 61.21 Part-A? [37] ? 0 37.60 38.15 37.76 65.29 65.88 64.31 76.38 73.79 68.56 59.76 59.27 56.88 Part-A? [37] ? 0 38.04 38.73 38.30 65.98 66.41 64.62 76.43 73.86 68.57 60.15 59.67 57.16 fog simulation in [2] * 41.07 41.63 40.81 65.91 65.84 63.91 76.61 73.84 68.31 61.20 60.44 57.68 our fog simulation * 42.16 42.75 41.70 68.12 67.76 65.19 76.64 73.86 68.05 62.31 61.46 58.32 PointPillars [23] ? 0 34.30 35.23 35.00 67.92 68.47 66.73 77.20 74.64 69.63 59.81 59.45 57.12 PointPillars [23] ? 0 34.89 35.84 35.47 67.97 68.52 66.76 77.27 74.66 69.59 60.04 59.67 57.27 fog simulation in [2] * 37.02 38.16 37.88 68.18 68.24 67.10 76.33 73.91 69.03 60.51 60.10 58.00 our fog simulation * 38.31 39.14 38.91 68.31 68.95 67.18 77.42 74.56 69.55 61.34 60.88 58.55</figDesc><table><row><cell>Method</cell><cell>?</cell><cell cols="3">Car AP@.5IoU easy mod. hard</cell><cell cols="3">Cyclist AP@.25IoU easy mod. hard</cell><cell cols="3">Pedestrian AP@.25IoU easy mod. hard</cell><cell>mAP over classes easy mod. hard</cell></row><row><cell>PV-Method</cell><cell>?</cell><cell>easy</cell><cell>dense fog mod.</cell><cell>hard</cell><cell>easy</cell><cell>light fog mod.</cell><cell>hard</cell><cell>easy</cell><cell>clear mod.</cell><cell>hard</cell><cell>mAP over conditions easy mod. hard</cell></row><row><cell>PV-RCNN [34]  ?</cell><cell cols="11">0 45.03 46.00 45.08 69.55 70.17 68.44 79.61 77.05 71.03 64.73 64.41 61.52</cell></row><row><cell>PV-RCNN [34]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Car 3D AP@.5IoU results on all relevant STF<ref type="bibr" target="#b1">[2]</ref> test splits. ? clear weather baseline ? clear weather baseline (same model as ?) with strongest ? last filter applied at test time * fog simulation gets applied to every training example with ? uniformly sampled from [0, 0.005, 0.01, 0.02, 0.03, 0.06]Figure 6: The (top) row shows predictions by PV-RCNN [34] trained on the original clear weather data (first row in tables above), the (bottom) row shows predictions by PV-RCNN [34] trained on a mix of clear weather and simulated foggy data (fourth row in tables above) on three example scenes from the STF [2] dense fog test split. Ground truth boxes in color, predictions of the model in white. Best viewed on a screen (and zoomed in).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Seeing through fog without seeing fog: Deep multimodal sensor fusion in unseen adverse weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bijelic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Mannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Heide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Libre: The multiple 3d lidar dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carballo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Monrroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Narksri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kitsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Motion-based detection and tracking in 3d lidar scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Caselitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Tipaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey on 3d lidar localization for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhousni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quantifying the influence of rain in lidar performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Filgueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Higinio</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susana</forename><surname>Lag?ela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><forename type="middle">D?az</forename><surname>Vilari?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Arias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohannes</forename><surname>Kassahun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mentar</forename><surname>Mahmudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Ricou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenz</forename><surname>Hauswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Viet Hoang Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>M?hlegg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiffany</forename><surname>Dorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudesh</forename><surname>J?nicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiragkumar</forename><surname>Mirashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Savani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vorobiov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Oelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Garreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuberth</surname></persName>
		</author>
		<title level="m">A2d2: Audi autonomous driving dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>arXiv preprint 2004.06320</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Predicting the influence of rain on lidar in adas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Goodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Carruth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Doude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hudson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic understanding of foggy scenes with purely synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hahner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Nico</forename><surname>Zaech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weather influence and classification with automotive lidar sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heinzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seekircher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-time loop closure in 2d lidar slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damon</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Rapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The apolloscape open dataset for autonomous driving and its application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Testing and validation of automotive point-cloud sensors in adverse weather conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Jokela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Kutila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasi</forename><surname>Pyyk?nen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gated3d: Monocular 3d object detection from temporal illumination cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Julca-Aguilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Bijelic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Mannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heide</surname></persName>
		</author>
		<idno>2102.03602</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kesten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nadhamuni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Omari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Platinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
		<ptr target="https://level5.lyft.com/dataset" />
		<title level="m">Lyft level 5 perception dataset 2020</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Comparison of laser beam propagation at 785 nm and 1550 nm in fog and haze for optical wireless communications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Mcarthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Korevaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optical Wireless Communications III</title>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">4214</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automotive lidar performance verification in fog and rain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kutila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pyyk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holzh?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Colomb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duthon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automotive lidar sensor development scenarios for harsh weather conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kutila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pyyk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sawade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?ufele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duthon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Colomb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibanez-Guzman</surname></persName>
		</author>
		<title level="m">What happens for a tof lidar in fog? IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">L3-net: Towards learning based lidar localization for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards characterizing the behavior of lidars in snowy conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Fran?ois</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gigu?re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The h3d dataset for full-surround 3d multi-object detection and tracking in crowded urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikanth</forename><surname>Malla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiming</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1568" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Pitropov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danson</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rebello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Waslander</surname></persName>
		</author>
		<title level="m">Canadian adverse driving conditions dataset</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Influences of weather phenomena on automotive laser radar systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rasshofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Spies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Radio Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ACDC: The adverse conditions dataset with correspondences for semantic driving scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Pointvoxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">PV-RCNN++: point-voxel feature set abstraction with local vector representation for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno>2102.00463</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Lopez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Openpcdet: An opensource toolbox for 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Openpcdet</forename><surname>Development Team</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/OpenPCDet" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rain rendering for evaluating and improving robustness to bad weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirsendu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><surname>Halder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Fran?ois</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Hdl-64e user&apos;s manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Velodyne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pointpainting: Sequential fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bassam</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Full waveform lidar for adverse weather conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Halimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Buller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Comparison of 905 nm and 1550 nm semiconductor laser rangefinders&apos; performance deterioration due to adverse environmental conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Wojtanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Zygmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miros?awa</forename><surname>Kaszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mierczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Muzal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opto-Electronics Review</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Synscapes: A photorealistic synthetic dataset for street scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Wrenninge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Unger</surname></persName>
		</author>
		<idno>1810.08705</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Loam : Lidar odometry and mapping in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems Conference (RSS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">People detection and tracking using lidar sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia?lvarez</forename><surname>Aparicio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Guerrero-Higueras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">Javier</forename><surname>Rodr?guez-Lera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonatan</forename><forename type="middle">Gin?s</forename><surname>Clavero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">Mart?n</forename><surname>Rico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Matell?n</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
