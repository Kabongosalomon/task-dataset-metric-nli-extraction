<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LIGAR: Lightweight General-purpose Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Izutov</surname></persName>
							<email>evgeny.izutov@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LIGAR: Lightweight General-purpose Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Growing amount of different practical tasks in a video understanding problem has addressed the great challenge aiming to design an universal solution, which should be available for broad masses and suitable for the demanding edge-oriented inference. In this paper we are focused on designing a network architecture and a training pipeline to tackle the mentioned challenges. Our architecture takes the best from the previous ones and brings the ability to be successful not only in appearance-based action recognition tasks but in motion-based problems too. Furthermore, the induced label noise problem is formulated and Adaptive Clip Selection (ACS) framework is proposed to deal with it. Together it makes the LIGAR framework the general-purpose action recognition solution. We also have reported the extensive analysis on the general and gesture datasets to show the excellent trade-off between the performance and the accuracy in comparison to the state-of-the-art solutions. Training code is available at: https: // github. com/ openvinotoolkit/ training_ extensions . For the efficient edge-oriented inference all trained models can be exported into the OpenVINO?format.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Introduction</head><p>N owadays Action Recognition (AR) plays the key role in many real-world applications, including human-robot interaction, behavior analysis, gesture translation. It aims to solve the classification problem in the video domain. The recent success of Deep Learning (DL) based solutions has brought the ability to solve the mentioned above problems close to the human level. Unfortunately, there is a big gap between a working solution developed for the academic purposes and an edge-oriented network design.</p><p>To connect the business goals and academic achievements the researchers have proposed several solutions for adverted tasks, like MobileNets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> for 2D image classification and the OSNet architecture <ref type="bibr" target="#b3">[4]</ref> for the Re-Identification problem. The AR domain still lacks the fast and edge-oriented solutions. So, we developed the lightweight action recognition network architecture called LIGAR (LIghtweigt General-purpose Action Recognition) to fill the gap between accurate and real-time solutions.</p><p>Motivation. Current SOTA Action Recognition approaches mostly use 3D-like backbone design <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> to capture the motion component paired with appearance one. It allows us to ingest huge datasets (e.g.</p><p>Kinetics <ref type="bibr" target="#b9">[10]</ref> and YouTube-8M <ref type="bibr" target="#b10">[11]</ref>) and demonstrates the impressive accuracy on the admissible robustness level. Unluckily the final 3D-like AR networks are too big for the edge-oriented inference and cannot be used somewhere except benchmarks.</p><p>The opposite segment of fast AR networks is represented by 2D network architectures <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, which split a video processing pipeline on independent image processing and the late feature fusion. The 2D networks work fast enough for the real-time inference. The main drawback of the adverted 2D solutions is an inability to capture complex motions, like hand movements in the sign language recognition problem (e.g. American Sign Language -ASL).</p><p>Summarizing, the development of a general-purpose network requires capturing the motion component as well as appearance one. In the proposed paper we are focused on merging representatives of 2D and 3D worlds of lightweight architectures -MobileNet-V3 <ref type="bibr" target="#b2">[3]</ref> and X3D <ref type="bibr" target="#b8">[9]</ref> respectively. The obtained architecture is suitable for motion capturing mostly and to enhance processing the appearance component the model is supplemented with additional global branch (proposed in the LGD paper <ref type="bibr" target="#b14">[15]</ref>).</p><p>Our contributions are as follows:</p><p>? Extending the family of efficient 3D networks for the general-purpose action recognition by II RELATED WORK merging X3D <ref type="bibr" target="#b8">[9]</ref> and LGD <ref type="bibr" target="#b14">[15]</ref> video processing frameworks with the lightweight edge-oriented MobileNet-V3 <ref type="bibr" target="#b2">[3]</ref> backbone architecture. ? Supplementing the training procedure with a bag of tricks (augmentations, multi-head pre-training, feature regularization, metric-learning based head, learning rate scheduler) to deal with limited size of AR datasets to reach robustness. ? Proposing the Adaptive Clip Selection (ACS) module to tackle with a weak temporal annotation in common AR datasets.</p><p>In addition, we release the training framework 1 that can be used in order to re-train or fine-tune our model with a custom database. The final model is inferenceready to use with Intel OpenVINO?toolkit 2 -the sample code on how to run the model in the demo mode is available at Intel OpenVINO?OMZ 3 .</p><p>Index terms: action recognition, network regularization, lightweight network, edge-oriented inference, metric learning head, label noise suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Related Work</head><p>i. Action Recognition I n the Action Recognition problem we are concentrated on solving the classification problem on a sequence of images, which can be derived from a live video stream or an untrimmed long video. First attempts to deal with it were based on extending the 2D classification networks into the 3D use case by addition an extra temporal dimension inside the CNN's kernels, like in C3D <ref type="bibr" target="#b15">[16]</ref> and I3D <ref type="bibr" target="#b4">[5]</ref> papers. The final networks were too large for the existed datasets and suffered from strong over-fitting. In the same time, purely 2D solutions perform independent image processing and after this the extracted features are merged at the decision <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b11">12]</ref> or feature <ref type="bibr" target="#b13">[14]</ref> levels. That design allows the 2D network to be the fastest video processing framework but not accurate.</p><p>The latter improvement is addressed to the publication of sizable datasets, like Kinetics <ref type="bibr" target="#b9">[10]</ref> and YouTube-8M <ref type="bibr" target="#b10">[11]</ref>, which are suitable for pre-training purposes. Next steps were focused on the reduction of parameters of 3D-based networks (e.g. R(2+1)D <ref type="bibr" target="#b6">[7]</ref>, S3D <ref type="bibr" target="#b7">[8]</ref>), extending 2D approaches with Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b17">[18]</ref> and merging 2D and 3D networks into the single one (e.g. SlowFast <ref type="bibr" target="#b18">[19]</ref>, Assem-bleNet <ref type="bibr" target="#b19">[20]</ref>, LGD <ref type="bibr" target="#b14">[15]</ref>) to achieve the trade-off between the accuracy of 3D networks and the low computation budget of 2D ones. We pursue the the same paradigm and designed the LGD-based architecture.</p><p>Other researchers pay attention to integrating specialized attention modules into the backbone <ref type="bibr" target="#b20">[21]</ref> or feature aggregation module <ref type="bibr" target="#b21">[22]</ref> or designing a specialized head for better spatio-temporal feature aggregation <ref type="bibr" target="#b22">[23]</ref>. Unfortunately, the listed above methods lead to the grow of computations and thereby are less suitable for the edge-oriented inference.</p><p>ii. Edge-oriented inference A fter the resounding success of DL-based approaches of solving a wide range of complex problems the necessity of porting solutions closer to the end users by adopting networks for the inference on the edge has appeared. The first 2D solutions are represented by the MobileNet family <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, the ShuffleNet architecture <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> and EfficientNets <ref type="bibr" target="#b25">[26]</ref> later.</p><p>The attempts to bring 3D networks closer to business include mostly introducing the X3D <ref type="bibr" target="#b8">[9]</ref> and MoViNet <ref type="bibr" target="#b26">[27]</ref> network families. The last one is designed automatically by the NAS-based approach. In the proposed paper we adhere to the X3D framework and merge it with the mentioned above edge-oriented 2D backbone -MobileNet-V3 architecture.</p><p>iii. Network regularization A nother question addressed to the researchers is the ability to train a network on the limited amount of a data without significant overfitting. The most common way to tackle it is to use advanced augmentation techniques, like MixUp <ref type="bibr" target="#b27">[28]</ref>, AugMix <ref type="bibr" target="#b28">[29]</ref>, CrossNorm <ref type="bibr" target="#b29">[30]</ref> and so on.</p><p>A different point of view is related to using featurelevel regularization methods, like Dropout family <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, mutual learning <ref type="bibr" target="#b34">[35]</ref> and self-feature regularization <ref type="bibr" target="#b35">[36]</ref>. We follow the mutual learning paradigm and blend it with the mentioned above Aug-Mix strategy.</p><p>Further, the network regularization can be performed by using more sophisticated losses instead of default one (the pair of the Softmax normalization and the CrossEntropy loss): transition to the Metric-Learning (ML) paradigm <ref type="bibr" target="#b36">[37]</ref> and using AM-Softmax loss <ref type="bibr" target="#b37">[38]</ref> III NETWORK DESIGN as a main target and some auxiliary losses from the Re-Identification task, like PushPlus <ref type="bibr" target="#b38">[39]</ref> and Center-Push <ref type="bibr" target="#b36">[37]</ref> losses. We act in accordance with the same practices and use ML head with the aforementioned auxiliary losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Network design</head><p>i. Motion-appearance trade-of O riginally the data sources for AR can be divided by the data representation onto continual and sparse. The last one is most common for general-purpose scenarios and described by appearance mostly rather than motion. In other words, the significant amount of data samples from that category can be viewed as a sequence of key frames, where each frame reflects a change in a global appearance. For better understanding imagine the "covering something" atomic action -to recognize the mentioned class it is needed to extract the sequence of two frames only -"uncovered object" and then "covered by something" one. As it can be seen the motion component in the above sample is rudiment. To recognize similar classes it is enough to model the relationship between key frames by a 2D network, like in TSN <ref type="bibr" target="#b11">[12]</ref> or TRN <ref type="bibr" target="#b13">[14]</ref>.</p><p>The continues segment of data representations consists of gesture recognition scenarios, like ASL (American Sign Language) recognition task. It is complicated by the difficult hand and fingers movements. The attempt to compress an image sequence into a sequence of key frames (comparable with 2D approaches) fails <ref type="bibr" target="#b39">[40]</ref> due to the unchanging appearance component (a person or background on a video) from the one hand and the rapidly changing motion component (fingers) on the other hand. The only way to model the motion component is using fully 3D networks.</p><p>In this paper we accept the common paradigm for "inflating" 2D network architectures into the 3D one, while keeping the original 2D weight initialization, as it was proposed in I3D <ref type="bibr" target="#b4">[5]</ref>. In terms of edge-oriented network design there are two main inflating frameworks: S3D <ref type="bibr" target="#b7">[8]</ref> and recently published X3D <ref type="bibr" target="#b8">[9]</ref>. The choice between S3D and X3D frameworks can be viewed as the choice between the motion-and the appearancepreferable architectures respectively due to the different placement of a temporal convolution kernel -inside a depth-wise kernel (X3D) and after it (S3D). As it was shown in <ref type="bibr" target="#b36">[37]</ref> the S3D framework is most suitable for the heavy gesture recognition scenarios, like the ASL gesture recognition problem. But our extensive experiments evinced that S3D is worse than X3D framework in general scenarios (e.g. UCF-101 <ref type="bibr" target="#b40">[41]</ref> or Acivi-tyNet <ref type="bibr" target="#b41">[42]</ref> datasets). Eventually, we have concluded to use the X3D framework.</p><p>ii. Multi-path architecture R ecently proposed Local and Global Diffusion (LGD) framework <ref type="bibr" target="#b14">[15]</ref> achieves SOTA results on several general-purpose datasets. The main idea of the mentioned method is to split the regular singlepath backbone design into two paths for separately processing the local and global context. The framework also offered the between paths communication modules. We found the proposed framework is preferable in terms of motion-appearance trade-off due to the possibility to move the lion's share of a work of appearance component processing into the lightweight global path. Such a find also allows us to compensate the loss for the gesture recognition issues, while making the choice in favor X3D instead of S3D framework.</p><p>In our paper we implemented the same LGD-based backbone architecture with several changes:</p><p>? We refused of using the kernel function to merge the global and local branches. Instead of it the iii Overall architecture IV NETWORK REGULARIZATION local branch is used as the backbone output and the global branch is shorter by single LGD block. We did not beneficiate from using any complex merging scheme because of following the end-toend training paradigm instead of separate path pre-training, proposed originally (see section i). ? Basically, the local-to-global diffusion module uses Global Average Pooling (GAP) operator to merge spatio-temporal representations into the single feature-vector. Experimentally we have found that the more accurate way is to use the attention module through the spatio-temporal dimensions to extract the most relevant features. Having multiple choice of possible architectures of the attention module we have focused on Global Context (GC) block <ref type="bibr" target="#b42">[43]</ref>.</p><p>iii. Overall architecture T he suggested edge-oriented backbone architecture is based on X3D framework but with MobileNet-V3 2D skeleton architecture instead of MobileNet-V2 originally. According to S3D framework the first convolution has only spatial dimensions (1 ? 3 ? 3 kernel) and places the temporal strides in different positions than the spational one. All the above changes allow us to design the efficient 3D network architecture.</p><p>The mentioned 3D architecture constitutes the local path in the described early LGD network design. The global path is implemented on the same way as in the original paper but with GC block instead of simple GAP operator. The output of the merged backbone is preserved the same as in X3D+MobileNet-V3 architecture without addition of kernel fusion module.</p><p>The backbone output followed the simple spatiotemporal reduction module implemented by GAP operator. On top of network the Metric-Learning based head is placed. It consists of two consecutive 1 ? 1 ? 1 convolutions with batch norms <ref type="bibr" target="#b43">[44]</ref> and forms the output of the network to be 256-dimensional feature-vector. Note, the network output is l 2 normalized. For more details see the <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Network regularization</head><p>i. Multi-head pre-training R ecent success of transfer learning approach <ref type="bibr" target="#b44">[45]</ref> through pre-training the network on tremendous general datasets (e.g. ImageNet <ref type="bibr" target="#b45">[46]</ref> for 2D im-age classification and Kinetics <ref type="bibr" target="#b9">[10]</ref> for the 3D use case) plays important role in the network regularization. It allows us to train the final model on the limited amount of data by using (aka. transferring) the original pretrained weights.</p><p>Another point of view on the pre-training stage is formulated as increasing the number of targets <ref type="bibr" target="#b46">[47]</ref> to reach the affluent semantics of the learned classes. Unfortunately, the data collection process of video datasets is time consuming and the existing datasets cannot boast of sufficient number of classes. The solution has been proposed in the paper <ref type="bibr" target="#b47">[48]</ref> by simultaneously training on the merged set of available datasets. The idea is to sample the batch from the joint set of samples but split the network heads according to the target datasets. In such framework each head spawn the independent decision space and there is no possible conflicting between similar classes in different datasets.</p><p>The mentioned above method allows us to use the original annotation of existing datasets but train the network on the full set of samples simultaneously. The proposed network architecture is trained according to the same idea on the joint set of Kinetics and YouTube-8M-Segments datasets but with ML-heads (see the next section ii).</p><p>ii. Feature regularization U nfortunately, the good network initialization is not enough for training the robust and accurate final model due to possible over-fitting on the limited-size target dataset. To overcome foregoing issue it is proposed to use a bag of regularization methods during the training stage.</p><p>The paper <ref type="bibr" target="#b36">[37]</ref> advocate the idea to combine a regular 3D classification network with Metric-Learning (ML) based design of heads and appropriate ML losses. It adds the two-layer's head on top of a backbone and l 2 normalizes the output 256-dimensional feature vector. Additionally, to control the structure of the learned manifold the extra losses are used: AM-Softmax loss <ref type="bibr" target="#b37">[38]</ref> instead of default CE-loss, Center-Push and LocalPush to model the sample-sample interactions and the confidence penalty to decrease the impact of overconfident samples. We use the same approach and for more details see the mentioned above paper.</p><p>The ML-based regularization is explicit and it is performed by forcing the extra properties for the decision space. The different (implicit) way to carry out the regularization is to restrict the expressiveness of fea-V IMPLEMENTATION DETAILS tures by some rules. Our experiments made to believe that there are several outstanding methods which are universal enough for the regularization purposes. The first method is the advanced version of the well-known dropout -Representation Self-Challenging (RSC) module <ref type="bibr" target="#b48">[49]</ref>. It drops the most relevant filters and thereby forces the network to find the diverse set of features.</p><p>The other regularization method is a part of the AugMix <ref type="bibr" target="#b28">[29]</ref> augmentation. It applies the different augmentation pipelines to the same sample in the batch and adds auxiliary loss to pull the predictions of the same instance to each other. Thereby the network tries to learn the augmentation-independent representation. We treat the method as some kind of self-mutual learning <ref type="bibr" target="#b35">[36]</ref> and use it as the way to fill the network capacity with utility task-relevant filters.</p><p>V. Implementation details i. Augmentations A s it was described in the previous section the AugMix <ref type="bibr" target="#b28">[29]</ref> augmentation is used to effectively regularize the network during the final training on the limited-size datasets. Unlike the original paper we have chosen the fixed set of operations used in the augmentation pipeline: random rotate, crop, horizontal flip and CrossNorm <ref type="bibr" target="#b29">[30]</ref> augmentations. Furthermore, we have found useful to integrate the random selection of time segment (aka. temporal crop during sampling a clip from the full input video) inside the augmentation pipeline. The last finding significantly increases the difficulty of the auxiliary task.</p><p>The other direction of augmenting consists of extending the background diversity of input frames by mixing the source video clips with some external spatial information. In our experiments the most impressive result have showed the MixUp augmentation <ref type="bibr" target="#b27">[28]</ref> but adopted for the video input (originally proposed in <ref type="bibr" target="#b36">[37]</ref>). It selects a randomly chosen image from the predefined set of images (commonly from ImageNet dataset <ref type="bibr" target="#b45">[46]</ref>) and mix it with a full sequence of selected frames in a video clip.</p><p>The same model quality can be also achieved by the recently proposed CrossNorm <ref type="bibr" target="#b29">[30]</ref> augmentation. To carry out the last one it is only need at source of pairs of clip mean and variance. Finally, we have chosen the CrossNorm augmentation in our pipeline due the simplicity for the end user to use that augmentation.</p><p>ii. Label noise suppression O ne more problem for training AR models is related to the induced label noise -it is possible while a network should be trained on an untrimmed dataset, like ActivityNet <ref type="bibr" target="#b41">[42]</ref>. In this way there is no accurate temporal segmentation of the target action classes and the clip sampling procedure can select background frames from the video thereby producing the sample with an incorrect label (aka. induced label noise -see <ref type="figure" target="#fig_1">Figure 2</ref>). According to the our observations the output label noise can exceeds more than 90% in a worse case. There are many solutions have been developed to tackle it, like MARVEL <ref type="bibr" target="#b49">[50]</ref> and PRISM <ref type="bibr" target="#b50">[51]</ref>. Unfortunately the mentioned methods reduce the quality in case of relatively clean datasets (e.g. UCF-101 <ref type="bibr" target="#b40">[41]</ref>) due to the inability to distinguish the noisy sample from the difficult one. The last thing does not allow us to use them in the universal framework suitable for all general use cases.</p><p>In our opinion, the more elegant way to tackle the mentioned problem is to design the procedure for the careful clip selection instead of fixing a label of an already sampled clip. In light of this, we have designed the Adaptive Clip Selection (ACS) procedure. The main idea of the method is to permanently collect the predicted accuracy of each sampled clip and then sample a new one according to the probability of temporal segment to be a correct carrier of a target action category.</p><p>More formally lets assume that F = {i : i ? 1, N} is set of frame indices in some video and N is a number of frames in it. The frame sampling procedure aims to select some continues subset of frames with length n. We can split the F into continuous segments S 1 , S 2 , ..., S N?n+1 , where S i = {k : i ? k &lt; i + n}. Additionally, lets associate each frame in a video with some positive score:</p><formula xml:id="formula_0">V = {v i : i ? 1, N, v i ? 0}. Hav- iii</formula><note type="other">Training VI EXPERIMENTAL RESULTS</note><p>ing the probabilities P i of selecting of each segment S i we can sample it from the defined above multinomial distribution. In this way P i can be expressed as follow:</p><formula xml:id="formula_1">P i = ? k?S i v k I[v k ? 0] + 1 ? t ? k?S t v k I[v k ? 0] + 1<label>(1)</label></formula><p>During the training procedure we store the predicted scores of classes after each forward pass. Lets assume the segment S i has been predicted as class y i with the probability P i . The true label of segment S i is y i . To enable the collection of statistics we update the associated with frames scores according to the following equation:</p><formula xml:id="formula_2">v new k = v old k + (I[y i = y i ] ? I[y i = y i ])P i , k ? S i (2)</formula><p>In practice we enable ACS for each video sample in dataset independently and do it after collecting the sufficient statistics of frames -number of frames with non-zero score should be not less than some threshold (70% in our experiments). Before that the ACS is disabled and clip sampling is performed uniformly. Note that collecting statistics is performed from the beginning to the end of training (except the very first epoch to omit the potential volatility of the beginning). Also we have experimented with labeling the sample as ignore (or negative) in case of all frames in a video predicted as incorrect (all v i &lt; 0) but the quality was worse because the method faced with the same as previous label correction methods problem -the mentioned above dilemma of a sample difficulty or a label noise.</p><p>iii. Training T he network training procedure is performed in multi-stage manner to preserve the pre-trained weights (see section i) and speed up training at all. Training stages are as follow:</p><p>1. Training the network head only. During this stage we freeze the whole backbone parameters (all batch norms in backbone are switched to the inference mode) and train the head only with high learning rate (10 ?2 ). That training allows to roughly optimize the class centroids and prevents the future gradient explosion after enabling the gradient propagation for all model parameters.</p><p>2. On the next stage we enable learning rate WarmUp <ref type="bibr" target="#b51">[52]</ref>. It increases the learning rate from 10 ?5 up to 10 ?3 by cosine schedule. It allows to sew together the gradients of the head and the rest network.</p><p>3. Finally the training procedure is switched to the default one.</p><p>The main training procedure is performed by cosing learning rate schedule starting from 10 ?3 and ending with 10 ?5 . We use slightly modified version of cosine schedule to increase a fraction of the time on a high learning rate. Practically it prevents from strong overfitting on the late phase of training in case of limitedsize datasets. The learning rate at the t-th iteration is given by the following expression:</p><formula xml:id="formula_3">? t = ? min + 1 2 (? max ? ? min )(1 + cos T cur T max ? ?</formula><p>(3) where ? min and ? max are the lower and upper bounds respectively for the learning rate. T cur represents the current iteration and T max is maximum number of iterations. The parameter ? allows us to control the fraction of high learning rate phase -in our experiments we have fixed it to the 1.5 value.</p><p>Furthermore, it was found that training with the recently proposed Adaptive Gradient Clipping (AGC) <ref type="bibr" target="#b52">[53]</ref> allows us to not only speed up training but increase the model accuracy too. In all experiments below we set it enabled for our network. The final network is trained on two GPUs by 12 clips per node with SGD optimizer and weight decay regularization using the PyTorch framework <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. Experimental Results</head><p>i. Data W e conduct experiments on several commonlyused benchmarks for general action and gesture recognition. The listed below datasets allow us to validate the proposed architecture in different scenarios: trimmed and untrimmed general purpose action recognition and continuous hand gesture recognition. As it was described early the first category checks the ability to learn the appearance-based action recognition while the last one -the ability to model the motion component of actions.</p><p>? UCF-101 <ref type="bibr" target="#b40">[41]</ref> is a middle-size general-purpose action recognition dataset of trimmed action videos, splitted on the 101 action categories. The total number of samples is 13320 and there are three train-val splits provided. As many other researchers we report the results on the first split only to reduce the training time. ? ActivityNet-200 <ref type="bibr" target="#b41">[42]</ref> is a middle-size generalpurpose action recognition dataset. Unlike the previous dataset the source videos are untrimmed and split on 200 action categories. Note that the dataset is designed to solve the temporal action detection/segmentation task mostly but we adopt it for action recognition purposes (see the next section for more details). The total number of available for downloading video instances is 17196. ? Jester-27 <ref type="bibr" target="#b54">[55]</ref> is large-size dataset of labeled video clips that shows humans performing pre-defined hand gestures. The dataset consists of 148092 unique video clips splitted on the 27 gesture categories.</p><p>ii. Evaluation Protocol T he main protocol to measure the performance of action recognition algorithms consists of top-1 and top-5 accuracies. The last metric is used to flatten the perturbations in case of noisy labels in the annotation. However as it was mentioned in the paper <ref type="bibr" target="#b36">[37]</ref> the transition to ML-based heads allows us to measure more noise resistant metric, like rank mAP (for more details see the reported above paper). In this paper we measure it too.</p><p>Another question is related to the choice of a procedure for the clip selection from an input video during the testing phase. Commonly used approach is to split the input video onto 10 temporal segments and apply a AR network for each segment independently. Sometimes AR network is run for several crops inside each temporal segment. For the end user it means that the reported performance metrics in terms of GFlops should be multiplied onto 30 (10 segments ? 3 crops). The mentioned method is suitable for the academic community (to show the best quality) but not for a business. To report the fair performance-accuracy pairs we follow the reduced protocol -single central spatio-temporal crop for each input video (for comparison purposes we report the metrics for both protocols in the section iii).</p><p>Note, the ActivityNet data is untrimmed, so the single temporal crop may not reflect the target class due to temporal mismatch with ground-truth (see the section ii). So, the 10-temporal-crop protocol is considered as a primary one for the ActivityNet benchmark.</p><p>One more possible difference in measurements is related to the frame sampling procedure. For the generalpurpose datasets it is reasonable to sample frames inside a segment uniformly to achieve the best temporal coverage. Unfortantely, the discussed method is not suitable for the real-time applications (e.g. hand gesture recognition) which process frames on-the-fly and does not have an access to the future frames. In the paper <ref type="bibr" target="#b36">[37]</ref> the continues protocol has been proposed -sampling the frames during the train and test phases with a fixed frame-rate. In the paper we follow the same continues protocol (the comparison between protocols can be found in the iii section). iii. Ablation study B elow we present the ablation study of the proposed framework in terms of the frame sampling strategy (comparison between sparse and continues protocols during the inference), the pre-training method and the influence of the introduced ACS module.</p><p>Frame sampling strategy. First, we would like to compare the frame sampling strategies. Depending on the target use case there are two possible solutions described early: sparse sampling and fixed frame rate sampling (or just with fixed temporal stride). The first strategy assumes the low impact of an ratio between a video and the network input lengths but is not suitable for the live demo mode due to inability to see the future frames. Recently the sparse sampling method <ref type="bibr" target="#b26">[27]</ref> allowed the authors to show the magnificent performance with single temporal crop. Our experiments (see the <ref type="table" target="#tab_1">Table 1</ref>) demonstrate that two-path network with separated global context branch is able to reduce the impact of an sampling strategy. Moreover we see the improvement in case of single crop for the Activi-tyNet dataset. In our opinion it is because the target clip with a valid action is significantly shorter than the full video and as a result the sparse sampling strategy collects a scarce number of valid frames. Generally speaking, the LIGAR framework allows us to close the question of the frame sampling strategy in case of live demo applications. Additionally, we have compared here the difference between testing protocols, especially the single-or multi-view video prediction. As it is expected increasing the number of views per an video improves the accuracy metric due to smoothing the impact of possible temporal mismatch between the unknown ground truth and the tested central temporal crop. Unfortunately most of papers report the multi-view metrics to get the best results and it is not suitable for the live demo scenario. Regarding the reported results the difference between the two testing protocols is comfortable with the exception of ActivityNet dataset (the reason of that is described early).</p><p>Network initialization. Another important question is about the network initialization and noteworthiness of a pre-training stage. Originally, pre-training is designed to transfer the knowledge from the big datasets to the small one thereby reducing a harmful effect of over-fitting. For the 3D-based networks the initialization have two sources: 2D initialization of a backbone only before the inflating procedure (see the I3D <ref type="bibr" target="#b4">[5]</ref> paper for more details) and direct 3D initialization of a full network. In the <ref type="table" target="#tab_2">Table 2</ref> we have summarized the potential strategies of the network initialization. Note, the last line is different from the previous one by enabling the mentioned early multi-head pre-training on the merged dataset. Overall, the model behavior reflects the intuition behind it -the method with more data during the pre-training stage exceeds the previous one in terms of all measured metrics.</p><p>Induced noise suppression. The last question is related to the suppression of the described early induced label noise. The main benchmark to measure the importance of the proposed Adaptive Clip Selection (ACS) module is ActivityNet dataset. As it was mentioned before we do not follow the original ActivityNet protocol and measure the action recognition metrics instead of localization one. Unfortunately we cannot measure the real performance of the model on this dataset due to the lack of an accurate temporal annotation of target actions and it is expected that the real quality is higher than it is announced. Nevertheless, we can see in the <ref type="table" target="#tab_3">Table 3</ref> the improvement over the baseline by using the proposed ACS module. Furthermore, the improvement is observed for the initially clean dataset like UCF-101. In our opinion, it is because the impact of induced label noise is much stronger than it is commonly believed even for the clean popular datasets. iv. Comparison with the State-of-the-Arts W e further demonstrate the advances of our proposed LIGAR framework in comparison with state-of-the-art methods for the generalpurpose action recognition. For fair comparison all methods use the RGB modality as an input. We report both results for possible testing protocols -1 and 10 temporal crops (spatial crops do not benefit in our experiments). In the <ref type="table" target="#tab_4">Table 4</ref> we have collected the best solutions in term of accuracy on two sufficiently different datasets.</p><p>Results on UCF-101. We first verify the appearance modeling ability on UCF-101. The model can achieve the superior results compared with other methods which are computationally more expensive. For example the BERT-like solution <ref type="bibr" target="#b22">[23]</ref> is ?32 times more expensive in term of GFLOPs but the accuracy drop is only 5% of top-1 metric. In case of equal testing protocol (10 views per video) the drop is slightly less. In comparison to the solution with a similar computation budget <ref type="bibr" target="#b57">[58]</ref> the accuracy of the reported approach is We also compare with other methods on Jester-27 to verify the model ability to make the prediction according to the motion component. Comparing with results on general dataset the gap between heavy solutions and the proposed one is even less. Specifically, we can observe that our proposed method drops less than 2 percentage in comparison to the SOTA solution <ref type="bibr" target="#b55">[56]</ref>. Like for the previous dataset the advantage in terms of a computation budget even more than 53 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. Conclusion</head><p>T his paper has presented the extension to the LGD framework for more efficient and accurate solving an action recognition problem for a wide range of applications, like general (appearancebased) and gesture (motion-based) recognition problems. Moreover, the paper proposed a novel clips selection module to tackle the induced label noise issues. The described training pipeline allows us to train a robust DL-based solution which is able to solve most of real-world action recognition problems in a fast and accurate manner. The reported results give us the hope that DL-based solutions will continue advance on the rest vital challenges of a humanity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overall network design. The model consists of Local and Global paths with intermediate fusion modules like in LGD network<ref type="bibr" target="#b14">[15]</ref>. The Local path is built upon X3D MobileNet-V3-Large backbone architecture. The Global path follows the original LGD design but with Global Context (GC) blocks instead of Global Average Pooling (GAP) operator. The lightweight head is represented by GAP operator and 1 ? 1 ? 1 convolution module. The network output is l 2 normalized to enable metric-learning losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Example of invalid clip sampling. F 1 ...F N -source video, F l ...F k -ground-truth borders of a target action clip, F m ...F n -incorrectly sampled action clip (according to the common used uniform sampling strategy).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :1 segment 10 segments 1 segment 10 segments 1 segment 10 segments</head><label>1</label><figDesc>The pivot table of different frame sampling strategies and test protocols. For each combination the Top-1 accuracy is reported.</figDesc><table><row><cell>Sampler</cell><cell></cell><cell>UCF-101</cell><cell cols="2">ActivityNet-200</cell><cell></cell><cell>Jester-27</cell></row><row><cell>sparse</cell><cell>93.79</cell><cell>94.71</cell><cell>59.94</cell><cell>74.32</cell><cell>95.25</cell><cell>95.68</cell></row><row><cell>continuous</cell><cell>93.63</cell><cell>94.85</cell><cell>64.19</cell><cell>75.27</cell><cell>95.43</cell><cell>95.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The ablation study of the backbone initialization on the UCF-101 dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Top-1 Top-5 mAP</cell></row><row><cell>from scratch</cell><cell>64.05</cell><cell>87.97 57.33</cell></row><row><cell>ImageNet (2D init)</cell><cell>71.90</cell><cell>90.67 77.74</cell></row><row><cell>Kinetics (3D init)</cell><cell>92.81</cell><cell>98.76 96.88</cell></row><row><cell cols="2">Kinetics+Ytb8M (3D init) 93.63</cell><cell>99.10 97.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The comparison of using ACS method on UCF-101 and ActivityNet-200 datasets. Note the metrics on the ActivityNet dataset is specified for the testing protocol with 10 temporal segments.</figDesc><table><row><cell>Method</cell><cell cols="3">UCF-101 Top-1 Top-5 mAP Top-1 Top-5 mAP ActivityNet-200</cell></row><row><cell cols="2">w/o ACS 93.52</cell><cell>99.07 97.50 74.89</cell><cell>92.77 73.33</cell></row><row><cell>w/ ACS</cell><cell>93.63</cell><cell>99.10 97.82 75.27</cell><cell>92.70 73.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison the proposed LIGAR framework with SOTA solutions on UCF-101 and Jester-27 datasets. For fairness we report the number of views for each measure if it is specified.</figDesc><table><row><cell>Name</cell><cell>Input frames</cell><cell>Views</cell><cell>Single GFLOPs</cell><cell>MParams</cell><cell cols="6">UCF-101 Top-1 Top-5 mAP Top1 Top-5 mAP Jester-27</cell></row><row><cell>R(2+1)D-BERT [23]</cell><cell>64</cell><cell>-</cell><cell>152.97</cell><cell>66.67</cell><cell>98.69</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LGD-3D RGB [15]</cell><cell>16</cell><cell>15</cell><cell>-</cell><cell>-</cell><cell>97.00</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PAN ResNet101 [56]</cell><cell>32</cell><cell>2</cell><cell>251.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">97.40 99.90</cell><cell>-</cell></row><row><cell>STM [57]</cell><cell>16</cell><cell>10</cell><cell>66.5</cell><cell>22.4</cell><cell>96.20</cell><cell>-</cell><cell>-</cell><cell cols="2">96.70 99.90</cell><cell>-</cell></row><row><cell>3D-MobileNetV2 1.0x [58]</cell><cell>16</cell><cell>10</cell><cell>0.45</cell><cell>3.12</cell><cell>81.60</cell><cell>-</cell><cell>-</cell><cell>94.59</cell><cell>-</cell><cell>-</cell></row><row><cell>LIGAR (our)</cell><cell>16</cell><cell>1 10</cell><cell>4.74</cell><cell>4.47</cell><cell>93.63 94.85</cell><cell cols="5">99.10 97.82 95.43 99.45 96.95 99.50 98.61 95.56 99.52 97.33</cell></row><row><cell>significantly better.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Results on Jester-27.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1801.04381</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1905.02244</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning generalisable omni-scale representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<idno>abs/1910.06827</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.07750</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno>abs/1711.10305</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1711.11248</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno>abs/1712.04851</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">X3D: expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno>abs/2004.04730</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A short note on the kinetics-700 human action dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1907.06987</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Youtube-8m: A large-scale video classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">References</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References S</surname></persName>
		</author>
		<idno>abs/1609.08675</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>abs/1608.00859</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/1811.08383</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/1711.08496</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal representation with local and global diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno>abs/1906.05571</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">C3D: generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1412.0767</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1604.06573</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">C3D: generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1412.0767</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1812.03982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Assemblenet: Searching for multi-stream neural connectivity in video architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<idno>abs/1905.13209</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient spatialtemporal context modeling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2103.11190</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>CoRR abs/1711.07971</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Late temporal modeling in 3d CNN architectures with BERT for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kalfaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alatan</surname></persName>
		</author>
		<idno>abs/2008.01232</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1707.01083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Shufflenet V2: practical guidelines for efficient CNN architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1807.11164</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
	<note type="report_type">CoRR abs</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Movinets: Mobile video networks for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<idno>abs/2103.11511</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno>abs/1710.09412</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Selfnorm and crossnorm for outof-distribution robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno>abs/2102.02811</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Continuous dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno>abs/1911.12675</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Informative dropout for robust representation learning: A shape-bias perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2008.04254</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Focuseddropout for convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR abs/2103.15425</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno>abs/1706.00384</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A new training framework for deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<idno>CoRR abs/2103.07350 (2021) REFERENCES REFERENCES</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">ASL recognition with metric-learning based lightweight network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Izutov</surname></persName>
		</author>
		<idno>abs/2004.05054</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<idno>abs/1801.05599</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Fast and accurate person reidentification with rmnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Izutov</surname></persName>
		</author>
		<idno>abs/1812.02465</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">MS-ASL: A large-scale data set and benchmark for understanding american sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R V</forename><surname>Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<idno>abs/1812.01053</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno>abs/2012.13375</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A comprehensive survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1911.02685</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Imagenet-21k pretraining for the masses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno>abs/2104.10972</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Divide and conquer the embedding space for metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tschernezki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>B?chler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<idno>abs/1906.05990</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Selfchallenging improves cross-domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/2007.02454</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning to combat noisy labels via classification margins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradic</surname></persName>
		</author>
		<idno>abs/2102.00751</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Noise-resistant deep metric learning with ranking-based instance selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<idno>abs/2103.16047</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1706.02677</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Highperformance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>abs/2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The jester dataset: A large-scale video dataset of human gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop</title>
		<imprint>
			<publisher>ICCVW</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2874" to="2882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">PAN: towards fast action recognition via learning persistence of appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gan</surname></persName>
		</author>
		<idno>abs/2008.03462</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">STM: spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1908.02486</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Resource efficient 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>K?p?kl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunduz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<idno>CoRR abs/1904.02422</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
