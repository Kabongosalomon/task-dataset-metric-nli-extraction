<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Global Self-Attention as a Replacement for Graph Convolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 14-18, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Md</roleName><forename type="first">Shamim</forename><surname>Hussain</surname></persName>
							<email>hussam4@rpi.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
							<email>zaki@cs.rpi.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharmashankar</forename><surname>Subramanian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Md</roleName><forename type="first">Shamim</forename><surname>Hussain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharmashankar</forename><surname>Subramanian</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Rensselaer Polytechnic Institute Troy</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Rensselaer Polytechnic Institute Troy</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">IBM T. J. Watson Research Center Yorktown Heights</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Global Self-Attention as a Replacement for Graph Convolution</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD &apos;22)</title>
						<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD &apos;22) <address><addrLine>Washington</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">August 14-18, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539296</idno>
					<note>ACM Reference Format: 2022. Global Self-Attention as a Replacement for Graph Convolution. In, DC, USA. ACM, New York, NY, USA, 11 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Neural networks; Artificial in- telligence KEYWORDS graph neural networks</term>
					<term>graph representation learning</term>
					<term>self-attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an extension to the transformer neural network architecture for general-purpose graph learning by adding a dedicated pathway for pairwise structural information, called edge channels. The resultant framework -which we call Edge-augmented Graph Transformer (EGT) -can directly accept, process and output structural information of arbitrary form, which is important for effective learning on graph-structured data. Our model exclusively uses global self-attention as an aggregation mechanism rather than static localized convolutional aggregation. This allows for unconstrained long-range dynamic interactions between nodes. Moreover, the edge channels allow the structural information to evolve from layer to layer, and prediction tasks on edges/links can be performed directly from the output embeddings of these channels. We verify the performance of EGT in a wide range of graph-learning experiments on benchmark datasets, in which it outperforms Convolutional/Message-Passing Graph Neural Networks. EGT sets a new state-of-the-art for the quantum-chemical regression task on the OGB-LSC PCQM4Mv2 dataset containing 3.8 million molecular graphs. Our findings indicate that global selfattention based aggregation can serve as a flexible, adaptive and effective replacement of graph convolution for general-purpose graph learning. Therefore, convolutional local neighborhood aggregation is not an essential inductive bias.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>structural information in graphs, they are powerful tools for compact and intuitive representation of data originating from a very wide range of sources. However, this flexibility comes at the cost of added complexity in processing and learning from graph-structured data, due to the arbitrary nature of the interconnectivity of the nodes. Recently the go-to solution for deep representation learning on graphs has been Graph Neural Networks (GNNs) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34]</ref>. The most commonly used GNNs follow a convolutional pattern whereby each node in the graph updates its state based on that of its neighbors <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b41">42]</ref> in each layer. On the other hand, the pure self-attention based transformer architecture <ref type="bibr" target="#b37">[38]</ref> has displaced convolutional neural networks for more regularly arranged data, such as sequential (e.g., text) and grid-like (images) data, to become the new state-of-the-art, especially in large-scale learning. Transformers have become the de-facto standard in the field of natural language processing, where they have achieved great success in a wide range of tasks such as language understanding, machine translation and question answering. The success of transformers has translated to other forms of unstructured data in different domains such as audio <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28]</ref> and images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref> and also on different (classification/generation, supervised/unsupervised) tasks.</p><p>Transformers differ from convolutional neural networks in some important ways. A convolutional layer aggregates a localized window around each position to produce an output for that position. The weights that are applied to the window are independent of the input, and can therefore be termed as static. Also, the sliding/moving window directly follows the structure of the input data, i.e., the sequential or grid-like pattern of positions. This is an apriori assumption based on the nature of the data and how it should be processed, directly inspired by the filtering process in signal processing. We call this assumption the convolutional inductive bias. On the other hand, in the case of a transformer encoder layer, the internal arrangement of the data does not directly dictate how it is processed. Attention weights are formed based on the queries and the keys formed at each position, which in turn dictate how each position aggregates other positions. The aggregation pattern is thus global and input dependent, i.e., it is dynamic. The positional information is treated as an input to the network in the form of positional encodings. In their absence, the transformer encoder is permutation equivariant and treats the input as a multiset. Information is propagated among different positions only via the global self-attention mechanism, which is agnostic to the internal arrangement of the data. Due to this property of global self-attention, distant points in the data can interact with each other as efficiently as nearby points. Also, the network learns to form appropriate aggregation patterns during the training process, rather than being constrained to a predetermined pattern.</p><p>Although it is often straightforward to adopt the transformer architecture for regularly structured data such as text and images by arXiv:2108.03348v3 <ref type="bibr">[cs.</ref>LG] 3 Jun 2022 <ref type="figure">Figure 1</ref>: A conceptual demonstration of Graph Convolution (left) and Global Self-Attention (right). It takes three stages of convolution for node 0 to aggregate node 6. With global self-attention, the model can learn to do so in a single step. The attention heads are formed dynamically for a given graph.</p><p>employing an appropriate positional encoding scheme, the highly arbitrary nature of structure in graphs makes it difficult to represent the position of each node only in terms of positional encodings. Also, it is not clear how edge features can be incorporated in terms of node embeddings. For graph-structured data, the edge/structural information can be just as important as the node information, and thus we should expect the network to process this information hierarchically, just like the node embeddings. To facilitate this, we introduce a new addition to the transformer, namely residual edge channels -a pathway that can leverage structural information. This is a simple yet powerful extension to the transformer framework in that it allows the network to directly process graph-structured data. This addition is also very general in the sense that it facilitates the input of structural information of arbitrary form, including edge features, and can handle different variants of graphs such as directed and weighted graphs in a systematic manner. Our framework can exceed the results of widely used Graph Convolutional Networks on datasets of moderate to large sizes, in supervised benchmarking tasks while maintaining a similar number of parameters. But our architecture deviates significantly from convolutional networks in that it does not impose any strong inductive bias such as the convolutional bias, on the feature aggregation process. We rely solely on the global self-attention mechanism to learn how best to use the structural information, rather than constraining it to a fixed pattern. Additionally, the structural information can evolve over layers and the network can potentially form new structures. Any prediction on the structure of the graph, such as link prediction or edge classification, can be done directly from the outputs of edge channels. However, these channels do add to the quadratic computational and memory complexity of global self-attention, with respect to the number of nodes, which restricts us to moderately large graphs. In addition to the edge channels, we generalize GNN concepts like gated aggregation <ref type="bibr" target="#b3">[4]</ref>, degree scalers <ref type="bibr" target="#b11">[12]</ref> and positional encodings <ref type="bibr" target="#b14">[15]</ref> for our framework.</p><p>Our experimental results indicate that given enough data and with the proposed edge channels, the model can utilize global selfattention to learn the best aggregation pattern for the task at hand. Thus, our results indicate that following a fixed convolutional aggregation pattern whereby each node is limited to aggregating its closest neighbors (based on adjacency, distance, intimacy, etc.) is not an essential inductive bias. With the flexibility of global selfattention, the network can learn to aggregate distant parts of the input graph in just one step as illustrated in <ref type="figure">Fig. 1</ref>. Since this pattern is learned rather than being imposed by design, it increases the expressivity of the model. Also, this aggregation pattern is dynamic and can adapt to each specific input graph. Similar findings have been reported for unstructured data such as images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32]</ref>. Some recent works have reported global self-attention as a means for better generalization or performance by improving the expressivity of graph convolutions <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b39">40]</ref>. Very recently, Graphormer <ref type="bibr" target="#b42">[43]</ref> performed well on graph level prediction tasks on molecular graphs by incorporating edges with specialized encodings. However, it does not directly process the edge information and therefore does not generalize well to edge-related prediction tasks. By incorporating the edge-channels, we are the first to propose global self-attention as a direct and general replacement for graph convolution for node-level, link(edge)-level and graph-level prediction, on all types of graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In relation to our work, we discuss self-attention based GNN models, where the attention mechanism is either constrained to a local neighborhood (local self-attention) of each node or unconstrained over the whole input graph (global self-attention). Methods like Graph Attention Network (GAT) <ref type="bibr" target="#b38">[39]</ref> and Graph Transformer (GT) <ref type="bibr" target="#b13">[14]</ref> constrain the self-attention mechanism to local neighborhoods of each node only, which is reminiscent of the graph convolution/local message-passing process. Several works have attempted to adopt the global self-attention mechanism for graphs as well. Graph-BERT <ref type="bibr" target="#b44">[45]</ref> uses a modified transformer framework on a sampled linkless subgraph (i.e., only node representations are processed) around a target node. Since the nodes do not inherently bear information about their interconnectivity, Graph-BERT uses several types of relative positional encodings to embed the information about the edges within a subgraph. Graph-BERT focuses on unsupervised representation learning by training the model to predict a single masked node in a sampled subgraph. GROVER <ref type="bibr" target="#b32">[33]</ref> used a modified transformer architecture with queries, keys and values produced by Message-Passing Networks, which indirectly incorporate the input structural information. This framework was used to perform unsupervised learning on molecular graphs only. Graph Transformer <ref type="bibr" target="#b5">[6]</ref> and Graphormer <ref type="bibr" target="#b42">[43]</ref> directly adopt the transformer framework for specific tasks. Graph Transformer separately encodes the nodes and the relations between nodes to form a fully connected view of the graph which is incorporated into a transformer encoder-decoder framework for graph-to-sequence learning. Graphormer incorporates the existing structure/edges in the graph as an attention bias, formed from the shortest paths between pairs of nodes. It focuses on graph-level prediction tasks on molecular graphs (e.g., classification/regression on molecular graphs). Unlike these models which handle graph structure in an ad-hoc manner and only for a specific problem, we directly incorporate graph structure into the transformer model via the edge channels and propose a general-purpose learning framework for graphs based only on the global self-attention mechanism, free of the strong inductive bias of convolution. Apart from being used for node feature aggregation, attention has also been used to form metapaths in heterogeneous graphs, such as the Heterogeneous Graph Transformer (HGT) <ref type="bibr" target="#b21">[22]</ref> and the Graph Transformer network (GTN) <ref type="bibr" target="#b43">[44]</ref>. However, these works are orthogonal to ours since metapaths are only relevant in the case of heterogeneous graphs and these methods use attention specifically to combine heterogeneous edges, over multiple hops. We focus only on homogeneous graphs, but more importantly, we use attention as a global aggregation mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NETWORK ARCHITECTURE 3.1 Preliminaries</head><p>The transformer architecture was proposed by Vaswani et al. <ref type="bibr" target="#b37">[38]</ref> as a purely attention-based model. The transformer encoder uses selfattention to communicate information between different positions, and thus produces the output embeddings for each position. In the absence of positional encodings, this process is permutation equivariant and treats the input embeddings as a multiset.</p><p>Each layer in the transformer encoder consists of two sublayers. The key component of the transformer is the multihead selfattention mechanism which takes place in the first sublayer, which can be expressed as:</p><formula xml:id="formula_0">Attn(Q, K, V) =?V (1) Where,? = softmax QK ?? (2)</formula><p>where Q, K, V are the keys, queries and values formed by learned linear transformations of the embeddings and is the dimensionality of the queries and the keys.? is known as the (softmax) attention matrix, formed from the scaled dot product of queries and keys. This process is done for multiple sets of queries, keys and values, hence the name multihead self-attention. The second sublayer is the feedforward layer which serves as a pointwise non-linear transformation of the embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Edge-augmented Graph Transformer (EGT)</head><p>The EGT architecture ( <ref type="figure" target="#fig_0">Fig. 2</ref>) extends the original transformer architecture. The permutation equivariance of the transformer is ideal for processing the node embeddings in a graph because a graph is invariant under the permutation of the nodes, given that the edges are preserved. We call the residual channels present in the original transformer architecture node channels. These channels transform a set of input node embeddings {? 0 1 , ? 0 2 , ..., ? 0 } into a set of output node embeddings (? ) (for 1 ? ? ), where ? ? ? R ? , ? is the node embeddings dimensionality, is the number of nodes, and is the number of layers. Our contribution to the transformer architecture is the introduction of edge channels, which start with an embedding for each pair of nodes. Thus, there are ? input edge embeddings 0 11 , 0 12 , ..., 0 1 , 0 21 , ..., 0 where, ? R , is the edge embeddings dimensionality. The input edge embeddings are formed from graph structural matrices and edge features. We define a graph structural matrix as any matrix with dimensionality ? , which can completely or partially define the structure of a graph (e.g., adjacency matrix, distance matrix). The edge embeddings are updated by EGT in each layer and finally, it produces a set of output edge embeddings ( ) (for 1 ? , ? ) from which structural predictions such as edge labeling and link prediction can be performed.  <ref type="formula">(1)</ref> and <ref type="formula">(2)</ref> we see that the attention matrix is comparable with a row-normalized adjacency matrix of a directed weighted complete graph. It dictates how the node features in a graph are aggregated, similarly to GCN <ref type="bibr" target="#b23">[24]</ref>. Unlike the input graph, this graph is dynamically formed by the attention mechanism. However, the basic transformer does not have a direct way to incorporate the input structure (existing edges) while forming these weighted graphs, i.e., the attention matrices. Also, these dynamic graphs are collapsed immediately after the aggregation process is done. To remedy the first problem we let the edge channels participate in the aggregation process as follows (as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>) -in the ?'th layer and for the 'th attention head,</p><formula xml:id="formula_1">Attn(Q ,? ? , K ,? ? , V ,? ? ) =? ,? V ,? ? (3) Where,? ,? = softmax(? ,? ) ? (G ,? )<label>(4)</label></formula><p>Where,? ,? = clip</p><formula xml:id="formula_2">Q ,? ? (K ,? ? ) ?? + E ,?<label>(5)</label></formula><p>where ? denotes elementwise product. E ,? , G ,? ? R ? are concatenations of the learned linear transformed edge embeddings. E ,? is a bias term added to the scaled dot product between the queries and the keys. It lets the edge channels influence the attention process. G ,? drives the sigmoid (?) function and lets the edge channels also gate the values before aggregation, thus controlling the flow of information between nodes. The scaled dot product is clipped to a limited range which leads to better numerical stability (we used [?5, +5]). To ensure that the network takes advantage of the full-connectivity the attention process is randomly masked by adding ?? to the inputs to the softmax with a small probability during training (i.e., random attention masking). Another approach is to apply dropout <ref type="bibr" target="#b36">[37]</ref> to the attention matrix.</p><p>To let the structural information evolve from layer to layer, the edge embeddings are updated by a learnable linear transformation of the inputs to the softmax function. The outputs of the attention heads are also mixed by a linear transformation. To facilitate training deep networks, Layer Normalization (LN) <ref type="bibr" target="#b0">[1]</ref> and residual connections <ref type="bibr" target="#b18">[19]</ref> are used. We adopted the Pre-Norm architecture whereby normalization is done immediately before the weighted sublayers <ref type="bibr" target="#b40">[41]</ref> rather than after, because of its better optimization characteristics. So,? ? = LN(? ??1 ),?? = LN( ??1 ). The residual updates can be expressed in an elementwise manner as:</p><formula xml:id="formula_3">? ? = ? ??1 + O ? ? =1 ?? =1? ,? (V ,??? ) (6) ? = ??1 + O ? =1? ,?<label>(7)</label></formula><p>Here, ? denotes concatenation. O ? ? ? R ? ? ? and O ? ? R ? are the learned output projection matrices, with edge embeddings dimensionality and attention heads.</p><p>The feed-forward sublayer following the attention sublayer consists of two consecutive pointwise fully connected linear layers with a non-linearity such as ELU <ref type="bibr" target="#b9">[10]</ref> in between. The updated em-</p><formula xml:id="formula_4">beddings are ? ? =? ? + FFN ? h (LN(? ? )), ? =?? + FFN ? e (LN(?? )).</formula><p>The Pre-Norm architecture also ends with a layer normalization over the final embeddings as (? ) = LN(? ), ( ) = LN( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dynamic Centrality Scalers</head><p>The attention mechanism in equation <ref type="formula">(3)</ref> is a weighted average of the gated node values, which is agnostic to the degree of the nodes. However, we may want to make the network sensitive to the degree/centrality of the nodes, in order to make it more expressive when distinguishing between non-isomorphic (sub-)graphs, similar to GIN <ref type="bibr" target="#b41">[42]</ref>. While this can be achieved by directly encoding the degrees of the nodes as an additional input like <ref type="bibr" target="#b42">[43]</ref>, we aimed for an approach that is adaptive to the dynamic nature of self-attention. Corso et al. <ref type="bibr" target="#b11">[12]</ref> propose scaling the aggregated values by a function of the degree of the node, more specifically a logarithmic degree scaler. But it is tricky to form a notion of degree/centrality for the dynamically formed graph represented by the attention matrix because this row-normalized matrix bears no notion of degree. In our network, the sigmoid gates control the flow of information to a particular node which are derived from the edge embeddings. So we use the sum of the sigmoid gates as a measure of centrality for a node and scale the aggregated values by the logarithm of this sum. With centrality scalers, equation <ref type="formula">(6)</ref> becomes:</p><formula xml:id="formula_5">? ? = ? ??1 + O ? ? =1 , ?? =1? ,? (V ,??? )<label>(8)</label></formula><p>Where,</p><formula xml:id="formula_6">, = ln 1 + ?? =1 (G ,? e , )<label>(9)</label></formula><p>Here, , is the centrality scaler for node , for attention head at layer ?. As pointed out by Ying et al. <ref type="bibr" target="#b42">[43]</ref>, with the addition of a centrality measure the global self-attention mechanism becomes at least as powerful as the 1-Weisfeiler-Lehman (1-WL) isomorphism test and potentially even more so, due to aggregation over multiple hops. Note that commonly used convolutional GNNs like GIN are at most as powerful as the 1-WL isomorphism test <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SVD-based Positional Encodings</head><p>While applying the transformer on regularly arranged data such as sequential (e.g., text) and grid-like (e.g., images) data it is customary to use sinusoidal positional encodings introduced by Vaswani et al. <ref type="bibr" target="#b37">[38]</ref>. However, the arbitrary nature of structure in graphs makes it difficult to devise a consistent positional encoding scheme. Nonetheless, positional encodings have been used for GNNs to embed global positional information within individual nodes and to distinguish isomorphic nodes and edges <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36]</ref>. Inspired by matrix factorization based node embedding methods for graphs <ref type="bibr" target="#b2">[3]</ref>, Dwivedi et al. <ref type="bibr" target="#b14">[15]</ref> proposed to use the smallest non-trivial eigenvectors of the Laplacian matrix of the graph as positional encodings. However, since the Laplacian eigenvectors can be complex-valued for directed graphs, this method is more relevant for undirected graphs which have symmetric Laplacian matrices. To remedy this we propose a method, that is more general and applies to all variants of graphs (e.g., directed, weighted). We propose a form of positional encoding based on precalculated SVD of the graph structural matrices. We use the largest singular values and corresponding left and right singular vectors to form our positional encodings. We use the adjacency matrix A (with self-loops) as the graph structural matrix, but it can be generalized to other structural matrices since the SVD of any real matrix produces real singular values and vectors.</p><formula xml:id="formula_7">A SVD ? U?V = (U ? ?) ? (V ? ?) =?V (10) ? =? ?V<label>(11)</label></formula><p>Where U, V ? R ? matrices contain the left and right singular vectors as columns, respectively, corresponding to the top singular values in the diagonal matrix ? ? R ? . Here, ? denotes concatenation along the columns. From <ref type="bibr" target="#b9">(10)</ref> we see that the dot product between 'th row of? and 'th row ofV can approximate A which denotes whether there is an edge between nodes and . Thus, the rows of?, namely?1,?2, ...,?, each with dimensionalit? ? R 2 , bear denoised information about the edges and can be used as positional encodings. Note that this form of representation based on the dot product is consistent with the scaled dot product attention used in the transformer framework. Since the signs of corresponding pairs of left and right singular vectors can be arbitrarily flipped, we randomly flip the signs of?during training for better generalization. Instead of directly adding?to the input embeddings of the node , we add a projection = W?, where W ? R ? ?2 is a learned projection matrix. This heuristically leads to better results. Since our architecture directly takes structure as input via the edge channels, the inclusion of positional encodings is optional for most tasks. However, positional encodings can help distinguish isomorphic nodes <ref type="bibr" target="#b45">[46]</ref> by serving as an absolute global coordinate system. Thus, they make the model more expressive.</p><p>However, the absolute coordinates may, in theory, hamper generalization, because they are specific to a particular reference frame that depends on the input graph. But in practice, we did not find any detrimental effect on the performance for any task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Embedding and Prediction</head><p>Given an input graph, both node and edge feature embeddings are formed by performing learnable linear transformations for continuous vector values, or vector embeddings for categorical/discrete values. In the case of multiple sets of features, their corresponding embeddings are added together. When positional encodings are used, they are added to the input node embeddings. The edge embeddings are formed by adding together the embeddings from the graph structural matrix and the input edge feature embeddings (when present). For non-existing edges, a masking value/vector is used in the place of an edge feature. As input structural matrix, we use the distance matrix clipped up to -hop distance, i.e., D ( ) where D ( ) ? {0, 1, ..., } are the shortest distances between nodes and , clipped to a maximum value of . We use vector embeddings of the discrete values contained in these matrices. For node and edge classification/regression tasks, we apply a few final MLP layers on the final node and edge embeddings, respectively, to produce the output. For graph-level classification/regression we adopt one of two different methods. In global average pooling method, all the output node embeddings are averaged to form a graph-level embedding, on which final linear layers are applied. In virtual nodes method, new virtual nodes with learnable input embeddings ? 0 +1 , ? 0 +2 , ..., ? 0 + are passed through EGT along with existing node embeddings. There are also different learnable edge embeddings?which are used as follows -the edge embedding between a virtual node and existing graph node is assigned 0 = 0 =?, and the edge embeddings between two virtual nodes , , are assigned 0 = 0 = 1 2 (?+?). Finally, the graph embedding is formed by concatenating the output node embeddings of the virtual nodes. This method is more flexible and better suited for larger models. The centrality scalers mentioned above are not applied to the virtual nodes, because by nature these nodes have high levels of centrality which are very different from the graph nodes. So a fixed scaler value of , = 1 is used instead for these virtual nodes.</p><p>For smaller datasets, we found that adding a secondary distance prediction objective alongside the graph-level prediction task in a multi-task learning setting serves as an effective means of regularization and thus improves the generalization of the trained model. This self-supervised objective is reminiscent of the unsupervised link prediction objective often used to pre-train GNNs to form node embeddings. In our case, we take advantage of the fact that we have output edge embeddings from the edge channels (alongside the node embeddings, which are used for graph-level prediction). We thus pass the output edge embeddings through a few (we used three) MLP layers and set the distance matrix up to -hop, D ( ) , as a categorical target. Hops greater than are ignored while calculating the loss. The loss from this secondary objective is multiplied by a small factor and added to the total loss. Note that in this case we always use the adjacency matrix, rather than the distance matrix as the input graph structural matrix so that the edge channels do not simply learn an identity transformation. We emphasize that this objective is only potentially beneficial as a regularization method for smaller datasets by guiding the aggregation process towards a Breadth-First Search pattern, which is a soft form of the convolutional bias. In the presence of enough data, the network is able to learn the best aggregation pattern for the given primary objective, which also generalizes to unseen data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND RESULTS</head><p>We evaluate the performance of our proposed EGT architecture in a supervised and inductive setting. We focus on a diverse set of supervised learning tasks, namely, node and edge classification, and graph classification and regression. We also experiment on the transfer learning performance of EGT. Datasets: In the medium-scale supervised learning setting, we experimented with the benchmarking datasets proposed by Dwivedi et al. <ref type="bibr" target="#b14">[15]</ref>, namely PATTERN (14K synthetic graphs, 44-188 nodes/graph) and CLUSTER (12K synthetic graphs, 41-190 nodes/graph) for node classification; TSP (12K synthetic graphs, 50-500 nodes/graph) for edge classification; and MNIST (70K superpixel graphs, 40-75 nodes/graph), CIFAR10 (60K superpixel graphs, 85-150 nodes/graph) and ZINC (12K molecular graphs, 9-37 nodes/graph) for graph classification/regression. To evaluate the performance of EGT at large-scale we consider the graph regression task on the PCQM4M and its updated version PCQM4Mv2 datasets <ref type="bibr" target="#b19">[20]</ref> which contain 3.8 million molecular graphs with 1-51 nodes/graph. We also experimented on tranfer learning from PCQM4Mv2 dataset to the graph classification tasks on OGB <ref type="bibr" target="#b20">[21]</ref> datasets MolPCBA (438K molecular graphs, 1-332 nodes/graph) and MolHIV (41K molecular graphs, 2-222 nodes/graph). Evaluation Setup: We use the PyTorch <ref type="bibr" target="#b29">[30]</ref> numerical library to implement our model. Training was done in a distributed manner on a single node with 8 NVIDIA Tesla V100 GPUs (32GB RAM/GPU), and 2 20-core 2.5GHz Intel Xeon CPUs (768GB RAM). Masked attention was used to process mini-batches containing graphs of different numbers of nodes. This allowed us to use highly parallel tensor operations on the GPU. The results are evaluated in terms of accuracy, F1 score, Mean Absolute Error (MAE), Average Precision (AP), or Area Under the ROC Curve (AUC), as recommended for each dataset. Hyperparameters were tuned on the validation set. Full details of hyperparameters are included in the appendix and the code is available at https://github.com/shamim-hussain/egt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Medium-scale Performance</head><p>For the benchmarking datasets, we follow the training setting suggested by Dwivedi et al. <ref type="bibr" target="#b14">[15]</ref> and evaluate the performance of EGT for a given parameter budget. Comparative results are presented in <ref type="table" target="#tab_0">Table 1</ref>. All datasets except PATTERN and CLUSTER include edge features. From the results, we see that EGT outperforms other GNNs (including GAT and GT which use local self-attention, and Graphormer which uses global self-attention but without edge channels) on all datasets except CIFAR10. We see a high level of overfitting for all models on CIFAR10, including our model which overfits the training dataset due to its higher capacity. The edge   <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref> 13.2M 0.1430 DeeperGCN-VN <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref> 25.5M 0.1398 GT <ref type="bibr" target="#b13">[14]</ref> 0.6M 0.1400 GT (bigger model) <ref type="bibr" target="#b13">[14]</ref> 83.2M 0.1408</p><p>Graphormer SMALL <ref type="bibr" target="#b42">[43]</ref> 12.5M 0.1264 Graphormer <ref type="bibr" target="#b42">[43]</ref> 47 channels allow us to use the distance prediction objective in a multitask learning setting, which helps lessen the overfitting problem on CIFAR10, ZINC and MNIST. Also, the output embeddings of the edge channels are directly used for edge classification on the TSP dataset which leads to very good results. Note that, Graphormer, which also uses global self-attention but does not have such edge channels, performs satisfactorily for other tasks but not so much on edge classification on the TSP dataset. Since we do not take advantage of the convolutional inductive bias our model shows various levels of overfitting on these medium-sized datasets. While EGT still outperforms other GNNs, we posit that it would further exceed the performance level of convolutional GNNs if more training data were present (we confirm this in the next section). Also, the results indicate that convolutional aggregation is not an essential inductive bias, and global attention can learn to make the best use of the structural information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Large-scale Performance</head><p>The results for the graph regression task on the OGB-LSC PCQM4M and PCQM4Mv2 datasets <ref type="bibr" target="#b19">[20]</ref> are presented in <ref type="table" target="#tab_1">Table 2</ref>. We show results for EGT models of small, medium and large network sizes based on number of parameters (details are included in the appendix). Note that the PCQM4M dataset was later deprecated in favor of PCQM4Mv2. So its test labels are no longer available and results are given over the validation set. We include these results for a thorough comparison with established models that report their results on the older dataset. We see that EGT achieves a much lower MAE than all the convolutional and local self-attention based (i.e., GT <ref type="bibr" target="#b13">[14]</ref>) GNNs. Its performance even exceeds Graphormer <ref type="bibr" target="#b42">[43]</ref>, which is also a global self-attention based model and can be thought of as an ablated variant of EGT with specialized encodings, such as centrality, spatial and edge encodings and requires similar training time and resources. We hypothesize that EGT gets a better result than Graphormer because of a combination of several factors, including its edge channels, unique gating mechanism and dynamic centrality scalers. Our model is currently the best performing model on the PCQM4Mv2 leaderboard. These results show the scalability of our framework and further confirm that given enough data, global self-attention based aggregation can outperform local convolutional aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transfer Learning Performance</head><p>In order to experiment on the transferability of the representations learned by EGT, we take an EGT model pre-trained on the largescale PCQM4Mv2 molecular dataset and fine-tune the weights on the OGB molecular datasets MolPCBA and MolHIV. Although the validation performance improvement seems to plateau for larger models on the PCQM4Mv2 dataset at a certain point, we found that larger pre-trained models perform better when fine-tuned on smaller datasets, so we select the largest model (EGT Larger ) with 30 layers for transfer learning experiments (it achieves a validation MAE of 0.0869 on PCQM4Mv2, same as EGT Large ). The results are presented in <ref type="table" target="#tab_3">Table 3</ref>. We see that both EGT and Graphormer achieve comparable results which exceed convolutional GNNs. Graphormer uses pre-trained models from PCQM4M and they found it essential to use the FLAG training method <ref type="bibr" target="#b24">[25]</ref> to achieve good fine-tuning results. FLAG uses an inner optimization loop to augment the node embeddings by adding adversarial perturbations to them. However, we do not use any form of specialized training during the finetuning process. This is due to two reasons -firstly, we wanted to evaluate our model in the conventional transfer learning setting where the weights of a pre-trained model are simply fine-tuned on a new dataset for a very few epochs which saves training time and resources -whereas, FLAG training takes several times longer training time with additional FLAG hyperparameter tuning. Another reason is that FLAG is an adversarial perturbation method for node embeddings and since we have both node and edge embeddings (including non-existing edges) it is not clear how this method should be adopted for our model -which requires further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>Our architecture is based upon two important ideas -global selfattention based aggregation and residual edge channels. To analyze the importance of these two features, we experiment with two ablated variants of EGT: i) EGT-Simple: incorporates global selfattention, but instead of having dedicated residual channels for edges, it directly uses a linear transformation of the input edge embeddings 0 (formed from adjacency matrix and edge features) to guide the self-attention mechanism. The absence of edge channels means that the edge embeddings are not updated from layer to layer. So, edge classification is performed by applying MLP layers on pairwise node-embeddings. It is architecturally similar  <ref type="table" target="#tab_1">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gated Attention Virtual Centrality Positional Validate Aggregation Dropout</head><p>Nodes</p><formula xml:id="formula_8">Scalers Encodings MAE ? - - - - - 0.0965 - - - - 0.0943 - - - 0.0926 - - 0.0919 - 0.0900 0.0899</formula><p>to Graphormer <ref type="bibr" target="#b42">[43]</ref>. While it is slightly less expensive in terms of computation and memory, it still scales quadratically with the number of nodes. ii) EGT-Constrained limits the self-attention process to the 1-hop neighborhood of each node, which allows us to compare global self-attention to convolutional local self-attention based aggregation. Also, it only keeps track of the edge embeddings in the edge channels if there is an edge from node to node or = (self-loop). Architecturally, this variant is similar to GT <ref type="bibr" target="#b13">[14]</ref> and can take advantage of the sparsity of the graph to reduce computational and memory costs. More details about these variants can be found in the appendix.</p><p>The results for the ablated variants are presented in <ref type="table" target="#tab_4">Table 4</ref>. We see that, EGT-Simple can come close to EGT, but is especially subpar when the targeted task is related to edges (e.g., edge classification on the TSP dataset) or when the distance objective cannot be applied (ZINC, CIFAR10) due to the lack of dedicated edge channels. Both EGT-Simple and EGT enjoy an advantage over EGT-Constrained on the large PCQM4Mv2 dataset due to their global aggregation mechanism. This indicates that given enough data, global self-attention based aggregation can outperform local selfattention based aggregation. Additionally to demonstrate the effect of the SVD based positional encodings we include results without positional encodings. Note that the positional encodings lead to a significant improvement for the ZINC and the CLUSTER datasets, but slight/no improvement in other cases. This is consistent with our statement that the positional encodings are optional for our model on some tasks, but their inclusion can often lead to a performance improvement.</p><p>To further examine the contribution of different features of our model we carried out a series of experiments on the PCQM4Mv2 dataset for the smallest EGT network. The results are presented in <ref type="table" target="#tab_5">Table 5</ref>. We see that the use of gates during aggregation leads to a significant improvement. Another important contributing factor is dropout on the attention matrix which encourages the network to take advantage of long-distance interactions. The dynamic centrality scalers also help by making the network more expressive. Virtual nodes and positional encodings lead to a more modest performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of Aggregation Patterns</head><p>To understand how global self-attention based aggregation translates to performance gains we examined the attention matrices dynamically formed by the network. These matrices dictate the weighted aggregation of the nodes and thus show how each node is looking at other nodes. This is demonstrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. We show the adjacency matrix and the distance matrix to demonstrate how far each node is looking. First, we look at an example attention matrix formed by an attention head. Next, for the sake of visualization, we merge the attention matrices for different heads together by averaging and normalizing them to values between [0 1]. We do this for two different layers at different depths of the model. Note that these patterns are specific to a particular input graph -since the aggregation process is dynamic they would be different for different inputs. To make a complete analysis of each layer's attention we also plot the weights assigned at different distances averaged over all the attention heads for all the nodes and all the graphs in a dataset. Note that a convolutional aggregation of immediate neighbors would correspond to non-zero weights being assigned to only 0/1 hop.</p><p>We see that the attention matrices for individual attention heads are quite sparse. So, the nodes are selective about where to look. For the ZINC dataset, from <ref type="figure" target="#fig_1">Fig. 3 (a)</ref>, at layer ? = 1 we see that EGT approximately follows a convolutional pattern. But as we go deeper, the nodes start to take advantage of global self-attention to look further. Finally, at ? = 10 we see highly non-local behavior. This shows why EGT has an advantage over local aggregation based convolutional networks because of its ability to aggregate global features. For PCQM4Mv2, in <ref type="figure" target="#fig_1">Fig. 3 (b)</ref>, we notice such non-local aggregation patterns starting from the lowest layers. This shows why a global aggregation based model such as EGT has a clear advantage over convolutional networks (as seen in <ref type="table" target="#tab_1">Table 2</ref>), because it would take a large number of consecutive convolutions to mimic such patterns. This non-local behavior is more subtle in TSP, where, except for the last layer, attention is mostly constrained to 1-3 hops, as seen from <ref type="figure" target="#fig_1">Fig. 3(c)</ref>. This also shows why EGT-Constrained achieves good results on this dataset <ref type="table" target="#tab_4">(Table 4</ref>). However, even the slight advantage of global aggregation gives pure EGT an edge over EGT-constrained. To conclude, the aggregation performed by our model is sparse and selective, like convolution, yet capable of being non-local and dynamic, which leads to a clear advantage over convolutional networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>We proposed a simple extension -edge channels -to the transformer framework. We preserve the key idea, namely, global attention, while making it powerful enough to take structural information of the graph as input and also to process it and output new structural information such as new links and edge labels. One of our key findings is that the incorporation of the convolutional aggregation pattern is not an essential inductive bias for GNNs and instead the model can directly learn to make the best use of structural information. We established this claim by presenting experimental results on both medium-scale, large-scale and transfer learning settings where our model achieves superior performance, beating convolutional GNNs. We also achieve a new state-of-the-art result on the large-scale PCQM4Mv2 molecular dataset. We demonstrated that the performance improvement is directly linked to the nonlocal nature of aggregation of the model. In future work, we aim to evaluate the performance of EGT in transductive, semi-supervised and unsupervised settings. Also, we plan to explore the prospect of reducing the computation and memory cost of our model to a sub-quadratic scale by incorporating linear attention <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref> and sparse edge channels.</p><p>A DATA AND CODE AVALABILITY Data: All datasets used in this work are publicly available. The medium-scale GNN benchmarking datasets by Dwivedi et al. <ref type="bibr" target="#b14">[15]</ref> are available at https://github.com/graphdeeplearning/benchmarkinggnns. The OGB-LSC <ref type="bibr" target="#b20">[21]</ref> PCQM4M and PCQM4Mv2 large-scale datasets, and the OGB datasets <ref type="bibr" target="#b19">[20]</ref> MolPCBA and MolHIV are available at https://ogb.stanford.edu. Code: The code to reproduce the results presented in this work is available at https://github.com/shamim-hussain/egt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B TRAINING METHOD AND HYPERPARAMETERS B.1 Medium-scale Experiments</head><p>For medium-scale experiments on the PATTERN, CLUSTER, MNIST, CIFAR10, TSP, and ZINC datasets we follow the benchmarking setting suggested by Dwivedi et al. <ref type="bibr" target="#b14">[15]</ref> and maintain a specified parameter budget of either 100K or 500K. The number of layers, the width of the node and the edge channels ( , ? and , correspondingly) were varied to get the best results on the validation set. We used the Adam optimizer and reduce the learning rate by a factor of 0.5 if the validation loss does not improve for a given number of epochs (Reduce LR when validation loss plateaus). We keep track of the validation loss at the end of each epoch and pick the set of weights that produces the least validation loss. No dropout or weight decay is used for a fair comparison with other GNNs. Each experiment (training and evaluation) was run 4 times with 4 different random seeds and the results were used to calculate the mean and standard deviations of the metric. The common hyperparameters and methods for all datasets are given in <ref type="table" target="#tab_6">Table 6</ref>, whereas the hyperparameters which are specific for each dataset are given in <ref type="table" target="#tab_7">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Large-scale Experiments</head><p>While training large models on the PCQM4M and PCQM4Mv2 datasets, we found it essential to use learning rate warmup. Following the warmup, we applied cosine decay to the learning rate. We used virtual nodes which is a more scalable method than global average pooling because the use of multiple virtual nodes allows the model to collect more graph-level information. Instead of random masking of the attention matrices, we applied dropout to the attention matrices, which showed better regularization performance. Attention dropout is the only regularization method used for all models. We trained all models for a fixed number (1 million) of gradient update steps. The hyperparameters are shown in <ref type="table" target="#tab_8">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Transfer Learning Experiments</head><p>We took the EGT Larger model pre-trained on the PCQM4Mv2 dataset ( <ref type="table" target="#tab_8">Table 8</ref>) and fine-tuned it on the OGB datasets MolPCBA and Mol-HIV. We used the same learning rate and warmup and cosine decay method mentioned above, although for a smaller number of total gradient update steps. Hyperparameters specific to the fine-tuning stage are shown in <ref type="table">Table 9</ref>. Other hyper hyperparameters were the same as in <ref type="table" target="#tab_8">Table 8</ref>. Each experiment (training and evaluation) was run 4 times with 4 different random seeds and the results were used to calculate the mean and standard deviations of the metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DETAILS OF ABLATED VARIANTS</head><p>For the ablation study presented in section 4.4 of the paper, we discuss here different ablation methods in more detail. EGT-Simple: EGT-simple uses global self-attention, but does not have dedicated residual channels for updating pairwise information (edges). The input edge embeddings (formed from graph structural matrix and edge features) directly participate in the aggregation process as follows:? ,? = softmax(? ,? ) ? (G ,? 0 )</p><p>Where,? ,? = clip</p><formula xml:id="formula_10">Q ,? ? (K ,? ? ) ?? + E ,? 0<label>(13)</label></formula><p>E ,? 0 , G ,? 0 ? R ? are directly formed by concatenations of the learned linear transformed input edge embeddings, i.e., E ,? 0 , G ,? 0 respectively. Also, dynamic centrality scalers are derived from 0 . The absence of edge channels means that the edge embeddings are not updated from layer to layer. So, edge classification is performed from pairwise node embeddings and input edge features. We denote this variant as EGT-Simple since it is architecturally simpler than EGT.</p><p>We use the same hyperparameters for this variant as the ones used for original EGT ( <ref type="table" target="#tab_7">Table 7</ref>, <ref type="table" target="#tab_8">Table 8</ref>; denotes only the dimensionality of the input edge embeddings) except, ? = 64, = 8 for CIFAR10, and ? = 80, = 8 for ZINC are used to make the number of parameters comparable. EGT-Constrained: EGT-Constrained is a convolutional variant of EGT which limits the self-attention process to the 1-hop neighborhood of each node. It only keeps track of the edge embeddings in the edge channels if there is an edge from node to node or = (self-loop). So, pairwise information corresponding to only the existing edges is updated by the edge channels. This architecture can be derived by taking the softmax over ? N ( ) ? { } while calculating the attention weights? ,? and limiting the aggregation  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Edge-augmented Graph Transformer (EGT)From equations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Analysis of aggregation patterns on three datasets -(a) ZINC, (b) PCQM4Mv2, (c) TSP. Left to right -adjacency (i) and distance matrices (ii), an example attention head (iii), average of attention heads in a middle layer (iv) and in a deeper layer (v) -for a particular input graph in the validation set (matrices have been cropped for the TSP dataset). On the right -weights assigned for different hops in different layers, averaged over all heads and all nodes in all the graphs in the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on 6 benchmarking datasets from Dwivedi et al.<ref type="bibr" target="#b14">[15]</ref>. Results on PATTERN and CLUSTER datasets are given in terms of weighted accuracy. Red: best model, Violet: good model; arrow next to a metric indicates whether higher or lower is better. Results not shown are not available for that method.</figDesc><table><row><cell></cell><cell cols="2">PATTERN</cell><cell>CLUSTER</cell><cell>MNIST</cell><cell>CIFAR10</cell><cell cols="2">TSP</cell><cell cols="2">ZINC</cell></row><row><cell></cell><cell cols="2">% Accuracy ?</cell><cell>% Accuracy ?</cell><cell>% Accuracy ?</cell><cell>% Accuracy ?</cell><cell cols="2">F1 ?</cell><cell cols="2">MAE ?</cell></row><row><cell></cell><cell>#Param</cell><cell>#Param</cell><cell>#Param</cell><cell>#Param</cell><cell>#Param</cell><cell>#Param</cell><cell>#Param</cell><cell>#Param</cell><cell>#Param</cell></row><row><cell>Model</cell><cell>?100K</cell><cell>?500K</cell><cell>?500K</cell><cell>?100K</cell><cell>?100K</cell><cell>?100K</cell><cell>?500K</cell><cell>?100K</cell><cell>?500K</cell></row><row><cell>GCN [24]</cell><cell>63.880 ? 0.074</cell><cell>71.892 ? 0.334</cell><cell>68.498 ? 0.976</cell><cell>90.705 ? 0.218</cell><cell>55.710 ? 0.381</cell><cell>0.630 ? 0.001</cell><cell></cell><cell>0.459 ? 0.006</cell><cell>0.367 ? 0.011</cell></row><row><cell>GraphSage [18]</cell><cell>50.516 ? 0.001</cell><cell>50.492 ? 0.001</cell><cell>63.844 ? 0.110</cell><cell>97.312 ? 0.097</cell><cell>65.767 ? 0.308</cell><cell>0.665 ? 0.003</cell><cell></cell><cell>0.468 ? 0.003</cell><cell>0.398 ? 0.002</cell></row><row><cell>GIN [42]</cell><cell>85.590 ? 0.011</cell><cell>85.387 ? 0.136</cell><cell>64.716 ? 1.553</cell><cell>96.485 ? 0.097</cell><cell>55.255 ? 1.527</cell><cell>0.656 ? 0.003</cell><cell></cell><cell>0.387 ? 0.015</cell><cell>0.526 ? 0.051</cell></row><row><cell>GAT [39]</cell><cell>75.824 ? 1.823</cell><cell>78.271 ? 0.186</cell><cell>70.587 ? 0.447</cell><cell>95.535 ? 0.205</cell><cell>64.223 ? 0.455</cell><cell>0.671 ? 0.002</cell><cell></cell><cell>0.475 ? 0.007</cell><cell>0.384 ? 0.007</cell></row><row><cell>GT [14]</cell><cell></cell><cell>84.808 ? 0.068</cell><cell>73.169 ? 0.622</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.226 ? 0.014</cell></row><row><cell>GatedGCN [4]</cell><cell>84.480 ? 0.122</cell><cell>86.508 ? 0.085</cell><cell>76.082 ? 0.196</cell><cell>97.340 ? 0.143</cell><cell>67.312 ? 0.311</cell><cell>0.808 ? 0.003</cell><cell>0.838 ? 0.002</cell><cell>0.375 ? 0.003</cell><cell>0.214 ? 0.013</cell></row><row><cell>PNA [12]</cell><cell>86.567 ? 0.075</cell><cell></cell><cell></cell><cell>97.690 ? 0.022</cell><cell>70.350 ? 0.630</cell><cell></cell><cell></cell><cell>0.188 ? 0.004</cell><cell>0.142 ? 0.010</cell></row><row><cell>DGN [2]</cell><cell>86.680 ? 0.034</cell><cell></cell><cell></cell><cell></cell><cell>72.700 ? 0.540</cell><cell></cell><cell></cell><cell>0.168 ? 0.003</cell><cell></cell></row><row><cell>Graphormer [43]</cell><cell></cell><cell>86.650 ? 0.033</cell><cell>74.660 ? 0.236</cell><cell>97.905 ? 0.176</cell><cell>65.978 ? 0.579</cell><cell></cell><cell>0.698 ? 0.007</cell><cell></cell><cell>0.122 ? 0.006</cell></row><row><cell>EGT</cell><cell cols="9">86.816 ? 0.027 86.821 ? 0.020 79.232 ? 0.348 98.173 ? 0.087 68.702 ? 0.409 0.822 ? 0.000 0.853 ? 0.001 0.143 ? 0.011 0.108 ? 0.009</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">PCQM4M</cell><cell cols="2">PCQM4Mv2</cell></row><row><cell>Model</cell><cell cols="2">#Param Validate</cell><cell>Test</cell><cell cols="2">Validate Test-dev</cell></row><row><cell>GCN [24]</cell><cell>2.0M</cell><cell>0.1684</cell><cell>0.1838</cell><cell>0.1379</cell><cell>0.1398</cell></row><row><cell>GIN [42]</cell><cell>3.8M</cell><cell>0.1536</cell><cell>0.1678</cell><cell>0.1195</cell><cell>0.1218</cell></row><row><cell>GCN-VN [16, 24]</cell><cell>4.9M</cell><cell>0.1510</cell><cell>0.1579</cell><cell>0.1153</cell><cell>0.1152</cell></row><row><cell>GIN-VN [16, 42]</cell><cell>6.7M</cell><cell>0.1396</cell><cell>0.1487</cell><cell>0.1083</cell><cell>0.1084</cell></row><row><cell>GINE-VN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Results on OGB-LSC PCQM4M and PCQM4Mv2 datasets in terms of Mean Absolute Error (lower is better). Results not shown are not available.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on OGB Mol datasets. EGT uses transfer learning from PCQM4Mv2, whereas GIN-VN and Graphormer use transfer learning from PCQM4M. AP stands for Average Precision and AUC for Area Under the ROC Curve, higher is better for both. Results not shown are not available.</figDesc><table><row><cell></cell><cell cols="2">MolPCBA</cell><cell></cell><cell>MolHIV</cell></row><row><cell>Model</cell><cell cols="4">#Param Test AP(%) #Param Test AUC(%)</cell></row><row><cell>DeeperGCN-FLAG [25, 27]</cell><cell>6.55M</cell><cell>28.42 ? 0.43</cell><cell>532K</cell><cell>79.42 ? 1.20</cell></row><row><cell>DeeperGCN-VN-FLAG</cell><cell>6.55M</cell><cell>28.42 ? 0.43</cell><cell></cell><cell></cell></row><row><cell>[16, 25, 27]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PNA [12]</cell><cell>6.55M</cell><cell>28.38 ? 0.35</cell><cell>326K</cell><cell>79.05 ? 1.32</cell></row><row><cell>DGN [2]</cell><cell>6.73M</cell><cell>28.85 ? 0.30</cell><cell>110K</cell><cell>79.70 ? 0.97</cell></row><row><cell>GINE-VN [5, 16]</cell><cell>6.15M</cell><cell>29.17 ? 0.15</cell><cell></cell><cell></cell></row><row><cell>PHC-GNN [26]</cell><cell>1.69M</cell><cell>29.47 ? 0.26</cell><cell>114K</cell><cell>79.34 ? 1.16</cell></row><row><cell>GIN-VN [16, 42]</cell><cell>3.4M</cell><cell>29.02 ? 0.17</cell><cell>3.3M</cell><cell>77.80 ? 1.82</cell></row><row><cell>(pre-trained)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Graphormer-FLAG [43]</cell><cell cols="2">119.5M 31.40 ? 0.34</cell><cell>47.2M</cell><cell>80.51 ? 0.53</cell></row><row><cell>(pre-trained)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EGT Larger (30 layers)</cell><cell>110.8M</cell><cell>29.61 ? 0.24</cell><cell>110.8M</cell><cell>80.60 ? 0.65</cell></row><row><cell>(pre-trained)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of results for two ablated variants of EGT (EGT-Constrained and EGT-Simple), along with the complete architecture with (EGT) and without (EGT w/o PE) SVD based positional encodings</figDesc><table><row><cell></cell><cell>PATTERN</cell><cell>CLUSTER</cell><cell>MNIST</cell><cell>CIFAR10</cell><cell>TSP</cell><cell>ZINC</cell><cell>PCQM4Mv2</cell></row><row><cell></cell><cell>% Accuracy ?</cell><cell>% Accuracy ?</cell><cell>% Accuracy ?</cell><cell>% Accuracy ?</cell><cell>F1 ?</cell><cell>MAE ?</cell><cell>MAE ?</cell></row><row><cell>Model</cell><cell cols="7">#Param?500K #Param?500K #Param?100K #Param?100K #Param?500K #Param?500K #Param?11.5M</cell></row><row><cell cols="2">EGT-Constrained 86.629 ? 0.041</cell><cell>76.701 ? 0.257</cell><cell>96.823 ? 0.204</cell><cell>65.192 ? 0.475</cell><cell>0.846 ? 0.001</cell><cell>0.174 ? 0.004</cell><cell>0.0934</cell></row><row><cell>EGT-Simple</cell><cell>86.813 ? 0.013</cell><cell>79.182 ? 0.213</cell><cell>98.148 ? 0.139</cell><cell>64.967 ? 1.263</cell><cell>0.831 ? 0.002</cell><cell>0.228 ? 0.020</cell><cell>0.0900</cell></row><row><cell>EGT w/o PE</cell><cell>86.812 ? 0.031</cell><cell cols="3">77.665 ? 0.343 99.218 ? 0.219 68.555 ? 0.624</cell><cell>0.853 ? 0.001</cell><cell>0.187 ? 0.005</cell><cell>0.0901</cell></row><row><cell>EGT</cell><cell cols="4">86.821 ? 0.020 79.232 ? 0.348 98.173 ? 0.087 68.702 ? 0.409</cell><cell>0.853 ? 0.001</cell><cell>0.108 ? 0.009</cell><cell>0.0899</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on the PCQM4Mv2 dataset for EGT</figDesc><table /><note>Small (from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Common hyperparameters used in medium-scale experiments on all datasets.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Number of attention heads,</cell><cell>8</cell></row><row><cell>Node channels FFN multiplier</cell><cell>2</cell></row><row><cell>Edge channels FFN multiplier</cell><cell>2</cell></row><row><cell>Final (two) MLP layers dimension</cell><cell>? /2, ? /4</cell></row><row><cell>Virtual nodes</cell><cell>Not used</cell></row><row><cell>SVD encoding rank,</cell><cell>8</cell></row><row><cell>Random attention masking rate</cell><cell>0.1</cell></row><row><cell>Dynamic Centrality Scalers</cell><cell>Not used</cell></row><row><cell>Dropout</cell><cell>Not used</cell></row><row><cell>Adam: initial LR</cell><cell>5 ? 10 ?4</cell></row><row><cell>Adam: 1</cell><cell>0.9</cell></row><row><cell>Adam: 2 Adam:</cell><cell>0.999 10 ?7</cell></row><row><cell>Reduce LR by factor</cell><cell>0.5</cell></row><row><cell>Minimum LR</cell><cell>5 ? 10 ?6</cell></row><row><cell>LR warmup</cell><cell>Not used</cell></row><row><cell>Cosine decay</cell><cell>Not used</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Specific hyperparameters used for each dataset in medium-scale experiments. D<ref type="bibr" target="#b15">(16)</ref> is the distance matrix clipped to 16 hops. A is the adjacency matrix with self-loops. Distance prediction objective is only used for MNIST, CIFAR10 and ZINC datasets.</figDesc><table><row><cell></cell><cell cols="2">PATTERN</cell><cell cols="3">CLUSTER MNIST CIFAR10</cell><cell cols="2">TSP</cell><cell cols="2">ZINC</cell></row><row><cell></cell><cell cols="2">#Param #Param</cell><cell>#Param</cell><cell cols="6">#Param #Param #Param #Param #Param #Param</cell></row><row><cell>Hyperparameter</cell><cell>?100K</cell><cell>?500K</cell><cell>?500K</cell><cell>?100K</cell><cell>?100K</cell><cell>?100K</cell><cell>?500K</cell><cell>?100K</cell><cell>?500K</cell></row><row><cell>Input structural matrix</cell><cell>D (16)</cell><cell>D (16)</cell><cell>D (16)</cell><cell>A</cell><cell>A</cell><cell>D (16)</cell><cell>D (16)</cell><cell>A</cell><cell>A</cell></row><row><cell>Batch size</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>8</cell><cell>8</cell><cell>128</cell><cell>128</cell></row><row><cell>Maximum no. of epochs</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>600</cell><cell>600</cell></row><row><cell>Reduce LR patience (epochs)</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>5</cell><cell>5</cell><cell>20</cell><cell>20</cell></row><row><cell>Distance prediction objective: (when used)</cell><cell></cell><cell></cell><cell></cell><cell>3 hops</cell><cell>3 hops</cell><cell></cell><cell></cell><cell>3 hops</cell><cell>3 hops</cell></row><row><cell>Distance prediction objective: (when used)</cell><cell></cell><cell></cell><cell></cell><cell>5 ? 10 ?4</cell><cell>5 ? 10 ?4</cell><cell></cell><cell></cell><cell cols="2">5 ? 10 ?2 5 ? 10 ?2</cell></row><row><cell>Number of layers,</cell><cell>4</cell><cell>16</cell><cell>16</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>16</cell><cell>4</cell><cell>10</cell></row><row><cell>Node channels width, ?</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>48</cell><cell>64</cell><cell>64</cell><cell>48</cell><cell>64</cell></row><row><cell>Edge channels width,</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>48</cell><cell>8</cell><cell>8</cell><cell>48</cell><cell>64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters used in large-scale experiments.</figDesc><table /><note>Hyperparameter Value</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the Rensselaer-IBM AI Research Collaboration, part of the IBM AI Horizons Network.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input structural matrix</head><p>Distance matrix (clipped up to <ref type="bibr" target="#b15">16</ref>  process to neighbors as:</p><p>Since this architecture is constrained to the existing edges we denote this as EGT-Constrained. It has the advantage that depending on the sparsity of the graph, it can have sub-quadratic computational and memory costs. However, it can be difficult to perform sparse aggregation in parallel on the GPU. Instead of sparse operations, we used masked attention to implement this architecture for faster training on datasets containing smaller graphs because we can take advantage of highly parallel tensor operations. For this variant, ? , bear their usual meanings in the hyperparameters tables ( <ref type="table">Table 7</ref>, <ref type="table">Table 8</ref>). We use the same hyperparameters for this variant as the ones used for original EGT. Ungated Variant: In EGT, the edge channels participate in the aggregation process in two ways -by an attention bias and also by gating the values before they are aggregated by the attention mechanism. To verify the utility of the gating mechanism used in EGT, an ungated variant can be formulated by simplifying the aggregation process as follows:</p><p>Where,? ,? = clip</p><p>E ,? ? R ? is a concatenation of the learned linear transformed edge embeddings, i.e., E ,??? . Note that we omitted the sigmoid gates. The edge channels influence the aggregation process only via an attention bias.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Directional graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saro</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="748" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nips</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Residual gated graph convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?my</forename><surname>Brossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriel</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dehaene</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.15069</idno>
		<title level="m">Graph convolutions that can finally model local structure</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph transformer for graph-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7464" to="7471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<title level="m">Afroz Mohiuddin, Lukasz Kaiser, and Others. 2020. Rethinking attention with performers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03584</idno>
		<title level="m">On the relationship between self-attention and convolutional layers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05718</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09699</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Yoshua Bengio, and Xavier Bresson. 2020. Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ogb-lsc: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09430</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<title level="m">Open graph benchmark: Datasets for machine learning on graphs</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nikolaos Pappas, and Fran?ois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mucong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09891</idno>
		<title level="m">Flag: Adversarial data augmentation for graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parameterized hypercomplex graph neural networks for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bertolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>No?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="204" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural speech synthesis with transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naihan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6706" to="6713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Puny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07846</idno>
		<title level="m">Global Attention Improves Graph Networks Generalization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Anselm Levskaya, and Jonathon Shlens</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02835</idno>
		<title level="m">Self-supervised graph transformer on large-scale molecular data</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Linear transformers are secretly fast weight memory systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imanol</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11174</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00452</idno>
		<title level="m">On the equivalence between positional node embeddings and structural graph representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the Global Self-attention Mechanism for Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyuan</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8531" to="8538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Do Transformers Really Perform Bad for Graph Representation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05234</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="11983" to="11993" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16103</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
