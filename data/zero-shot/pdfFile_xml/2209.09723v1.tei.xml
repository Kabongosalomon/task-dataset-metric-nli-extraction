<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GANet: Goal Area Network for Motion Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Wang</surname></persName>
							<email>wangmingkun95@qq.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
							<email>zhuxinge123@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Changqian</surname></persName>
							<email>yuchangqian@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meituan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
							<email>mayuexin@shanghaitech.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochun</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchun</forename><forename type="middle">Ren</forename><surname>Meituan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxu</forename><surname>Wang</surname></persName>
							<email>wang_mingxu@126.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Yang</surname></persName>
							<email>wenjing.yang@nudt.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Academy of Military Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GANet: Goal Area Network for Motion Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Corresponding author Preprint. Under review. arXiv:2209.09723v1 [cs.CV] 20 Sep 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting the future motion of road participants is crucial for autonomous driving but is extremely challenging due to staggering motion uncertainty. Recently, most motion forecasting methods resort to the goal-based strategy, i.e., predicting endpoints of motion trajectories as conditions to regress the entire trajectories, so that the search space of solution can be reduced. However, accurate goal coordinates are hard to predict and evaluate. In addition, the point representation of the destination limits the utilization of a rich road context, leading to inaccurate prediction results in many cases. Goal area, i.e., the possible destination area, rather than goal coordinate, could provide a more soft constraint for searching potential trajectories by involving more tolerance and guidance. In view of this, we propose a new goal area-based framework, named Goal Area Network (GANet), for motion forecasting, which models goal areas rather than exact goal coordinates as preconditions for trajectory prediction, performing more robustly and accurately. Specifically, we propose a GoICrop (Goal Area of Interest) operator to effectively extract semantic lane features in goal areas and model actors' future interactions, which benefits a lot for future trajectory estimations. GANet ranks the 1st on the leaderboard of Argoverse Challenge among all public literature (till the paper submission), and its source codes will be released.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As one of the most critical subtasks in autonomous driving, motion forecasting targets to understand and predict the future behaviors of other road participants (called actors). It is essential for the self-driving car to make safe and reasonable decisions in the subsequent planning and control module. The recent emergence of large-scale datasets with high-definition maps (HD maps) and sensor data such as Argoverse <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref> and Waymo Open Dataset <ref type="bibr" target="#b23">[24]</ref> have boosted the research in motion forecasting. These HD maps provide rich geometric and semantic information, e.g., the map topology, that constrains the vehicle's motion. Meanwhile, actors also follow driving etiquette and interact with each other. Thus, how to effectively incorporate driving context to predict multiple plausible and accurate trajectories becomes the core challenge for motion forecasting.</p><p>Some works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b15">16]</ref> encode maps and motion trajectories into 2D images and apply convolutional neural networks (CNN) to process. Others <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1]</ref> use vectorized and graph-structured data to represent maps. For instance, LaneGCN <ref type="bibr" target="#b3">[4]</ref> applies a multi-stride graph neural network to encode maps. However, since the traveling mode of an actor is highly diverse, the fixed size stride cannot effectively model distant relevant map features and thus limits the prediction performance (see <ref type="figure" target="#fig_2">Figure 3</ref>). While most works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4]</ref> focus on map encoding and motion history modeling, another family of motion forecasting methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, which is built on goal-based prediction, captures the actor's intentions in the future explicitly. Specifically, these methods follow a three-stage scheme: first, candidate goals are sampled from the lane centerlines; second, a set of goals are proposed by goal prediction; third, trajectories are estimated conditioning on candidate goals and a set of high-scored trajectories are selected as outputs. Although these methods have achieved competitive results, there remain two main drawbacks. (1) These methods merely use a limited number of isolated goal coordinates as conditions, which contain limited information and hinder accurate motion forecasting. As goal coordinates of different distances to the road edge carry different information, using a limited number of goal coordinates as conditions constrains the full utilization of a road context. <ref type="bibr" target="#b1">(2)</ref> The competitive performance of these methods heavily depends on the well-designed goal space, which may be violated in practice. Well-designed goal space is required for sampling, refining and scoring candidate goals, due to the difficulty of predicting and evaluating accurate goal coordinates. For example, vehicles' candidate goals are sampled from the lane centerlines while pedestrians' candidate goals are sampled from a virtual grid around themselves in TNT <ref type="bibr" target="#b1">[2]</ref>. These hard-encoded candidate goals abide by driving rules, e.g., never depart far from lanes or go beyond the road's edge, so the predicted trajectories are less likely to miss. However, these methods may fail once these hard-encoded candidate goals are violated in the real world.</p><p>Compared with accurate goal coordinates, a potential goal area with a relatively richer context of the road is able to provide more tolerance and better guidance for accurate trajectory prediction through a soft constraint. Also, as driving history of actors is critical for goal area estimation, we make full use of this clue for accurate localization of goal areas. For example, a fast-moving vehicle's goal areas may be far away, while the goal areas of a stationary vehicle should be limited around itself.</p><p>Motivated by these observations, we propose Goal Area Network framework (GANet) that predicts potential goal areas as conditions for motion forecasting. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, there are three stages in GANet, which are trained in an end-to-end way, and we construct a series of GANet models following this framework. They overcome the shortcomings of the aforementioned goal-based prediction methods. First, an efficient encoding backbone is adopted to encode motion history and scene context. Then, we predict approximate goals and crop their surrounding goal areas as more robust conditions. Moreover, we introduce a GoICrop operator to explicitly query and aggregate the rich semantic features of lanes in the goal areas. GoICrop learns the interactions between maps and actors in the goal areas, and it implicitly captures the interactions among actors in the future. Finally, we make the formal motion forecasting conditioned on motion history, scene context, and the aggregated goal area features. Extensive experiments on the large-scale Argoverse 1 and Argoverse 2 motion forecasting benchmark demonstrate the effectiveness and generality of our method, where GANet achieves state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Interactions. Early motion prediction methods mainly focus on motion and interaction modeling. They attempt to explain actors' complex movements by exploring their potential "interactions". Traditional methods such as Social Force <ref type="bibr" target="#b10">[11]</ref> use hand-crafted features and rules to model interactions and constraints. Later, deep learning methods bring significant progress to this task. Social LSTM <ref type="bibr" target="#b11">[12]</ref> and SR-LSTM <ref type="bibr" target="#b12">[13]</ref> use variants of LSTM to implicitly model interactions. GNN-TP <ref type="bibr" target="#b13">[14]</ref> introduces a GNN method for interaction inference and trajectory prediction. The approach of <ref type="bibr" target="#b14">[15]</ref> applies multi-head attention to incorporate interaction. mmTransformer <ref type="bibr" target="#b5">[6]</ref> applies a transformer architecture to fuse actors' motion histories, maps, and their interactions.</p><p>HD maps encoding. According to the HD maps' processing manner, methods can be divided into three categories. Rasterization-based methods rasterize the elements of HD maps and actors' motion histories into an image. Then, they use a CNN network to extract features and perform coordinates prediction. IntentNet <ref type="bibr" target="#b21">[22]</ref> develops a multi-task model with a CNN-based detector to extract features from rasterized maps. MultiPath <ref type="bibr" target="#b15">[16]</ref> uses the Scene CNN to extract mid-level features, which encode the states of actors and their interactions on a top-down scene representation. However, these 2D-CNN-based methods suffer from low efficiency in extracting features of graph-structured maps. Graph-based methods <ref type="bibr" target="#b0">[1]</ref> construct graph-structured representations from HD maps, which preserve the connectivity of lanes. VectorNet <ref type="bibr" target="#b16">[17]</ref> encodes map elements and actor trajectories as polylines and then uses a global interactive graph to fuse map and actor features. LaneGCN <ref type="bibr" target="#b3">[4]</ref> constructs a map node graph and proposes a novel graph convolution. Point cloud-based methods use points to represent actors' trajectories and maps. TPCN <ref type="bibr" target="#b4">[5]</ref> takes each actor as an unordered point set and applies a point cloud learning model.</p><p>Multimodality. The multi-modal prediction has become an indispensable part of motion forecasting, which deals with the uncertainty in motion forecasting. Generative methods, such as variational auto-encoder <ref type="bibr" target="#b25">[26]</ref> and generative adversarial network <ref type="bibr" target="#b17">[18]</ref>, can be used to generate multi-modal predictions. However, each prediction requires multiple independent sampling and forward pass, which cannot guarantee the diversity of samples. Other methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16]</ref> add some prior knowledge, such as pre-defined or model-based anchor trajectories. mmTransformer <ref type="bibr" target="#b5">[6]</ref> designs a region-based training strategy, which ensures that each proposal captures a specific pattern. Recently, goal-based forecasting methods <ref type="bibr" target="#b19">[20]</ref> have proven effective. TNT <ref type="bibr" target="#b1">[2]</ref> first samples dense goal candidates along the lane and generates trajectories conditioned on high-scored goals. LaneRCNN <ref type="bibr" target="#b0">[1]</ref> regards each lane segment as an anchor. DenseTNT <ref type="bibr" target="#b2">[3]</ref> introduces a trajectory prediction model to output a set of trajectories from dense goal candidates. Heatmap-based methods <ref type="bibr" target="#b7">[8]</ref> focus on outputting a heatmap to represent the trajectories' future distribution. HOME <ref type="bibr" target="#b6">[7]</ref> method predicts a future probability distribution heatmap and designs a deterministic sampling algorithm for optimization.</p><p>Our method is different from previous works as follows. (1) We give the definition of goal area and propose a new goal area-based framework. We experimentally verify the effectiveness of modeling goal areas, predicting goal areas and returning crucial map information. (2) We employ a GoICrop operator to extract rich semantic map features in goal areas, which effectively models distant relevant map features slighted by previous methods. These map features provide more robust information than the goal coordinates embedding. Specifically, the distance-based attention implicitly captures the interactions between maps and trajectories in goal areas. It constrains the trajectories to follow driving rules and map topology in a data-driven manner rather than relying on a well-designed goal space. (3) Since our predicted goal is just a potential destination, we take it as a handle to model agents' interactions in the future, which is also crucial for collision avoidance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>This section describes our proposed GANet framework in a pipelined manner. An overview of the GANet architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>, and each module in this framework is pluggable. First, we discuss our formulation. Then, we present the proposed stages: context encoding, goal prediction, GoICrop, and motion forecasting.</p><p>Formulation. Given a sequence of past observed states a P = [a ?T +1 , a ?T +2 , ..., a 0 ] for an actor, we aim to predict its future states a F = [a 1 , a 2 , ..., a T ] up to a fixed time step T . We predict all actors' trajectories simultaneously. Running in a specific environment, each actor will interact with static HD maps m and the other dynamic actors. Therefore, the probabilistic distribution we want to capture is p(a F |m, a P , a O P ), where a O P denotes the other actors' observed states. The output of our model is</p><formula xml:id="formula_0">A F = {a k F } k?[0,K?1] = {(a k 1 , a k 2 , ..., a k T )} k?[0,K?1]</formula><p>for each actor, while motion forecasting tasks usually expect us to submit K trajectories for evaluation.</p><p>TNT-like methods can be expressed as</p><formula xml:id="formula_1">p(a F |m, a P , a O P ) ? ? ?T (m,a P ,a O P ) p(? |m, a P , a O P )p(a F |?, m, a P , a O P )<label>(1)</label></formula><p>where T (m, a P , a O P ) is the space of candidate goals depending on the driving context m, a P , a O P . However, the spatial map space m is large and the goal space T (m, a P , a O P ) requires careful design. Some methods expect to predict the actor's motion accurately by extracting good features. For example, LaneGCN <ref type="bibr" target="#b3">[4]</ref> try to approximate p(a F |m, a P , a O P ) by modeling p(a F |M a0 , a P , a O P ), where M a0 is the "local" map features that is related to the actor's state a 0 at final observed step t = 0. To form M a0 , they use a 0 as an anchor to explicitly retrieve its surrounding map elements and aggregate their features. We found that not only the "local" map information is important, but also the goal area maps information is of great importance for the accurate trajectory prediction. So, we reconstructed the probability distribution as:</p><formula xml:id="formula_2">p(a F |m, a P , a O P ) ? ? p(? |M a0 , a P , a O P )p(M ? |m, ? )p(a F |M ? , M a0 , a P , a O P )<label>(2)</label></formula><p>We directly predict possible goals ? based on actors' motion histories and driving context. Therefore, GANet is genuinely end-to-end and adaptive. Then, we apply the predicted goals as anchors to retrieve the map elements in goal areas explicitly and aggregate their map features as M ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motion history and scene context encoding</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the first stage of motion forecasting is driving context encoding, which extracts actors' motion features and maps features. We adopt LaneGCN's <ref type="bibr" target="#b3">[4]</ref> backbone to encode motion history and scene context for its outstanding performance. Specifically, we apply a 1D CNN with Feature Pyramid Network (FPN) to extract actors' motion features. The input is observed past trajectories of all actors in a scenario. We represent each trajectory as a sequence of Bird's Eye View</p><formula xml:id="formula_3">(BEV) coordinate displacements {?p ?(T +1) , ..., ?p ?1 , ?p 0 },</formula><p>where ?p t is the 2D coordinates displacements from step t ? 1 to t. We observe T steps. For trajectories with observed steps less than T , we pad them with zeros by adding a binary 1 ? T mask to indicate whether the element is padded or not. We concatenate the displacements and the mask to obtain a 3 ? T input tensor.</p><p>Following <ref type="bibr" target="#b3">[4]</ref>, we use a multi-scale LaneConv network to encode map features. The vectorized map data we adopted consists of lanes and their connectivity. We construct a lane node graph from the map data. A lane node is a short lane segment between two consecutive points of the lane centerline, which is represented by the location (the averaged coordinates of its two endpoints) and the shape (the vector between its two endpoints). The multi-scale LaneConv operator encodes lane node features and aggregates the map topology at a larger scale based on the lane node graph. Finally, A fusion network captures the interactions among actors and lane nodes and updates their fused features. After driving context encoding, we obtain a 2D feature matrix X where each row X i indicates the feature of the i-th actor, and a 2D feature matrix Y where each row Y i indicates the feature of the i-th lane node. We can also use other methods to encode motion history and scene context. For example, we implement a VectorNet++ method in the ablation study section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Goal prediction</head><p>In stage two, we predict possible goals for the i-th actor based on X i . We apply intermediate supervision and calculate the smooth L1 loss between the best predicted goal and the ground-truth trajectory's endpoint to backpropagate, making the predicted goal close to the actual goal as much as possible. The goal prediction stage serves as a predictive test to locate goal areas, which is different from goal-based methods using the predicted goals as the final predicted trajectories' endpoint. In practice, a driver's driving intent is highly multi-modal. For example, he or she may stop, go ahead, turn left, or turn right when approaching an intersection. Therefore, we try to make a multiple-goals prediction. We construct a goal prediction header with two branches to predict E possible goals and their confidence scores for each actor. We apply a residual Multi Layer-Perception Neural Network (MLP) in the regression branch to regress E BEV coordinates G n,end = {g e n,end } e?[0,E?1] , where g e n,end is the e-th predicted goal coordinates of the n-th actor. For the classification branch, we apply an MLP to output E confidence scores C n,end = {c e n,end } e?[0,E?1] , where c e n,end is the e-th predicted goal confidence of the n-th actor.</p><p>We train this stage using the sum of classification loss and regression loss. Given E predicted goals, we find a positive goal? that has the minimum Euclidean distance with the ground truth trajectory's endpoint. For classification, we use the max-margin loss:</p><formula xml:id="formula_4">L cls_end = 1 N (E ? 1) N n=1 e =? max(0, c e n,end + ? c? n,end )<label>(3)</label></formula><p>where N is the total number of actors and = 0.2 is the margin. The margin loss expects each goal to capture a specific pattern and pushes the goal closest to the ground truth to have the highest score. For regression, we apply the smooth L1 loss to the positive goals:</p><formula xml:id="formula_5">L reg_end = 1 N N n=1 reg(g? n,end ? a * n,end )<label>(4)</label></formula><p>where a * n,end is the ground truth BEV coordinates of the n-th actor trajectory's endpoint, reg(z) = i d(z i ), z i is the i-th element of z, and d(z i ) is a smooth L1 loss. Additionally, we also try to add a "one goal prediction" module at each trajectory's middle position aggregating map features to assist the endpoint goal prediction and the whole trajectory prediction. Similarly, we apply a residual MLP to regress a middle goal g n,mid for the n-th actor. The loss term for this module is given by:</p><formula xml:id="formula_6">L reg_mid = 1 N N n=1 reg(g n,mid ? a * n,mid )<label>(5)</label></formula><p>where a * n,mid is the ground truth BEV coordinates of the n-th actor trajectory's middle position. The total loss at the goal prediction stage is:</p><formula xml:id="formula_7">L 1 = ? 1 L cls_end + ? 1 L reg_end + ? 1 L reg_mid<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GoICrop</head><p>We choose the predicted goal with the highest confidence among E goals as an anchor. This anchor is the approximate destination with the highest possibility that the actor may reach based on its motion history and driving context. Because actors' motion is highly uncertain, we crop maps within 6 meters of the anchor as the goal area of interest, which relaxes the strict goal prediction requirement. The actual endpoint is more likely to appear in candidate areas compared with being hit by scattered endpoint predictions. Moreover, the actor's behavior highly depends on its destination area's context, i.e., the maps and other actors. Although previous works have explored the interactions between actors, the interactions between actors and maps in goal areas and the interactions among actors in the future have received less attention. Thus, we retrieve the lane nodes in goal areas and apply a GoICrop module to aggregate these map node features as follows:</p><formula xml:id="formula_8">x i = ? 1 (x i W 0 + j ? 2 (concat(x i W 1 , ? i,j , y j )W 2 ))W 3<label>(7)</label></formula><p>where x i is the feature of i-th actor and and y j is the feature of j-th lane node, W i is a weight matrix, ? i is a layer normalization with ReLU function, and</p><formula xml:id="formula_9">? i,j = ?(M LP (v i ? v j )),</formula><p>where v i denotes the anchor's coordinates of i-th actor and v j denotes the j-th lane node's coordinates. GoICrop serves as spatial distance-based attention and updates the goal area lane nodes' features back to the actors.</p><p>We transpose x i with W 1 as a query embedding. The relative distance feature between the anchor of i-th actor and j-th lane node are extracted by ? i,j . Then, we concatenate the query embedding, relative distance feature, and the lane node feature. An M LP is employed to transpose and encode these features. Finally, the goal area features are aggregated for i-th actor.</p><p>Previous motion forecasting methods usually focus on the interactions in the observation history.</p><p>However, actors will interact with each other in the future to follow driving etiquette, such as avoiding collisions. Since we have performed predictive goal predictions and gotten possible goals for each actor, our framework can model the actors' future interactions. Hence, we utilize the predicted anchor positions and apply a GoICrop module as equation 7 to implicitly model actors' interactions in the future. We consider the other actors whose future anchor's distance from the anchor of i-th actor is smaller than 100 meters. In this case, y j in equation 7 denotes the features of j-th actor, v i denotes the anchor's coordinates of i-th actor, and v j denotes the anchor's coordinates of j-th actor in</p><formula xml:id="formula_10">? i,j = ?(M LP (v i ? v j )).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Motion estimation and scoring</head><p>We take the updated actor features X as input to predict K final future trajectories and their confidence scores in stage three. Specifically, we construct a two-branch multi-modal prediction header similar to the goal prediction stage, with one regression branch estimating the trajectories and one classification branch scoring the trajectories. For each actor, we regress K sequences of BEV coordinates A n,F = {(a k n,1 , a k n,2 , ..., a k n,T )} k?[0,K?1] , where a k n,t denotes the n-th actor's future coordinates of the k-th mode at t-th step. For the classification branch, we output K confidence scores C n,cls = {c k n } k?[0,K?1] corresponding to K modes. We find a positive trajectory of modek, whose endpoint has the minimum Euclidean distance with the ground truth endpoint.</p><p>For classification, we use the margin loss similar to the goal prediction stage:</p><formula xml:id="formula_11">L cls = 1 N (K ? 1) N n=1 k =k max(0, c k n + ? ck n )<label>(8)</label></formula><p>For regression, we apply the smooth L1 loss on all predicted steps of the positive trajectories:</p><formula xml:id="formula_12">L reg = 1 N T N n=1 T t=1 reg(ak n,t ? a * n,t )<label>(9)</label></formula><p>where a * n,t is the n-th actor's ground truth coordinates at step t. To emphasize the importance of the goal, we add a loss term stressing the penalty at the endpoint:</p><formula xml:id="formula_13">L end = 1 N N n=1</formula><p>reg(ak n,end ? a * n,end ) where a * n,end is the n-th actor's ground truth endpoint coordinates and ak n,end is the n-th actor's positive trajectory's endpoint.</p><p>The loss function for training at this stage is given by:</p><formula xml:id="formula_15">L 2 = ? 2 L cls + ? 2 L reg + ? 2 L end<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training and inference</head><p>As all the modules are differentiable, we train our model with the loss function:</p><formula xml:id="formula_16">L = L 1 + L 2<label>(12)</label></formula><p>Although the various losses may seem complicated, their structures are almost identical. The parameters are chosen to balance the training process. More details can be found in Appendix B.</p><p>For inference, GANet (1) encodes motion and scene context; (2) predicts a goal at the middle position, crops a goal area, and aggregates its map features; (3) predicts three goals and chooses the one with the highest confidence to crop a goal area of interest; (4) aggregates the goal area map features and models actors' interactions in the future; (5) estimates K trajectories and their confidence scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental settings</head><p>Dataset. Argoverse 1 <ref type="bibr" target="#b9">[10]</ref> is a large-scale motion forecasting dataset with high-definition maps and sensor data, which consists of over 30K real-world driving sequences collected in Pittsburgh and Miami, split into train, validation, and test sequences without geographical overlap. Each training and validation sequence is 5 seconds long, while each test sequence presents only 2 seconds to the model, and another 3 seconds are withheld for the leaderboard evaluation. Each sequence includes one interesting tracked actor labeled as the "agent." Given an initial 2-second observation, the task is to predict the agent's future coordinates in the next 3 seconds.</p><p>Spanning 2,000+ km over six geographically diverse cities, Argoverse 2 <ref type="bibr" target="#b22">[23]</ref> is a high-quality motion forecasting dataset whose scenario is paired with a local map. Each scenario is 11 seconds long. We observe five seconds and predict six seconds for the leaderboard evaluation. Compared to Argoverse 1, the scenarios in Argoverse 2 are approximately twice longer and more diverse.</p><p>Metrics. We follow the experimental settings of the benchmark and widely accepted evaluation metrics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref>. Specifically, MR is the ratio of predictions where none of the predicted K trajectories is within 2.0 meters of ground truth according to the endpoint's displacement error. Minimum Final Displacement Error (minFDE) is the L2 distance between the endpoint of the best-forecasted trajectory and the ground truth. The best-forecasted trajectory is the one that has the minimum finalstep displacement error. Minimum Average Displacement Error (minADE) is the average L2 distance between the best-forecasted trajectory and the ground truth. Brier minimum Final Displacement Error (brier-minFDE) is similar to minFDE. It adds a (1.0 ? p) 2 penalty to the endpoint's L2 distance error, where p corresponds to the predicted probability of the best-forecasted trajectory. Argoverse Motion Forecasting leaderboard is ranked by Brier minimum Final Displacement Error (brier-minFDE6).</p><p>Implementation. We train our model on 2 A100 GPUs using a batch size of 128 with the Adam optimizer for 42 epochs. The initial learning rate is 1 x 10-3, decaying to 1 x 10-4 at 32 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-art</head><p>We compare our approach with state-of-the-art methods. As shown in <ref type="table" target="#tab_0">Table 1</ref>, our GANet outperforms existing goal-based approaches such as TNT <ref type="bibr" target="#b1">[2]</ref>, LaneRCNN <ref type="bibr" target="#b0">[1]</ref>, and DenseTNT <ref type="bibr" target="#b2">[3]</ref>. Specifically, we make a detailed comparison with LaneGCN because we adopt their backbone to encode motion history and scene context, which demonstrate the effectiveness of GANet. Public results on the official motion forecasting challenge leaderboard show that our GANet method significantly beats LaneGCN by decreases of 28%, 15%, 13% and 9% in MR6, minFDE6, brier-minFDE6, and minFDE1, respectively. We also conduct experiments on Argoverse 2 Motion Forecasting Dataset <ref type="bibr" target="#b22">[23]</ref>, and GANet achieves state-of-the-art performance in CVPR 2022 Argoverse 2 motion forecasting challenge, where the top ten entries are shown in <ref type="table" target="#tab_0">Table 1</ref>. Since many methods, such as TENET and OPPred, apply model ensemble to boost their performance, we report their results without ensemble for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation studies</head><p>Component study. We perform ablation studies on the validation set to investigate the effectiveness of each component. We take the LaneGCN model as the baseline for the ablation study. Then we add other components progressively and discuss the impact of each part according to the results shown in <ref type="table" target="#tab_1">Table 2</ref>. First, to emphasize the motion's temporal modeling, we construct an enhanced version of LaneGCN called LaneGCN++. Specifically, we apply an LSTM network on FPN's output features for motion history encoding to get a series of hidden states with temporal information. Then, we sum the final step features of the LSTM and FPN as the actor's motion features. We also use two identical parallel networks to extract motion features and sum the output together to enhance the feature encoding. As shown in <ref type="table" target="#tab_1">Table 2</ref>, LaneGCN++ improves the ADE1 and FDE1 metrics performance with the enhanced temporal modeling. However, the enhanced bigger network shows little improvement in multi-modal prediction.</p><p>Second, to verify GANet's effectiveness, we adopt LaneGCN++'s backbone and add a "one goal prediction" module to construct the GANet_1 model, which only predicts M = 1 goals. Since we only predict one goal in this model, we omit the classification loss term L cls_end and L reg_mid in L 1 . The performance of the GANet_1 model outperforms LaneGCN++ dramatically, with more than 10% improvement on minFDE(K=6). In addition, considering the multimodality, we apply a "three goals predictions" module in our GANet_3 model, which performs better. Moreover, we also try to add a "one goal prediction" module at the trajectory's middle position to aggregate the middle position's map information in GANet_M_3. The performance has been further improved. Our model improves all the metrics compared to the LaneGCN++, demonstrating the effectiveness of our GANet framework.</p><p>Backbone. To demonstrate the generality of our GANet, we implement a VectorNet++ method as another backbone, whose polylines idea is similar to VectorNet <ref type="bibr" target="#b16">[17]</ref>. Specifically, the HD map elements and actor trajectories are encoded as polylines. Two graph neural networks based on self-attention are applied to encode actor polylines and map polylines, respectively. Then, three crossattention networks are used to transfer information between actors and maps. Finally, a global graph neural network is applied to model the interactions between all polylines and output a 2D feature matrix X. Our stage-three module is used to make the final predictions based on X. Benefiting from cross-attention and our stage-three module, VectorNet++ outperforms VectorNet by a large margin.</p><p>We construct our GANet models based on the VectorNet++ backbone and compare them to the VectorNet++ method. As shown in <ref type="table" target="#tab_1">Table 2</ref>, we can see that with our GANet, the performance improves by 9.9% and 5.2% in minFDE(K=6) and minADE(K=6), respectively, which shows the generality of GANet when adopting different scene context encoding methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of goals.</head><p>We also evaluate the effect of the goal number. <ref type="table" target="#tab_1">Table 2</ref> shows the model performance under different numbers of goals, where the goal number only has marginal effects on the overall performance. We adopt the "three goals prediction" scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative results</head><p>We visualize the predicted results on the validation set. For challenging sequences, almost all results of GANet models are more reasonable and smoother following map constraints than outputs of LaneGCN. We show the multi-modal prediction trajectories of two cases in <ref type="figure" target="#fig_2">Figure 3</ref>  The second row presents a case where the agent performs a right turn at a complex intersection. Due to the lack of motion history, maps are essential to produce reasonable trajectories. LaneGCN produces divergent, non-traffic-rule compliant trajectories, while our method produces reasonable trajectories following the lane topology. This is because our GANet model can capture the complex map topology and give trajectories consistent with the lane's topology. In contrast, LaneGCN fails to capture the actual motion of the agent. Moreover, we conduct visualization to compare predictions with and without agents' future interactions, and the related qualitative results can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposes a Goal Area Network (GANet), a new framework for motion forecasting. GANet predicts potential goal areas as conditions for prediction, which introduces more reliable intentions than the hard-estimated goal coordinates. We design a GoICrop operator to extract and aggregate the rich semantic lane features in goal areas instead of the simple goal coordinates embedding. It implicitly models the interactions between trajectories and maps in the goal area and the interactions between actors in the future in a data-driven manner. Experiments on the Argoverse motion forecasting benchmark demonstrate the effectiveness of our GANet method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Qualitative Results</head><p>In this section, we present more qualitative visualizations. <ref type="figure">Figure 4</ref> shows qualitative comparisons between LaneGCN and GANet. <ref type="figure" target="#fig_4">Figure 5</ref> shows results of models with and without agents' future interactions. We only show the model's prediction with the highest confidence for a clear presentation. The visualizations show that the model incorporating the agent's future interactions is more likely to output reasonable predictions following traffic rules and driving etiquette, such as collision avoidance.</p><p>Although the evaluation metrics do not consider the future interactions between agents, this issue is also essential. <ref type="figure">Figure 4</ref>: More qualitative comparisons between LaneGCN (the left two columns) and GANet (the right two columns) on the Argoverse 1 validation set. Lanes are shown in grey, the agent's past trajectory is in orange, the ground truth future trajectory is in red, and the predicted six trajectories are in green. The results of different methods are shown in different columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>This section describes the architecture and implementation details of GANet models.</p><p>Inputs and model structure. We use all actors and lanes within 100 meters of the agent's final observed position as the inputs. To normalize the map and trajectories, we take the agent's final observed position as the origin of the coordinate system. We take the direction from the agent's location at t = ?1 to the location at t = 0 as the x-axis. As shown in <ref type="figure" target="#fig_6">Figure 6</ref>, each block is represented in the form of [layer type, output channels, stride (for CNN layers)]. Upsample and Concatenate denote bilinear upsampling and feature concatenation, respectively. ResBlock denotes The important vehicles are shown in blue, while others are shown in white. The predicted trajectory is shown in green, which starts at the beginning of the prediction time. a 1D CNN network with a residual summation. Linear ResBlock denotes an MLP network with a residual summation. ResSum denotes a residual summation, which sums the output from the Linear block and the input to GoICrop in the goal prediction stage. Linear Softmax denotes a softmax function following a Linear block to normalize the confidence scores. Actor-Map FusionNet and LaneConv are adopted from LaneGCN <ref type="bibr" target="#b3">[4]</ref>. The actor input1 and map input1 shown in <ref type="figure" target="#fig_6">Figure 6</ref> have been described in section 3.1. The actor input2 is the actor's final observed step's coordinate, which serves as an input to initialize the hidden layer of the LSTM. <ref type="figure" target="#fig_7">Figure 7</ref> shows our implemented VectorNet++ backbone, whose input representation is different from LaneGCN++. Following VectorNet <ref type="bibr" target="#b16">[17]</ref>, we convert the lanes and trajectories into sequences of vectors named polylines. For actors, each vector is represented as [p ctr , ?p t , step_t], where p ctr is the coordinate at step t, and ?p t is the displacement with mask at step t as stated in section 3.1. We take these polylines as the actor input3. As shown in <ref type="figure" target="#fig_7">Figure 7</ref>, first, the Self-Attention block constructs subgraphs and aggregates the information at the vector level, where all vector nodes belonging to the same polyline are connected. Then, the Cross-Attention block transfers information between actors and lanes. In the A2L Cross-Attention block, lane features serve as queries while actor features serve as key and value features. Finally, a global self-attention module fuses the actor and lane features. Moreover, we adopt a MapNet to output lane segment node features similar to the LaneGCN++ backbone because we will crop and aggregate fine-grained map features in the goal prediction stage.</p><p>Parameters. The parameters are chosen to balance the training process. We discount the intermediate supervision losses in the goal prediction stage, and try to keep 1:1:1 between L cls , L reg and L end loss, and 1:1 between the intermediate losses L reg_mid and L reg_end on the validation set when the training converges by setting ? 2 = 2, ? 2 = 1, ? 2 = 1, ? 1 = 1, ? 1 = 0.2 and ? 1 = 0.02 for Argoverse 1 Dataset and setting ? 2 = 2, ? 2 = 1, ? 2 = 1, ? 1 = 1, ? 1 = 0.2 and ? 1 = 0.1 for Argoverse 2 Dataset.</p><p>Inference time. We conduct experiments on an A100 server, which has an inference speed of 89 scenarios per second, regardless of the time for preprocessing to generate the input files.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of GANet framework, which consists of three stages: (a) Context encoding encodes motion history and scene context; (b) Goal prediction predicts possible goals. GoICrop retrieves and aggregates goal area map features and models the actors' interactions in the future; (c) Motion forecasting estimates multi-feasible trajectories and their corresponding confidence scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The GANet_M_3 model overview. (a) A feature extracting model encodes and fuses map and motion features. (b) The "one goal prediction" module predicts a goal area in the trajectory's middle position and aggregates its features. (c) The "three goals predictions" module predicts three goal areas, aggregates their features, and models the actors' future interactions. (d) The final prediction stage predicts K trajectories and their confidence scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results on the Argoverse 1 validation set. Lanes are shown in grey, the agent's past trajectory is in orange, the ground truth future trajectory is in red, and the predicted six trajectories are in green. The results of different methods are shown in different columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>and compare GANet with LaneGCN qualitatively. For illustration purpose, we only draw the agent's trajectory for an intuitive check while other actors are omitted. The first row shows a case where the direction of the lane has changed over a long distance. LaneGCN is unaware of this distant change and gives six straight predictions. Our GANet_1 model captures this change and generates trajectories that follow the lane topology, while our GANet_M_3 model generates smoother trajectories than GANet_1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative comparisons between models with (the second row) and without (the first row) agents' future interactions on the Argoverse 2 validation set. Lanes are shown in grey. The orange represents the ground truth trajectory of the agent vehicle. The autonomous vehicle is shown in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>For</head><label></label><figDesc>lanes, each vector is represented as [p ctr , ?p, index, len, turn, control, intersect], where p ctr is the lane segment's center coordinate, ?p is the lane segment's displacement vector, index is the polyline's index, len denotes the number of lane segments in each lane, turn is a two-bit bool value, indicating that the lane turns left or right or does not turn. control and intersect are bool indicators indicating whether this lane is controlled by a traffic light or in an intersection. These polylines serve as the map input2 in Figure 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Detailed GANet model (LaneGCN++ backbone) architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Detailed VectorNet++ backbone architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on Argoverse 1 (upper group) and Argoverse 2 (lower group) motion forecasting test dataset. The "-" denotes that this result was not reported in their paper.</figDesc><table><row><cell>Method</cell><cell>b-minFDE (K=6)</cell><cell>MR (K=6)</cell><cell>minFDE (K=6)</cell><cell>minADE (K=6)</cell><cell>minFDE (K=1)</cell><cell>minADE (K=1)</cell><cell>MR (K=1)</cell></row><row><cell>LaneRCNN [1]</cell><cell>2.147</cell><cell>0.123</cell><cell>1.453</cell><cell>0.904</cell><cell>3.692</cell><cell>1.685</cell><cell>0.569</cell></row><row><cell>TNT[2]</cell><cell>2.140</cell><cell>0.166</cell><cell>1.446</cell><cell>0.910</cell><cell>4.959</cell><cell>2.174</cell><cell>0.710</cell></row><row><cell>DenseTNT (MR)[3]</cell><cell>2.076</cell><cell>0.103</cell><cell>1.381</cell><cell>0.911</cell><cell>3.696</cell><cell>1.703</cell><cell>0.599</cell></row><row><cell>LaneGCN [4]</cell><cell>2.059</cell><cell>0.163</cell><cell>1.364</cell><cell>0.868</cell><cell>3.779</cell><cell>1.706</cell><cell>0.591</cell></row><row><cell>mmTransformer[6]</cell><cell>2.033</cell><cell>0.154</cell><cell>1.338</cell><cell>0.844</cell><cell>4.003</cell><cell>1.774</cell><cell>0.618</cell></row><row><cell>GOHOME [8]</cell><cell>1.983</cell><cell>0.105</cell><cell>1.450</cell><cell>0.943</cell><cell>3.647</cell><cell>1.689</cell><cell>0.572</cell></row><row><cell>HOME [7]</cell><cell>-</cell><cell>0.102</cell><cell>1.45</cell><cell>0.94</cell><cell>3.73</cell><cell>1.73</cell><cell>0.584</cell></row><row><cell>DenseTNT (FDE)[3]</cell><cell>1.976</cell><cell>0.126</cell><cell>1.282</cell><cell>0.882</cell><cell>3.632</cell><cell>1.679</cell><cell>0.584</cell></row><row><cell>TPCN [5]</cell><cell>1.929</cell><cell>0.133</cell><cell>1.244</cell><cell>0.815</cell><cell>3.487</cell><cell>1.575</cell><cell>0.560</cell></row><row><cell>GANet(Ours)</cell><cell>1.790</cell><cell>0.118</cell><cell>1.161</cell><cell>0.806</cell><cell>3.455</cell><cell>1.592</cell><cell>0.550</cell></row><row><cell>DirEC</cell><cell>3.29</cell><cell>0.52</cell><cell>2.83</cell><cell>1.26</cell><cell>6.82</cell><cell>2.67</cell><cell>0.73</cell></row><row><cell>drivingfree</cell><cell>3.03</cell><cell>0.49</cell><cell>2.58</cell><cell>1.17</cell><cell>6.26</cell><cell>2.47</cell><cell>0.72</cell></row><row><cell>LGU</cell><cell>2.77</cell><cell>0.37</cell><cell>2.15</cell><cell>1.05</cell><cell>6.91</cell><cell>2.77</cell><cell>0.73</cell></row><row><cell>Autowise.AI(GNA)</cell><cell>2.45</cell><cell>0.29</cell><cell>1.82</cell><cell>0.91</cell><cell>6.27</cell><cell>2.47</cell><cell>0.71</cell></row><row><cell>Timeformer [28]</cell><cell>2.16</cell><cell>0.20</cell><cell>1.51</cell><cell>0.88</cell><cell>4.71</cell><cell>1.95</cell><cell>0.64</cell></row><row><cell>QCNet</cell><cell>2.14</cell><cell>0.24</cell><cell>1.58</cell><cell>0.76</cell><cell>4.79</cell><cell>1.89</cell><cell>0.63</cell></row><row><cell>OPPred w/o Ensemble [31]</cell><cell>2.03</cell><cell>0.180</cell><cell>1.389</cell><cell>0.733</cell><cell>4.70</cell><cell>1.84</cell><cell>0.615</cell></row><row><cell>TENET w/o Ensemble [30]</cell><cell>2.01</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Polkach(VILaneIter)</cell><cell>2.00</cell><cell>0.19</cell><cell>1.39</cell><cell>0.71</cell><cell>4.74</cell><cell>1.82</cell><cell>0.61</cell></row><row><cell>GANet(Ours)</cell><cell>1.969</cell><cell>0.171</cell><cell>1.352</cell><cell>0.728</cell><cell>4.475</cell><cell>1.775</cell><cell>0.597</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study results on the Argoverse 1 validation set. VectorNet* is the result reported in<ref type="bibr" target="#b16">[17]</ref>, LaneGCN++ and VectorNet++ are enhanced methods that we implement.</figDesc><table><row><cell>Method</cell><cell>backbone</cell><cell>minFDE (K=6)</cell><cell>minADE (K=6)</cell><cell>minFDE (K=1)</cell><cell>minADE (K=1)</cell></row><row><cell>LaneGCN</cell><cell>lanegcn</cell><cell>1.080</cell><cell>0.710</cell><cell>3.010</cell><cell>1.359</cell></row><row><cell>LaneGCN++</cell><cell>lanegcn++</cell><cell>1.076</cell><cell>0.703</cell><cell>2.819</cell><cell>1.286</cell></row><row><cell>GANet_1</cell><cell>lanegcn++</cell><cell>0.961</cell><cell>0.684</cell><cell>2.743</cell><cell>1.269</cell></row><row><cell>GANet_3</cell><cell>lanegcn++</cell><cell>0.949</cell><cell>0.679</cell><cell>2.719</cell><cell>1.264</cell></row><row><cell>GANet_M_3</cell><cell>lanegcn++</cell><cell>0.934</cell><cell>0.673</cell><cell>2.707</cell><cell>1.259</cell></row><row><cell>VectorNet*</cell><cell>vectornet++</cell><cell>-</cell><cell>-</cell><cell>3.67</cell><cell>1.66</cell></row><row><cell cols="2">VectorNet++ vectornet++</cell><cell>1.156</cell><cell>0.772</cell><cell>3.256</cell><cell>1.507</cell></row><row><cell>GANet_1</cell><cell>vectornet++</cell><cell>1.076</cell><cell>0.744</cell><cell>3.050</cell><cell>1.429</cell></row><row><cell cols="2">GANet_M_3 vectornet++</cell><cell>1.042</cell><cell>0.732</cell><cell>3.100</cell><cell>1.449</cell></row><row><cell>GANet_2</cell><cell>lanegcn++</cell><cell>0.971</cell><cell>0.689</cell><cell>2.756</cell><cell>1.280</cell></row><row><cell>GANet_6</cell><cell>lanegcn++</cell><cell>0.966</cell><cell>0.683</cell><cell>2.784</cell><cell>1.289</cell></row><row><cell>GANet_9</cell><cell>lanegcn++</cell><cell>0.967</cell><cell>0.685</cell><cell>2.759</cell><cell>1.282</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributed representations for graphcentric motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanercnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TNT: Target-driven Trajectory Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="895" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Densetnt: End-to-end trajectory prediction from dense goal sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15303" to="15312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning lane graph representations for motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="541" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tpcn: Temporal point cloud networks for motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11318" to="11327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multimodal motion prediction with stacked transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7577" to="7586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Home: Heatmap output for future motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabatini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsishkou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stanciulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Intelligent Transportation Systems Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="500" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gohome: Graph-oriented heatmap output for future motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabatini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsishkou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stanciulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="9107" to="9114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-modal motion prediction with transformer-based neural network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2605" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8748" to="8757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4282</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sr-lstm: State refinement for lstm towards pedestrian trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12085" to="12094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised pedestrian trajectory prediction with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 31st International Conference on Tools with Artificial Intelligence</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="832" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multihead attention for multi-modal joint vehicle motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mercat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>El Zoghby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sandou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beauvois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Gil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="9638" to="9644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for Behavior Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL</title>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vectornet: Encoding hd maps and agent dynamics from vectorized representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11525" to="11533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2255" to="2264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Covernet: Multimodal behavior prediction using trajectory sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Phan-Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Grigore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Boulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Wolff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14074" to="14083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Map-Adaptive Goal-Based Trajectory Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchetti-Bowick</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1371" to="1383" />
		</imprint>
	</monogr>
	<note>, October)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<title level="m">Scene transformer: A unified multi-task model for behavior prediction and planning. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2106</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Intentnet: Learning to predict intention from raw sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="947" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021-08" />
		</imprint>
	</monogr>
	<note>Round 2</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9710" to="9719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">HiVT: Hierarchical Vector Transformer for Multi-Agent Motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zikang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Kejie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Desire: Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="336" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multipath++: Efficient information fusion and trajectory aggregation for behavior prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hefny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Refaat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayakanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cornman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-05" />
			<biblScope unit="page" from="7814" to="7821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">THOMAS: Trajectory Heatmap Output with learned Multi-Agent Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabatini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsishkou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stanciulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05859</idno>
		<title level="m">DCMS: Motion Forecasting with Dual Consistency and Multi-Pseudo-Target Supervision</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<idno>arXiv-2207</idno>
		<title level="m">TENET: Transformer Encoding Network for Effective Temporal Flow on Motion Prediction. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07934</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report for Argoverse2 Challenge 2022-Motion Forecasting Task. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
