<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HPTQ: Hardware-Friendly Post Training Quantization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07">July 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">Victor</forename><surname>Habi</surname></persName>
							<email>hai.habi@sony.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuven</forename><surname>Peretz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Cohen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Dikstein</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oranit</forename><surname>Dror</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idit</forename><surname>Diamant</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Jennings</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnon</forename><surname>Netzer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sony</forename><surname>Semiconductor</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename></persName>
						</author>
						<title level="a" type="main">HPTQ: Hardware-Friendly Post Training Quantization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07">July 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural network quantization enables the deployment of models on edge devices. An essential requirement for their hardware efficiency is that the quantizers are hardware-friendly: uniform, symmetric and with power-oftwo thresholds. To the best of our knowledge, current post-training quantization methods do not support all of these constraints simultaneously. In this work we introduce a hardware-friendly post training quantization (HPTQ) framework, which addresses this problem by synergistically combining several known quantization methods. We perform a large-scale study on four tasks: classification, object detection, semantic segmentation and pose estimation over a wide variety of network architectures.</p><p>Our extensive experiments show that competitive results can be obtained under hardware-friendly constraints.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks have shown state-of-art performance in many real-world computer vision tasks, such as image classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, object detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, semantic segmentation <ref type="bibr" target="#b5">[6]</ref> and pose estimation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. However, the deployment of deep neural networks on edge devices is still considered a challenging task due to limitations on available memory, computational power and power consumption.</p><p>Quantization <ref type="bibr" target="#b8">[9]</ref> is a common approach to tackle this challenge with minimal performance loss, by reducing the bit-width of network weights and activations. Quantization methods can be roughly divided into two categories: quantization aware training (QAT) and post-training quantization (PTQ). QAT methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> retrain the network in order to recover the accuracy degradation caused by quantization and usually achieve better results than PTQ methods. PTQ methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> are simpler and add quantization to a given network model without any training process. These methods are usually based on a representative unlabeled dataset that is used for selecting the quantization parameters.</p><p>Recently, several works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> have focused on hardware friendly quantization schemes. Namely, that their quantizers are uniform, symmetric and with power-of-two thresholds. Such quantizers optimize computational costs as they allow integer arithmetic without any cross-terms due to zero-points and floating-point scaling <ref type="bibr" target="#b10">[11]</ref>.</p><p>In this work, we introduce a hardware-friendly post-training quantization (HPTQ) method. To the best of our knowledge, current hardware friendly quantization methods are based on quantization aware training (QAT). This might be due to the difficulty of using power-of-two thresholds as stated in <ref type="bibr" target="#b19">[20]</ref>. HPTQ offers a post-training quantization flow that adapts and synergistically combines several known techniques, namely, threshold selection, shift negative correction, channel equalization, per channel quantization and bias correction.</p><p>We extensively examine the performance of our method using 8-bit quantization. We evaluate HPTQ on different network architectures over a variety of tasks, including classification, object detection, semantic segmentation and pose estimation. Additionally, we provide an ablation study demonstrating the effect of each technique on the network performance. To summarize, our contributions are:</p><p>? Introducing HPTQ, a method for hardware friendly post-training quantization.</p><p>? A large-scale study of post-training quantization on a variety of tasks: classification, object detection, semantic segmentation and pose estimation.</p><p>? We demonstrate that competitive results can be obtained under hardware friendly constraints of uniform, symmetric 8-bit quantization with powerof-two thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Basic Notions</head><p>In this section we give a short overview of uniform quantization and the hardware friendly constraints that will be applied in this work, namely, symmetric quantization with power-of-two thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uniform Affine Quantization.</head><p>A quantizer can be formalized as a right to left composition Q = Q de ? Q int of an integer valued function Q int : R ? Z and a recovering affine operation Q de : Z ? R (known as de-quantization). The discrete range of Q is called a quantization grid and if it is uniformly spaced, then Q is said to be a uniform quantizer.</p><p>The constant gap between two adjacent points in the quantization grid of a uniform quantizer is called its step size and the affine shift is called the zero point z. Using these parameters, a uniform quantizer can be formalized as:</p><formula xml:id="formula_0">Q(x) = Q de (Q int (x)) = s ? x int + z ? x<label>(1)</label></formula><p>where x int is the image of Q int (x) and is called the quantized integer value of x. Practically, Q int is defined by a clipping range of real values [a, b] ? R and the number of bits n b ? N for representing the quantized integer values:</p><formula xml:id="formula_1">x int = Q int (x, a, b, n b ) = clip (x, a, b) ? a s<label>(2)</label></formula><p>where s = b?a 2 n b ?1 is the step size, clip (x, a, b) = min(max(x, a), b) and ? is the rounding function to the nearest integer. The zero-point is then defined as z = a s and a uniform quantizer can be formalized as:</p><formula xml:id="formula_2">Q (x, a, b, n b ) = Q de Q int (x, a, b, n b ) = s clip (x, a, b) ? a s + a<label>(3)</label></formula><p>Note that usually the clipping boundaries a, b are selected so that the real value 0.0 is a point on the quantization grid.</p><p>Symmetric Quantization. Symmetric quantization is a simplified case of a uniform quantizer that restricts the zero-point to 0. This eliminates the need for zero-point shift in Eq. 1 and thus enables efficient hardware implementation of integer arithmetic without any cross-terms <ref type="bibr" target="#b10">[11]</ref>.</p><p>The zero-point restriction to 0 requires the selection of either a signed or unsigned quantization grid. Let t ? R + be a clipping threshold of the quantization range. A signed quantizer is then formalized as:</p><formula xml:id="formula_3">x int = clip x s , ?2 n b ?1 , 2 n b ?1 ? 1<label>(4)</label></formula><p>where s = 2t 2 n b is the step-size. Similarly, an unsigned quantizer is formalized as:</p><formula xml:id="formula_4">x int = clip x s , 0, 2 n b ? 1<label>(5)</label></formula><p>where s = t 2 n b is the step size.</p><p>Power-of-Two Thresholds. A uniform, symmetric quantizer (either signed or unsigned) with a power-of-two integer threshold is said to be a hardwarefriendly quantizer <ref type="bibr" target="#b17">[18]</ref>. Restricting the threshold of a symmetric quantizer to power-of-two integers (i.e. t = 2 M , where M ? Z) enables an efficient hardware implementation that uses integer arithmetic without floating-point scaling <ref type="bibr" target="#b10">[11]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates uniform, symmetric and hardware-friendly 4-bit quantization grids for the same range of real numbers [-0.3,4.2] to be quantized. Specifically, the figure demonstrates how the symmetry and a power-of-two threshold constraints imply sub-optimal clipping ranges compared to the general uniform quantizer. These clipping ranges lead to a loss in representation bins and thus increase the potential rounding noise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Given a trained floating point network and a representative dataset D of independent and identically distributed samples, our aim is to quantize the network in post-training with hardware-friendly quantizers, namely that are uniform, symmetric and with power-of-two thresholds. Hardware Friendly Post Training Quantization (HPTQ) is a three-tier method for addressing this goal. HPTQ consists of a pre-processing stage followed by activation quantization and weight quantization (see <ref type="figure" target="#fig_1">Fig. 2</ref>). In the resulting network, activations are quantized per tensor and weights are quantized per channel.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-Processing</head><p>The pre-processing stage consists of folding batch normalization layers into their preceding convolution layers <ref type="bibr" target="#b9">[10]</ref>, collecting activation statistics using the representative dataset and finally removing outliers from the collected statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch-Normalization Folding.</head><p>A common technique to reduce model size and computational complexity is batch-normalization folding <ref type="bibr" target="#b9">[10]</ref> (also known as batch-normalization fusing) in which batch-normalization layers are folded into the weights of their preceding convolution layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics Collection.</head><p>In this stage we infer all of the samples in the representative dataset D and collect activation statistics of each layer. Specifically, for each layer l denote the collection of its activations over D by F l (D). Based on F l (D) we collect histograms for each tensor as well as the minimum, maximum and mean values per channel. In the reset of this work we assume that activation tensors X ? R h?w?c have three dimensions where h, w and c are the height, weight and number of channels, respectively.</p><p>Outlier Removal. In this step we filter out outliers in the activation histograms using the z-score approach described in <ref type="bibr" target="#b20">[21]</ref>. Specifically, we remove histogram bins for which the absolute z-score value is larger than a predefined threshold. This implies that we restrict the range of each histogram bin to a predefined number of standard deviations from its activation mean value. See <ref type="figure" target="#fig_2">Figure 3</ref> for an example. Note that since this step updates the histograms, it applies only to the Threshold Selection step (see <ref type="figure" target="#fig_1">Figure 2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Activation Quantization</head><p>This stage consists of three steps: threshold selection, shift negative correction (SNC) and activation equalization. In the threshold selection step, we set power-of-two thresholds per tensor. The SNC step is a trick that improves the quantization of signed activation functions with a small negative range <ref type="bibr" target="#b21">[22]</ref>. In the activation equalization step we equalize the expected dynamic ranges of activation channels by applying a modified version of a technique that appears in <ref type="bibr" target="#b22">[23]</ref>.</p><p>Threshold Selection. Given a fixed bit width n b , our aim is to find a powerof-two threshold t that minimizes the noise caused by the quantization of each layer l in the network. Formally, for each layer l in the network, our objective is to find a threshold t that minimizes</p><formula xml:id="formula_5">ERR (t) = 1 n s X?F l (D) d (Q(X, t, n b ), X) ,<label>(6)</label></formula><p>where n s is the size of the representative dataset, F l (D) is the collection of activation tensors in the l-th layer and d is some error measurement.</p><p>In an ablation study we examine the effect of several possible quantization error measurements on the actual task accuracy, including L p Norms <ref type="bibr" target="#b23">[24]</ref> and Kullback-Leibler (KL) divergence <ref type="bibr" target="#b24">[25]</ref>. Our results show that Mean Square Error (MSE) <ref type="bibr" target="#b23">[24]</ref> achieves the best performance (see <ref type="table" target="#tab_7">Table 7</ref>). Thus, the objective of the threshold selection is to minimize</p><formula xml:id="formula_6">ERR (t) = 1 n s X?F l (D) (Q(X, t, n b ) ? X) 2 .<label>(7)</label></formula><p>In practice, we approximate a solution to this minimization problem by estimating the noise based on the histogram corresponding to layer l collected in the Statistics Collection step above. The restriction of the threshold to power-of-two values implies that the search space is discrete.</p><formula xml:id="formula_7">Let M = max X?F l (D) max i,j,k |X| i,j,k</formula><p>be the maximal absolute value of an activation in X over the representative dataset D that was collected in the Statistics Collection step above and define the no-clipping threshold:</p><formula xml:id="formula_8">t nc = 2 log 2 M .<label>(8)</label></formula><p>Note that the clipping noise induced by the threshold t nc is zero and that for any power-of-two threshold larger than t nc , the noise is increased. Thresholds smaller than t nc may reduce the noise, albeit, at the cost of increasing the clipping noise. Therefore, we search for a threshold minimizing the quantization error starting with t nc and iteratively decreasing it (see. Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Constraint threshold selection</head><p>Data: quantization error estimator ERR ; no-clipping threshold t nc ; bit-width n b ; n iterations Result: t threshold value e min = ? ; t = t nc ; for i in 0 to n do</p><formula xml:id="formula_9">t i = tnc 2 i ; e i = ERR (t i , n b ) ; if e i &lt; e min then t = t i ; e min = e i return t</formula><p>Shift Negative Correction (SNC). Recent works have shown benefits in using signed, non-linear activation functions, such as Swish <ref type="bibr" target="#b25">[26]</ref>, PReLU and HSwish <ref type="bibr" target="#b26">[27]</ref>. However, a signed symmetric quantization of these functions can be inefficient due to differences between their negative and positive dynamic ranges. The main idea in SNC is to reduce the quantization noise of an unsigned activation function with a small negative range (relatively to its positive range). This is done by adding a positive constant to the activation values (shifting its values) and using an unsigned quantizer with the same threshold. This effectively doubles the quantization grid resolution. Note that shifting the values can imply added clipping noise on the one hand but reduced rounding noise on the other.</p><p>This step can be viewed as an adaptation to PTQ of a technique that appears in <ref type="bibr" target="#b21">[22]</ref>, where activations are shifted and scaled in order to match a given dynamic range of a quantizer. Here, we do not add scaling due to its implied added complexity. Specifically, let ? be the activation function in some layer l in the network, let t be its threshold, calculated in the Threshold Selection step above and let s = min X?F l (D) min i,j,k X i,j,k be its minimal (negative) activation value over the representative dataset D, collected in the Statistics Collection step above. If |s| t &lt; ? for a hyperparameter ?, then we replace ? with a shifted version? = ? + |s| and replace the signed quantizer with an unsigned quantizer followed by another shift operation as follows:</p><formula xml:id="formula_10">Q s (?(X), t, n b ) ? ? Q us (?(X), t, n b ) ? |s|,<label>(9)</label></formula><p>where Q s (?(X), t, n b ) is the signed quantizer, Q us (?(X), t, n b ) is the unsigned quantizer and n b is the bit-width. In practice, the last subtraction of |s| is folded into the following operation in the network.</p><p>Activation Equalization. In this step, we equalize activation ranges per channel similarly to the methods presented in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28]</ref>. Here, we set the scale-perchannel factor according to the value of the threshold that is selected per-tensor. The motivation to use this scaling factor in order to equalize the activation ranges is to use the maximum range of the quantization bins for each channel (see <ref type="figure">Figure 4</ref>). The authors in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28]</ref> suggest to perform channel equalization by exploiting the positive scale equivariance property of activation functions. It holds for any piece-wise linear activation function in its relaxed form: ? (Sx) = S? (x) where ? is a piece-wise linear function,? is its modified version that fits this requirement and S = diag (s) is a diagonal matrix with s k denoting the scale factor for channel k.</p><p>The positive scaling equivariance can be applied on the following set of consecutive layers: a linear operation, a piece-wise linear function ? and an additional linear operation. This is demonstrated in the following equation:</p><formula xml:id="formula_11">y = W 2 ? (W 1 x + b 1 ) + b 2 = W 2 ? SS ?1 (W 1 x + b 1 ) + b 2 = W 2 S?(S ?1 (W 1 x + b 1 )) + b 2 ,<label>(10)</label></formula><p>where W 1 and b 1 are the first layer's weights and bias, W 2 and b 2 are the second layer's weights and bias. Although Eq. 10 demonstrates the case of fully-connected layers, it can be also extended for CNNs where the scaling is performed per channel. We present a use case of channel equalization named Max Channel Equalization which can be applied in any quantization scheme. We assume that? is one of the following non-linear functions: ReLU, ReLU8 or PReLU. Given the quantization threshold t of a non-linear function as well as the maximal activation value of the k th channel v k = max</p><formula xml:id="formula_12">X?F l (D) max i,j |X i,j,k |,</formula><p>where X is the activation tensor of the l th layer, we set:</p><formula xml:id="formula_13">s k = min v k t , 1 ,<label>(11)</label></formula><p>so that the maximal value of each channel in tensor X will be the threshold value (see <ref type="figure">Figure 4</ref>). <ref type="figure">Figure 4</ref>: An example of Max Channel Equalization using MobileNetV2 <ref type="bibr" target="#b28">[29]</ref> . Left: the max value v max of each channel. Middle: the inverse scale factor 1 s k for each channel k. Right: the max value of each channel after equalization using this scaling factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Weight Quantization</head><p>In the Weight Quantization stage we quantize the network's weights. It was shown in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> that weight quantization with scaling per channel improves accuracy. Moreover, this work presents an efficient dot product and convolution implementation supporting per-channel quantization. Our Weight Quantization stage consists of per-channel threshold selection and bias correction <ref type="bibr" target="#b22">[23]</ref>.</p><p>Threshold Selection. As noted above, weight quantization is performed perchannel. Its thresholds are selected similarly to activation thresholds (see Algorithm 1). However, a key difference is that here the search is performed directly on the weight values, opposed to the statistical values that are used for activation. More precisely, given the weights w ? R n of some channel in the network, the initial no-clipping threshold is</p><formula xml:id="formula_14">t nc = 2 log 2 max i |wi| ,<label>(12)</label></formula><p>where w i ? R are the entries of w. Additionally, the error induced by a threshold t is</p><formula xml:id="formula_15">ERR (t) = M SE(Q(w, t, n b ), w) = 1 n i (Q(w i , t, n b ) ? w i ) 2 .<label>(13)</label></formula><p>Note that as with activations, MSE is selected as an error measurement since it yields the best performance (see <ref type="table" target="#tab_1">Table 10</ref>).</p><p>Bias Correction. Quantization of weights induce bias shifts to activation means that may lead to detrimental behaviour in the following layers <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32]</ref>. can be expressed as follows:</p><formula xml:id="formula_16">E [y] ? E [?] = E W ?W E [x] = E [x] .<label>(14)</label></formula><p>Several works propose approaches to correct the quantization induced bias. These include using batch-normalization statistics <ref type="bibr" target="#b22">[23]</ref>, micro training <ref type="bibr" target="#b31">[32]</ref> and applying scale and shift per channel <ref type="bibr" target="#b32">[33]</ref>.</p><p>We adopt the solution in <ref type="bibr" target="#b22">[23]</ref>, in which the bias shift is fixed by modifying the layer's bias vectorb</p><formula xml:id="formula_17">= b ? E [x] ,<label>(15)</label></formula><p>where E [x] is the per channel empirical mean obtain in the Statistic Collection stage above. Note that although the above is written for a fully connected layer, it applies to convolutional layers as well, as shown in <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section we evaluate the performance of HPTQ with 8-bit quantization over different tasks and a variety of network architectures. The experiments are divided into two parts. The first part presents an overall performance comparison to the floating point baseline as well as to state-of-the-art quantization approaches. The second part presents an ablation study that analyzes the influence of each technique in HPTQ separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Performance Evaluation</head><p>We evaluate the performance of HPTQ on four different tasks: image classification, object detection, semantic segmentation and pose estimation. For each task, we present a comparison between the performance of models quantized by HPTQ and their floating point baselines. Furthermore, for classification and segmentation we provide a comprehensive performance comparison of HPTQ with both PTQ and QAT state-of-the art quantization methods. We use the same set of hyper-parameters for all our experiments. Specifically, the number of image samples in the representative dataset D is 500. The zscore threshold in the outlier removal step is z th = 24. The SNC threshold is ? = 0.25. Last, for both activations and weights, the number of iterations performed in Algorithm 1 in the threshold selection search is set to n = 10. One should note that fine-tuning the hyper-parameters per network may lead to further improvement. In all of the tables below ? is the difference between the performance of the floating point model and the quantized model, PC indicates the use of weights per channel quantization and PoT indicates power-of-two thresholds.</p><p>Classification. We evaluate HPTQ on the ImageNet classification task <ref type="bibr" target="#b33">[34]</ref> using MobileNetV1 <ref type="bibr" target="#b1">[2]</ref> , MobileNetV2 <ref type="bibr" target="#b28">[29]</ref> and ResNet50 <ref type="bibr" target="#b0">[1]</ref> architectures 2 . Tables 1, 2 and 3 present comparisons of HPTQ with other quantization methods, both PTQ and QAT, for the three architectures. The results show that HPTQ achieves competitive performance despite the hardware friendly constraints. In the tables below F-Acc is the floating point accuracy and Q-Acc is the accuracy of the quantized model.    <ref type="table" target="#tab_4">Table 4</ref> shows that HPTQ achieves competitive results compared to other PTQ methods. Object Detection. We evaluate HPTQ on COCO <ref type="bibr" target="#b44">[45]</ref> using the SSD detector <ref type="bibr" target="#b3">[4]</ref> with several backbones 4 . HPTQ achieves similar Mean Average Precision (mAP) to the floating point baseline as demonstrated in <ref type="table" target="#tab_5">Table 5</ref>. Pose-Estimation. We evaluate HPTQ on the single-person pose estimation task using LPN network <ref type="bibr" target="#b6">[7]</ref> on the LIP (Look into Person) dataset <ref type="bibr" target="#b45">[46]</ref>. We use the PCKh metric <ref type="bibr" target="#b45">[46]</ref> for evaluation, which is the head-normalized probability of correct keypoints. HPTQ achieves similar performance to the floating point baseline with only a slight degradation from 81.65 to 81.53 PCKh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>We provide an ablation study of HPTQ's performance on the ImageNet classification task <ref type="bibr" target="#b33">[34]</ref> using eleven networks 5 . The study is divided into two parts analyzing activation quantization and weight quantization. <ref type="table" target="#tab_6">Table 6</ref> compares the performance of HPTQ between four cases: full floatingpoint, activation quantization, weight quantization and joint quantization of both. The comparison shows that activation quantization causes a larger degradation in performance compared to weight quantization, especially for Efficient-Net with Swish activations functions. This might be due to the fact that activation equalization is not applied for these activations. Activation Quantization Analysis. In this analysis we evaluate the influence of the different methods used for quantizing the activations (without quantizing the weights). The analysis is performed with eleven different network architectures 67 on the ImageNet classification <ref type="bibr" target="#b33">[34]</ref> task. <ref type="table" target="#tab_7">Table 7</ref> shows an accuracy comparison using four different threshold selection methods without applying any other of the activation quantization steps. NC indicates using the no-clipping threshold. Mean Square Error (MSE), Mean Average Error (MAE) and Kullback-Leibler (KL) are three different error measurements d in Equation 6.  <ref type="table" target="#tab_8">Table 8</ref> shows the incremental accuracy influence on ImageNet classification <ref type="bibr" target="#b33">[34]</ref> of the methods used by HPTQ for activation quantization (without quantizing weights). Note that SNC is applied in all of the experiments in the table and its influence is studied separately below. The table shows that all of the methods result in an improvement. Note that fine-tuning the z-score threshold z th per network may lead to further improvement.  <ref type="table" target="#tab_9">Table 9</ref> shows the accuracy improvement achieved by applying Shift Negative Correction (SNC). Specifically, the table compares the performance of several versions of MobileNetV1, each with different non-linear functions, with a full flow of activation quantization. Weight Quantization Analysis. In this analysis we evaluate the influence of the different methods used for quantizing weights (without quantizing activations). The analysis is performed with eleven different network architectures 89 on the ImageNet classification <ref type="bibr" target="#b33">[34]</ref> task. <ref type="table" target="#tab_1">Table 10</ref> shows an accuracy comparison of each quantized network using four different threshold selection methods (without applying bias correction).</p><p>NC indicates using the no-clipping threshold. Mean Square Error (MSE), Mean Average Error (MAE) and Kullback-Leibler (KL) are three different error measurements d in Equation <ref type="bibr" target="#b5">6</ref>. Similarly to the results for activation quantization in <ref type="table" target="#tab_7">Table 7</ref>, the MSE error measurement achieves the best results.  <ref type="table" target="#tab_1">Table 11</ref> shows the incremental accuracy influence of the two methods (per channel quantization and bias correction) used in HPTQ for weight quantization (without quantizing activations) on the ImageNet classification task <ref type="bibr" target="#b33">[34]</ref>. This table shows that both of our methods result in improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work we propose HPTQ, a method for hardware-friendly post-training quantization. HPTQ offers a flow that adapts and synergistically combines several known quantization techniques both for weights and activations. We extensively evaluated the performance of HPTQ on four tasks: classification, object detection, semantic segmentation and pose estimation. Notably, for all of the tasks we demonstrated that competitive results can be obtained under our hardware-friendly constraints of uniform and symmetric quantization with power-of-two thresholds. In addition, we performed an ablation study in which we presented the contributions of each of the methods used by HPTQ.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Uniform, Symmetric and Hardware-Friendly Quantizers. Illustration of the loss in quantization bins due to hardware friendly constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The HPTQ framework. Dashed lines represent statistical information passing, which include also their updates, dotted lines represent data passing and solid lines represent an updated network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Outlier Removal. Left: an input data distribution. Middle: the respective distribution of absolute z-score values. Right: data distribution after outlier removal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Explicitly, let y = Wx + b be the floating point output of a fully connected layer where x, W, b are the floating-point input activation, weight and bias, respectively. Denote the quantized weights of the layer byW = Q(W, t, n b ) and the corresponding output by? =Wx + b. The induced bias shift E [y] ? E [?]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>ImageNet classification<ref type="bibr" target="#b33">[34]</ref> with MobileNetV1<ref type="bibr" target="#b1">[2]</ref> </figDesc><table><row><cell>Type</cell><cell>Method</cell><cell cols="2">PC PoT F-Acc Q-Acc</cell><cell>?</cell></row><row><cell>QAT</cell><cell>QT [10] TQT [11]</cell><cell>70.9 71.1</cell><cell>70.0 71.1</cell><cell>0.9 0.0</cell></row><row><cell></cell><cell>SSBD [28]</cell><cell>70.9</cell><cell>69.95</cell><cell>0.95</cell></row><row><cell>PTQ</cell><cell>Krishnamoorthi [30] Wu et al [35] Lee et al [36]</cell><cell>70.9 71.88 69.5</cell><cell>70.3 70.39 68.84</cell><cell>0.6 1.49 0.66</cell></row><row><cell></cell><cell>HPTQ (Our)</cell><cell>70.55</cell><cell>70.41</cell><cell>0.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>ImageNet classification<ref type="bibr" target="#b33">[34]</ref> with MobileNetV2<ref type="bibr" target="#b28">[29]</ref> </figDesc><table><row><cell>Type</cell><cell>Method</cell><cell cols="2">PC PoT F-Acc Q-Acc</cell><cell>?</cell></row><row><cell>QAT</cell><cell>QT [10] RVQuant [37] TQT [11]</cell><cell>71.9 70.10 71.7</cell><cell>70.9 70.29 71.8</cell><cell>1.0 -0.19 -0.10</cell></row><row><cell></cell><cell>AdaQuant [38]</cell><cell>73.03</cell><cell>73.03</cell><cell>0.0</cell></row><row><cell></cell><cell>ZeroQ [15]</cell><cell>73.03</cell><cell>72.91</cell><cell>0.12</cell></row><row><cell></cell><cell>SSBD [28]</cell><cell>71.9</cell><cell>71.29</cell><cell>0.61</cell></row><row><cell></cell><cell>Wu et al [35]</cell><cell>71.88</cell><cell>71.14</cell><cell>0.74</cell></row><row><cell>PTQ</cell><cell>Krishnamoorthi [30] Nagel et al [20]</cell><cell>71.9 71.72</cell><cell>69.7 70.99 71.16</cell><cell>2.2 0.73 0.56</cell></row><row><cell></cell><cell>DFQ [23]</cell><cell>71.72</cell><cell>70.92</cell><cell>0.8</cell></row><row><cell></cell><cell>Lee et al [36]</cell><cell>71.23</cell><cell>69.5</cell><cell>1.73</cell></row><row><cell></cell><cell>HPTQ (Our)</cell><cell cols="2">71.812 71.46</cell><cell>0.352</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>ImageNet classification<ref type="bibr" target="#b33">[34]</ref> with ResNet50<ref type="bibr" target="#b0">[1]</ref> </figDesc><table><row><cell>Type</cell><cell>Method</cell><cell cols="2">PC PoT F-Acc Q-Acc</cell><cell>?</cell></row><row><cell></cell><cell>QT [10]</cell><cell>76.4</cell><cell>74.9</cell><cell>1.5</cell></row><row><cell></cell><cell>RVQuant [37]</cell><cell>75.92</cell><cell>75.67</cell><cell>0.25</cell></row><row><cell>QAT</cell><cell>HAWQ-V3 [39] LSQ [40]</cell><cell>77.72 76.9</cell><cell>77.58 76.8</cell><cell>0.14 0.1</cell></row><row><cell></cell><cell>TQT [11]</cell><cell>76.9</cell><cell>76.5</cell><cell>0.4</cell></row><row><cell></cell><cell>FAQ [41]</cell><cell>75.4</cell><cell>75.4</cell><cell>0.0</cell></row><row><cell></cell><cell>ZeroQ [15]</cell><cell>77.72</cell><cell>77.67</cell><cell>0.05</cell></row><row><cell></cell><cell>OCS [42]</cell><cell>76.1</cell><cell>75.9</cell><cell>0.2</cell></row><row><cell></cell><cell>SSBD [28]</cell><cell>75.2</cell><cell>74.95</cell><cell>0.25</cell></row><row><cell></cell><cell>He et al [43]</cell><cell>75.3</cell><cell>75.03</cell><cell>0.27</cell></row><row><cell>PTQ</cell><cell>Wu et al [35] Nagel et al [20]</cell><cell>76.16 76.07</cell><cell>76.05 75.87 75.88</cell><cell>0.11 0.2 0.19</cell></row><row><cell></cell><cell>Krishnamoorthi [30]</cell><cell>75.2</cell><cell>75.00 75.1</cell><cell>0.20 0.1</cell></row><row><cell></cell><cell>HPTQ (Our)</cell><cell cols="3">75.106 75.018 0.088</cell></row><row><cell cols="5">Semantic Segmentation. We evaluate HPTQ on Pascal VOC [44] using</cell></row><row><cell cols="3">DeepLab V3 3 [6] with MobileNetV2 [29] as a backbone.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Semantic segmentation on Pascal VOC<ref type="bibr" target="#b43">[44]</ref> using DeepLab V3 with Mo-bileNetV2<ref type="bibr" target="#b28">[29]</ref> as a backbone. F-mIoU is the floating point mean Intersectionover-Union (mIoU) and Q-mIoU is the mIoU of the quantized model.</figDesc><table><row><cell>Type</cell><cell>Method</cell><cell cols="2">PC PoT F-mIoU Q-mIoU</cell><cell>?</cell></row><row><cell></cell><cell>DFQ [23]</cell><cell>72.45</cell><cell>72.33</cell><cell>0.12</cell></row><row><cell>PTQ</cell><cell>Nagel et al [20]</cell><cell>72.94</cell><cell>72.44 72.27</cell><cell>0.50 0.67</cell></row><row><cell></cell><cell>HPTQ (Our)</cell><cell>75.57</cell><cell>75.38</cell><cell>0.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Object detection results with HPTQ on COCO<ref type="bibr" target="#b44">[45]</ref> using MobileNetV2<ref type="bibr" target="#b28">[29]</ref> and ResNet50<ref type="bibr" target="#b0">[1]</ref> as backbones. F-mAP is the floating point mAP and Q-mAP is the mAP of the quantized model.</figDesc><table><row><cell>Model</cell><cell cols="2">F-mAP Q-mAP</cell></row><row><cell>SSD MobileNetV2 [29] 320x320</cell><cell>20.2</cell><cell>20.21</cell></row><row><cell cols="2">SSD MobileNetV2 [29] FPN Lite 320x320 22.2</cell><cell>21.93</cell></row><row><cell>SSD ResNet50 [1] V1 FPN 640x640</cell><cell>34.3</cell><cell>34.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>ImageNet classification<ref type="bibr" target="#b33">[34]</ref> accuracy with HPTQ in four cases: full floating-point, activation quantization, weight quantization and both activation and weight quantization.</figDesc><table><row><cell>Network</cell><cell>F-Acc</cell><cell>Q-Acc (Activation)</cell><cell>Q-Acc (Weights)</cell><cell>Q-Acc (Both)</cell></row><row><cell>MobileNetV1 [2]</cell><cell cols="2">70.558 70.48</cell><cell>70.394</cell><cell>70.418</cell></row><row><cell>MobileNetV2 [29]</cell><cell cols="2">71.812 71.616</cell><cell>71.668</cell><cell>71.46</cell></row><row><cell>NasnetMobile [47]</cell><cell cols="2">74.376 74.068</cell><cell>74.352</cell><cell>73.888</cell></row><row><cell>VGG16 [48]</cell><cell cols="2">70.956 70.834</cell><cell>70.946</cell><cell>70.81</cell></row><row><cell>InceptionV3 [49]</cell><cell cols="2">77.908 77.872</cell><cell>77.844</cell><cell>77.85</cell></row><row><cell cols="3">InceptionResNetV2 [50] 80.284 80.154</cell><cell>80.32</cell><cell>80.14</cell></row><row><cell>ResNet50 [1]</cell><cell cols="2">75.106 75.072</cell><cell>75.06</cell><cell>75.018</cell></row><row><cell>EfficientNet-B0 [51]</cell><cell>77.2</cell><cell>74.3</cell><cell>77.012</cell><cell>74.216</cell></row><row><cell>EfficientNet-B0 ReLU 6</cell><cell>77.65</cell><cell>77.1</cell><cell>77.568</cell><cell>77.092</cell></row><row><cell>DenseNet-121 [52]</cell><cell cols="2">74.848 73.252</cell><cell>74.784</cell><cell>73.356</cell></row><row><cell>Xception [53]</cell><cell>79.05</cell><cell>79.048</cell><cell>79.062</cell><cell>78.972</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>ImageNet classification<ref type="bibr" target="#b33">[34]</ref> accuracy with activations quantized using different threshold selection methods (weights are in floating point).</figDesc><table><row><cell>Network</cell><cell>NC</cell><cell>MSE MAE</cell><cell>KL</cell></row><row><cell>MobileNetV1 [2]</cell><cell cols="3">70.406 70.434 60.218 70.418</cell></row><row><cell cols="2">MobileNetV2 [29] 71.25</cell><cell cols="2">71.458 65.918 71.482</cell></row><row><cell>VGG16 [48]</cell><cell>70.8</cell><cell>70.764 58.37</cell><cell>65.096</cell></row><row><cell>ResNet50 [1]</cell><cell cols="3">74.612 74.996 67.896 59.556</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>The accuracy influence of the different activation quantization methods used by HPTQ for ImageNet classification<ref type="bibr" target="#b33">[34]</ref> when keeping all weights in floating point. Baseline is quantization with no-clipping thresholds, +Eq. means adding max channel equalization, +MSE Th. means replacing the no-clipping thresholds with MSE and +z-score means applying z-score outlier removal.</figDesc><table><row><cell>Network Name</cell><cell cols="4">Baseline +Eq. +MSE Th. +z-score</cell></row><row><cell>MobileNetV1 [2]</cell><cell>70.406</cell><cell cols="2">70.418 70.48</cell><cell>70.48</cell></row><row><cell>MobileNetV2 [29]</cell><cell>71.25</cell><cell>71.34</cell><cell>71.528</cell><cell>71.616</cell></row><row><cell>NasnetMobile [47]</cell><cell>18.572</cell><cell cols="2">18.484 73.486</cell><cell>74.068</cell></row><row><cell>VGG16 [48]</cell><cell>70.8</cell><cell cols="2">70.696 70.888</cell><cell>70.834</cell></row><row><cell>InceptionV3 [49]</cell><cell>77.658</cell><cell cols="2">77.646 77.832</cell><cell>77.872</cell></row><row><cell cols="2">InceptionResNetV2 [50] 49.132</cell><cell cols="2">49.238 80.014</cell><cell>80.154</cell></row><row><cell>ResNet50 [1]</cell><cell>74.612</cell><cell cols="2">74.654 75.086</cell><cell>75.072</cell></row><row><cell>EfficientNet-B0 [51]</cell><cell>13.562</cell><cell cols="2">13.736 74.096</cell><cell>74.3</cell></row><row><cell>EfficientNet-B0 ReLU 6</cell><cell>74.298</cell><cell cols="2">76.298 76.956</cell><cell>77.1</cell></row><row><cell>DenseNet-121 [52]</cell><cell>56.08</cell><cell cols="2">55.916 73.28</cell><cell>73.252</cell></row><row><cell>Xception [53]</cell><cell>48.718</cell><cell cols="2">48.784 78.87</cell><cell>79.048</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>ImageNet classification accuracy<ref type="bibr" target="#b33">[34]</ref> using HPTQ with and without SNC of MobileNetV1<ref type="bibr" target="#b1">[2]</ref> trained with different non-linear functions.</figDesc><table><row><cell></cell><cell>Swish</cell><cell>Leaky ReLU (? = 0.1)</cell><cell cols="2">PReLU SELU</cell></row><row><cell>Float</cell><cell cols="2">73.522 72.866</cell><cell>73.114</cell><cell>72.032</cell></row><row><cell cols="2">Without SNC 60.98</cell><cell>71.966</cell><cell>72.548</cell><cell>69.726</cell></row><row><cell>With SNC</cell><cell cols="2">71.146 72.588</cell><cell>72.548</cell><cell>70.902</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>ImageNet classification<ref type="bibr" target="#b33">[34]</ref> accuracy with weights quantized using different threshold selection methods (activations are in floating point).</figDesc><table><row><cell>Network</cell><cell>NC</cell><cell cols="2">MSE MAE</cell><cell>KL</cell></row><row><cell>MobileNetV1 [2]</cell><cell>68.75</cell><cell cols="3">68.756 64.242 64.968</cell></row><row><cell>MobileNetV2 [29]</cell><cell cols="3">69.562 69.758 67.57</cell><cell>62.394</cell></row><row><cell>NasnetMobile [47]</cell><cell cols="3">74.188 74.232 72.79</cell><cell>73.358</cell></row><row><cell>VGG16 [48]</cell><cell cols="2">70.944 70.94</cell><cell cols="2">67.486 70.472</cell></row><row><cell>InceptionV3 [49]</cell><cell cols="2">77.768 77.82</cell><cell>70.91</cell><cell>74.28</cell></row><row><cell cols="5">InceptionResNetV2 [50] 80.244 80.276 78.676 77.112</cell></row><row><cell>ResNet50 [1]</cell><cell cols="2">75.068 75.11</cell><cell cols="2">72.352 73.418</cell></row><row><cell>EfficientNet-B0 [51]</cell><cell cols="3">76.822 76.822 75.86</cell><cell>75.554</cell></row><row><cell>EfficientNet-B0 ReLU 6</cell><cell cols="4">77.078 77.218 76.916 76.674</cell></row><row><cell>DenseNet-121 [52]</cell><cell cols="4">74.734 74.736 72.102 60.17</cell></row><row><cell>Xception [53]</cell><cell cols="3">79.006 79.006 77.47</cell><cell>75.374</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>The incremental influence of applying per-channel threshold selection (Per ch.) and bias correction (Bias corr.) on ImageNet<ref type="bibr" target="#b33">[34]</ref> classification accuracy. Baseline means quantization with MSE threshold applied per tensor.</figDesc><table><row><cell>Network</cell><cell cols="3">Baseline Per ch. +Bias corr.</cell></row><row><cell>MobileNetV1 [2]</cell><cell>0.966</cell><cell>68.756</cell><cell>70.394</cell></row><row><cell>MobileNetV2 [29]</cell><cell>0.398</cell><cell>69.758</cell><cell>71.668</cell></row><row><cell>NasnetMobile [47]</cell><cell>73.494</cell><cell>74.232</cell><cell>74.352</cell></row><row><cell>VGG16 [48]</cell><cell>70.814</cell><cell>70.94</cell><cell>70.946</cell></row><row><cell>InceptionV3 [49]</cell><cell>76.42</cell><cell>77.82</cell><cell>77.844</cell></row><row><cell cols="2">InceptionResNetV2 [50] 80.066</cell><cell>80.276</cell><cell>80.32</cell></row><row><cell>ResNet50 [1]</cell><cell>74.718</cell><cell>75.11</cell><cell>75.06</cell></row><row><cell>EfficientNet-B0 [51]</cell><cell>2.524</cell><cell>76.822</cell><cell>77.012</cell></row><row><cell>EfficientNet-B0 ReLU 6</cell><cell>0.682</cell><cell>77.218</cell><cell>77.568</cell></row><row><cell>DenseNet-121 [52]</cell><cell>72.986</cell><cell>74.736</cell><cell>74.784</cell></row><row><cell>Xception [53]</cell><cell>78.786</cell><cell>79.006</cell><cell>79.062</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.tensorflow.org/api_docs/python/tf/keras/applications</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/ model_zoo.md</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/tensorflow/models/blob/master/research/object_detection/ g3doc/tf2_detection_zoo.md 5 https://www.tensorflow.org/api_docs/python/tf/keras/applications</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">EfficientNet-B0 ReLU is a trained version of EfficientNet-B0 with ReLU activation function instead of swish 7 https://keras.io/api/applications/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">6EfficientNet-B0 ReLU is a trained version of EfficientNet-B0 with ReLU activation function instead of swish 9 https://keras.io/api/applications/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ssd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
	<note>Feature pyramid networks for object detection</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Simple and lightweight human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10346</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="172" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A survey of quantization methods for efficient neural network inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13630</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Trained quantization thresholds for accurate and efficient fixed-point inference of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Sambhav R Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gural</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08066</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Jen</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayalakshmi</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailash</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gopalakrishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06085</idno>
		<title level="m">Pact: Parameterized clipping activation for quantized neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Differentiable soft quantization: Bridging full-precision and low-bit neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazhen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4852" to="4861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Post-training 4-bit quantization of convolution networks for rapid-deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Nahshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05723</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zeroq: A novel zero shot quantization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13169" to="13178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Up or down? adaptive rounding for post-training quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rana</forename><forename type="middle">Ali</forename><surname>Amjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart</forename><surname>Van Baalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7197" to="7206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Post-training piecewise linear quantization for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamzah</forename><surname>Abdel-Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Thorsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Georgiadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">H</forename><surname>Hassoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="69" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hmq: Hardware friendly mixed precision quantization block for cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">Victor</forename><surname>Habi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnon</forename><surname>Netzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="448" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mixed precision dnns: All you need is a good parametrization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Cardinaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Yoshiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><forename type="middle">Alonso</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11452</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Fournarakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rana</forename><forename type="middle">Ali</forename><surname>Amjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelysei</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart</forename><surname>Van Baalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08295</idno>
		<title level="m">A white paper on neural network quantization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Outlier analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="237" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lsq+: Improving low-bit quantization through learnable offsets and better initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Bhalgat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="696" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data-free quantization through weight equalization and bias correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart</forename><surname>Van Baalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Nahshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Chmiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaim</forename><surname>Baskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenii</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Mendelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07190</idno>
		<title level="m">Loss aware post-training quantization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Szymon Migacz. 8-bit inference with tensorrt</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Same, same but different-recovering neural network quantization error through weight factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Meller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Almog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Grobman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01917</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Quantizing deep convolutional networks for efficient inference: A whitepaper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08342</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Almog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Grobman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03193</idno>
		<title level="m">Fighting quantization bias with bias</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Post training 4-bit quantization of convolutional networks for rapid-deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Nahshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7950" to="7958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius. Integer quantization for deep learning inference: Principles and empirical evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Judd</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09602</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwon</forename><surname>Jun Haeng Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saerom</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Won-Jo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05488</idno>
		<title level="m">Quantization for rapid deployment of deep neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Value-aware quantization for training and inference of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunhyeok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="580" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improving post training neural quantization: Layer-wise calibration and integer programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Nahshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Hanani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10518</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hawq-v3: Dyadic neural network quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangcheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiali</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11875" to="11886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepika</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bablani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08153</idno>
		<title level="m">Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discovering low-precision networks close to full-precision networks for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rathinakumar</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepika</forename><surname>Appuswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bablani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Izzet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharmendra S</forename><surname>Yildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving neural network quantization without retraining using outlier channel splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritchie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Dotzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiru</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7543" to="7552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning compression from limited unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="752" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Look into person: Joint body parsing &amp; pose estimation network and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="871" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
