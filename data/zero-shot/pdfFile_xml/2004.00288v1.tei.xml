<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University ? Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University ? Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University ? Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University ? Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University ? Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University ? Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University ? Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University ? Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University ? Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Tencent</roleName><forename type="first">Youtu</forename><surname>Lab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University ? Michigan State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As an emerging topic in face recognition, designing margin-based loss functions can increase the feature margin between different classes for enhanced discriminability. More recently, the idea of mining-based strategies is adopted to emphasize the misclassified samples, achieving promising results. However, during the entire training process, the prior methods either do not explicitly emphasize the sample based on its importance that renders the hard samples not fully exploited; or explicitly emphasize the effects of semi-hard/hard samples even at the early training stage that may lead to convergence issue. In this work, we propose a novel Adaptive Curriculum Learning loss (Cur-ricularFace) that embeds the idea of curriculum learning into the loss function to achieve a novel training strategy for deep face recognition, which mainly addresses easy samples in the early training stage and hard ones in the later stage. Specifically, our CurricularFace adaptively adjusts the relative importance of easy and hard samples during different training stages. In each stage, different samples are assigned with different importance according to their corresponding difficultness. Extensive experimental results on popular benchmarks demonstrate the superiority of our CurricularFace over the state-of-the-art competitors. * denotes Ying Tai and Shaoxin Li are corresponding authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hard Hard</head><p>Early Stage</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Later Stage</head><p>Harder Harder</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The success of Convolutional Neural Networks (CNNs) on face recognition can be mainly credited to: enormous training data, network architectures, and loss functions. Recently, designing effective loss functions that enhance discriminative power is pivotal for training deep face CNNs.</p><p>Current state-of-the-art (SOTA) face recognition methods mainly adopt softmax-based classification loss. Since the learned features with the original softmax is not suf- <ref type="figure">Figure 1</ref>. Different training strategies for modulating negative cosine similarities of hard samples (i.e., the mis-classified samples) in ArcFace <ref type="bibr" target="#b7">[8]</ref>, MV-Arc-Softmax <ref type="bibr" target="#b30">[31]</ref> and our Curricular-Face. Left: The modulation coefficients I(t, cos ?j) for negative cosine similarities of hard samples in different methods, where t is an adaptively estimated parameter and ?j denotes the angle between the hard sample and the non-ground truth j-class center. Right: The corresponding hard samples' negative cosine similarities N (t, cos ?j) = I(t, cos ?j) cos ?j +c after modulation, where c indicates a constant. On one hand, during early training stage (e.g., t is close to 0), hard sample's negative cosine similarities are usually reduced, and thus leads to smaller hard sample loss than the original one. Therefore, easier samples are relatively emphasized; during later training stage (e.g., t is close to 1), the hard sample's negative cosine similarities are enhanced, and thus leads to larger hard sample loss. On the other hand, in the same training stage, we modulate the hard samples' negative cosine similarities with cos ?j. Specifically, the smaller the angle ?j is, the larger the modulation coefficient should be.</p><p>ficiently discriminative for the practical face recognition problem <ref type="bibr" target="#b13">[14]</ref>, which means that the testing identities are usually disjoint from the training set, several margin-based variants have been proposed to enhance features' discriminative power. For example, explicit margin, i.e., Cos-Face <ref type="bibr" target="#b29">[30]</ref>, Sphereface <ref type="bibr" target="#b13">[14]</ref>, ArcFace <ref type="bibr" target="#b7">[8]</ref>, and implicit margin, i.e., Adacos <ref type="bibr" target="#b37">[38]</ref>, supplement the original softmax function to enforce greater intra-class compactness and inter-class discrepancy, which result in more discriminate features. However, these margin-based loss functions do not explicitly emphasize each sample according to its im-portance.</p><p>As demonstrated in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>, hard sample mining is also a critical step to further improve the final accuracy. As a commonly-used hard sample mining method, OHEM <ref type="bibr" target="#b25">[26]</ref> focuses on the large-loss samples in one mini-batch, in which the percentage of hard samples is empirically decided and easy samples are completely discarded. Focal loss <ref type="bibr" target="#b15">[16]</ref> is a soft mining variant that rectifies the loss function to a elaborately designed form, in which two hyperparameters should be tuned with a lot of efforts to decide the weights of each sample and hard samples are emphasized by reducing the weights of easy samples. Recently, Triplet loss <ref type="bibr" target="#b22">[23]</ref> and MV-Arc-Softmax <ref type="bibr" target="#b30">[31]</ref> are motivated by integrating both margin and mining into one framework. Triplet loss adopts a semi-hard mining strategy to obtain semi-hard triplets and enlarges the margin between triplet samples. MV-Arc-Softmax <ref type="bibr" target="#b30">[31]</ref> clearly defines hard samples as misclassified samples and emphasizes them by increasing the weights of their negative cosine similarities with a preset constant. In a nutshell, mining-based loss functions explicitly emphasize the effects of semi-hard or hard samples <ref type="bibr" target="#b22">[23]</ref>.</p><p>However, there are drawbacks in training strategies of both margin-and mining-based loss functions. The general softmax-based loss function can be formulated as follows:</p><formula xml:id="formula_0">L = ? log e sT (cos ?y i ) e sT (cos ?y i )+ n j=1,j =y i e sN (t,cos ? j ) ,<label>(1)</label></formula><p>where T (cos ? yi ) and N (t, cos ? j ) = I(t, cos ? j ) cos ? j + c are the functions to define the positive and negative cosine similarities, respectively. I(t, cos ? j ) denotes the modulation coefficients of negative cosine similarities and c is a constant. For margin-based methods, mining strategy is ignored and thus the difficultness of each sample is not exploited, which may lead to convergence issues when using a large margin on small backbones, e.g., MobileFaceNet <ref type="bibr" target="#b5">[6]</ref>. As shown in <ref type="figure">Fig. 1</ref>, the modulation coefficients I(?) for the negative cosine similarities are fixed as a constant of 1 in Ar-cFace for all samples during the entire training process. For mining-based methods, over-emphasizing hard samples in early training stage may hinder the model to converge. MV-Arc-Softmax emphasizes hard samples by modulating the negative cosine similarity as N (t, cos ? j ) = t cos ? j + t ? 1, i.e., I(t, cos ? j ) = t, where t is a manually defined constant. As MV-Arc-Softmax claimed, t plays a key role in the model convergence property and a slight larger value (e.g., &gt;1.4) may cause the model difficult to converge. Thus t needs to be carefully tuned.</p><p>In this work, we propose a novel adaptive curriculum learning loss, termed CurricularFace, to achieve a novel training strategy for deep face recognition. Motivated by the nature of human learning that easy cases are learned first and then come the hard ones <ref type="bibr" target="#b1">[2]</ref>, our CurricularFace incorporates the idea of Curriculum Learning (CL) into face recognition in an adaptive manner, which differs from the traditional CL in two aspects. First, the curriculum construction is adaptive. In traditional CL, the samples are ordered by the corresponding difficultness, which are often defined by a prior and then fixed to establish the curriculum. In CurricularFace, the samples are randomly selected in each mini-batch, while the curriculum is established adaptively via mining the hard samples online, which shows the diversity in samples with different importance. Second, the importance of hard samples are adaptive. On one hand, the relative importance between easy and hard samples is dynamic and could be adjusted in different training stages. On the other hand, the importance of each hard sample in current mini-batch depends on its own difficultness.</p><p>Specifically, the mis-classified samples in mini-batch are chosen as hard samples and weighted by adjusting the modulation coefficients I(t, cos? j ) of cosine similarities between the sample and the non-ground truth class center vectors, i.e., negative cosine similarity cos? j . To achieve the goal of adaptive curricular learning in the entire training, we design a novel coefficient function I(?) that is determined by two factors: 1) the adaptively estimated parameter t that utilizes moving average of positive cosine similarities between samples and the corresponding ground-truth class center to unleash the burden of manually tuning; and 2) the angle ? j that defines the difficultness of hard samples to achieve adaptive assignment. To sum up, the contributions of this work are:</p><p>? We propose an adaptive curriculum learning loss for face recognition, which automatically emphasizes easy samples first and hard samples later. To the best of our knowledge, it is the first work to introduce the idea of adaptive curriculum learning for face recognition.</p><p>? We design a novel modulation coefficient function I(?) to achieve adaptive curriculum learning during training, which connects positive and negative cosine similarity simultaneously without the need of manually tuning any additional hyper-parameter.</p><p>? We conduct extensive experiments on popular facial benchmarks, which demonstrate the superiority of our CurricularFace over the SOTA competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Margin-based loss function. Loss design is pivotal for large-scale face recognition. Current SOTA deep face recognition methods mostly adopt softmax-based classification loss <ref type="bibr" target="#b27">[28]</ref>. Since the learned features with the original softmax loss are not guaranteed to be discriminative enough for practical face recognition problem <ref type="bibr" target="#b13">[14]</ref>, margin-based losses <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b7">8]</ref> are proposed. Though the margin-based loss functions are verified to obtain good performance, they do not take the difficultness of each sample into consideration, while our CurricularFace emphasizes easy samples first and hard samples later, which is more reasonable and effective.</p><p>Mining-based loss function. Though some mining-based loss function such as Focal loss <ref type="bibr" target="#b15">[16]</ref>, Online Hard Sample Mining (OHEM) <ref type="bibr" target="#b25">[26]</ref> are prevalent in the field of object detection, they are rarely used in face recognition. OHEM focuses on the large-loss samples in one mini-batch, in which the percentage of the hard samples is empirically determined and easy samples are completely discarded. Focal loss emphasizes hard samples by reducing the weights of easy samples, in which two hyper-parameters should be manually tuned. The recent work, MV-Arc-Softmax <ref type="bibr" target="#b30">[31]</ref> fuses the motivations of both margin and mining into one framework for deep face recognition. They define hard samples as misclassified samples and enlarge the weights of hard samples with a preset constant. Our method differs from MV-Arc-Softmax in three aspects: 1) We do not always emphasize hard samples, especially in the early training stages. 2) We assign different weights for hard samples according to their corresponding difficultness. 3) We adaptively estimate the additional hyper-parameter t without manual tuning.</p><p>Curriculum Learning. Learning from easier samples first and harder samples later is a common strategy in Curriculum Learning (CL) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b41">42]</ref>. The key problem in CL is to define the difficultness of each sample. For example, <ref type="bibr" target="#b0">[1]</ref> takes the negative distance to the boundary as the indicator for easiness in classification. However, the ad-hoc curriculum design in CL turns out to be difficult to implement in different problems. To alleviate this issue, <ref type="bibr" target="#b11">[12]</ref> designs a new formulation, called Self-Paced Learning (SPL), where examples with lower losses are considered to be easier and emphasized during training. The key differences between our CurricularFace with SPL are: 1) Our method focuses on easier samples in the early training stage and emphasizes hard samples in the later stage. 2) Our method proposes a novel function N (?) for negative cosine similarities, which achieves not only adaptive assignment on modulation coefficients I(?) for different samples in the same training stage, but also adaptive curriculum learning strategy in different stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed CurricularFace</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary Knowledge on Loss Function</head><p>The original softmax loss is formulated as follows:</p><formula xml:id="formula_1">L = ? log e Wy i x i +by i n j=1 e W j x i +b j ,<label>(2)</label></formula><p>where x i ? R d denotes the deep feature of i-th sample which belongs to the y i class, W j ? R d denotes the j-th column of the weight W ? R d?n and b j is the bias term. The class number and the embedding feature size are n and d, respectively. In practice, the bias is usually set to b j = 0 and the individual weight is set to ||W j ||= 1 by l 2 normalization. The deep feature is also normalized and re-scaled to s. Thus, the original softmax can be modified as follows:</p><formula xml:id="formula_2">L = ? log e s(cos ?y i ) e s(cos ?y i ) + n j=1,j =y i e s(cos ? j ) .<label>(3)</label></formula><p>Since the learned features with original softmax loss may not be discriminative enough for practical face recognition problem, several variants are proposed and can be formulated in a general form:</p><formula xml:id="formula_3">L = ?G(p(xi)) log e sT (cos ?y i ) e sT (cos ?y i ) + n j=1,j =y i e sN (t,cos ? j ) ,<label>(4)</label></formula><p>where p(x i ) = e sT (cos ?y i ) e sT (cos ?y i ) + n j=1,j =y i e sN (t,cos ? j ) is the predicted ground truth probability and G(p(x i )) is an indicator function.</p><p>T (cos ? yi ) and N (t, cos ? j ) = I(t, cos ? j ) cos ? j + c are the functions to modulate the positive and negative cosine similarities, respectively, where c is a constant, and I(t, cos ? j ) denotes the modulation coefficients of negative cosine similarities. In margin-based loss function, e.g., ArcFace, G(p(x i )) = 1, T (cos ? yi ) = cos(? yi + m), and N (t, cos ? j ) = cos ? j . It only modifies the positive cosine similarity of each sample to enhance the feature discrimination. As shown in <ref type="figure">Fig. 1</ref>, the modulation coefficients I(?) of each sample's negative cosine similarities are fixed as 1. The recent work, MV-Arc-Softmax emphasizes hard samples by increasing I(t, cos ? j ) for hard samples. That is, G(p(x i )) = 1 and N (t, cos ? j ) is formulated as follows:</p><formula xml:id="formula_4">N (t, cos ? j ) = cos ?j, T (cos ?y i ) ? cos ?j ? 0 t cos ?j + t ? 1, T (cos ?y i ) ? cos ?j &lt; 0.<label>(5)</label></formula><p>If a sample is defined to be easy, its negative cosine similarity is kept the same as the original one, cos ? j ; if as a hard sample, its negative cosine similarity becomes t cos ? j +t?1. That is, as shown in <ref type="figure">Fig. 1</ref>, I(?) is a constant and determined by a preset hyper-parameter t. Meanwhile, since t is always larger than 1, t cos ? j + t ? 1 &gt; cos ? j always holds true, which means the model always focuses on hard samples, even in the early training stage. However, the parameter t is sensitive that a large pre-defined value (e.g., &gt; 1.4) may lead to convergence issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adaptive Curricular Learning Loss</head><p>Next, we present the details of our proposed adaptive curriculum learning loss, which is the first attempt to intro- </p><formula xml:id="formula_5">W (k+1) = W (k) ? ? (k) ?L ?W , ? (k+1) = ? (k) ? ? (k) ?L ?x i ?x i ?? (k) ; k ? k + 1;</formula><p>Update the parameter t by Eq. 9; end Output: W , ?.</p><p>duce adaptive curriculum learning into deep face recognition. The formulation of our loss function is also contained in the general form, where G(p(x i )) = 1, positive and negative cosine similarity functions are defined as follows:</p><formula xml:id="formula_6">T (cos ?y i ) = cos(?y i + m),<label>(6)</label></formula><p>N (t, cos ? j ) = cos ?j, T (cos ?y i ) ? cos ?j ? 0 cos ?j(t + cos ?j), T (cos ?y i ) ? cos ?j &lt; 0.</p><p>It should be noted that the positive cosine similarity can adopt any margin-based loss functions and here we adopt ArcFace as an example. As shown in <ref type="figure">Fig. 1</ref>, the modulation coefficient I(t, ? j ) of hard sample negative cosine similarity depends on both the value of t and ? j . In the early training stage, learning from easy samples is beneficial to model convergence. Thus, t should be close to zero and I(?) = t + cos ? j is smaller than 1. Therefore, the weights of hard samples are reduced and easy samples are emphasized relatively. As training goes on, the model gradually focuses on the hard samples, i.e., the value of t shall increase and I(?) is larger than 1. Thus, the hard samples are emphasized with larger weights. Moreover, within the same training stage, I(?) is monotonically decreasing with ? j so that harder sample can be assigned with larger coefficient according to its difficultness. The value of the parameter t is automatically estimated in our CurricularFace, otherwise it may require lots of efforts for manual tuning.</p><p>Optimization. Next, we show our CurricularFace can be easily optimized by the conventional stochastic gradient de- scent. Assuming x i denotes the deep feature of i-th sample which belongs to the y i class, the input of the proposed function is the logit f j , where j denotes the j-th class.</p><p>In the forwarding process, when j = y i , it is the same as the ArcFace, i.e., f j = sT (cos ? yi ), T (cos ? yi ) = cos(? yi + m). When j = y i , it has two cases, if x i is an easy sample, it is the the same as the original softmax, i.e., f j = s cos ? j . Otherwise, it will be modulated as f j = sN (t, cos ? j ), where N (t, cos ? j ) = (t + cos ? j ) cos ? j . In the backward propagation process, the gradients w.r.t. x i and W j can also be divided into three cases and computed as follows:</p><formula xml:id="formula_8">?L ?xi = ? ? ? ? ? ? ? ?L ?fy i (s sin(?y i +m) sin ?y i )Wy i , j = yi ?L ?f j sWj, j = yi, easy ?L ?f j s(2 cos ?j + t)Wj j = yi, hard ?L ?Wj = ? ? ? ? ? ? ? ?L ?fy i (s sin(?y i +m) sin ?y i )xi, j = yi ?L ?f j sxi, j = yi, easy ?L ?f j s(2 cos ?j + t)xi j = yi, hard<label>(8)</label></formula><p>Based on the above formulations, we can find the gradient modulation coefficients of hard samples are determined by M (?) = 2 cos ? j + t, which consists of two parts, the negative cosine similarity cos ? j and the value of t. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, on the one hand, the coefficients increase with the adaptive estimation of t (described in the next subsection) to emphasize hard samples. On the other hand, these coefficients are assigned with different importance according to their corresponding difficultness (cos ? j ). Therefore, the values of M in <ref type="figure" target="#fig_0">Fig. 2</ref> are plotted as a range at each training iteration. However, the coefficients are fixed to be 1 and a constant t in ArcFace and MV-Arc-Softmax, respectively. Adaptive Estimation of t. It is critical to determine appropriate values of t in different training stages. Ideally the value of t can indicate the model training stages. We empirically find the average of positive cosine similarities is a good indicator. However, mini-batch statistic-based methods usually face an issue: when many extreme data are sampled in one mini-batch, the statistics can be vastly noisy and the estimation will be unstable. Exponential Moving Average (EMA) is a common solution to address this issue <ref type="bibr" target="#b12">[13]</ref>. Specifically, let r (k) be the average of the positive cosine similarities of the k-th batch and be formulated as r (k) = i cos ? yi , we have:</p><formula xml:id="formula_9">t (k) = ?r (k) + (1 ? ?)t (k?1) ,<label>(9)</label></formula><p>where t 0 = 0, ? is the momentum parameter and set to 0.99. With the EMA, we avoid the hyper-parameter tuning and make the modulation coefficients of hard sample negative cosine similarities I(?) adaptive to the current training stage. To sum up, the loss function of our CurricularFace is formulated as follows: L = ? log e s cos(?y i +m) e s cos(?y i +m) + n j=1,j =y i e sN (t (k) ,cos ? j )</p><p>, <ref type="bibr" target="#b9">(10)</ref> where N (t (k) , cos ? j ) is defined in Eq. 7. The entire training process is summarized in Algorithm 1. <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates how the loss changes from ArcFace to our CurricularFace during training. Here are some observations: 1) As we excepted, hard samples (B and C) are suppressed in early training stage but emphasized later.</p><p>2) The ratio is monotonically increasing with cos? j , since the larger cos? j is, the harder the sample is.</p><p>3) The positive cosine similarity of a perceptual-well image is often large. However, during the early training stage, the negative cosine similarities of the perceptual-well image (A) may also be large so that it could be classified as the hard one.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussions with SOTA Loss Functions</head><p>Comparison with ArcFace and MV-Arc-Softmax. We first discuss the difference between our CurricularFace and the two competitors, ArcFace and MV-Arc-Softmax, from the perspective of the decision boundary in Tab. 1. Arc-Face introduces a margin function T (cos ? yi ) = cos(? yi + m) from the perspective of the positive cosine similarity. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, its decision condition changes from cos ? yi = cos ? j (i.e., blue line) to cos(? yi + m) = cos ? j (red line) for each sample. MV-Arc-Softmax introduces additional margin from the perspective of negative cosine similarity for hard samples, and the decision boundary becomes cos(? yi + m) = t cos ? j + t ? 1 (green line). Conversely, we adaptively adjust the weights of hard samples in different training stages. The decision condition becomes cos(? yi + m) = (t + cos ? j ) cos ? j (purple line). During training, the decision boundary for hard samples changes from one purple line (early stage) to another (later stage), which emphasizes easy samples first and hard samples later.</p><p>Comparison with Focal Loss. Focal loss is formulated as: G(p(x)) = ?(1 ? p(x i )) ? , where ? and ? are modulating factors to be tuned manually. The definition of hard samples in Focal loss is ambiguous, since it focuses on relatively hard samples by reducing the weight of easier samples during entire training process. In contrast, the definition of hard samples in our CurricularFace is more clear, i.e., misclassified samples. Meanwhile, the weights of hard samples are adaptively determined in different training stages. , CFP-FP <ref type="bibr" target="#b23">[24]</ref>, CPLFW <ref type="bibr" target="#b40">[41]</ref>, AgeDB <ref type="bibr" target="#b19">[20]</ref>, CALFW <ref type="bibr" target="#b39">[40]</ref>, IJB-B <ref type="bibr" target="#b32">[33]</ref>, IJB-C <ref type="bibr" target="#b18">[19]</ref>, and MegaFace <ref type="bibr" target="#b10">[11]</ref>.</p><p>Training Setting. We follow <ref type="bibr" target="#b7">[8]</ref> to crop the 112 ? 112 faces with five landmarks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b26">27]</ref>. For the embedding network, we adopt ResNet50 and ResNet100 as in <ref type="bibr" target="#b7">[8]</ref>. Our framework is implemented in Pytorch <ref type="bibr" target="#b20">[21]</ref>. We train models on 4 NVIDIA Tesla P40 GPU with batch size 512. The models are trained with SGD algorithm, with momentum 0.9 and weight decay 5e ? 4. On CASIA-WebFace, the learning rate starts from 0.1 and is divided by 10 at 28, 38, 46 epochs. The training process is finished at 50 epochs. On MS1MV2, we divide the learning rate at 10, 18, 22 epochs and finish at 24 epochs. We follow the common setting as <ref type="bibr" target="#b7">[8]</ref> to set scale s = 64 and margin m = 0.5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>Effects on Fixed vs. Adaptive Parameter t. We first investigate the effect of adaptive estimation of t. We choose four fixed values between 0 and 1 for comparison. Specifically, 0 means the modulation coefficient I(?) of each hard sample's negative cosine similarity is always reduced based on its difficultness. In contrast, 1 means the hard samples are always emphasized. 0.3 and 0.7 are between the two cases. Tab. 2 shows that it is more effective to learn from easier samples first and hard samples later based on our adaptively estimated parameter t. Effects on Different Statistics for Estimating t. We now investigate the effects of several other statistics, i.e., mode of positive cosine similarities in a mini-batch, or mean of the predicted ground truth probability for estimating t in our loss. As Tab. 3 shows: 1) The mean of positive cosine similarities is better than mode.</p><p>2) The positive cosine similarity is more accurate than the predicted ground truth probability to indicate the training stages.</p><p>Robustness on Training Convergence. As claimed in <ref type="bibr" target="#b14">[15]</ref>, ArcFace exhibits the divergence issue when using small backbones like MobileFaceNet. As a result, softmax loss must be incorporated for pre-training. To illustrate the robustness of our loss function on convergence issue with small backbones, we use the MobileFaceNet as the network architecture and train it on CASIA-WebFace. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, when the margin m is set to 0.5, the model trained with our loss achieves 99.25% accuracy on LFW, while the model trained with ArcFace does not converge and the loss is NAN at about 2, 400-th step. When the margin m is set to 0.45, both losses can converge, but our loss achieves better performance (99.20% vs. 99.10%). Comparing the yellow and red curves, since the losses of hard samples are reduced in early training stages, our loss converges much faster in the beginning, leading to lower loss than ArcFace. Later on, the value of our loss is slightly larger than ArcFace, because we emphasize the hard samples in later stages. The results illustrate that learning from easy samples first and hard samples later is beneficial to model convergence.    the performance is near saturated. While for both CFP-FP and CPLFW, our method shows superiority over the baselines including general methods, e.g., <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b3">[4]</ref>, and crosspose methods, e.g., <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b6">[7]</ref>. As a recent face recognition method, MV-Arc-Softmax achieves better performance than ArcFace, but still worse than Our Curricular-Face. Finally, for AgeDB and CALFW, as Tab. 4 shows, our CurricularFace again achieves the best performance than all of the other SOTA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with SOTA Methods</head><p>Results on IJB-B and IJB-C. The IJB-B dataset contains 1, 845 subjects with 21.8K still images and 55K frames from 7, 011 videos. In the 1:1 verification, there are 10, 270 positive matches and 8M negative matches. The IJB-C dataset is a further extension of IJB-B, which contains about 3, 500 identities with a total of 31, 334 images and 117, 542 unconstrained video frames. In the 1:1 verification, there are 19, 557 positive matches and 15, 638, 932 negative matches. On IJB-B and IJB-C datasets, we employ MS1MV2 and the ResNet100 for a fair comparison with recent methods. We follow the testing protocol in Ar-cFace and take the average of the image features as the corresponding template representation without bells and whistles. Note that our method is not proposed for set-based face recognition task, and DOES not adopt any specific strategies for set-based face recognition. The experiments on these two datasets are just to prove that our loss can obtain more discriminate features than the baselines like Arc-Face, which are also generic methods for face recognition. Tab. 5 exhibits the performance of different methods, e.g., Multicolumn <ref type="bibr" target="#b34">[35]</ref>, DCN <ref type="bibr" target="#b33">[34]</ref>, Adacos <ref type="bibr" target="#b37">[38]</ref>, P2SGrad <ref type="bibr" target="#b38">[39]</ref>, PFE <ref type="bibr" target="#b24">[25]</ref> and MV-Arc-Softmax [31] on IJB-B and IJB-C 1:1 verification, our method again achieves the best performance. <ref type="figure" target="#fig_5">Fig. 6</ref> shows the ROC curves of CurricularFace and ArcFace on IJB-B/C with the backbone ResNet100, our method achieves better performance.</p><p>Results on MegaFace. Finally, we evaluate the performance on the MegaFace Challenge. The gallery set of MegaFace includes 1M images of 690K subjects, and the probe set includes 100K photos of 530 unique individuals from FaceScrub. We report the two testing results under two protocols (large or small training set). Here, we use CASIA-WebFace and MS1MV2 under the small protocol and large protocol, respectively. In Tab. 6, our method  achieves the best single-model identification and verification performance under both protocols, surpassing the recent strong competitors, e.g., CosFace, ArcFace, Adacos, P2SGrad and PFE. We also report the results following the ArcFace testing protocol, which refines both the probe set and the gallery set. As shown in <ref type="figure" target="#fig_7">Fig. 8</ref>, our method still clearly outperforms the competitors and achieves the best performance on identification. Compared with ArcFace, our loss shows better performance under both identification and verification scenarios as shown in <ref type="figure" target="#fig_8">Fig. 9</ref>. Adapitve-Face <ref type="bibr" target="#b16">[17]</ref> is another recent margin-based loss function for face recognition. We train our model with the same training data MS1MV2 and the same backbone ResNet50 <ref type="bibr" target="#b7">[8]</ref> as AdaptiveFace for a fair comparison. The results in Tab. 6 demonstrate the superiority of our method.</p><p>Time Complexity. The proposed method only brings small burden on training complexity, but has the same cost as the backbone model during inference. Specifically, compared with the conventional margin-based loss functions, our loss only additionally adjusts the negative cosine similarity of hard samples. Under the same environment and batchsize, ArcFace <ref type="bibr" target="#b7">[8]</ref> costs 0.370s for each iteration on NVIDIA P40 GPUs, while ours costs 0.378s. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion on Easy and Hard Samples During Training.</head><p>Finally, <ref type="figure" target="#fig_6">Fig. 7</ref> shows the easy and hard samples classified by our method in different training stages. As we can see, the front and clear faces are usually considered as easy samples in early training stage, and our model mainly learns the identity information from these samples. With the model continues training, slightly harder samples (i.e., Blue box) are gradually focused and corrected as the easy ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a novel Adaptive Curriculum Learning Loss that embeds the idea of adaptive curriculum learning into deep face recognition. Our key idea is to address easy samples in the early training stage and hard ones in the later stage. Our method is easy to implement and robust to converge. Extensive experiments on popular facial benchmarks demonstrate the effectiveness of our method compared to the SOTA competitors. Following the main idea of this work, future research can be expanded in various aspects, including designing a better function N (?) for negative cosine similarity that shares similar adaptive characteristic during training, and investigating the effects of noise samples that might be optimized as hard samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The adaptive parameter t (red line) and gradient modulation coefficients M of ours (green area) and MV-Arc-Softmax (blue line) in training. Since the number of mined hard samples reduces as training progresses, the green area, i.e., the range of M values, is relatively smooth in early stage and exhibits burrs in later stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustrations on (ratio between our loss and ArcFace, maximum cos?j) in different training stages. Top: Early training stage. Bottom: Later training stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>cos ?y i = cos ?j SphereFace cos(m?y i ) = cos ?j CosFace cos ?y i ? m = cos ?j ArcFace cos(?y i + m) = cos ?j MV-Arc-Softmax cos(?y i + m) = cos ?j (easy) cos(?y i + m) = t cos ?j + t ? 1 (hard) CurricularFace (Ours) cos(?y i + m) = cos ?j (easy) cos(?y i + m) = (t + cos ?j) cos ?j(hard)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Blue line, red line, green line and purple line denote the decision boundary of Softmax, ArcFace, MV-Arc-Softmax, and ours, respectively. m denotes the angular margin added by Ar-cFace. d denotes the additional margin of MV-Arc-Softmax and ours. In MV-Arc-Softmax, d = (t ? 1) cos ?j + t ? 1. In ours, d = (t + cos ?j ? 1) cos ?j.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Illustrations on loss curves of our CurricularFace and ArcFace with the small backbone MobileFaceNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>ROC of 1:1 verification protocol on IJB-B and IJB-C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Easy and hard examples from two subjects classified by our CurricularFace on early and later training stage, respectively. Green box indicates easy samples. Red box indicates hard samples. Blue box means samples are classified as hard in early stage but relabeled as easy in later stage, which indicates samples' transformation from hard to easy during the training procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>The rank-1 face identification accuracy on MegaFace Challenge 1 with both the 1M distractors and the probe set refined by ArcFace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Illustrations on Top 1 of different distractors and ROC on Megaface. Results are evaluated on refined MegaFace dataset. The results of ArcFace are from the official ResNet100 pre-trained with MS1M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>CurricularFaceInput: The deep feature of i-th sample xi with its label yi, last fully-connected layer parameters W , cosine similarity cos ?j of two vectors, embedding network parameters ?, learning rate ?, and margin m iteration number k ? 0, parameter t ? 0, m ? 0.5; while not converged do if cos(?y i + m) ? cos ?j then N (t, cos ?j) = cos ?j; else N (t, cos ?j) = (t (k) + cos ?j) cos ?j ; end T (cos ?y i ) = cos(?y i + m); Compute the loss L by Eq. 10; Compute the gradients of xi and Wj by Eq. 8; Update the parameters W and ? by:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The decision boundaries of popular loss functions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Verification performance (%) of different values of t.</figDesc><table><row><cell>Methods (%)</cell><cell>LFW</cell><cell>CFP-FP</cell></row><row><cell>t = 0</cell><cell>99.32</cell><cell>95.90</cell></row><row><cell>t = 0.3</cell><cell>99.37</cell><cell>96.47</cell></row><row><cell>t = 0.7</cell><cell>99.42</cell><cell>96.66</cell></row><row><cell>t = 1</cell><cell>99.45</cell><cell>93.94</cell></row><row><cell>Adaptive t</cell><cell>99.47</cell><cell>96.96</cell></row><row><cell cols="3">Table 3. Verification performance (%) of different strategies</cell></row><row><cell>for setting t.</cell><cell></cell><cell></cell></row><row><cell>Methods (%)</cell><cell>LFW</cell><cell>CFP-FP</cell></row><row><cell>Mode(cos ?y i )</cell><cell>99.42</cell><cell>96.49</cell></row><row><cell>Mean(px i )</cell><cell>99.42</cell><cell>95.39</cell></row><row><cell cols="2">Mean(cos ?y i ) 99.47</cell><cell>96.96</cell></row><row><cell>4. Experiments</cell><cell></cell><cell></cell></row><row><cell cols="2">4.1. Implementation Details</cell><cell></cell></row></table><note>Datasets. We separately employ CASIA-WebFace [36] and refined MS1MV2 [8] as our training data for fair com- parisons with other methods. CASIA-WebFace contains about 0.5M of 10 individuals, and MS1MV2 contains about 5.8M images of 85K individuals. We extensively test our method on several popular benchmarks, including LFW [9]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Verification comparison with SOTA methods on LFW, two pose benchmarks: CFP-FP and CPLFW, and two age benchmarks: AgeDB and CALFW. * denotes our re-implemented results with the backbone ResNet100<ref type="bibr" target="#b7">[8]</ref>.</figDesc><table><row><cell>Methods (%)</cell><cell>LFW</cell><cell>CFP-FP</cell><cell>CPLFW</cell><cell>AgeDB</cell><cell>CALFW</cell></row><row><cell>Center Loss (ECCV'16)</cell><cell>98.75</cell><cell>?</cell><cell>77.48</cell><cell>?</cell><cell>85.48</cell></row><row><cell>SphereFace (CVPR'17)</cell><cell>99.27</cell><cell>?</cell><cell>81.40</cell><cell>?</cell><cell>90.30</cell></row><row><cell>DRGAN (CVPR'17)</cell><cell>?</cell><cell>93.41</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Peng et al. (ICCV'17)</cell><cell>?</cell><cell>93.76</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>VGGFace2 (FG'18)</cell><cell>99.43</cell><cell>?</cell><cell>84.00</cell><cell>?</cell><cell>90.57</cell></row><row><cell>Dream (CVPR'18)</cell><cell>?</cell><cell>93.98</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Deng et al. (CVPR'18)</cell><cell>99.60</cell><cell>94.05</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>ArcFace (CVPR'19)</cell><cell>99.77</cell><cell>98.27</cell><cell>92.08</cell><cell>98.15</cell><cell>95.45</cell></row><row><cell>MV-Arc-Softmax (AAAI'20)</cell><cell>99.78</cell><cell>-</cell><cell>-</cell><cell>?</cell><cell>?</cell></row><row><cell>MV-Arc-Softmax *</cell><cell>99.80</cell><cell>98.28</cell><cell>92.83</cell><cell>97.95</cell><cell>96.10</cell></row><row><cell>CurricularFace (Ours)</cell><cell>99.80</cell><cell>98.37</cell><cell>93.13</cell><cell>98.32</cell><cell>96.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell cols="3">1:1 verification TAR (@FAR=1e?4) on the IJB-B and</cell></row><row><cell cols="3">IJB-C datasets.  *  denotes our re-implemented results with the</cell></row><row><cell>backbone ResNet100 [8].</cell><cell></cell><cell></cell></row><row><cell>Methods (%)</cell><cell>IJB-B</cell><cell>IJB-C</cell></row><row><cell>ResNet50+SENet50 (FG'18)</cell><cell>80.0</cell><cell>84.1</cell></row><row><cell>Multicolumn (BMVC'18)</cell><cell>83.1</cell><cell>86.2</cell></row><row><cell>DCN (ECCV'18)</cell><cell>84.9</cell><cell>88.5</cell></row><row><cell>ArcFace-VGG2-R50 (CVPR'19)</cell><cell>89.8</cell><cell>92.1</cell></row><row><cell>ArcFace-MS1MV2-R100 (CVPR'19)</cell><cell>94.2</cell><cell>95.6</cell></row><row><cell>Adocos (CVPR'19)</cell><cell>?</cell><cell>92.4</cell></row><row><cell>P2SGrad (CVPR'19)</cell><cell>?</cell><cell>92.3</cell></row><row><cell>PFE (ICCV'19)</cell><cell>?</cell><cell>93.3</cell></row><row><cell>MV-Arc-Softmax *  (AAAI'20)</cell><cell>93.6</cell><cell>95.2</cell></row><row><cell>Ours-MS1MV2-R100</cell><cell>94.8</cell><cell>96.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Verification comparison with SOTA methods on MegaFace Challenge 1 using FaceScrub as the probe set. Id refers to the rank-1 face identification accuracy with 1M distractors, and Ver refers to the face verification TAR at 1e ?6 FAR. The column R refers to data refinement on both probe set and 1M distractors. * denotes our re-implemented results with the backbone ResNet100<ref type="bibr" target="#b7">[8]</ref>.</figDesc><table><row><cell>Methods (%)</cell><cell>Protocol</cell><cell>R</cell><cell>Id</cell><cell>Ver</cell></row><row><cell>Triplet (CVPR'15)</cell><cell>Small</cell><cell></cell><cell>64.79</cell><cell>78.32</cell></row><row><cell>Center Loss (ECCV'16)</cell><cell>Small</cell><cell></cell><cell>65.49</cell><cell>80.14</cell></row><row><cell>SphereFace (CVPR'17)</cell><cell>Small</cell><cell></cell><cell>72.73</cell><cell>85.56</cell></row><row><cell>CosFace (CVRP'18)</cell><cell>Small</cell><cell></cell><cell>77.11</cell><cell>89.88</cell></row><row><cell>AM-Softmax (SPL'18)</cell><cell>Small</cell><cell></cell><cell>72.47</cell><cell>84.44</cell></row><row><cell>ArcFace-R50 (CVPR'19)</cell><cell>Small</cell><cell></cell><cell>77.50</cell><cell>92.34</cell></row><row><cell>ArcFace-R50</cell><cell>Small</cell><cell></cell><cell>91.75</cell><cell>93.69</cell></row><row><cell>Ours-R50</cell><cell>Small</cell><cell></cell><cell>77.65</cell><cell>92.91</cell></row><row><cell>Ours-R50</cell><cell>Small</cell><cell></cell><cell>92.48</cell><cell>94.55</cell></row><row><cell>CosFace-R100</cell><cell>Large</cell><cell></cell><cell>80.56</cell><cell>96.56</cell></row><row><cell>CosFace-R100</cell><cell>Large</cell><cell></cell><cell>97.91</cell><cell>97.91</cell></row><row><cell>ArcFace-R100</cell><cell>Large</cell><cell></cell><cell>81.03</cell><cell>96.98</cell></row><row><cell>ArcFace-R100</cell><cell>Large</cell><cell></cell><cell>98.35</cell><cell>98.48</cell></row><row><cell>PFE (ICCV'19)</cell><cell>Large</cell><cell></cell><cell>78.95</cell><cell>92.51</cell></row><row><cell>Adacos (CVPR'19)</cell><cell>Large</cell><cell></cell><cell>97.41</cell><cell>?</cell></row><row><cell>P2SGrad (CVPR'19)</cell><cell>Large</cell><cell></cell><cell>97.25</cell><cell>?</cell></row><row><cell>MV-Arc-Softmax (AAAI'20)</cell><cell>Large</cell><cell></cell><cell>97.14</cell><cell>97.57</cell></row><row><cell>MV-Arc-Softmax*</cell><cell>Large</cell><cell></cell><cell>80.59</cell><cell>96.22</cell></row><row><cell>MV-Arc-Softmax*</cell><cell>Large</cell><cell></cell><cell>97.76</cell><cell>97.80</cell></row><row><cell>Ours-R100</cell><cell>Large</cell><cell></cell><cell>81.26</cell><cell>97.26</cell></row><row><cell>Ours-R100</cell><cell>Large</cell><cell></cell><cell>98.71</cell><cell>98.64</cell></row><row><cell>AdaptiveFace-R50 (CVPR19)</cell><cell>Large</cell><cell></cell><cell>95.02</cell><cell>95.61</cell></row><row><cell>Ours-R50</cell><cell>Large</cell><cell></cell><cell>98.25</cell><cell>98.44</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Teaching classification boundaries to humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janara</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pose-robust face recognition via deep residual equivariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In FG</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Anshumali Shrivastava, and Anima Anandkumar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Phenomena</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Angular visual hardness</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CCBR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uv-gan: Adversarial facial uv map completion for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ArcFace: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distribution distillation loss: Generic approach for improving face recognition from hard samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03662</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selfpaced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>M Pawan Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient harmonized single-stage detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Airface: Lightweight and efficient model for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianyang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12256</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptiveface: Adaptive margin and sampling for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11947" to="11956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark-c: Face dataset and protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><surname>Niggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICB</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Agedb: the first manually collected, in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reconstruction-based disentanglement for poseinvariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probabilistic face embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards highly accurate and stable face alignment for high-resolution videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Disentangled representation learning GAN for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mis-classified vector guided softmax loss for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark-b face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Whitelam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Comparator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multicolumn networks for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adacos: Adaptively scaling cosine logits for effectively learning deep face representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">P2sgrad: Refined gradients for optimizing deep face models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengya</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cross-age lfw: A database for studying cross-age face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08197</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Cross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<idno>18-01</idno>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
		<respStmt>
			<orgName>Beijing University of Posts and Telecommunications</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Minimax curriculum learning: Machine teaching with desirable difficulties and scheduled diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
