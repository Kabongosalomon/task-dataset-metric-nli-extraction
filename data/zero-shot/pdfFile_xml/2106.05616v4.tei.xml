<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SVMAC: Unsupervised 3D Human Pose Estimation from a Single Image with Single-view-multi-angle Consistency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Kyushu University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Jiaotong University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqi</forename><surname>Sun</surname></persName>
							<email>yqsun@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Jiaotong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SVMAC: Unsupervised 3D Human Pose Estimation from a Single Image with Single-view-multi-angle Consistency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recovering 3D human pose from 2D joints is still a challenging problem, especially without any 3D annotation, video information, or multi-view information. In this paper, we present an unsupervised GAN-based model consisting of multiple weight-sharing generators to estimate a 3D human pose from a single image without 3D annotations. In our model, we introduce single-view-multi-angle consistency (SVMAC) to significantly improve the estimation performance. With 2D joint locations as input, our model estimates a 3D pose and a camera simultaneously. During training, the estimated 3D pose is rotated by random angles and the estimated camera projects the rotated 3D poses back to 2D. The 2D reprojections will be fed into weightsharing generators to estimate the corresponding 3D poses and cameras, which are then mixed to impose SVMAC constraints to self-supervise the training process. The experimental results show that our method outperforms the stateof-the-art unsupervised methods on Human 3.6M and MPI-INF-3DHP. Moreover, qualitative results on MPII and LSP show that our method can generalize well to unknown data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human pose estimation from monocular images has always been a problem in computer vision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref> with numerous applications such as motion recognition, virtual reality, and human-computer interaction. Although some currently presented fully supervised and weakly supervised methods based on deep learning have achieved good results <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">36]</ref>, these methods have two problems. Firstly, most of them learn a simple correspondence from 2D to 3D, which usually cannot be generalized to unknown actions and camera positions well. Sec-ondly, these methods usually require much 3D annotation data, while there are currently few datasets with 3D annotations, especially for pose datasets in the wild, which is extremely difficult to be labeled with 3D annotation. Hence, the study of unsupervised methods for 3D human pose estimation is of great significance. Recently, several unsupervised methods have been proposed which don't require any 3D data. Some of them estimate 3D poses based on monocular images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, while the performance remains to be improved. Some of them use multi-view information and achieve good results <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">40]</ref>, however setting up multi-view cameras in the wild is also extremely difficult.</p><p>In this paper, we propose an unsupervised adversarial training method for 3D human pose and camera estimation from 2D joint locations extracted from a single image. <ref type="figure">Figure 1</ref> shows our training pipeline. In our model, a generator named GEN is used to estimate a 3D pose and a camera simultaneously from an input 2D pose, and then we can reproject the estimated 3D pose to obtain the corresponding 2D pose.</p><p>Consider that a plausible 3D pose can be rotated by random angles and then be reprojected to obtain reasonable 2D poses. We propose single-view-multi-angle consistency(SVMAC) to improve the estimation accuracy. Specifically, we use generators that share weights with GEN to impose SVMAC constraint. The estimated 3D pose obtained from GEN is rotated from multiple angles and projected back to 2D reprojections. Then the 2D reprojections are fed to weight-sharing generators, which output the corresponding 3D poses and cameras. Since the estimated 3D poses from different angles are from a single view, we define the SVMAC loss, which will be described in detail in Section 3.2.</p><p>For the discriminator of our model, the input is the 2D reprojections or the 2D poses sampled from the real distribution. The discriminator aims to determine whether the <ref type="figure">Figure 1</ref>. The main structure of our adversarial training framework. The generators involved in our model share weights. The random reprojection of the estimated 3D pose are fed to the weight-sharing generator to be lifted to 3D again, allowing the network to impose SVMA consistency constraints. The 2D reprojection or a real 2D pose is fed to a discriminator for discrimination. 2D reprojections are from the real pose distribution, making our model learn a mapping of distribution from 2D poses to 3D poses, instead of a simple 2D-3D correspondence.</p><p>To verify the effectiveness of our method, we perform experiments on four datasets Human3.6M <ref type="bibr" target="#b15">[16]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b26">[27]</ref>, MPII <ref type="bibr" target="#b0">[1]</ref>, and Leed Sports Pose(LSP) <ref type="bibr" target="#b17">[18]</ref>. Results show that our method outperforms state-of-the-art methods, and the ablation studies on Human 3.6M and MPI-INF-3DHP datasets show that our SVMAC constraint significantly improves the performance of our model. In addition, the model trained on a specific dataset can also perform well on other datasets, which shows that our method has the excellent ability of generalization.</p><p>In summary, our contributions are as follows:</p><p>? We present the first unsupervised adversarial training method to simultaneously estimate a 3D pose and a camera from a 2D pose without requiring any other information.</p><p>? Our method use weight-sharing networks to generate poses and cameras from multiple angles and mix them to impose single-view-multi-angle consistency (SV-MAC) constraints, which significantly improves the estimation accuracy.</p><p>? The experimental results show that our method outperforms the state-of-the-art methods, and can be generalized to unknown 3D human poses and cameras well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Fully Supervised Methods There are several methods that make full use of both 2D and 3D ground truth based on large datasets which contain millions of images with corresponding 3D pose annotations. Madadi et al. <ref type="bibr" target="#b24">[25]</ref> use CNNbased 3D joint predictions to regress SMPL pose and shape parameters and then get the estimated 3D pose with these parameters. Sun et al. <ref type="bibr" target="#b34">[35]</ref> propose an end-to-end model to regress a 3D human pose from 2D heat maps. Dushyant et al. <ref type="bibr" target="#b27">[28]</ref> present a CNN-based model, which regresses 2D and 3D joint coordinates and motion skeletons to produce a real-time stable 3D reconstruction of motion. In addition to the above end-to-end methods, there are also some methods whose estimation process includes two stages. The first stage is to perform 2D pose detection on a single image and predict its 2D joint coordinates <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">37]</ref>, while the second stage is to predict 3D joint coordinates from the 2D joint coordinates through regression analysis or model fitting <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">39]</ref>. Recently, some methods have been proposed for the second stage, and these methods aim to learn the 2D-3D correspondence with the given paired 2D and 3D data. Martinez et al. <ref type="bibr" target="#b25">[26]</ref> propose a simple but effective regression network to estimate a 3D human pose directly from a 2D pose, considered to be the baseline due to its simplicity and high accuracy estimation. Hossain et al. <ref type="bibr" target="#b12">[13]</ref> extend the baseline by employing a recurrent neural network for a human pose sequence. Although these methods have achieved outstanding performance, they require a <ref type="figure">Figure 2</ref>. Network structure of the generator. A fully connected layer and a shared residual block are used to upscale the input and extract its features. Then the network splits into two paths that predict the 3D pose and the camera, respectively. The upper path and lower path both have two residual blocks followed by a fully connected layer which outputs the 3D pose and the camera.</p><p>lot of data with 3D annotations, and only work well on similar datasets.</p><p>Weakly Supervised Methods Weakly supervised methods only require limited 3D annotations or unpaired 2D-3D data. Zhou et al. <ref type="bibr" target="#b45">[45]</ref> propose a two-stage transfer learning method to generate 2D heat maps and regress the joint depths. Yang et al. <ref type="bibr" target="#b44">[44]</ref> present an adversarial training method based on multiple representations. They introduce a discriminator that makes full use of RGB images, geometric representations, and heat maps. Drover et al. <ref type="bibr" target="#b8">[9]</ref> learn a mapping of distribution from 2D to 3D with the help of 2D projections based on a GAN <ref type="bibr" target="#b13">[14]</ref>. However, they require extra data generated by utilizing ground-truth 3D data for training. Considering the reprojection constraint, Wandt et al. <ref type="bibr" target="#b39">[39]</ref> propose a GAN-based model named RepNet to estimate 3D pose and camera simultaneously, and use a discriminator to evaluate the estimated 3D pose and the corresponding KCS matrix. Although these methods somewhat solve the problem of generalization, they still require 3D annotations, which are time-consuming and labor-intensive to acquire. Unsupervised Methods Unsupervised methods make full use of images or 2D data, and do not require any 3D annotation. Rhodin et al. <ref type="bibr" target="#b30">[31]</ref> propose an encoder-decoder model to perform 3D human pose estimation based on unsupervised geometry-aware representations. Their method requires multi-view 2D data to learn the appearance representation. Kudo et al. <ref type="bibr" target="#b22">[23]</ref> consider that the random 2D reprojections are reasonable if the estimated 3D pose is accurate enough. Chen et al. <ref type="bibr" target="#b7">[8]</ref> extend the method <ref type="bibr" target="#b22">[23]</ref> by lifting the 2D reprojection to 3D again to impose geometric self-consistency constraints. Kocabas et al. <ref type="bibr" target="#b21">[22]</ref> adopt traditional methods to generate a 3D pose using multi-view 2D poses, and take it as self-supervised information. Kundu et al. <ref type="bibr" target="#b23">[24]</ref> propose a differentiable and modular self-supervised method for 3D human pose estimation along with the discovery of 2D part segments from unlabeled video frames. However, these methods can't be generalized to unknown motions and camera positions well. In this paper, we propose an unsupervised adversarial training method to simultaneously estimate a 3D human pose and a camera from 2D joint locations extracted from a single image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we introduce our unsupervised method which lifts 2D joint locations to a 3D pose. Let</p><formula xml:id="formula_0">x real ? R 2N = (x 1 , y 1 , x 2 , y 2 ...x N , y N ) be 2D joint locations and X real ? R 3N = (X 1 , Y 1 , Z 1 , X 2 , Y 2 , Z 2 , ..., X N , Y N , Z N )</formula><p>be the real 3D pose, where N represents the number of human joints. We take the hip joint as the root joint to align all 2D and 3D pose coordinates. Then, we assume a perspective camera, and we have</p><formula xml:id="formula_1">x i = f * X i /Z i , i = 1, 2, ..., N,<label>(1)</label></formula><formula xml:id="formula_2">y i = f * Y i /Z i , i = 1, 2, ..., N,<label>(2)</label></formula><p>where f is the focal length of the camera. Here we assume f = 1. And we assume that the distance from the camera to the 3D skeleton is c = 10. Then we normalize 2D joint locations so that the average distance from other joints to the root joint (i.e., hip joint) is 1/c, and correspondingly normalize the 3D poses so that the average distance from the other joints to the root joint is 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D pose and camera estimation</head><p>Given the input x real , we use a generator named GEN to estimate the 3D pose and camera simultaneously.</p><formula xml:id="formula_3">X pred , K = GEN (x real ).<label>(3)</label></formula><p>Specifically, GEN contains two modules: pose estimation module and camera estimation module. Let D = (d 1 , d 2 , ..., d N ) denotes the output of the pose estimation module, which represents the depth of each joint relative to the root joint. Then we have Z = D + c, and X pred = (X, Y, Z) by Eq.1 and Eq.2. Considering the difference among the average depths of the estimated 3D poses from different angles, the camera estimation module outputs a more simplified weak perspective camera K ? R 2 * 3 , which is used to mix the estimated 3D poses from the source angle and other angles to impose SVMAC constraints. <ref type="figure">Figure 2</ref> shows the network structure of the generator. We first use a fully connected layer and a shared residual block to extract the feature of input 2D joint locations. Then, for the pose estimation module, we add two residual blocks after the shared residual block and finally add a fully connected layer to output a N -dimensional vector, while the camera estimation module contains two residual blocks after the shared residual block and a fully connected layer to output a 6-dimensional vector. In addition, each layer is followed by batch-normalization <ref type="bibr" target="#b14">[15]</ref>, leaky ReLUs <ref type="bibr" target="#b2">[3]</ref>, and dropout <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Single-view-multi-angle consistency</head><p>Notice that a plausible 3D pose can be rotated by random angles and then be reprojected to obtain reasonable 2D poses. We consider a sequence of angles: angle 0 , angle 1 , ..., angle n?1 , where n represents the number of angles. Let ? i represent the difference between angle i and angle 0 , i = 0, 1, ..., n ? 1, that is,</p><formula xml:id="formula_4">? i = angle i ? angle 0 , i = 0, 1, ..., n ? 1,<label>(4)</label></formula><p>specifically ? 0 = 0. Hence, angle i is obtained by rotating around the y-axis by ? i radian from angle 0 , which is the source angle of X pred . Let X ?i rot and K ?i represent the 3D pose and camera from the angle i , we have</p><formula xml:id="formula_5">X ?0 rot = X pred ,<label>(5)</label></formula><p>and</p><formula xml:id="formula_6">K ?0 = K.<label>(6)</label></formula><p>Then we define the rotation matrix for ? i</p><formula xml:id="formula_7">R ?i = ? ? cos? i 0 ?sin? i 0 1 0 sin? i 0 cos? i ? ? , i = 1, 2, ..., n ? 1. (7)</formula><p>So the rotated 3D poses can be obtained by</p><formula xml:id="formula_8">X ?i rot = (X pred -[0, 0, c]) * R ?i + [0, 0, c], i = 1, 2, ..., n ? 1.<label>(8)</label></formula><p>Through X ?i rot and the estimated camera K, we have the rotated 2D reprojections</p><p>x ?i proj = KX ?i rot , i = 1, 2, ..., n ? 1.</p><p>Then x ?i proj is fed to a weight-sharing generator, we have X ?i rot , K ?i = GEN (x ?i proj ), i = 1, ..., n ? 1.</p><p>Since X ?i rot and X ?i rot should be similar, we define the loss function as</p><formula xml:id="formula_11">L 3D = 1 n ? 1 n?1 i=1 1 N X ?i rot ? X ?i rot 2 2 .<label>(11)</label></formula><p>Thus, we can define the single-view-multi-angle consistency(SVMAC), which means that the estimated poses and cameras from n angles can be mixed to generate n 2 2D reprojections, and the 2D reprojections from the same angle should be consistent. <ref type="figure" target="#fig_0">Figure 3</ref> shows an example of the SVMA consistency in the case of two angles. The 2D reprojections can be calculated by P ?i a = K ?a X ?i rot , a, i = 0, 1, ..., n ? 1,</p><p>where X ?0 rot = X pred .</p><p>Then the loss function can be defined as</p><formula xml:id="formula_14">L svmac = 1 n n?1 i=0 mi a,b=1,a&lt;b 1 N m i P ?i a ? P ?i b 2 2 ,<label>(14)</label></formula><p>where m i is the number of 2D poses from angle i . Note that the 2D poses involved in Eq. 14 contain the input 2D pose at the source angle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adversarial training</head><p>To enhance the performance of our generators, we introduce a discriminator to determine the reality of the 2D reprojections to learn a mapping of the distribution from 2D poses to 3D poses instead of a simple 2D-3D correspondence. The input of the discriminator is the rotated 2D reprojection x ?i proj or the 2D pose sampled from real samples x i sam . The output represents the probability that the current input is from real distribution.</p><p>The network structure of our discriminator includes a fully connected layer to increase the dimensionality of the input, two residual blocks and a fully connected layer to produce the output. For the activation functions we use leaky ReLUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss functions</head><p>For our GAN, the standard WGAN-GP loss function <ref type="bibr" target="#b10">[11]</ref> is used,</p><formula xml:id="formula_15">min G max D L adv = n?1 i=1 E(D(x ?i proj )) ? E(D(x i sam )) +? gp (?xi(D(x i ) ? 1),<label>(15)</label></formula><p>wherex i represents the linear combination of x i sam and x ?i proj . For the loss functions of camera, similar to <ref type="bibr" target="#b39">[39]</ref>, each of the estimated cameras should satisfy</p><formula xml:id="formula_16">K ?i K ?iT = s 2 I 2 , i = 0, 1, ..., n ? 1,<label>(16)</label></formula><p>where s is the scale of the projection and I 2 is the 2*2 indentity matrix. Since s equals to the largest singular value (or the l 2 -norm) of K ?i and the trace of K ?i K ?iT is the sum of the squared singular values, we can calculate s for all angles as s = trace(K ?i K ?iT )/2, i = 0, 1, ..., n ? 1. <ref type="formula" target="#formula_1">(17)</ref> Then we can define the loss function of weak perspective camera as</p><formula xml:id="formula_17">L cam = n?1 i=0 2 trace(K ?i K ?iT ) K ?i K ?iT ? I 2 F ,<label>(18)</label></formula><p>where ? F represents the Frobenius norm.</p><p>In addition, we consider the constraint of the bone length of the human body. There are several pairs of bones in the human body that have a symmetrical relationship in length. So we have the symmetric loss</p><formula xml:id="formula_18">L sym = 1 q q i B i ? B i 2 2 ,<label>(19)</label></formula><p>where q is the number of pairs of bones that have a symmetrical relationship, B i and B i are the i-th pair of two bones with a symmetrical relationship. Then, we introduce another loss L angle following the idea of Kudo et al. <ref type="bibr" target="#b22">[23]</ref>.</p><p>It guarantees that the zcomponents of the generated 3D pose will not be inverted. Similarly, we define the face orientation vector v = [v x , v y , v z ] = j nose ? j neck ? R 3 and shoulder orientation vector w = [w x , w y , w z ] = j ls ? j rs ? R 3 , where j nose , j neck , j ls , j rs ? R 3 represent the 3D coordinates of the nose, neck, left shoulder, and right shoulder joints, respectively. According to the above mentioned constraints, the angle ? between v and w on the z ? x plane should satisfy</p><formula xml:id="formula_19">sin ? = v z w x ? v x w z v w ? 0.<label>(20)</label></formula><p>Thus, the loss function can be defined as</p><formula xml:id="formula_20">L angle = max(0, ? sin ?) = max(0, v x w z ? v z w x v w ).</formula><p>(21) In the end, we define the total loss function of the generator as follows:</p><formula xml:id="formula_21">L = L adv + ? 1 L angle + ? 2 L cam + ? 3 L sym + ? 4 L 3D + ? 5 L svmac ,<label>(22)</label></formula><p>where ? i for 1 ? i ? 5 represent the weight coefficients of the loss terms, L 3D and L svmac are calculated by Eq.11 and Eq.14, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training details</head><p>As mentioned above, we use the standard WGAN-GP loss function and other loss functions to train our GAN. We use Adam optimizer <ref type="bibr" target="#b20">[21]</ref> for networks with learning rate of 5.5e-5, beta 1 = 0.7 and beta 2 = 0.9. The loss weights are set as ? 1 =1, ? 2 =1, ? 3 =0.01, ? 4 =0.1 and ? 5 =10 in Eq.22.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and results</head><p>In this section, we conduct several experiments to evaluate our model on the Human3.6M <ref type="bibr" target="#b15">[16]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b26">[27]</ref> datasets and show quantitative results. In addition, we also show qualitative results on the in-the-wild datasets MPII <ref type="bibr" target="#b0">[1]</ref> and Leeds Sports Pose(LSP) <ref type="bibr" target="#b17">[18]</ref>, where 3D ground truth data is not available. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and metrics</head><p>Human3.6M Human3.6M is one of the largest 3D human pose datasets, consisting of 3.6 million 3D human poses. It contains video and MoCap data captured from 4 different viewpoints from 11 subjects performing typical activities such as directing, walking, sitting, etc. We evaluate the accuracy of pose estimation in terms of mean per joint position error(MPJPE) in millimeters after scaling and rigid alignment on the ground truth skeleton, i.e., P-MPJPE. We train our model on subjects S1, S5, S6, S7, S8 and evaluate it on subjects S9, S11.</p><p>MPI-INF-3DHP The MPI-INF-3DHP is a large human pose dataset consists of 3D data captures using a markerless multi-camera MoCap system. It contains both indoor and outdoor scenes and has eight actors performing several actions, which are more diverse than the Human 3.6M dataset. We evaluate valid images from the test-set containing 2929 frames following <ref type="bibr" target="#b18">[19]</ref> and report P-MPJPE, Percentage of Correct Keypoints (PCK) @150mm, and Area Under the Curve(AUC) computed for a range of PCK thresholds. For PCK and AUC, there are two cases of using or not using a rigid alignment. <ref type="table" target="#tab_0">Table 1</ref> shows the quantitative results on Human 3.6M. The lower value is better for P-MPJPE. The results show that our method outperforms state-of-the-art unsupervised methods by 2.6%. In addition, we compare our method with several fully supervised and weakly supervised methods. It can be seen that our method even outperforms several weakly supervised methods and fully supervised methods. The results marked by SH/GT are obtained by training on 2D locations detected by stacked hourglass <ref type="bibr" target="#b28">[29]</ref> and testing on 2D ground truth, and they indicate that our model works well for estimating the depth of human poses. Note that Chen et al. <ref type="bibr" target="#b7">[8]</ref> achieve better performance by using temporal information and extra data for training, we will employ <ref type="table">Table 2</ref>. The results of 3D human pose estimation on the MPI-INF-3DHP dataset without using a scaling and rigid alignment. ( ?) denotes the method using temporal information, ( ?) denotes the method using multi-view information, (+) denotes the method using extra data for training.    <ref type="table">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref> show the qualitative results on MPI-INF-3DHP. Higher values of PCK and AUC signify better performance. Comparing with unsupervised methods which estimate a 3D pose from a single image, the experimental results show that our method outperforms stateof-the-art by 15.0% in terms of P-MPJPE with training on MPI-INF-3DHP, and 21.6% in terms of P-MPJPE with training on Human 3.6M, respectively. Besides, our method is even better than several unsupervised methods that use multi-view data or temporal data and extra data for training. In addition to comparing with the state-of-the-art unsupervised methods, we also show results from top fully supervised and weakly supervised methods. Hence, the experimental results show that our model can be applied to multiple datasets and achieves better performance for human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative results on Human 3.6M</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative results on MPI-INF-3DHP</head><p>As shown in <ref type="table" target="#tab_2">Table 3</ref>, we notice that there is only a minor difference of experimental results between the training datasets of MPI-INF-3DHP and Human3.6M. This indicates that our method can converge to a similar distribution of feasible human poses for both training sets, which also means our method can be well generalized to unseen data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies</head><p>In this section, we conduct ablation studies on Human 3.6M and MPI-INF-3DHP to evaluate the effectiveness of the adversarial training method and our single-view-multiangle consistency in terms of P-MPJPE. The experimental results are shown in <ref type="table" target="#tab_3">Table 4</ref>. 'w/o Disc' represents the experimental results when we train our model without using a discriminator. The results on Human 3.6M and MPI-INF-3DHP show that the adversarial training method helps learn a mapping from 2D poses to 3D poses. 'n = k' represents the results when we use the SVMAC loss, see Eq.14, in the case of k angles. Specially, the case of 'n = 1' means that we train the model without the SV-MAC loss. For the case of n = 2, The experimental results show that the SVMA consistency loss further improves the performance of our model by 20% compared with n = 1. Hence, we have the conclusion that the simple reprojection constraints and adversarial training are not enough for estimating reasonable 3D poses. The SVMAC loss significantly improves the accuracy of our model for estimating plausible 3D poses and cameras. In order to further explore the effectiveness of the value of n, we perform experiments in the case of n = 3 as shown in <ref type="table" target="#tab_3">Table 4</ref>. We can find that increasing the number of angles does not significantly improve performance, but makes training process much slower. Hence we set n = 2 to train our model and compare it with other methods.</p><p>In addition, we also perform experiments on both datasets with 5% ground truth 3D data for supervision, and the results are shown at the bottom of table 4. The experimental results show that our model can outperform most of weakly supervised methods and even fully supervised methods by using a little 3D ground truth for supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative results</head><p>In this section, we conduct qualitative experiments on Human 3.6M dataset and <ref type="figure" target="#fig_1">Figure 4</ref> shows several reconstruction examples. The results show that our method can perform very well for even more complicated actions. However, our model cannot work well for certain special views and scenes.</p><p>In order to verify the generalization of our model, we conduct experiments on MPII and LSP datasets without training on them but on Human 3.6M, and show several examples of estimated 3D poses in <ref type="figure" target="#fig_2">Figure 5</ref> and <ref type="figure" target="#fig_3">Figure 6</ref>, respectively. It can be seen that our model performs well with standard 2D pose datasets, which contain more complicated in-the-wild poses and actions that are not involved in the training dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>For 3D human pose estimation, the acquisition of 3D annotation data is time-consuming and expensive, which is still a difficult problem. In this paper, we present an unsupervised GAN-based model to estimate a 3D pose and a camera simultaneously from a 2D pose extracted from a single image without requiring any other information. Considering that a plausible 3D pose can be projected back to reasonable 2D poses even if it is rotated by random angles, we first propose single-view-multi-angle consistency(SVMAC). We use weight-sharing generators to impose SVMAC constraints, which forces the estimated 3D pose reasonable from any angle. The experimental results show that our method outperforms the state-of-the-art unsupervised methods by 2.6% on Human 3.6M and 15.0% on MPI-INF-3DHP in terms of P-MPJPE. We also perform qualitative experiments on MPII and LSP, which demonstrate that our method can be generalized to unknown actions and camera positions well. In the future, we plan to improve the model's performance by applying it to video sequences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>An example of the single-view-multi-angle consistency in the case of 2 angles. The estimated 3D poses and cameras from different angles can be mixed to generate rich 2D reprojections, and the 2D reprojections from the same angle should be consistent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results on Human 3.6M dataset. The top shows some examples of successful reconstruction, and the bottom shows some failures. Each example includes image, ground-truth 3D pose and estimated 3D pose (top to bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Examples of reconstruction on the MPII dataset. Each example shows an image and the corresponding reconstructed 3D pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Examples of reconstruction on the LSP dataset. Each example shows the image with overlaid 2D pose and the corresponding reconstructed 3D pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The results of 3D human pose estimation on the Human 3.6M dataset. GT and IMG denote the results obtained using ground-truth 2D joint locations and estimated 2D joint locations by SH/CPM<ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b41">41]</ref>, respectively.</figDesc><table><row><cell>Type</cell><cell>Methods</cell><cell>GT</cell><cell>IMG</cell></row><row><cell>Full</cell><cell>Chen and Ramanan[7]</cell><cell>57.5</cell><cell>82.7</cell></row><row><cell></cell><cell>Martinez et al.[26]</cell><cell>37.1</cell><cell>52.1</cell></row><row><cell>Weak</cell><cell>3DInterpreter[42]</cell><cell>88.6</cell><cell>98.4</cell></row><row><cell></cell><cell>AIGN[10]</cell><cell>79.0</cell><cell>97.2</cell></row><row><cell></cell><cell>RepNet[39]</cell><cell>38.2</cell><cell>65.1</cell></row><row><cell>Unsupervised</cell><cell>Kudo et al.[23]</cell><cell>73.2</cell><cell>110.2</cell></row><row><cell></cell><cell>Chen et al.[8]</cell><cell>58.0</cell><cell>-</cell></row><row><cell></cell><cell>Kundu et al.[24]</cell><cell>-</cell><cell>99.2</cell></row><row><cell></cell><cell>Ours</cell><cell>56.5</cell><cell>98.3</cell></row><row><cell></cell><cell>Ours(SH/GT)</cell><cell>-</cell><cell>64.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">The results of 3D human pose estimation on the MPI-</cell></row><row><cell cols="5">INF-3DHP dataset using a scaling and rigid alignment, in which</cell></row><row><cell cols="5">Various refers to the combination of datasets H36M, MPI-INF-</cell></row><row><cell cols="2">3DHP and LSP.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Supervision Methods</cell><cell>Training Data</cell><cell cols="2">Rigid Alignment</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">PCK AUC P-MPJPE</cell></row><row><cell>Full</cell><cell>VNect[28]</cell><cell cols="2">H36M+MPI 83.9 47.3</cell><cell>98.0</cell></row><row><cell></cell><cell>DenseRac[43]</cell><cell cols="2">H36M+MPI 86.3 47.8</cell><cell>89.8</cell></row><row><cell>Weak</cell><cell>SPIN[42]</cell><cell>Various</cell><cell>87.0 48.5</cell><cell>80.4</cell></row><row><cell></cell><cell>HMR[19]</cell><cell cols="2">H36M+MPI 77.1 40.7</cell><cell>113.2</cell></row><row><cell cols="3">Unsupervised PoseNet3D[38] H36M</cell><cell>81.9 43.2</cell><cell>102.4</cell></row><row><cell></cell><cell cols="2">Kundu et al.[24] H36M</cell><cell>82.1 56.3</cell><cell>103.8</cell></row><row><cell></cell><cell cols="2">Kundu et al.[24] MPI</cell><cell>84.6 60.8</cell><cell>93.9</cell></row><row><cell></cell><cell>Ours</cell><cell>H36M</cell><cell>86.9 51.7</cell><cell>80.3</cell></row><row><cell></cell><cell>Ours</cell><cell>MPI</cell><cell>86.6 53.1</cell><cell>79.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Ablation Studies.</cell></row><row><cell></cell><cell>P-MPJPE</cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">Human 3.6M MPI-INF-3DHP</cell></row><row><cell>w/o Disc</cell><cell>90.0</cell><cell>134.2</cell></row><row><cell>n = 1</cell><cell>70.9</cell><cell>98.7</cell></row><row><cell>n = 2</cell><cell>56.8</cell><cell>79.8</cell></row><row><cell>n = 3</cell><cell>56.5</cell><cell>78.5</cell></row><row><cell>n = 2 with 5% 3D Sup</cell><cell>35.4</cell><cell>51.0</cell></row><row><cell cols="3">these information in novel ways to improve the performance</cell></row><row><cell>in our future work.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Computational studies of human motion: Tracking and motion synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ikemoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human Pose Estimation with Iterative Error Feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Can 3d pose be learned from 2d projections alone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference of Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y. Fish</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-based vision: A program to see a walking person</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="20" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly-supervised 3d human pose learning via multi-view images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Endto-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A cnn-based 3d human pose estimation based on projection of depth and ridge data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">107462</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised adversarial learning of 3d human pose from 2d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ogaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Odagiri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08244</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-supervised 3d human pose estimation via part guided novel image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="6151" to="6161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Smplr: Deep learning based smpl reverse for 3d human pose and shape recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bertiche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">107472</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly-supervised 3d pose estimation from a single image using multi-view consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rochette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Integral Human Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<imprint>
			<biblScope unit="volume">11210</biblScope>
			<biblScope unit="page" from="536" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<title level="m">Series Title: Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DeepPose: Human Pose Estimation via Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Posenet3d: Learning temporally consistent 3d human pose via knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ranade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="311" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Canonpose: Self-supervised monocular 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3d pose and shape estimation by dense render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation in the Wild by Adversarial Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
