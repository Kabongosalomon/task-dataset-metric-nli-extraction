<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequential Graph Convolutional Network for Active Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Caramalau</surname></persName>
							<email>r.caramalau18@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
							<email>b.bhattarai@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
							<email>tk.kim@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequential Graph Convolutional Network for Active Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel pool-based Active Learning framework constructed on a sequential Graph Convolution Network (GCN). Each image's feature from a pool of data represents a node in the graph and the edges encode their similarities. With a small number of randomly sampled images as seed labelled examples, we learn the parameters of the graph to distinguish labelled vs unlabelled nodes by minimising the binary cross-entropy loss. GCN performs message-passing operations between the nodes, and hence, induces similar representations of the strongly associated nodes. We exploit these characteristics of GCN to select the unlabelled examples which are sufficiently different from labelled ones. To this end, we utilise the graph node embeddings and their confidence scores and adapt sampling techniques such as CoreSet and uncertainty-based methods to query the nodes. We flip the label of newly queried nodes from unlabelled to labelled, re-train the learner to optimise the downstream task and the graph to minimise its modified objective. We continue this process within a fixed budget. We evaluate our method on 6 different benchmarks: 4 real image classification, 1 depth-based hand pose estimation and 1 synthetic RGB image classification datasets. Our method outperforms several competitive baselines such as VAAL, Learning Loss, CoreSet and attains the new stateof-the-art performance on multiple applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning has shown great advancements in several computer vision tasks such as image classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref> and 3D Hand Pose Estimation (HPE) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b24">25]</ref>. This has been possible due to the availability of both the powerful computing infrastructure and the large-scale datasets. Data annotation is a time-consuming task, needs experts and is also expensive. This gets even more challenging to some of the specific domains such as robotics or medical image analysis. Moreover, while optimizing deep neural network architectures, a gap is present concerning the representative- ness of the data <ref type="bibr" target="#b3">[4]</ref>. To overcome these issues, active learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref> has been successfully deployed to efficiently select the most meaningful samples. Essentially, there are three distinct components in any Active Learning (AL) framework. These components are learner, sampler, and annotator. In brief, a learner is a model trained to minimize the objective of the target task. The sampler is designed to select the representative unlabelled examples within a fixed budget to deliver the highest performance. Finally, an annotator labels the queried data for learner re-training. Based on the relationship between learner and sampler, AL frameworks can be categorised into two major groups: task-dependent and task-agnostic. Taskdependents are the ones where the sampler is specially designed for a specific task of the learner. Majority of the previous works in active learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref> are task-dependent in nature. In other words, the sampling function is dependent on the objective of the learner. This design limits the model to become optimal for a specific type of task while suffering from its scalability problem. Some of the recent works such as VAAL <ref type="bibr" target="#b32">[33]</ref> and Learning Loss <ref type="bibr" target="#b41">[42]</ref> tackle such a problem. VAAL trains a variational auto-encoder (VAE) that learns a latent space for better discrimination between labelled and unlabelled images in an adversarial manner. Similarly, Learning Loss introduces a separate loss-prediction module to be trained together with the learner. The major drawback of these approaches is the lack of a mechanism that exploits the correlation between the labelled and the unlabelled images. Moreover, VAAL has no way to communicate between the learner and the sampler. Graph Convolutional Networks(GCNs) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5]</ref> are capable of sharing information between the nodes via message-passing operations. In the AL domain, the application of GCNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b37">38]</ref> is also slowly getting priority. However, these methods are applied only to specific kind of datasets i.e. graph-structured data such as Cora, CiteSeer, and PubMed <ref type="bibr" target="#b39">[40]</ref>. In this work, we are exploring the image domain beyond graph-structured data.</p><p>To address the above-mentioned issues, we propose a sequential GCN for Active Learning in a task-agnostic manner. <ref type="figure" target="#fig_0">Figure 1</ref> shows the pipeline of the proposed method. In the Figure, Phase I implements the learner. This is a model trained to minimize the objective of the downstream task. Phase II, III and IV compose our sampler where we deploy the GCN and apply the sampling techniques on graphinduced node embeddings and their confidence scores. Finally, in Phase V, the selected unlabelled examples are sent for annotation. At the initial stage, the learner is trained with a small number of seed labelled examples. We extract the features of both labelled and unlabelled images from the learner parameters. During Phase II, we construct a graph where features are used to initialise the nodes of the graph and similarities represent the edges. Unlike VAAL <ref type="bibr" target="#b32">[33]</ref>, the initialisation of the nodes by the features extracted from the learner creates an opportunity to inherit uncertainties to the sampler. This graph is passed through GCN layers (Phase III) and the parameters of the graph are learned to identify the nodes of labelled vs unlabelled example. This objective of the sampler is independent of the one from the learner. We convolve on the graph which does message-passing operations between the nodes to induce the higher-order representations. The graph embedding of any image depends primarily upon the initial representation and the associated neighbourhood nodes. Thus, the images bearing similar semantic and neighbourhood structure end up inducing close representations which will play a key role in identifying the sufficiently different unlabelled examples from the labelled ones. The nodes after convolutions are classified as labelled or unlabelled. We sort the examples based on the confidence score, apply an uncertainty sampling approach (Phase IV), and send the selected examples to query their labels(Phase V). We called this sampling method as Uncer-tainGCN. <ref type="figure" target="#fig_1">Figure 2</ref> simulates the UncertainGCN sampling technique. Furthermore, we adapt the higher-order graph node information under the CoreSet <ref type="bibr" target="#b30">[31]</ref> for a new sampling technique by introducing latent space distancing. In principle, CoreSet <ref type="bibr" target="#b30">[31]</ref> uses risk minimisation between core-sets on the learner feature space while we employ this operation over GCN features. We called this sampling technique as CoreGCN. Our method has a clear advantage due to the aforementioned strengths of the GCN which is demonstrated by both the quantitative and qualitative experiments (see <ref type="bibr">Section 4)</ref>. Traversing from Phase I to Phase V as shown in <ref type="figure" target="#fig_0">Figure 1</ref> completes a cycle. In the next iteration, we flip the label of annotated examples from unlabelled to labelled and re-train the whole framework.</p><p>We evaluated our sampler on four challenging real domain image classification benchmarks, one depth-based dataset for 3D HPE and a synthetic image classification benchmark. We compared with several competitive sampling baselines and existing state-of-the-arts methods including CoreSet, VAAL and Learning Loss. From both the quantitative and the qualitative comparisons, our proposed framework is more accurate than existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Model-based methods. Recently, a new category of methods is explored in the active learning paradigm where a separate model from the learner is trained for selecting a subset of the most representative data. Our method is based on this category. One of the first approaches <ref type="bibr" target="#b41">[42]</ref> attached a loss-learning module so that loss can be predicted offline for the unlabelled samples. In <ref type="bibr" target="#b32">[33]</ref>, another task-agnostic solution deploys a variational auto-encoder (VAE) to map the available data on a latent space. Thus, a discriminator is trained in an adversarial manner to classify labelled from unlabelled. The advantage of our method over this approach is the exploitation of the relative relationship between the examples by sharing information through message-passing operations in GCN. GCNs in active learning. GCNs <ref type="bibr" target="#b17">[18]</ref> have opened new active learning methods that have been successfully applied in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref>. In comparison to these methods, our approach has distinguished learner and sampler. It makes our approach task-agnostic and also gets benefited from modelbased methods mentioned just before. Moreover, none of these methods is trained in a sequential manner. <ref type="bibr" target="#b37">[38]</ref> proposes K-Medoids clustering for the feature propagation between the labelled and unlabelled nodes. A regional uncertainty algorithm is presented in <ref type="bibr" target="#b0">[1]</ref> by extending the PageRank <ref type="bibr" target="#b26">[27]</ref> algorithm to the active learning problem. Similarly, <ref type="bibr" target="#b5">[6]</ref> combines node uncertainty with graph centrality for selecting the new samples. A more complex algorithm is introduced in <ref type="bibr" target="#b10">[11]</ref> where a reinforcement scheme with multi-armed bandits decides between the three query measurements from <ref type="bibr" target="#b5">[6]</ref>. However, these works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b37">38]</ref> derive their selection mechanism on the assumption of a Graph learner. This does not make them directly comparable with our proposal where a GCN is trained separately for a different objective function than the learner. Uncertainty-based methods. Earlier techniques for sampling unlabelled data have been explored through uncertainty exploration of the convolutional neural networks (CNNs). A Bayesian approximation introduced in [9] produce meaningful uncertainty measurements by variational inference of a Monte Carlo Dropout (MC Dropout) adapted architecture. Hence, it is successfully integrated into active learning by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref>. With the rise of GPU computation power, <ref type="bibr" target="#b1">[2]</ref> ensemble-based method outperformed MC Dropout. Geometric-based methods. Although there have been studies exploring the data space through the representations of the learning model ( <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b13">14]</ref>), the first work applying it for CNNs as an active learning problem, CoreSet, has been presented in <ref type="bibr" target="#b30">[31]</ref>. The key principle depends on minimising the difference between the loss of a labelled set and a small unlabelled subset through a geometric-defined bound. We furthermore represent this baseline in our experiments as it successfully over-passed the uncertainty-based ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we describe the proposed method in details. First, we present the learners in brief for the image classification and regression tasks under the pool-based active learning scenario. In the second part, we discuss our two novel sampling methods: UncertainGCN and CoreGCN. UncertainGCN is based on the standard AL method uncertainty sampling <ref type="bibr" target="#b30">[31]</ref> which tracks the confidence scores of the designed graph nodes. Furthermore, CoreGCN adapts the highly successful CoreSet <ref type="bibr" target="#b30">[31]</ref> on the induced graph embeddings by the sequentially trained GCN network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learner</head><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, Phase I depicts the learner. Its goal is to minimise the objective of the downstream task. We have considered both the classification and regression tasks. Thus, the objective of this learner varies with the nature of the task we are dealing with. Classification: For the classification tasks, the learner is a CNN image classifier. We deploy a deep model M that maps a set of inputs x ? X to a discriminatory space of outputs y ? Y with parameters ?. We took ResNet-18 <ref type="bibr" target="#b14">[15]</ref> as the CNN model due to its relatively higher performance in comparison to other networks with comparable parameter complexity. Any other model like VGG-11 <ref type="bibr" target="#b31">[32]</ref> can also be easily deployed (refer to Supplementary Material B.4). A loss function L(x, y; ?) is minimized during the training process. The objective function of our classifier is crossentropy defined as below: <ref type="bibr" target="#b0">(1)</ref> where N l is the number of labelled training examples and f (x i , y i ; ?) is the posterior probability of the model M. Regression: To tackle the 3D HPE, we deploy a wellknown DeepPrior <ref type="bibr" target="#b25">[26]</ref> architecture as model M. Unlike the previous case, we regress the 3D hand joint coordinates from the hand depth images. Thus, the objective function of the model changes as in Equation 2. In the Equation, J is the number of joints to construct the hand pose.</p><formula xml:id="formula_0">L c M (x, y; ?) = ? 1 N l N l i=1 y i log(f (x i , y i ; ?)),</formula><formula xml:id="formula_1">L r M (x, y; ?) = 1 N l N l i=1 1 J J j=1 y i,j ? f (x i,j , y i,j ; ?) 2 ,<label>(2)</label></formula><p>To adapt our method to any other type of task, we just need to modify the learner. The rest of our pipeline remains the same which we discuss in more details in the following Sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sampler</head><p>Moving to the second Phase from <ref type="figure" target="#fig_0">Figure 1</ref>, we adopt a pool-based scenario for active learning. This has become a standard in deep learning system due to its successful deployment in recent methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42]</ref>. In this scenario, from a pool of unlabeled dataset D U , we randomly select an initial batch for labelling D 0 ? D U . Without loss of generality, in active learning research, the major goal is to optimize a sampler's method for data acquisition, A in order to achieve minimum loss with the least number of batches D n . This scope can be simply defined for n number of active learning stages as following:</p><formula xml:id="formula_2">min n min L M A(L M (x, y; ?)|D 0 ? ? ? ? ? D n ? D U ). (3)</formula><p>We aim to minimise the number of stages so that fewer samples (x, y) would require annotation. For the sampling method A, we bring the heuristic relation between the discriminative understanding of the model and the unlabelled data space. This is quantified by a performance evaluation metric and traced at every querying stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Sequential GCN selection process</head><p>During sampling as shown in <ref type="figure" target="#fig_0">Figure 1</ref> from Phase II to IV, our contribution relies on sequentially training a GCN initialised with the features extracted from the learner for both labelled and unlabelled images at every active learning stage. As stated before, similar to VAAL <ref type="bibr" target="#b32">[33]</ref>, we consider this methodology as model-based where a separate architecture is required for sampling. Our motivation in introducing the graph is primarily in propagating the inherited uncertainty on the learner feature space between the samples (nodes). Thus, message-passing between the nodes induces higher-order representations of the nodes after applying convolution on the graph. Finally, our GCN will act as a binary classifier deciding which images are annotated. Graph Convolutional Network. The key components of a graph, G are the nodes, also called vertices V and the edges E. The edges capture the relationship between the nodes and encoded in an adjacency matrix A. The nodes v ? R (m?N ) of the graph encode image-specific information and are initialised with the features extracted from the learner. Here, N represents the total number of both labelled and unlabelled examples while m represents the dimension of features for each node. After we apply l 2 normalisation to the features, the initial elements of A result as vector product between every sample of v i.e. (S ij = v i v j , {i, j} ? N ). This propagates the similarities between nodes while falling under the same metric space as the learner's objective. Furthermore, we subtract from S the identity matrix I and then we normalise by multiplying with its degree D. Finally, we add the self-connections back so that the closest correlation is with the node itself. This can simply be summarised under:</p><formula xml:id="formula_3">; D 0 D 0 D 0 D 0 D 1 D 1 D 1 D 2 D 2 D 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First selection stage Second selection stage Third selection stage All selected images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edges -Unlabelled sample -Selected sample -Labelled sample</head><formula xml:id="formula_4">A = D ?1 (S ? I) + I.<label>(4)</label></formula><p>To avoid over-smoothing of the features in GCN <ref type="bibr" target="#b17">[18]</ref>, we adopt a two-layer architecture. The first GCN layer can be described as a function f 1</p><formula xml:id="formula_5">G (A, V; ? 1 ) : R N ?N ? R m?N ? R h?N</formula><p>where h is number of hidden units and ? 1 are its parameters. A rectified linear unit activation <ref type="bibr" target="#b23">[24]</ref> is applied after the first layer to maximise feature contribution. However, to map the nodes as labelled or unlabelled, the final layer is activated through a sigmoid function. Thus, the output of f G is a vector length of N with values between 0 and 1 (where 0 is considered unlabelled and 1 is for labelled). We can further define the entire network function as:</p><formula xml:id="formula_6">f G = ?(? 2 (ReLU (? 1 A)A).<label>(5)</label></formula><p>In order to satisfy this objective, our loss function will be defined as:</p><formula xml:id="formula_7">L G (V, A; ? 1 , ? 2 ) = ? 1 N l N l i=1 log(f G (V, A; ? 1 , ? 2 ) i )? ? ? N ? N l N i=N l +1 log(1 ? f G (V, A; ? 1 , ? 2 ) i ),<label>(6)</label></formula><p>where ? acts as a weighting factor between the labelled and unlabelled cross-entropy. UncertainGCN: Uncertainty sampling on GCN. Once the training of the GCN is complete, we move forward to selection. From the remaining unlabelled samples D U , we can draw their confidence scores f G (v i ; D U ) as outputs of the GCN. Similarly to uncertainty sampling, we propose to select with our method, UncertainGCN, the unlabelled images with the confidence depending on a variable s margin . While querying a fixed number of b points for a new subset D L , we apply the following equation:</p><formula xml:id="formula_8">D L = D L ? arg max i=1???b |s margin ? f G (v i ; D U )|. (7)</formula><p>For selecting the most uncertain unlabelled samples, s margin should be closer to 0. In this manner, the selected images are challenging to discriminate, similarly to the adversarial training scenario <ref type="bibr" target="#b11">[12]</ref>. This stage is repeated as long as equation 3 is satisfied. Algorithm 1 summarises the GCN sequential training with the UncertainGCN sampling method.</p><p>Algorithm 1 UncertainGCN active learning algorithm 1: Given: Initial labelled set D 0 , unlabelled set D U and query budget b 2: Initialise (x L , y L ), (x U ) -labelled and unlabelled im-</p><formula xml:id="formula_9">ages 3: repeat 4: ? ? f (x L , y L )</formula><p>Train learner with labelled 5:</p><formula xml:id="formula_10">V = [v L , v U ] ? f (x L ? x U ; ?)</formula><p>Extract features for labelled and unlabelled <ref type="bibr">6:</ref> Compute adjacency matrix A according to Equation 4 <ref type="bibr" target="#b6">7</ref>:</p><formula xml:id="formula_11">? ? f G (V, A)</formula><p>Train the GCN 8: To integrate geometric information between the labelled and unlabelled graph representation, we approach a CoreSet technique <ref type="bibr" target="#b30">[31]</ref> in our sampling stage. This has shown better performance in comparison to uncertainty-based methods <ref type="bibr" target="#b37">[38]</ref>. <ref type="bibr" target="#b30">[31]</ref> shows how bounding the difference between the loss of the unlabelled samples and the one of the labelled is similar to the k-Centre minimisation problem stated in <ref type="bibr" target="#b36">[37]</ref>.</p><formula xml:id="formula_12">for i = 1 ? b do 9: D L = D L ? arg max i |s margin ? f G (v; D U )|</formula><p>In this approach, the sampling is based on the l2 distances between the features extracted from the trained classifier. Instead of that, we will make use of our GCN architecture by applying CoreSet method on the features represented after the first layer of the graph. To this, the CoreSet method benefits from the cyclical dependencies. The sampling method is adapted to our mechanism for each b data point under the equation:</p><formula xml:id="formula_13">D L = D L ?arg max i?D U min j?D L ?(f 1 G (A, v i ; ? 1 ), f 1 G (A, v j ; ? 1 )),<label>(8)</label></formula><p>where ? is the Euclidean distance between the graph features of the labelled node v i and the ones from the unlabelled node v j . We define this method as CoreGCN.</p><p>Finally, given the model-based mechanism, we claim that our sampler is task-agnostic as long as the learner is producing a form of feature representations. In the following section, we will experimentally demonstrate the performance of our methods quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We performed experiments on sub-sampling RGB and grayscale real images for classification, depth real images for regression and RGB synthetic-generated for classification tasks. We describe them in details below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classification</head><p>Datasets and Experimental Settings. We evaluated the proposed AL methods on four challenging image classification benchmarks. These include three RGB image datasets, CIFAR-10 <ref type="bibr" target="#b20">[21]</ref>, CIFAR-100 <ref type="bibr" target="#b20">[21]</ref> and SVHN <ref type="bibr" target="#b12">[13]</ref>, and a grayscale dataset, FashionMNIST <ref type="bibr" target="#b38">[39]</ref>. Initially, for every benchmark, we consider the entire training set as an unlabelled pool (D U ). As a cold-start, we randomly sample a small subset and query their labels, D L . For CIFAR-10, SVHN and FashionMNIST, the size of the seed labelled examples is 1,000. Whereas, for CIFAR-100 we select 2,000 due to their comparatively more number of classes (100 vs 10). We conduct our experiments for 10 cycles. At every stage, the budget is fixed at 1,000 images for the 10class benchmarks and at 2,000 for CIFAR-100 which is a 100-class benchmark. Similar to the existing works of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b41">42]</ref>, we apply our selection on randomly selected subsets D S ? D U of unlabelled images. This avoids the redundant occurrences which are common in all datasets <ref type="bibr" target="#b3">[4]</ref>. The size of D S is set to 10,000 for all the experiments. Implementation details. ResNet-18 <ref type="bibr" target="#b14">[15]</ref> is the favourite choice as learner due to its relatively higher accuracy and better training stability. During training the learner, we set a batch size of 128. We use Stochastic Gradient Descent (SGD) with a weight decay 5 ? 10 ?4 and a momentum of 0.9. At every selection stage, we train the model for 200 epochs. We set the initial learning rate of 0.1 and decrease it by the factor of 10 after 160 epochs. We use the same set of hyper-parameters in all the experiments. For the sampler, GCN has 2 layers and we set the dropout rate to 0.3 to avoid over-smoothing <ref type="bibr" target="#b42">[43]</ref>. The dimension of initial representations of a node is 1024 and it is projected to 512. The objective function is binary cross-entropy per node. We set the value of ? = 1.2 to give more importance to the larger unlabelled subset. We choose Adam <ref type="bibr" target="#b16">[17]</ref> optimizer with a weight decay of 5 ? 10 ?4 and a learning rate of 10 ?3 . We initialise the nodes of the graph with the features of the images extracted from the learner. We set the value of s margin Testing accuracy on CIFAR10 Random UncertainGCN(Ours) VAAL <ref type="bibr" target="#b32">[33]</ref> Learning Loss <ref type="bibr" target="#b41">[42]</ref> CoreSet <ref type="bibr" target="#b30">[31]</ref> CoreGCN(Ours) FeatProp <ref type="bibr" target="#b37">[38]</ref> 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000  to 0.1. For the empirical comparisons, we suggest readers to refer Supplementary Material. Compared Methods and Evaluation Metric: We compare our method with a wide range of baselines which we describe here. Random sampling is by default the most common sampling technique. CoreSet <ref type="bibr" target="#b30">[31]</ref> on learner feature space is one of the best performing geometric techniques to date and it is another competitive baseline for us. VAAL <ref type="bibr" target="#b32">[33]</ref> and Learning Loss <ref type="bibr" target="#b41">[42]</ref> are two state-of-the-art baselines from task-agnostic frameworks. Finally, we also compare with FeatProp <ref type="bibr" target="#b37">[38]</ref> which is a representative baseline for the GCN-based frameworks. This method is designed for cases where a static fixed graph is available. To approximate their performance, we construct a graph from the features extracted from learner and similarities between the features as edges. We then compute the k-Medoids distance on this graph. For quantitative evaluation, we report the mean average accuracy of 5 trials on the test sets. Quantitative Comparisons. We train the ResNet-18 learner with all the available training examples on every dataset separately and report the performance on the test set. Our implementation obtains 93.09% on CIFAR-10, 73.02% on CIFAR-100, 93.74% on FashionMNIST, and 95.35% on SVHN. This is comparable as reported on the official implementation <ref type="bibr" target="#b14">[15]</ref>. These results are also set as the upperbound performance of the active learning frameworks. CoreSet is the closest competitor for our methods. After selecting 10,000 labelled examples, the CoreGCN achieves 90.7% which is the highest performance amongst reported in the literature <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b32">33]</ref>. Likewise, <ref type="figure" target="#fig_5">Figure 3</ref> (right) shows the accuracy comparison on CIFAR-100. We observe almost similar trends as on CIFAR-10. With only 40% of the training data, we achieve 69% accuracy by applying CoreGCN. This performance is just 4% lesser than when training with the entire dataset. Compared to CIFAR-10, we observe the better performance on VAAL in this benchmark. The reason is that VAE might favour a larger query batch size (&gt;1,000). This exhaustively annotates large batches of data when the purpose of active learning is to find a good balance between exploration and exploitation as we constrain the budget and batches sizes. We further continue our evaluation on the image classification by applying our methods on FashionMNIST and SVHN. In <ref type="figure">Figure 4</ref>, the left and the right graphs show the comparisons on FashionMNIST and SVHN respectively. As in the previous cases, our methods achieve at minimum similar performance to that of existing methods or outperforming them. From the studies on these datasets, we observed consistent modest performance of FeatProp <ref type="bibr" target="#b37">[38]</ref>. This may be because it could not generalise on the unstructured data like ours. Qualitative Comparisons. To further analyse the sampling behaviour of our method we perform qualitative comparison with existing method. We choose CoreSet for its consistently better performance in empirical evaluations when compared to the other baselines. We made this comparison on CIFAR-10. For the two algorithms, we generate the t-SNE <ref type="bibr" target="#b35">[36]</ref> plots of both labelled and unlabelled extracted features from the learner at the first and fourth selection stage. To make a distinctive comparison of sampling behaviour from early stage, we choose to keep a difference of 3 stages. <ref type="figure">Figure 5</ref>, t-SNE plots, compares the sampling behaviour of CoreSet and UncertainGCN. In the first sampling stage, the selected samples distribution is uniform which is similar for both techniques. Without loss of generality, the learner trained with a small number of seed annotated examples is sub-optimal, and, hence the features of both labelled and unlabelled are not discriminative enough. This makes the sampling behaviour for both methods near random. At the fourth selection stage, the learner becomes relatively more discriminative. This we can notice from the clusters representing each class of CIFAR-10. Now, these features are robust to capture the relationship between the  continue the qualitative investigation for the CoreGCN acquisition method. CoreGCN avoids over-populated areas while tracking out-of-distribution unlabelled data. Compared to UncertainGCN, the geometric information from CoreGCN maintains a sparsity throughout all the selection stages. Consequently, it preserves the message passing through the uncertain areas while CoreSet keeps sampling closer to cluster centres. This brings a stronger balance in comparison to CoreSet between in and out-of-distribution selection with the availability of more samples. Ablation Studies To further motivate the GCN proposal, we conduct ablation studies on the sampler architecture. In <ref type="figure" target="#fig_9">Figure 7</ref>, still on CIFAR-10, we replace the GCN with a 2 Dense layer discriminator, UncertainDiscriminator. This approach over-fits at early selection stages. Although, GCN with 2 layers <ref type="bibr" target="#b17">[18]</ref> has been a de-facto optimal design choice, we also report the performance with 1 layer (hinders longrange propagation) and 3 (over-smooths). However, to further quantify the significance of our adjacency matrix with feature correlations, we evaluate GCN samplers with identity (UncertainGCN Eye) and filled with 1s matrices (Un-certainGCN Ones). Finally, a study on two important hyperparameters: drop-out (0.3, 0.5, 0.8) and the number of hidden units (128, 256, 512) is in the Supplementary B.2. We also fine-tune these parameters to obtain the optimal solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of labelled images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Regression</head><p>Dataset and Experimental Settings: We further applied our method on a challenging dataset for 3D Hand Pose Estimation benchmarks from depth images. ICVL <ref type="bibr" target="#b33">[34]</ref> contains 16,004 hand depth-images in the training set and the test set has 1,600. At every selection stage, similar to the experimental setup of image classification, we randomly presample 10% of entire training examples D S and apply the AL methods on this subset of the data. Out of this presampled images subset, we apply our sampler to select the most influencing 100 examples. Implementation Details: 3D HPE is a regression problem which involves estimating the 3D coordinates of the hand joints from depth images. Thus, we replace ResNet-18 by commonly used DeepPrior <ref type="bibr" target="#b25">[26]</ref> as learner. The sampler and the other components in our pipeline remain the same as in the image classification task. This is yet another evidence for our sampling method being task-agnostic. For all the experiments, we train the 3D HPE with Adam <ref type="bibr" target="#b16">[17]</ref> optimizer and with a learning rate of 10 ?3 . The batch size is 128. As pre-processing, we apply a pre-trained U-Net <ref type="bibr" target="#b29">[30]</ref> model to detect hands, centre, crop and resize images to the dimension of 128x128. Compared Methods and Evaluation Metric: We compare our methods from the two ends of the spectrum of baselines. One is random sampling which is the default mechanism. The other is CoreSet <ref type="bibr" target="#b30">[31]</ref>, one of the best performing baselines from the previous experiments. We report the performance in terms of mean squared error averaged from 5 different trials and its standard deviation.</p><p>Quantitative Evaluations: <ref type="figure" target="#fig_10">Figure 8</ref> shows the performance comparison on ICVL dataset. In the <ref type="figure">Figure,</ref> we can observe that both our sampling methods, CoreGCN and Un-certainGCN, outperform the CoreSet and Random sampling consistently from the second selection stage. The slope of decrease in error for our methods sharply falls down from the second till the fifth selection stage for UncertainGCN and till the sixth for CoreGCN. This gives us an advantage over the other methods when we have a very limited budget. At the end of the selection stage, CoreGCN gives the least error of 12.3 mm. In terms of performance, next to it is UncertainGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sub-sampling of Synthetic Data.</head><p>Unlike previous experiments of sub-sampling real images, we applied our method to select synthetic examples obtained from StarGAN <ref type="bibr" target="#b6">[7]</ref> trained on RaFD <ref type="bibr" target="#b22">[23]</ref> for translation of face expressions. Although Generative Adversarial Networks <ref type="bibr" target="#b11">[12]</ref> are closing the gap between real and synthetic data <ref type="bibr" target="#b28">[29]</ref>, still the synthetic images and its associated labels are not yet suitable to train a downstream discriminative model. Recent study <ref type="bibr" target="#b2">[3]</ref> recommends sub-sampling the  <ref type="figure">Figure 9</ref>: Performance comparison on sub-sampling synthetic data to augment real data for expression classification synthetic data before augmenting to the real data. Hence, we apply our algorithm to get a sub-set of the quality and influential synthetic examples. The experimental setup is similar to that of the image classification which we described in our previous Section. Learner and Sampler remain the same. The only difference will be in the nature of the pool images. Instead of real data, we have StarGAN synthetic images. <ref type="figure">Figure 9</ref> shows the performance comparison of random selection vs our UncertainGCN method in 5 trials. From the experiment, we can observe our method achieving higher accuracy with less variance than commonly used random sampling. The mean accuracy drops for both the methods from the fourth selection stage. Only a small fraction of synthetic examples are useful to train the model <ref type="bibr" target="#b2">[3]</ref>. After the fourth stage, we force sampler to select more examples which may end up choosing noisy data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented a novel methodology of active learning in image classification and regression using Graph Convolutional Network. After systematical and comprehensive experiments, our adapted sampling techniques, Uncer-tainGCN and CoreGCN, produced state-of-the-art results on 6 benchmarks. We have shown through qualitative distributions that our selection functions maximises informativeness within the data space. The design of our sampling mechanism permits integration into other learning tasks. Furthermore, this approach enables further investigation in this direction where optimised selection criteria can be combined GCN sampler. Here, we present a extended description of the datasets we used to evaluate our algorithms and the compared baselines. We evaluated our methods together with the others on four challenging image classification benchmarks: CIFAR-10 <ref type="bibr" target="#b20">[21]</ref>, CIFAR-100 <ref type="bibr" target="#b20">[21]</ref>, FashionMNIST <ref type="bibr" target="#b38">[39]</ref> and SVHN <ref type="bibr" target="#b12">[13]</ref>. Each of the datasets has different properties and present new challenges for the active learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>FashionMNIST is a grey scale image dataset. Whereas, others are RGB image datasets. CIFAR-10 consists of 50,000 images for training and 10,000 for testing. There are 5,000 samples for each of the 10 object categories. CIFAR-100 is constructed in a similar fashion with the same size of the training and testing set. The difference lies in the granularity of the data distribution as 100 classes are categorised (500 images corresponding to each class). The SVHN dataset represent 10 digit classes with 73,257 train images and 26,032 test images. Finally, FashionMNIST contains training and testing sets of the size 60,000 and 10,000, respectively, with annotations of 10 clothing designs. From an input image resolution perspective, despite FashionMNIST with a 28x28 size, the other datasets have 32x32 scale.</p><p>Together with the classification task, we shift the learner's objective to regression. As we tackle the 3D Hand Pose Estimation task, we benchmark our baselines on one of the most challenging, widely been used and first of depth based datasets, ICVL <ref type="bibr" target="#b33">[34]</ref>. This is composed of 16,004 images for training and 1,600 for testing. The dataset has a single frontal viewpoint and a wide range of articulation and hand positions. The initial resolution is 320x240, but we pre-process by hand centring and scaling to 128x128.</p><p>The last benchmark we deployed in the experiment section is the face expression dataset, Radboud Faces Database (RaFD) <ref type="bibr" target="#b22">[23]</ref>. This is formed of 7,200 training images, 800 for each of the 8 expressions. However, the test set contains only 840 images. Although the initial image dimensions are 256x256x3, for efficiency, we downscale them by a factor of 2. As we consider the entire training set as labelled in this experiment, we generate with StarGAN <ref type="bibr" target="#b6">[7]</ref> 57,600 images for the unlabelled set. Similar to the CIFAR-10 evaluation settings, we initially create a randomly distributed subset D S of 10,000 images from which we further apply the selection given a budget b of 1,000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments</head><p>CIFAR-10 imbalanced dataset In the experimental part, we evaluated quantitatively in a systematic manner the active learning methods over four image classification datasets. Although, before selection, we randomise the unlabelled samples to a subset, the dataset is still relatively balanced to each class distribution. However, this is not commonly the case where there is no prior information related to the data space. Therefore, we are simulating an imbalanced CIFAR-10 in a quantitative experiment. Beforehand we considered the 50,000 training set as unlabeled, given 5,000 samples for each of the 10 categories. We custom the dataset so that 5 of the 10 classes contain 10 % of their original data (500 samples each). Therefore, the new unlabelled pool is composed of 27,500 images. The experiment architecture and settings are similar to the one on the full scale.  CoreGCN, out-stand once again the other model-based selections like VAAL and Learning Loss. UncertainGCN scores 2% more than those methods with 80.05% mean average accuracy at 10,000 labelled samples. Meanwhile, CoreGCN achieves 84.5% top performance together with CoreSet. Thus, the geometric information is more useful in scenarios where the dataset is imbalanced.  <ref type="figure">Figure B</ref>.3, we modified the architecture of the learner from CIFAR-10 experiment to VGG-11 <ref type="bibr" target="#b31">[32]</ref>. Therefore, we analyse how the AL methods are affected in terms of accuracy at the fourth sampling stage.</p><p>In training the VGG-11 network, we kept the same hyperparameters. We also had to trace the features after the first four Max Pooling layers for the Learning Loss baseline. Our proposed methods present robustness to this change whilst GCN settings were left unchanged. Hence, they surpass all state-of-the-arts at this early stage. This also demonstrates how the batch size and the feature representation play an important role in the performances of the other baselines. The most affected baseline in this context is CoreSet. Hyper-parameters Study Here, we present the analysis of two important hyper-parameters in the objective of the sampler. These are GCN uncertainties margin s margin and ?, the labelled vs unlabelled data loss weighing factor. <ref type="figure">Figure B</ref>.4 summarises these studies. From the <ref type="figure">Figure,</ref> we observe that the performance improves when we decrease margin from 0.4 to 0.1. Afterwards, the performance is stable. This shows that our method is stable in the range of an optimal margin. Similarly, ? influences the performance. However, the drift in performance is smooth with the change in the value of ?.</p><p>Extended qualitative analysis on the AL method In <ref type="figure">Figure B</ref>.5, we extend our qualitative analysis by visualising the initial, the unlabelled and the last selected samples from CIFAR-10, ICVL and RaFD. The last selection stage for CIFAR-10 and ICVL is the 10th, while in the synthetic RaFD experiment is the 4th. The seed labelled images are acquired randomly before the first selection stage. The RaFD seed examples are from the entire training set as the AL selection is applied on StarGAN generated images. For all the three benchmarks we evaluated the selected examples with our proposed AL method, UncertainGCN. Although the seed labelled samples for CIFAR-10 are randomly selected, the top query images from the "cat" class consist of difficult examples. On the other hand, the remained unlabelled images present distinguishable features, easy for the learner to predict. These observations have been quantified in the main paper as well. However, in the ICVL dataset case, the selected samples show closer and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seed labelled images</head><p>Selected images Unlabelled images CIFAR-10 ICVL RaFD <ref type="figure">Figure B</ref>.5: Extended qualitative analysis on labelled/unlabelled images at the last selection stage for CIFAR-10, ICVL and RaFD easier hand articulations compared to the initial labelled set. This is because of the highly complex set that was used as seed examples. The unlabelled images might have a lack of representativeness in the learner's perception after all the 10 sampling stages. Finally, in the RaFD synthetic subsampling process, we can clearly denote the noisy images that were left unlabelled. These present more artefacts than the selected group.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The diagram outlines the proposed pipeline. Phase I: Train the learner to minimise the objective of downstream task from the available annotations, Phase II: Construct a graph with the representations of images extracted from the learner as vertices and their similarities as edges, Phase III: Apply GCN layers to minimise binary cross-entropy between labelled and unlabelled, Phase IV: Apply uncertainty sampling to select unlabelled examples to query their labels, Phase V: Annotate the selection, populate the number of labelled examples and repeat the cycle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>This Figure simulates the sampling behaviour of the proposed UncertainGCN sampler at its first three selection stages. We run a toy experiment just taking 100 examples from ICVL [34] Hand Pose dataset. Each node is initialised by the features extracted from the learner and edges capture their relation. Each concentric circle represents a cluster of strongly connected nodes. Here, in our case, a group of images having similar viewpoints are in a concentric circle. Considering two labelled examples as seed labelled examples in the centre-most circle, in the first selection stage, our algorithm selects samples from another concentric circle which is out-of-distribution than selecting the remaining examples from the innermost circle.Similarly, in the second stage, our sampler chooses images residing in another outer concentric circle which are sufficiently different from those selected before.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Label y U given new D L 12: until Equation 3 is satisfied CoreGCN: CoreSet sampling on GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Quantitative comparison on CIFAR-10(top) and CIFAR-100(bottom) (Zoom in the view)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 (Figure 4 :</head><label>34</label><figDesc>left) shows the performance comparison of Quantitative comparison on FashionMNIST(top) and SVHN(bottom) (Zoom in the view) UncertainGCN and CoreGCN with the other five existing methods on CIFAR-10. The solid line of the representation is the mean accuracy and the faded colour shows the standard deviation. Both our sampling techniques surpass almost every other compared methods in every selection stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Exploration comparison on CIFAR-10 between CoreSet and UncertainGCN (Zoom in the view) Exploration comparison on CIFAR-10 between CoreSet and CoreGCN (Zoom in the view)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>labelled and unlabelled examples which we encode in the adjacency matrix. Message-passing operations on GCN exploits the correlation between the labelled and unlabelled examples by inducing similar representations. This enables our method to target on the out-of-distribution unlabelled samples and areas where features are hardly distinguished. This characteristics we can observe on the plot of the fourth selection stage of UncertainGCN. Similarly, in Figure 6, we</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Ablation studies (Please zoom in)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Quantitative comparison on 3D Hand Pose Estimation (lower is better)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure A. 1 :</head><label>1</label><figDesc>This Figure shows some of the randomly sampled images from the data sets we use to validate our methods. Effectiveness of our method on these diverse characteristics of datasets demonstrate its generic nature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure B. 1 :</head><label>1</label><figDesc>Quantitative results -CIFAR-10 imbalanced dataset Figure B.1 shows the progressions of the presented baselines. Our proposed methods, UncertainGCN and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure B. 2 :Figure B. 3 :</head><label>23</label><figDesc>CIFAR-10 GCN Hyper-parameters Drop rate = 0.3 Hidden units = 128 Drop rate = 0.5 Hidden units = 128 Drop rate = 0.8 Hidden units = 128 Drop rate = 0.5 Hidden units = 256 Drop rate = 0.5 Hidden units = 512 Ablation studies -CIFAR-10 GCN Hyperparameters tuningAblation study -GCN parameter search While varying the architectural parameters of the GCN binary classifier, we encountered a poorer selection with the increase of the Dropout rate from 0.3 to 0.5 or 0.8. However, when changing the size of the hidden units to 256 and 512, the Un-certainGCN sampling was not affected on CIFAR-10. This might require further optimisation for different datasets although robustness is being shown. CIFAR-10 Learner VGG-11 -3 selection stages VGG-11 learner for CIFAR-10 image classification for 3 selection stages In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>4 Figure B. 4 :</head><label>44</label><figDesc>Hyper-parameter study on UncertainGCN margin (s margin ) (left) and labelled vs unlabelled data loss weighing factor, ? (right) (Zoom in the view)</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is partially supported by Huawei Technologies Co. and by EPSRC Programme Grant FACER2VM (EP/N007743/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Regional based query in graph active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Louzoun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The power of ensembles for active learning in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William H Beluch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bcai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan M K?hler</forename><surname>N?rnberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bcai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sampling strategies for gan synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryul</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumeysa</forename><surname>Bodur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semantic redundancies in image-classification datasets: The 10% you don&apos;t need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vighnesh</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Active Learning for Graph Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin Chen-Chuan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Support-vector networks. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Bayesian Active Learning with Image Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riashat</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active discriminative network representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sacha</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Smaller coresets for kmedian and k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sariel</forename><surname>Har</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Peled</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Kushal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCG</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bayesian Active Learning for Classification and Preference Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M?t?</forename><surname>Lengyel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ADAM: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Active nearest-neighbor learning in metric spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aryeh</forename><surname>Kontorovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Sabato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Urner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-05" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Presentation and validation of the radboud faces database. Cognition and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Dotsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gijsbert</forename><surname>Bijlstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skyler</forename><forename type="middle">T</forename><surname>Wigboldus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hawk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Knippenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepprior++: Improving fast and accurate 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hands deep in deep learning for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVWW</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bayesian Batch Active Learning as Sparse Subset Approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pinsler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos? Miguel</forename><surname>Hernandez-Lobato</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Seeing is not necessarily believing: Limitations of biggans for data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Active Learning for Convolutional Neural Networks: A Core-set approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Network for Large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Variational Adversarial Active Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Latent regression forest: Structured estimation of 3d articulated hand posture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danhang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Core vector machines: Fast svm training on very large data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pak-Ming</forename><surname>Cheung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Facility location: concepts, models, algorithms and case studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contributions to Management Science</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Active Learning for Graph Neural Networks via Node Feature Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artur</forename><surname>Dubrawski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno>1708.07747v2</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spatial Attention Deep Net with Partial PSO for Hierarchical Hybrid Hand Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning Loss for Active Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pairnorm: Tackling oversmoothing in gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
