<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Object Detection in Art Images Using Only Style Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kadish</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Design Department IT</orgName>
								<orgName type="institution">University of Copenhagen Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Risi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Digital Design Department IT</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><forename type="middle">Sundnes</forename><surname>L?vlie</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Digital Design Department IT</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Object Detection in Art Images Using Only Style Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite recent advances in object detection using deep learning neural networks, these neural networks still struggle to identify objects in art images such as paintings and drawings. This challenge is known as the cross depiction problem and it stems in part from the tendency of neural networks to prioritize identification of an object's texture over its shape. In this paper we propose and evaluate a process for training neural networks to localize objects -specifically people -in art images. We generate a large dataset for training and validation by modifying the images in the COCO dataset using AdaIn style transfer. This dataset is used to fine-tune a Faster R-CNN object detection network, which is then tested on the existing People-Art testing dataset. The result is a significant improvement on the state of the art and a new way forward for creating datasets to train neural networks to process art images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Image recognition systems -especially ones based on deep neural networks -have improved significantly in recent years <ref type="bibr" target="#b0">(LeCun et al., 2015)</ref>, yet they are often brittle and struggle if images are grainy or noisy. More worryingly, they can be manipulated into making incorrect classifications by malicious pixel changes, sometimes on as little as a single pixel <ref type="bibr" target="#b1">(Su et al., 2019)</ref>. Neural networks struggle even more to recognise objects depicted in different styles such as drawings or paintings, something the human mind can do readily even as a small child. This is known as the cross-depiction problem <ref type="bibr" target="#b2">(Cai et al., 2015;</ref><ref type="bibr" target="#b3">Boulton and Hall, 2019)</ref>.</p><p>Meanwhile, in the museum sector there is a growing interest in the use of object detection in artworks <ref type="bibr" target="#b4">(Ciecko, 2020)</ref>, partly for the purpose of improving metadata in order to better support search, e.g. by making it possible to search for visual motifs; and partly in order to set up new creative possibilities for artists and designers, to create innovative art experiences e.g. through apps such as Smartify or Send Me SFMoma <ref type="bibr" target="#b5">(Mollica, 2018)</ref>. As is the case with many AI problems, a major challenge for improving object detection in artworks is the lack of large, high quality datasets for training.</p><p>In this paper, we introduce a technique inspired in part by <ref type="bibr" target="#b6">Geirhos et al. (2018)</ref> to generate a dataset and train an object detector to locate people in artworks. <ref type="bibr">Geirhos et al.</ref> The research presented here has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 727040 (the GIFT project).</p><p>(2018) used style transfer to demonstrate that neural networks prioritize texture over shape in classifying images and to train a neural network to be more robust to textural distortions such as noise. Here, style transfer is used to address the cross-depiction problem. We create a large training dataset (58,672 images) of artistically-styled images from already-labelled photographs, train an object detector, and test it on the artwork dataset People-Art <ref type="bibr" target="#b2">(Cai et al., 2015)</ref>, which results in a significant improvement on past detection efforts on this dataset.</p><p>This research contributes to the field of computer vision by expanding the visual domains available for object detection, for instance making it possible to search for motifs in nonphotographic databases. Furthermore, it is a step towards developing vision algorithms that have more general representations of objects that resemble the ways humans see objects, allowing for more robust object detection in photos and video (e.g. algorithms that can better handle noise and distortions).</p><p>Improving object detection will enable new ways to search and study particular objects in art collections. This may enable new strategies for doing quantitative research on art history and cultural heritage in the field of digital humanities <ref type="bibr" target="#b7">(Berry and Fagerjord, 2017;</ref><ref type="bibr" target="#b8">Manovich, 2018</ref><ref type="bibr" target="#b8">Manovich, , 2020</ref>. In combination with existing metadata this would make it possible to search for specific depictions of people, for instance based on the colour of their skin, in order to quantitatively explore depictions of race in art (searching for images based on colour profile is already a well-established technique). Another use case might be to search the collections of a national museum for depictions of objects from other countries to explore international influences on culture in different historical eras. Such metadata could enable the design of more engaging experiences with museum collections, e.g. by giving users the means to search for objects relating to their personal interests <ref type="bibr" target="#b4">(Ciecko, 2020;</ref><ref type="bibr" target="#b9">Merritt, 2017;</ref><ref type="bibr" target="#b10">Bailey, 2019;</ref><ref type="bibr">Smith, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>A perpetual problem in artificial intelligence (AI) is that the creation of large-scale, high-quality datasets is timeconsuming and expensive <ref type="bibr" target="#b11">(Chang et al., 2017)</ref>. For some tasks an effort has been made to compile a canonical dataset that can be widely used by the AI community <ref type="bibr" target="#b12">(Deng et al., 2009;</ref><ref type="bibr" target="#b13">Lin et al., 2014)</ref>; for less common tasks the challenge persists. The major datasets for object detection training are comprised mainly of photographs. This leads to the cross-depiction problem <ref type="bibr" target="#b2">(Cai et al., 2015)</ref>, where Neural Networks (NNs) struggle to correctly identify objects depicted in different styles -they may be able to pick out a car in a photograph, but struggle with a pencil sketch of the same vehicle. <ref type="bibr">Cai et al. attempt</ref> to tackle this problem using neural networks pretrained on a large database of photographs and fine-tuned on a smaller dataset of labelled artwork. Achieving a maximum AP 50 score of 0.4, they conclude that deep learning has yet to demonstrate that it is able to solve the cross-depiction problem.</p><p>One of the potential reasons for this is that neural networks are not good at recognizing shapes. They base their decisions mainly on the texture of the objects that they are examining <ref type="bibr" target="#b6">(Geirhos et al., 2018)</ref>. While neural networks tend to be biased towards textures, humans tend to exhibit a strong bias towards shape <ref type="bibr" target="#b6">(Geirhos et al., 2018)</ref>. Artistic representations tend to also be shape-biased, making artwork especially difficult for most neural networks to process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The problem of detecting objects in artwork has proven to be particularly challenging. For image classification -categorizing the overall content of an image -a large training set can be created using metadata like image titles, descriptions, and artist-and crowd-generated tags, as in the Behance Artistic Media (BAM) dataset <ref type="bibr" target="#b14">(Wilber et al., 2017)</ref>. Object detection, however, requires training data with labelled bounding boxes which indicate where in an image an object is localized, and few sources of this type of data exist (see <ref type="table" target="#tab_0">table I</ref>).</p><p>The main data source used for object detection in artwork is the People-Art dataset, originally described by <ref type="bibr" target="#b2">Cai et al. (2015)</ref>. People-Art consists of a set of images of artwork in which people have been labelled with bounding boxes. The dataset includes a training set, a validation set and a test set. <ref type="table" target="#tab_0">Table II</ref> shows the breakdown of total images, positive images (containing people) and labelled people in the dataset. The dataset encompasses a wide range of artistic styles, but is notably small for training a deep neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Object detection</head><p>Despite the limited availability of training data, a number of efforts have been made to detect people using deep neural networks -and a variety of other strategies -in the People-Art test set. <ref type="bibr" target="#b2">Cai et al. (2015)</ref> used Region Based Convolutional Neural Networks (R-CNNs) pre-trained on ImageNet and attempted fine-tuning them on the VOC2007 dataset and People-Art. <ref type="bibr" target="#b15">Westlake et al. (2016)</ref> produced a similar attempt, using Fast Region Based Convolutional Neural Network (Fast R-CNN) with a variety of architectures also pre-trained on Im-ageNet and fine-tuned on VOC 2007 or People-Art. <ref type="bibr" target="#b16">Redmon et al. (2016)</ref> also tested the YOLO network on People-Art, in order to see how well it performed in a cross-depiction test. Evaluating these efforts is non-trivial, but the standard metrics are a set of average precision and recall measurements that assess the performance of the detector at different levels of bounding box overlap with the labelled box. The classic metric was devised as part of the PASCAL VOC challenge <ref type="bibr">(Everingham et al. 2010</ref>) and is the mean average precision (mAP) with a 50% overlap between bounding boxes. The results of these trials using this metric are shown in table III and <ref type="figure">fig. 5</ref>. The Fast R-CNN network using a VGG16 backbone, pre-trained on ImageNet and fine-tuned on the People-Art training set performed the best, scoring 0.58 in the AP 50 test.</p><p>Is this a good result? It can be difficult to know what to compare this to. Obviously, a score of 1.0 would be desirable, but likely unrealistic. The best score for person detection in photographs in the latest COCO detection challenge was 0.85 1 , but this detector was trained to look for objects in 80 different categories -a much more difficult task than just searching for people. Clearly there is room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATASET GENERATION USING STYLE TRANSFER</head><p>Given the assertion of <ref type="bibr" target="#b6">Geirhos et al. (2018)</ref> that Convolutional Neural Networks (CNNs) trained on ImageNet were generally texture-biased, we first tried using their Stylized-ImageNet-trained ResNet-50 network as a backbone for a Faster Region Based Convolutional Neural Network (Faster R-CNN) object detector to see if it would improve over a backbone trained on ImageNet. This failed to yield a significant improvement, but the process inspired an alternative use of style transfer. <ref type="bibr" target="#b6">Geirhos et al. (2018)</ref> had applied a style transfer to the images from ImageNet to generate a set of images they call Stylized-ImageNet (SIN). If we could use a similar process on a database of images that was already labelled for object detection, we could create a massive, pre-labelled dataset of images that could be used for object detection in art images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Style transfer</head><p>AdaIn style transfer <ref type="bibr" target="#b17">(Huang and Belongie, 2017</ref>) is a neural network that takes two input images and applies the style of one to the content of the other. It is the mechanism used in <ref type="bibr" target="#b6">Geirhos et al. (2018)</ref> to create the stylized dataset that they call Stylized-ImageNet.</p><p>We follow the basic process described in <ref type="bibr" target="#b6">Geirhos et al. (2018)</ref>, using the Painter by Numbers dataset from Kaggle 2 as the source of stylized images, modifying each input image with a single style with a stylization weight of 1 and not changing the size or crop of the input files. The stylization script used is from https://github.com/bethgelab/stylize-datasets.</p><p>For the source images, we chose the COCO 2017 dataset -a library of 123,287 photographs with bounding box object detection labels <ref type="bibr" target="#b13">(Lin et al., 2014)</ref>. The resulting dataset is referred to as StyleCOCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. StyleCOCO</head><p>StyleCOCO is a subset of the original COCO dataset <ref type="bibr" target="#b13">(Lin et al., 2014)</ref> with images stylized to look like artwork using AdaIn style transfer. It consists of a training set of 58,672 images and a validation set of 2,688 images 3 . The images are annotated for object detection for the person class and bounding box and segmentation information is retained. A total of 239,845 annotated people are found in the training set and a further 10,997 are found in the validation set.</p><p>Images from the original COCO dataset containing no people were removed from the dataset as the pixels not containing people provide sufficient negative examples for neural network training. No testing set was stylized as the dataset was only intended for training a neural network to be tested on data from the People-Art dataset. A breakdown of the number of images and labelled people is shown in table II and sample images from the StyleCOCO dataset are shown in <ref type="figure">fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MODEL TRAINING</head><p>The models employed in this study were trained using the PyTorch library and its built-in FasterRCNN model with a ResNet-152 backbone pre-trained on ImageNet. Models were fine-tuned on the StyleCOCO training dataset and evaluated during training using a random subset of 2,000 images from the StyleCOCO validation dataset. Early stopping was engaged to prevent overfitting of models to the training data. After finetuning was complete, models were tested on the full People-Art test dataset. The reported AP scores are from this testing phase.</p><p>The code for creating the dataset and training and testing the model is available at https://github.com/dkadish/Style-Transfer-for-Object-Detection-in-Art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments</head><p>Two experiments were conducted to test the validity of using stylized photographs to fine-tune a neural network for object detection in artwork. In the first (Experiment 1), we tested the impact of the number of training images on the performance of the object detector. We trained 7 models with exponentially 4 increasing amounts of input data, starting at 1000 images. Images were randomly selected from the full dataset for each model and only those images were used in training. The models were then tested on the People-Art testing dataset. These results were compared against each other in order to test the impact of additional training data on model performance.</p><p>The second experiment (Experiment 2) compares performance of a model fine-tuned on StyleCOCO on the People-Art testing dataset with other results reported in the literature. This experiment establishes the performance of the method detailed here against the state-of-the-art in object detection in artwork.</p><p>It is important to note that in both of these experiments, the testing results are achieved without having to use any labelled data from target distribution in training. The People-Art training datasets are not used in any way to train the NN that eventually detects objects in the People-Art testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model and training parameters</head><p>The model used in both experiments is a Faster R-CNN network with a ResNet-152 backbone pre-trained on Ima-geNet. Models were fine-tuned using the StyleCOCO training and validation datasets -or a subset thereof in the case of Experiment 1. Default training parameters from PyTorch were used except where modifications were shown to improve performance and those non-default parameters are listed below. 2 layers of the backbone were made trainable and the remaining 3 were frozen to retain the pre-trained ImageNet weights. Models were trained for 15 epochs, though early stopping (patience=3.0) was employed in order to avoid overfitting.</p><p>The model was optimized using stochastic gradient descent (SGD), using an initial learning rate of 0.005, a momentum of 0.9, and weight decay of 0.0005. The learning rate was adjusted over the course of the training using a stepped learning rate scheduler, which multiplied the rate by 0.2 every 5 epochs. A warm-up period of 5,000 iterations was also implemented to ease into the initial learning rate. The resulting learning rate curve is shown in <ref type="figure">fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment 1: Number of training images</head><p>The first experiment tests how the performance of a NN in detecting people in an unseen distribution of artistic images shifts depending on the number of stylized training images seen during fine-tuning. <ref type="figure" target="#fig_1">Figure 3</ref> shows the performance of the network using the average precision metrics (AP, AP 50 , and AP 75 ) defined in the COCO detection evaluation metrics 5 .</p><p>The precision rises sharply below about 10,000 images, but continue a slow improvement as more images are added to the training set. On the AP 50 metric, there is actually a small decrease in score between the NN trained on 32,000 images and the one trained on 58,672 images. However, the AP and AP 75 scores continue to rise.</p><p>To understand why, it is important to consider the meaning of the three scores. AP 50 and AP 75 are mean average precision  scores with different intersection over union (IoU) thresholds and AP is the average of a series of scores at different IoU thresholds from .50 to .95. The AP 50 score reflects object detections that are less exact than the AP 75 score. So, it appears as though the network reaches a limit for moderately precise detections somewhere around 32,000 training images. With more images, it is able to improve the precision of some detections while sacrificing some of the less precise bounding boxes. Note that the decline in AP 50 is relatively minor (0.4%) compared to the rise in AP 75 (2.0%) between the networks trained on 32,000 and 58,672 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment 2: Performance on People-Art dataset</head><p>Experiment 2 tests the performance of the trained networks against the state of the art, as described in literature. Four previous papers have published results for object detection on the People-Art testing dataset using a variety of methods <ref type="bibr" target="#b2">(Cai et al., 2015;</ref><ref type="bibr" target="#b15">Westlake et al., 2016;</ref><ref type="bibr" target="#b16">Redmon et al., 2016;</ref><ref type="bibr" target="#b18">Gonthier et al., 2020)</ref>. Each reported the PASCAL VOC metric which corresponds to the AP.50 score used here, so a comparison is made based on this value. <ref type="bibr" target="#b2">Cai et al. (2015)</ref> and <ref type="bibr" target="#b15">Westlake et al. (2016)</ref> are the creators of the People-Art dataset. <ref type="bibr" target="#b2">Cai et al. (2015)</ref> used a Deformable Parts Model (DPM) as well as 2 R-CNN models pre-trained on ImageNet and fine-tuned on VOC 2012 and People-Art. <ref type="bibr" target="#b15">Westlake et al. (2016)</ref> used Fast R-CNN models with a mix of backbones (CaffeNet, VGG1024, and VGG16) pre-trained on ImageNet and fine-tuned on either VOC 2007 or People-Art, achieving the best result (AP.50=.58) with the VGG16 backbone fine-tuned on People-Art. A YOLO model was tested on the People-Art dataset by <ref type="bibr" target="#b16">Redmon et al. (2016)</ref> with mid-range success (AP.50=0.45) though this model was not fine-tuned on the People-Art dataset nor was it trained or designed to recognize art specifically. Finally, <ref type="bibr" target="#b18">Gonthier et al. (2020)</ref> tested a number of Multiple Instance Learning (MIL) approaches in a Weakly Supervised Object Detection (WSOD) task on the People-Art dataset, achieving scores equal to those of <ref type="bibr" target="#b15">Westlake et al. (2016)</ref> in their best models.</p><p>The best result from each of the four papers is listed alongside our result in table II; full results from those papers 6 and our tests in Experiment 1 are plotted in <ref type="figure">fig. 5</ref>. The approach proposed here achieves a full 10-percentage-point improvement over these methods, with an AP.50 score of 0.68 on the People-Art testing set, compared to the next best score of 0.58. For a more detailed picture of the performance, <ref type="bibr">6</ref> Most of the papers attempted more than just one method and reported results from all of the trials. the precision-recall curve for the network trained with 58,672 images is shown in <ref type="figure" target="#fig_3">fig. 6</ref>. <ref type="figure" target="#fig_4">Figure 7</ref> shows some examples of the NN's range of performance on images from the People-Art testing dataset. The figure includes images where the NN was inaccurate (A,B) as well as some where it failed to detect people <ref type="bibr">(C,D)</ref>. In other images the NN performed relatively well (E,F), labelling the correct number of people with reasonable accuracy, though it seemed to struggle to precisely label people who are occluded by other people. In a final pair of images it correctly detected people that were unlabelled in the dataset (G,H), outperforming the humans that created the dataset. In G the NN correctly found a faint impression of a person in the left side of the image, though it appears to have been confused by the white space in the middle of the figure's body and has separately labelled the head, the lower body, and the entire person. In H the NN correctly found an additional head that was unlabelled in the right side of the image, but also mislabelled a mark at the top-right corner of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Failure Modes</head><p>The leap in performance achieved with fine-tuning on Style-COCO is itself useful, but it is just as interesting to examine the modes of failure of the method. We have studied a variety of images to attempt to understand the NN's failings, and the examples in <ref type="figure" target="#fig_4">fig. 7</ref> illustrate some relevant issues. Higher levels of abstraction such as that found in image D seem to confound the NN -especially in terms of the shape of the people. It also failed to detect stick figures in another example image. This is likely due to the way in which style transfer modifies images, largely shifting the colour and texture of the people in them without significantly warping their shapes. This means that while the Faster R-CNN network is fine-tuned with people depicted in a wide range of colours and textures, it has not seen this combination of shapes as a person before and is therefore unable to recognize it as such.</p><p>Visual complexity appears to also pose a challenge for NN. In <ref type="figure" target="#fig_4">images A and B (fig. 7)</ref>, the NN returns some wildly inaccurate bounding boxes and outright misses some of the people in the image. In B in particular, the resulting bounding boxes are dispersed almost randomly throughout the image. B also points to another likely source of error in the data.</p><p>Given the well-documented <ref type="bibr" target="#b19">(Buolamwini and Gebru, 2018;</ref><ref type="bibr" target="#b20">Dulhanty and Wong, 2019)</ref> problem of gender, race, and age biases in machine learning, it is likely that this bias plays a role in the NN's failure on some images. The Faster R-CNN network uses a backbone pre-trained on ImageNet which has documented imbalances in racial representation <ref type="bibr">(Yang et al., 2020)</ref> that are shown to lead to biased models <ref type="bibr" target="#b23">(Steed and Caliskan, 2021)</ref>.</p><p>The NN presented here likely suffers a second type of bias related to artistic style. The Painter-by-Numbers database used for style transfer in the creation of StyleCOCO draws most of its image set from WikiArt.org. WikiArt catalogues around  <ref type="figure">Fig. 5</ref>: AP 50 scores on the People-Art testing set for prior work (in blue) and results from this paper (in orange). Includes results from <ref type="bibr" target="#b2">Cai et al. (2015)</ref> [a], <ref type="bibr" target="#b15">Westlake et al. (2016)</ref> [b], <ref type="bibr" target="#b16">Redmon et al. (2016)</ref> [c], <ref type="bibr" target="#b18">Gonthier et al. (2020)</ref> [d] and this paper [e]. The method proposed here outperforms past detection efforts on the People-Art dataset for every network trained with more than 4,000 images. The two networks trained with over 32,000 images achieve improvements of 10percentage-points over the prior state of the art without having been trained on any of the images in the People-Art dataset. 250,000 artworks from more that 100 countries 7 , but it appears to skew towards those living and educated in North America and Europe. It is reasonable to assume, therefore, that an NN trained on StyleCOCO would perform better, for example, on Renaissance paintings than on a print from the Japanese shin-hanga movement. This was seen in testing as the NN performed poorly on Kiyokata Kaburagi's At the Shore.</p><formula xml:id="formula_0">R-CNN, f.t. VOC12, [a] DPM [a] Fast R-CNN (CaffeNet), f.t. VOC07, [b] Fast R-CNN (VGG1024), f.t. VOC07, [b] R-CNN, f.t. People-Art, [a] Fast R-CNN (VGG16), f.t. VOC07, [b] YOLO, f.t. V10, [c] Fast R-CNN (CaffeNet), f.t. People-Art, [b] Fast R-CNN (VGG1024), f.t. People-Art, [b] MI-max [d] MI-max-HL [d] Fast R-CNN (VGG16), f.t. People-Art, [b] Polyhedral MI-max [d] Faster R-CNN, f.t. StyleCOCO (n=1000), [e] Faster R-CNN, f.t. StyleCOCO (n=2000), [e] Faster R-CNN, f.t. StyleCOCO (n=4000), [e] Faster R-CNN, f.t. StyleCOCO (n=8000), [e] Faster R-CNN, f.t. StyleCOCO (n=16000), [e] Faster R-CNN, f.t. StyleCOCO (n=32000), [e] Faster R-CNN, f.t. StyleCOCO (n=58672), [e]</formula><p>Future work must consider these sources of bias -the ImageNet backbone, the COCO base image dataset, and the artistic images used for style transfer -and ways to identify these biases more specifically and mitigate its effects. One way to more precisely identify the issues would be to test the NN against the different image styles contained within the People-Art dataset independently. This could help to pinpoint the types of images that are difficult for the NN. Examination of saliency maps could also help to illuminate the source of difficulty in identifying people in specific artworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Applications</head><p>The technique in this paper can be applied to detect objects in non-photographic images, such as in museum collections and online databases. This type of localization of people can be used to extract more detailed metadata such as the number and relative positions of people in art images.</p><p>There is an additional opportunity to extend this process beyond the recognition of people in artwork and into other object categories. Using this technique it is possible to generate useful training data for all the object categories that are represented in the COCO dataset, thus circumventing one of the main limitations with existing datasets of non-photographic images, which include very few object categories (from 1 in In images A and B, the network did a poor job, failing to detect a number of people and placing wildly inaccurate bounding boxes for others in the busy scenes. Many of the people in images C and D were likely too abstract and the NN failed to detect people in them, though it managed to quite accurately find the person in the middle of image C. Images E and F represent pretty good results. Each image has bounding boxes that are imperfectly placed, but both feature the correct number of detections with some bounding boxes that overlap the ground truth almost exactly. Finally, images G and H represent cases where the detector found people in the images that had not been labelled with a ground truth in the dataset, but that were correct nonetheless. These detections show how the NNs can occasionally outperform humans at the task of detecting people in artwork.</p><p>People-Art to 7 in PACS). <ref type="bibr" target="#b2">Cai et al. (2015)</ref> note that objects do not appear with similar frequencies in artwork and people are over-represented, making it easier to collect databases of people in artwork, but harder to collect artwork depicting other minor categories. If style transfer can be used to train object detectors for people in artwork using the COCO database as pre-labelled data, then it is likely possible to train detectors for the other 79 object categories in COCO as well. Furthermore, the technique presented here could be used as a bootstrapping technique to build larger datasets of nonphotographic images for object detection. Using a NN trained with the stylization technique one could annotate a large dataset (e.g. BAM), which holds 2.5 million images <ref type="bibr" target="#b14">(Wilber et al., 2017)</ref>. These bounding boxes could be verified and corrected using a crowdsourcing approach, resulting in a potentially very large dataset of non-photographic images which could be used to further improve object detection algorithms.</p><p>Other application areas for our approach are domains in which users can create their own content such as in video games, social networks, or internet forums. In the game iNNk <ref type="bibr" target="#b24">(Villareale et al., 2020)</ref>, an AI has to guess what object a person is drawing before the other players are able to do so. For such systems it is challenging to obtain a large enough training set for accurate object detection, and they could therefore benefit from a dataset created using styletransfer. Other games feature user-generated content that is moderated fully-or semi-automatically to remove or restrict drawn images that are disturbing or inappropriate for the audience. A system trained on a style-transferred dataset could help to make these AI-driven processes more accurate and attuned to the styles of images produced in a particular game.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Future Work</head><p>In this initial exploration of the use of style transferred images for training object detectors in artwork, we used a general database of artistic images from the Painter-by-Numbers dataset to perform style transfer. However, since the target set of artwork was known in advance it would have been possible to use that set instead. A specialized version of StyleCOCO could be generated using unlabelled images from the target dataset as the style sources and that database could be used to train a NN that is specially tuned to the styles of representation found in the target images. This could yield additional improvements as the styled images in the training set more closely reflect the eventual targets.</p><p>Finally, though not used here, COCO also contains data for object segmentation, person keypoint detection and pose estimation, and detailed scene segmentation. It would be interesting to see how well these could be translated to artwork using the style transfer method described here.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :Fig. 2 :</head><label>12</label><figDesc>Sample images from the StyleCOCO dataset (above) paried with the original image from the COCO dataset (below) Learning rate for the model by iteration. The learning rate was adjusted using a stepped scheduler (step size= 2, ? = 0.2) and a warm-up (5000 iterations) to increase the rate at the beginning of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>AP scores for networks trained using different numbers of training images in the StyleCOCO dataset and tested on the People-Art dataset. The plot shows the 3 main AP measures (AP, AP 50 , and AP 75 ). The availability of more training images from StyleCOCO appears to improve the performance of the networks on the People-Art testing set -rapidly at first and then more gradually as the network sees tens of thousands of images in training. At 58,672 training images, AP=0.36, AP 50 =0.68, and AP 75 =0.33.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Ground truth (red bounding boxes) and results of person detection (green bounding boxes) with Faster R-CNN finetuned on StyleCOCO with increasing numbers of training images from 1000 to 58,672. Networks with fewer training images (left) produced far too many candidates. As the networks were able to train with a larger dataset, they narrowed their search and became more accurate overall. Note that in this example the network with 32,000 training images actually outperformed the network with 58,672 training images as the latter network added an extra, unmatched candidate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>7 www.wikiart.org/en/about Precision-Recall curve for AP 50 and AP 75 metrics from evaluation on the People-Art dataset for the Faster R-CNN network fine-tuned on 58,672 images from StyleCOCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Sample images from the People-Art testing dataset with the ground truth (red bounding boxes) and people detected (green bounding boxes) using Faster R-CNN fine-tuned on 58,672 images from StyleCOCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Labelled art image datasets</figDesc><table><row><cell>Name</cell><cell>Images</cell><cell cols="2">Labels Classes Task</cell></row><row><cell>Paintings Database</cell><cell>10000</cell><cell>8629</cell><cell>10 Classification</cell></row><row><cell>BAM</cell><cell cols="2">2500000 393000</cell><cell>n/a Classification</cell></row><row><cell>Photo-Art-50</cell><cell>?5000</cell><cell>?5000</cell><cell>50 Classification</cell></row><row><cell>PACS</cell><cell>9991</cell><cell>n/a</cell><cell>7 Classification</cell></row><row><cell>People-Art</cell><cell>4631</cell><cell>3487</cell><cell>1 Obj. Detection</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Art image object detection datasets</figDesc><table><row><cell>Dataset</cell><cell>Subset</cell><cell cols="2">Images Positive</cell><cell>People</cell></row><row><cell></cell><cell>training</cell><cell>1,627</cell><cell>521</cell><cell>1,324</cell></row><row><cell>People-Art</cell><cell>validation</cell><cell>1,387</cell><cell>442</cell><cell>1,080</cell></row><row><cell></cell><cell>testing</cell><cell>1,617</cell><cell>520</cell><cell>1,083</cell></row><row><cell>StyleCOCO</cell><cell>training validation</cell><cell>58,672 2,688</cell><cell cols="2">58,672 239,845 2,688 10,997</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>AP scores on the People-Art dataset, best results</figDesc><table><row><cell>Paper</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell></row><row><cell>Cai et al. (2015)</cell><cell>-</cell><cell>0.4</cell><cell>-</cell></row><row><cell>Westlake et al. (2016)</cell><cell>-</cell><cell>0.58</cell><cell>-</cell></row><row><cell>Redmon et al. (2016)</cell><cell>-</cell><cell>0.45</cell><cell>-</cell></row><row><cell>Gonthier et al. (2020)</cell><cell>-</cell><cell>0.58</cell><cell>-</cell></row><row><cell cols="2">This paper (n=58,672) 0.36</cell><cell>0.68</cell><cell>0.33</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">From https://cocodataset.org/#detection-leaderboard, this is the Chal17 score for person-person category by Megvii (Face++).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.kaggle.com/c/painter-by-numbers/ (accessed 14 Oct 2020).3  The set of images is smaller than the full COCO dataset because we excluded images that did not contain a labelled person.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Following the exponential pattern, the final experiment should have had 64000, but due to the limitations of the COCO dataset there were only 58,672 images available, so the entire dataset was used. 5 These metrics are used for all evaluation in this paper and are detailed on the COCO website at https://cocodataset.org/#detection-eval</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">One Pixel Attack for Fooling Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakurai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="828" to="841" />
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The Cross-Depiction Problem: Computer Vision Algorithms for Recognising Objects in Artwork and in Photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Corradi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00110</idno>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Artistic Domain Generalisation Methods are Limited by their Deep Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12622</idno>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">AI Sees What? The Good, the Bad, and teh Ugly of Machine Vision for Museum Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ciecko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Museums and the Web 2020. Online: Museums and the Web</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Send Me SFMOMA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mollica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MW18: Museums and the</title>
		<meeting><address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12231</idno>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Digital humanities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fagerjord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Polity Press</publisher>
			<pubPlace>Cambridge, UK and Malden, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Manovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Aesthetics</surname></persName>
		</author>
		<title level="m">Cultural Analytics</title>
		<meeting><address><addrLine>Moscow; Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Artificial Intelligence The Rise Of The Intelligent Machine. American Alliance of Museums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Merritt</surname></persName>
		</author>
		<ptr target="https://www.aam-us.org/2017/05/01/artificial-intelligence-the-rise-of-the-intelligent-machine/" />
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Solving Art&apos;s Data Problem -Part One, Museums. Artnome</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<ptr target="https://medium.com/smk-open/smks-collection-search-levels-up-cf8e967e9346" />
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
	<note>) SMK&apos;s collection search levels up. SMK Open</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Revolt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2017 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-05" />
			<biblScope unit="page" from="2334" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context,&quot; in Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="740" to="755" />
			<date type="published" when="2014" />
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BAM! The Behance Artistic Media Dataset for Recognition Beyond Photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="1211" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting people in artwork with CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Westlake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9913</biblScope>
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>October. Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2017-12" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multiple instance learning on deep features for weakly supervised object detection with extreme domain shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gonthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ladjal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01178</idno>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning</title>
		<editor>Research, S. A. Friedler and C. W. Abstract</editor>
		<meeting>Machine Learning<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-01" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Auditing ImageNet: Towards a Model-driven Framework for Annotating Demographic Attributes of Large-Scale Image Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dulhanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01347</idno>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards fairer datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020</title>
		<meeting>the 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<title level="m">Conference on Fairness, Accountability, and Transparency</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-01" />
			<biblScope unit="page" from="547" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Steed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caliskan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-03" />
			<biblScope unit="page" from="701" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villareale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Acosta-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Arcaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Freed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nuchprayoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sladek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weigelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Risi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">Extended Abstracts of the 2020 Annual Symposium on Computer-Human Interaction in Play</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="33" to="37" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
