<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PEDENET: IMAGE ANOMALY LOCALIZATION VIA PATCH EMBEDDING AND DENSITY ESTIMATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitai</forename><surname>Zhang</surname></persName>
							<email>kaitaizh@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">Jay</forename><surname>Kuo</surname></persName>
							<email>cckuo@sipi.usc.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PEDENET: IMAGE ANOMALY LOCALIZATION VIA PATCH EMBEDDING AND DENSITY ESTIMATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A neural network targeting at unsupervised image anomaly localization, called the PEDENet, is proposed in this work. PEDENet contains a patch embedding (PE) network, a density estimation (DE) network, and an auxiliary network called the location prediction (LP) network. The PE network takes local image patches as input and performs dimension reduction to get low-dimensional patch embeddings via a deep encoder structure. Being inspired by the Gaussian Mixture Model (GMM), the DE network takes those patch embeddings, and then predicts the cluster membership of an embedded patch. The sum of membership probabilities is used as a loss term to guide the learning process. The LP network is a Multi-layer Perception (MLP), which takes embeddings from two neighboring patches as input and predicts their relative location. The performance of the proposed PEDENet is evaluated extensively and benchmarked with that of state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image anomaly detection is a binary classification problem that decides whether an input image contains an anomaly or not. Image anomaly localization is to further localize the anomalous region at the pixel level. Due to recent advances in deep learning and availability of new datasets, recent research works are no longer limited to the image-level anomaly detection result, but also show a significant interest in the pixel-level localization of anomaly regions. Image anomaly detection and localization find real-world applications such as manufacturing process monitoring <ref type="bibr" target="#b0">[1]</ref>, medical image analysis <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, and video surveillance analysis <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Most well-studied localization solutions (e.g., semantic segmentation) rely on heavy supervision, where a large number of pixel-level labels and many labeled images are needed. However, in the context of image anomaly detection and localization, a typical assumption is that only normal (i.e., artifact-free) images are available in the training stage. This is because anomalous samples are few in general. Besides, they are often hard to collect and expensive in labeling. To this end, image anomaly localization is usually done in an unsupervised manner, and traditional supervised solutions are not applicable.</p><p>Several methods that integrate local image features and anomaly detection models for anomaly localization have been proposed recently, e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. They first employ a deep neural network to extract local image features and then apply the anomaly detection technique to local regions. Generally speaking, stage-by-stage training is only sub-optimal. However, when the stages are relatively independent, the loss could be little. Actually, it is observed in Sec. 4 that this approach offers excellent (and even the best) performance among various benchmarking models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. This is attributed to the adopt of a very large pretrained network. On one hand, the pre-trained network plays an important role in their superior performance. On the other hand, it demands higher computational complexity and memory requirement. Thus, it is costly to deploy in real-world applications.</p><p>To address these issues, we present a new neural network model, called the PEDENet, for unsupervised image anomaly localization in this work. PEDENet contains a patch embedding (PE) network, a density estimation (DE) network and an auxiliary network called the location prediction (LP) network. The PE network utilizes a deep encoder to get a low-dimensional embedding for local patches. Inspired by the Gaussian mixture model (GMM), the DE network models the distribution of patch embeddings and computes the membership of a patch belonging to certain modalities. arXiv:2110.15525v1 [cs.CV] 29 Oct 2021 It helps identify outlying artifact patches. The sum of membership probabilities is used as a loss term to guide the learning process. The LP network is a Multi-layer Perception (MLP), which takes patch embeddings as input and predicts the relative location of corresponding patches. The performance of the proposed PEDENet is evaluated and compared with state-of-the-art benchmarking methods by extensive experiments.</p><p>Our work has the following three main contributions.</p><p>? We propose PEDENet for unsupervised image anomaly localization. It can be trained in an end-to-end manner with only normal images (i.e. unsupervised learning).</p><p>? Being inspired by the GMM, the DE network models the distribution of patch embeddings and computes the membership of a patch belonging to certain modalities. It helps find outlying artifact patches.</p><p>? Experiments show that PEDENet achieves state-of-the-art performance on unsupervised anomaly localization for the MVTec AD dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reconstruction-based Approach</head><p>Since normal samples are only available in training, the reconstruction-based approach utilizes the extraordinary capability of neural networks to focus on normal samples' characteristics. Early work mainly uses the autoencoder and its variants <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Since they are trained with only normal training data, it is unlikely for them to reconstruct abnormal images in the testing stage. As a result, the pixel-wise difference between the input abnormal image and its reconstructed image can indicate the region of abnormality. However, this approach is problematic due to inaccurate reconstructions or poorly calibrated likelihoods. More recently, several methods exploit factors on top of the reconstruction loss such as the incorporation of an attention map <ref type="bibr" target="#b12">[13]</ref> or a student-teacher network to exploit intrinsic uncertainty in reconstruction <ref type="bibr" target="#b13">[14]</ref>. A similar idea is to adopt the inpainting model as an alternative <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. It first removes part of an image and then reconstructs the missing part based on the visible part. The difference between the removed region and its inpainted result could tell the abnormality level of that region. Although they show promising results, the resolution of anomaly maps is somehow coarse due to the heavy computational burden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pretrained Network-based Approach</head><p>The performance of some computer vision algorithms can be improved by transfer learning using discriminative embeddings from pretrained networks. Along this line of thought, some models combine image features obtained by pretrained networks with anomaly detection algorithms. For example, the nearest-neighbor algorithm is used in <ref type="bibr" target="#b6">[7]</ref> to examine whether an image patch of a test image is similar to any known normal image patches in the training set. The Gaussian distribution is adopted by <ref type="bibr" target="#b5">[6]</ref> to fit the distribution of local features that are extracted from a pretrained network. However, since the pretrained networks are not specially optimized for the image anomaly detection task, the resulting model usually has a large model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">One-class Classification-based Approach</head><p>Another natural idea is to adopt the Support Vector Data Description (SVDD) classifier. SVDD is a classic one-class classification algorithm derived from the Support Vector Machine (SVM). It maps all normal training data into a kernel space and seeks the smallest hypersphere that encloses the data in the space. Anomalies are expected to be located outside the learned hypersphere. Ruff et al. <ref type="bibr" target="#b16">[17]</ref> first incorporated this idea in a deep neural network for non-image data anomaly detection and then extended it to an unsupervised setting <ref type="bibr" target="#b17">[18]</ref>. They used a neural network to mimic the kernel function and trained it with the radius of the hypersphere. This modification allows an encoder to learn a data-dependent transformation, thus enhancing detection performance on high-dimensional and structured data. Later, Liznerski et al. <ref type="bibr" target="#b18">[19]</ref> generalized it to image anomaly detection by applying a SVDD-inspired pseudo-Huber loss to the output matrix of a Fully Convolutional Network (FCN). It offers further improvements in a semi-supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Non-image Data</head><p>For anomaly detection on non-image data, a Deep Autoencoding Gaussian Mixture Model(DAGMM) was proposed in <ref type="bibr" target="#b19">[20]</ref>. It combines dimensionality reduction and density estimation for unsupervised anomaly detection. Our proposed PEDENet is different from DAGMM in three aspects. First, PEDENet is designed for image anomaly localization while DAGMM targets at identifying an anomalous entity. Second, DAGMM employs an autoencoder (AE) to reconstruct the input data and concatenates latent features with reconstruction errors for density estimation. PEDENet adopts a PE network to learn local features and then applies the DE network to patch embeddings. Third, DAGMM demonstrates its performance on non-image data whose dimension is relatively lower. For example, the dimension of the latent space can be as low as one. In contrast, the dimension of patch embedding features is significantly higher in PEDENet.</p><p>3 Proposed Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PEDENet</head><p>An overview of the PEDENet is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, where the Hazelnut class from the MVTec AD dataset is used as an illustrative input. PEDENet conducts class-specific training and testing. It contains a patch embedding (PE) network, a density estimation (DE) network, and an auxiliary network called the location prediction (LP) network. The PE network takes an image patch as its input and performs dimension reduction to get low-dimensional patch embeddings via a deep encoder structure. The DE network takes a patch embedding as the input and predicts its cluster membership under the framework of the Gaussian Mixture Model (GMM). These two networks are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The LP network is a multi-layer perception (MLP). It takes a pair of patch embeddings as the input and predicts the relative location of them as depicted in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PE Network.</head><p>With an image patch as the input, the PE network outputs its low-dimensional embedding. Inspired by <ref type="bibr" target="#b20">[21]</ref>, we adopt a hierarchical encoder that embodies a larger encoder with several smaller encoders. That is, we divide an input patch, p, into 2 ? 2 sub-patches and feed each of smaller sub-patches into a smaller encoder. The outputs of the smaller sub-patches are aggregated based on their original positions. Then, they are encoded by a larger encoder to get the embedding of patch p. The above process can be summarized as</p><formula xml:id="formula_0">z = PEN(p; ? P EN ),<label>(1)</label></formula><p>where PEN denotes the PE network and z ? R Z is the low-dimensional embedding output of p, and Z is the dimensionality of the embedding space. The PE network is implemented as a composite mapping consisting of smaller encoders, a larger encoder and the aggregation process with learnable parameters ? P EN as shown in the right-hand-side of Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DE Network.</head><p>Given the low-dimensional embedding of an input patch, the DE network predicts its cluster membership values with respect to multiple Gaussian modalities under the Gaussian Mixture Model (GMM). A GMM assumes data samples are generated from a finite number of Gaussian distributions with unknown parameters. The Expectation-Maximization (EM) algorithm can be used to optimize those parameters iteratively. In the EM algorithm, the Expectation step computes the posterior probability (i.e. the so-called cluster membership) of each sample while the Maximization step updates the GMM parameters, including the mean vector, the covariance matrix and the mixture coefficient of each Gaussian component.</p><p>Note that GMM may suffer from a sub-optimal solution. Furthermore, it is challenging to combine it with a neural network to achieve end-to-end learning. By following the idea in DAGMM <ref type="bibr" target="#b19">[20]</ref>, we propose the DE network to estimate the cluster membership for each patch embedding, which corresponds to the expectation step of the EM algorithm:</p><formula xml:id="formula_1">? = softmax(DEN(z; ? DEN )),<label>(2)</label></formula><p>where DEN indicates the DE network, z is the low-dimensional embedding generated by the PE network, and ? DEN denotes parameters of the DE network. After the softmax normalization, ? is a vector in R K , where K is a hyper-parameter representing the number of Gaussian components in the GMM.</p><p>After getting the membership prediction ?, we update GMM parameters. This corresponds to the maximization step of the EM algorithm. Here, we use ? k , ? k and ? k to represent the mixture coefficient, the mean vector, the covariance matrix of the k-th component, 1 ? k ? K. For a batch of N patch embeddings, we have Then, we can express the probability of a patch embedding z i in form of</p><formula xml:id="formula_2">? k = N i=1 ? ik N ? R,<label>(3)</label></formula><formula xml:id="formula_3">? k = N i=1 ? ik z i N i=1 ? ik ? R Z ,<label>(4)</label></formula><formula xml:id="formula_4">? k = N i=1 ? ik (z i ? ? k )(z i ? ? k ) T N i=1 ? ik ? R Z?Z .<label>(5)</label></formula><formula xml:id="formula_5">P (z i ) = K k=1 ? k exp(? 1 2 (z i ? ? k )? ?1 k (z i ? ? k ) T ) 2?|? k | ? [0, 1],<label>(6)</label></formula><p>where |?| means the determinant of a matrix. If a patch embedding, z i , has a large probability, it means the corresponding patch is likely to be a normal patch, and vice versa.</p><p>LP Network. Inspired by Patch-SVDD <ref type="bibr" target="#b20">[21]</ref>, we employee the LP network as an auxiliary network that predicts the relative location of two neighboring patches in a self-supervised manner. For an arbitrary input patch, p, we sample another patch p from one of its eight neighbors as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. We feed patches p and p into the PE network to get their patch embeddings, z and z , respectively. The relative location of patch p against patch p is encoded to a one-hot vector l ? R 8 , used as the label in the training as:</p><formula xml:id="formula_6">l = LPN(z, z ; ? LP N ),<label>(7)</label></formula><p>where LPN denotes the LP network and ? LP N denotes its network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss Function</head><p>We propose the following loss function to train the PE, DE and LP networks jointly:</p><formula xml:id="formula_7">L = ? 1 L DEN + ? 2 L LP N + ? 3 L reg ,<label>(8)</label></formula><p>where ? 1 , ? 2 , and ? 3 are parameters adjustable by users to assign a different weight to each loss terms. The three terms in the right of Eq. (8) are elaborated below.</p><p>The first loss term is needed for the DE network. Since the total probability, P (z i ), models the likelihood of observing z i as a normal patch. By maximizing the average total probability of input patches,</p><formula xml:id="formula_8">L DEN = ? 1 N log N i=1 P (z i ),<label>(9)</label></formula><p>the DE network is trained to describe the distribution of patches embedding with an implicit GMM while the PE network can be optimized simultaneously. The second loss term is the cross-entropy loss between l andl for the LP network as given in Eq. <ref type="formula" target="#formula_8">(9)</ref>:</p><formula xml:id="formula_9">L LP N = ? 8 i=1 l i ? log(l i ).<label>(10)</label></formula><p>Finally, we add a regularization term to prevent singularity in the GMM, which occurs when the determinant of any ? k degenerates to zero. We add the regularization loss term to penalize small values of diagonal elements:</p><formula xml:id="formula_10">L reg = K k=1 Z z=1 1 ? kz .<label>(11)</label></formula><p>End-to-end Training. With the total loss function given in Eq. (8), all three networks could be jointly optimized using the back-propagation algorithm. The DE network and the LP network are two parallel branches concatenated to the PE network. That is, the output of the PE network, i.e. patch embedding, serves as the input to the DE and the LP networks. The DE network takes a patch embedding directly. The LP network takes the difference of embeddings of a pair of adjacent patches and use their relative position as the training label. In the training, parameters of all three networks are updated simultaneously so as to achieve end-to-end training for the whole PEDENet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Anomaly Localization</head><p>We obtain low-dimensional patch embeddings from the PE network so as to localize anomalies. Low-dimensional patch embeddings can be integrated with various anomaly detection models such as one-class SVM <ref type="bibr" target="#b21">[22]</ref> and SVDD <ref type="bibr" target="#b22">[23]</ref>. In this work, to illustrate the effectiveness of learned low-dimensional patch embeddings, we adopt the nearest neighbor rule in the embedding space to localize anomalous pixels. This was also used in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>For every patch p with stride S in test image X, we use its L 2 distance to the nearest normal patch in the embedding space to define its anomaly score:</p><formula xml:id="formula_11">AS(p) = min L 2 (p, p N ormal ).<label>(12)</label></formula><p>The pixel-wise anomaly score can be calculated by averaging anomaly scores of all patches to which this pixel belongs to. An approximate algorithm is adopted to mitigate the computational cost of the nearest neighbor search. The maximum anomaly score of pixels in an image is set to the image-level anomaly score.   <ref type="figure" target="#fig_0">Fig. 1</ref>. We train and evaluate anomaly localization algorithms for each class separately, which is known as class-specific evaluation. To train a model, all training images are first resized to 256 ? 256 and, then, patches of size 64 ? 64 are randomly sampled from these resized images.</p><p>Our proposed solution consists of three networks: 1) the PE network, 2) the DE network, and 3) the LP network. The PE network consists of eight convolutional layers and one output layer. The filter size in all convolutional layers is the same,  We train all networks using the Adam optimizer with learning rate 0.0001. The batch sizes are 128 image patches for the DE network and 36 pairs of adjacent patches for the LP network. All experiments are conducted on a machine equipped with an Intel i7-5930K CPU and an NVIDIA GeForce Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Evaluation</head><p>Anomaly Localization. We compare the image anomaly localization performance of several methods with respect to the MVTec AD dataset <ref type="bibr" target="#b23">[24]</ref> in <ref type="table" target="#tab_1">Table 1</ref>, where the evaluation metric is the pixel-wise area under the receiver operating characteristic curve (AUC-ROC). The benchmarking methods are listed below.</p><p>? Reconstruction approach: AE L2 <ref type="bibr" target="#b23">[24]</ref>, AE SSIM <ref type="bibr" target="#b8">[9]</ref>, Variational Autoencoder(VAE) and AnoGAN <ref type="bibr" target="#b1">[2]</ref>.</p><p>? Pre-trained network based approach: SPADE <ref type="bibr" target="#b6">[7]</ref>.</p><p>? One-class classification approach: Patch-SVDD <ref type="bibr" target="#b20">[21]</ref>, and FCDD <ref type="bibr" target="#b18">[19]</ref>.</p><p>Our proposed PEDENet achieves 97.0% for the average of 10 object classes (the 2nd best), 93.6% for the average of 5 texture classes (the 2nd best), and 95.9% for the average of all 15 classes (the 2nd best). Note that there is no single method that performs the best in all cases. PEDENet is second to SPADE in the average of 10 object classes while it is second to Patch-SVDD in the average of the 5 texture classes. The performance differences among SPADE, Patch-SVDD and PEDENet are actually quite small. Thus, it is fair to say that PEDENet is one of the state-of-the-art methods for image anomaly localization.</p><p>Anomaly Detection. We compare image anomaly detection performance in <ref type="table" target="#tab_2">Table 2</ref>, where the evaluation metric is the image-level area under the receiver operating characteristic curve (AUC-ROC). The benchmarking methods include: GANomaly <ref type="bibr" target="#b25">[26]</ref>, ITAE <ref type="bibr" target="#b26">[27]</ref>, Patch-SVDD <ref type="bibr" target="#b20">[21]</ref>, SPADE <ref type="bibr" target="#b6">[7]</ref> and MahalanobisAD <ref type="bibr" target="#b5">[6]</ref>.  <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>Model Size. Most state-of-the-art methods that give the best performance take the pre-trained network approach such as SPADE <ref type="bibr" target="#b6">[7]</ref>, and MahalanobisAD <ref type="bibr" target="#b5">[6]</ref>. As a result, their model sizes are usually quite large. We compare the model sizes of these three methods in <ref type="table" target="#tab_4">Table 3</ref> in terms of the number of model parameters. The numbers of model parameters of SPADE and MahalanobisAD are 145? and 36? of that of PEDENet. A smaller model size means lower memory requirement which is critical in real-world deployment. Besides being a light-weighted network, PEDENet is trained using the training data in the MVTec AD dataset only. In contrast, the two benchmarking methods leverage giant models that are pre-trained by millions of labeled images.  Ablation Study. We conduct ablation study to understand the impact of each loss term in Eq. (8) to the performance of the proposed PEDENet. That is, we remove either L DEN or L LP N and train the model under the same condition. We see from <ref type="table" target="#tab_5">Table 4</ref> that the adoption of both L DEN and L DEN as shown in Eq. (8) improves the anomaly localization performance.</p><p>Challenge of Texture Classes. It is worthwhile to point out that almost all methods have an obvious performance gap between the object classes and the texture classes. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the performance gap varies from 3% [21] to almost 20% <ref type="bibr" target="#b8">[9]</ref>. As mentioned in <ref type="bibr" target="#b20">[21]</ref>, the optimal hyper-parameters for objects and textures are likely to be different and there is a trade-off to get the best average performance. Actually, texture and regular images are treated differently in image processing. There many unique methods developed specifically for textures <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. Unlike regular images, there are strong self-similarity and quasi-periodicity in textures, which could be exploited to achieve further performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>A new neural network model, called the PEDENet, was proposed for image anomaly detection and localization. It is a lightweight model and offers state-of-the-art anomaly detection and localization performance. There are some future research topics. First, the MVTec AD dataset is still too small. It is valuable to collect more samples and build a larger dataset. Second, it is desired to find a more effective solution for texture anomaly detection and localization. Third, there are many hyper-parameters in the proposed PEDENnet, which are finetuned by trial and error. A more systematic approach is needed. Finally, it is interesting to find an altenative image anomaly detection and localization solution that is effective and interpretable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Image anomaly localization examples (from left to right): normal images, anomalous images, ground truth of the anomalous region and the predicted anomalous region by the proposed PEDENet, where the red region indicates the detected anomalous region. These examples are taken from the MVTec AD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An overview of the proposed PEDENet. Image are first divided into patches and, then, fed into the Patch Embedding (PE) network for patch embeddings. The Density Estimation (DE) network conducts clustering in the embedding space. After training, normal patches are clustered, and outliers could be treated as abnormal patches in the inference stage. Three anomaly localization results from thee Hazelnut class are shown as examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The diagram of the location prediction (LP) network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of anomalous images, labeled ground truths and localization results of the proposed PEDENet for 10 object classes in the MVTec AD dataset, where the red color is used to indicate detected anomaly regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of anomalous images, labeled ground truths and localization results of the proposed PEDENet for 5 texture classes in the MVTec AD dataset, where the red color is used to indicate detected anomaly regions. i.e. 3 ? 3. The filter numbers for convolutional layers are 32, 64, 128, 128, 64, 64, 32, 32, 64 and LeakyReLU [25] with slope 0.1 is used as activation function. The tanh activation is used in the output layer to normalize the output to the range of [-1.0,1.0]. Both the DE and LP networks are multilayer perceptrons (MLPs) with the LeakyReLU activation of slope 0.1. The DE network has three hidden layers of 128, 64, 32 neurons, respectively. The LP network has two hidden layers of 128 neurons per layer. The input to the LP network is subtraction of features from two neighboring patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The rest of this letter is organized as follows. Related previous work is reviewed in Sec. 2. The proposed PEDENet is presented in Sec. 3. Experimental results are shown in Sec. 4. Finally, concluding remarks are given in Sec. 5.</figDesc><table><row><cell>2 Related Work</cell></row><row><cell>Some related previous work is reviewed in this section. For image anomaly detection and localization, there are three</cell></row><row><cell>commonly used approaches: 1) reconstruction-based, 2) pretrained network-based and 3) one-class classification-based</cell></row><row><cell>approaches. They are reviewed in Secs. 2.1, 2.2 and 2.3, respectively. Finally, work on non-image anomaly data is</cell></row><row><cell>briefly mentioned in Sec. 2.4.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of image anomaly localization performance, where the evaluation metric is pixel-wise AUC-ROC. The best and the second best results are shown in bold face and with an underline, respectively.</figDesc><table><row><cell></cell><cell cols="8">AE L2 AE SSIM AnoGAN VAE SPADE Patch-SVDD FCDD PEDENet (ours)</cell></row><row><cell>Bottle</cell><cell>0.86</cell><cell>0.93</cell><cell>0.86</cell><cell>0.831</cell><cell>0.984</cell><cell>0.981</cell><cell>0.80</cell><cell>0.984</cell></row><row><cell>Cable</cell><cell>0.86</cell><cell>0.82</cell><cell>0.78</cell><cell>0.831</cell><cell>0.972</cell><cell>0.968</cell><cell>0.80</cell><cell>0.971</cell></row><row><cell>Capsule</cell><cell>0.88</cell><cell>0.94</cell><cell>0.84</cell><cell>0.817</cell><cell>0.990</cell><cell>0.958</cell><cell>0.88</cell><cell>0.943</cell></row><row><cell>Hazelnut</cell><cell>0.95</cell><cell>0.97</cell><cell>0.87</cell><cell>0.877</cell><cell>0.991</cell><cell>0.975</cell><cell>0.96</cell><cell>0.970</cell></row><row><cell>Metal</cell><cell>0.86</cell><cell>0.89</cell><cell>0.76</cell><cell>0.787</cell><cell>0.981</cell><cell>0.980</cell><cell>0.88</cell><cell>0.973</cell></row><row><cell>Pill</cell><cell>0.85</cell><cell>0.91</cell><cell>0.87</cell><cell>0.813</cell><cell>0.965</cell><cell>0.951</cell><cell>0.86</cell><cell>0.960</cell></row><row><cell>Screw</cell><cell>0.96</cell><cell>0.96</cell><cell>0.80</cell><cell>0.753</cell><cell>0.989</cell><cell>0.957</cell><cell>0.87</cell><cell>0.972</cell></row><row><cell>Teeth brush</cell><cell>0.93</cell><cell>0.92</cell><cell>0.90</cell><cell>0.919</cell><cell>0.979</cell><cell>0.981</cell><cell>0.90</cell><cell>0.979</cell></row><row><cell>Transistor</cell><cell>0.86</cell><cell>0.90</cell><cell>0.80</cell><cell>0.754</cell><cell>0.941</cell><cell>0.970</cell><cell>0.80</cell><cell>0.982</cell></row><row><cell>Zipper</cell><cell>0.77</cell><cell>0.88</cell><cell>0.78</cell><cell>0.716</cell><cell>0.965</cell><cell>0.951</cell><cell>0.81</cell><cell>0.962</cell></row><row><cell>All 10 Object Classes</cell><cell>0.88</cell><cell>0.91</cell><cell>0.83</cell><cell>0.810</cell><cell>0.976</cell><cell>0.967</cell><cell>0.86</cell><cell>0.970</cell></row><row><cell>Carpet</cell><cell>0.59</cell><cell>0.87</cell><cell>0.54</cell><cell>0.597</cell><cell>0.975</cell><cell>0.926</cell><cell>0.93</cell><cell>0.922</cell></row><row><cell>Grid</cell><cell>0.90</cell><cell>0.94</cell><cell>0.58</cell><cell>0.612</cell><cell>0.937</cell><cell>0.962</cell><cell>0.87</cell><cell>0.959</cell></row><row><cell>Leather</cell><cell>0.75</cell><cell>0.78</cell><cell>0.64</cell><cell>0.671</cell><cell>0.976</cell><cell>0.974</cell><cell>0.98</cell><cell>0.976</cell></row><row><cell>Tile</cell><cell>0.51</cell><cell>0.59</cell><cell>0.50</cell><cell>0.513</cell><cell>0.874</cell><cell>0.914</cell><cell>0.92</cell><cell>0.926</cell></row><row><cell>Wood</cell><cell>0.73</cell><cell>0.73</cell><cell>0.62</cell><cell>0.666</cell><cell>0.885</cell><cell>0.908</cell><cell>0.89</cell><cell>0.900</cell></row><row><cell>All 5 Texture Classes</cell><cell>0.70</cell><cell>0.78</cell><cell>0.29</cell><cell>0.612</cell><cell>0.929</cell><cell>0.937</cell><cell>0.92</cell><cell>0.936</cell></row><row><cell>Average of 15 Classes</cell><cell>0.82</cell><cell>0.87</cell><cell>0.74</cell><cell>0.744</cell><cell>0.965</cell><cell>0.957</cell><cell>0.88</cell><cell>0.959</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of image anomaly detection performance, where the evaluation metric is the image-level AUC-ROC. The best and the second best results are shown in bold face and with an underline, respectively.To verify the effectiveness of the proposed PEDENet, we conducted experiments on the MVTec AD dataset<ref type="bibr" target="#b23">[24]</ref>, which is a comprehensive anomaly localization dataset collected from real world scenarios. It consists of images belonging to 15 classes, including 10 object classes and 5 texture classes. For each class, there are 60-391 training images and 40-167 test images, where image sizes varies from 700 ? 700 to 1024 ? 1024. The training set contains only normal images while the test set contains both normal and anomalous images. Examples of normal and anomalous images are shown in</figDesc><table><row><cell></cell><cell cols="6">GANomaly ITAE Patch-SVDD SPADE MahalanobisAD PEDENet (ours)</cell></row><row><cell>Image-Level AUC-ROC</cell><cell>0.762</cell><cell>0.839</cell><cell>0.921</cell><cell>0.855</cell><cell>0.958</cell><cell>0.928</cell></row><row><cell>4 Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1 Experimental Setup</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Our proposed PEDENnet reaches 92.8%. It outperforms both SPADE and Patch-SVDD. It is only second to MahalanobisAD. Thus, PEDENnet also offers state-of-the-art image anomaly detection performance. Visualization of Localized Anomalies. Anomalous images, labeled ground truths and localization results of the proposed PEDENet for 10 object classes and 5 texture classes in the MVTec AD dataset are visualized in Figs. 4 and 5, respectively. Anomalous regions are highlighted in red. A region is more likely to be anomalous if it has stronger red color. As shown in those figures, anomalous regions could be accurately detected and localized by the proposed PEDENet. Note that relatively small and inconspicuous defects can still be spotted such as Capsule, Hazelnut and Pill examples in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Model Size Comparison</figDesc><table><row><cell>Methods</cell><cell cols="2">Pre-trained Model # of Parameters</cell></row><row><cell>SPADE</cell><cell>Wide ResNet-50</cell><cell>68M</cell></row><row><cell>MahalanobisAD</cell><cell>EfficientNet-B4</cell><cell>17M</cell></row><row><cell>PEDENet (ours)</cell><cell>None</cell><cell>0.47M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation Study</figDesc><table><row><cell>L DEN L LP N AUC-ROC ? 0.810 ? 0.894 ? ? 0.959</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Anomaly detection and classification in a laser powder bed additive manufacturing process using a trained computer vision algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Scime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Beuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Additive Manufacturing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="114" to="126" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">f-anogan: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on local statistical aggregates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2112" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Anomalynet: An anomaly detection network for video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick Siow Mong</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2537" to="2550" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Modeling the distribution of normal data in pre-trained deep features for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorit</forename><surname>Merhof</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14140</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sub-image anomaly detection with deep pyramid correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02357</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Anomalyhop: An ssl-based image anomaly localization method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Sohrab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moncef</forename><surname>Gabbouj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-C Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visual Communications and Image Processing</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sindy</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</title>
		<editor>Alain Tr?meau, Giovanni Maria Farinella, and Jos? Braz</editor>
		<meeting>the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>SciTePress</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="372" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainaz</forename><surname>Eftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niousha</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad Hossein Rohban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamid R Rabiee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12959</idno>
		<title level="m">Puzzle-ae: Novelty detection in images through solving puzzles</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards visually explaining variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bir</forename><surname>Bhanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavia</forename><surname>Radke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Camps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8642" to="8651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Same same but differnet: Semi-supervised defect detection with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1907" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention guided anomaly localization in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashanka</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat Vikram</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahalanobis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="485" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Superpixel masking and inpainting for self-supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reconstruction by inpainting for visual anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitjan</forename><surname>Zavrtanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijel</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">107706</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep semi-supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>G?rnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02694</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Liznerski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billy</forename><forename type="middle">Joe</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Franks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>M?ller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01760</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Explainable deep one-class classification. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeki</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Patch-SVDD: Patch-level SVDD for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">One-class svm for learning in image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2001 International Conference on Image Processing (Cat. No. 01CH37205)</title>
		<meeting>2001 International Conference on Image Processing (Cat. No. 01CH37205)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="34" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MVTec AD-A comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ganomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="622" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attribute restoration framework for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Jinkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Texture analysis and classification with tree-structured wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhorng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-Cj</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="429" to="441" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Texture analysis via hierarchical spatial-spectral correlation (HSSC)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4419" to="4423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A data-centric approach to unsupervised texture segmentation using principle representative patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1912" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dynamic texture synthesis by incorporating long-range spatial and temporal correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05940</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
