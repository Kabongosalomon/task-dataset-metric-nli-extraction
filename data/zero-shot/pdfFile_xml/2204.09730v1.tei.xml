<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer Decoders with MultiModal Regularization for Cross-Modal Food Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Shukor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Couairon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Meta AI 3 Meero</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asya</forename><surname>Grechka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transformer Decoders with MultiModal Regularization for Cross-Modal Food Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-modal image-recipe retrieval has gained significant attention in recent years. Most work focuses on improving cross-modal embeddings using unimodal encoders, that allow for efficient retrieval in large-scale databases, leaving aside cross-attention between modalities which is more computationally expensive. We propose a new retrieval framework, T-Food (Transformer Decoders with MultiModal Regularization for Cross-Modal Food Retrieval) that exploits the interaction between modalities in a novel regularization scheme, while using only unimodal encoders at test time for efficient retrieval. We also capture the intra-dependencies between recipe entities with a dedicated recipe encoder, and propose new variants of triplet losses with dynamic margins that adapt to the difficulty of the task. Finally, we leverage the power of the recent Vision and Language Pretraining (VLP) models such as CLIP for the image encoder. Our approach outperforms existing approaches by a large margin on the Recipe1M dataset. Specifically, we achieve absolute improvements of 8.1 % (72.6 R@1) and +10.9 % (44.6 R@1) on the 1k and 10k test sets respectively. The code is available here:https://github.com/mshukor/TFood.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multimodal learning, especially Vision and Language Pretraining (VLP), has become an attractive research field with many applications such as cross-modal retrieval <ref type="bibr" target="#b45">[45]</ref>, Visual Question Answering (VQA) <ref type="bibr" target="#b0">[1]</ref> and visual reasoning <ref type="bibr" target="#b63">[63]</ref>. An interesting playground for multimodal learning is computational cooking, which encompasses different tasks such as food categorization <ref type="bibr" target="#b40">[40]</ref>, food perception <ref type="bibr" target="#b43">[43]</ref>, recommendation <ref type="bibr" target="#b18">[18]</ref>, or retrieval <ref type="bibr" target="#b48">[48]</ref>.</p><p>In this work, we focus on recipe-image retrieval, which consists in retrieving the image corresponding to a given recipe and vice versa. This task has gained much attention in the recent years since the release of Recipe1M <ref type="bibr" target="#b48">[48]</ref>, a dataset containing one million textual recipes and their corresponding images. The textual recipe is composed of 3 entities: title, ingredients, and instructions.</p><p>Most previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b64">64]</ref> rely on specific (dual) encoders for images and text recipes that embed the two modalities in a shared latent space. The recipe encoder usually considers the recipe entities as independent and encodes them in separate modules <ref type="bibr" target="#b58">[58]</ref>. These dual encoders are trained using pairwise, triplet or any contrastive loss optimization to align the visual and textual representations.</p><p>We propose a novel strategy for cross-modal recipe retrieval based on a new architecture and learning framework. Since recipe entities (title, ingredients, instructions) are highly correlated, we propose a hierarchical transformer that explicitly leverages the intra and inter entity dependencies. For the visual encoder, we consider vision transformers such as ViT, but also recent models such as CLIP-ViT pretrained on large multimodal datasets. We expect the latter to be more robust to the noise in the Recipe1M dataset <ref type="table">Table 1</ref>. Qualitative results; we plot the best 5 retrieved images for each recipe query on the Recipe1M test set with 1k setup. All 5 images share semantic similarities with the query image.</p><p>which has been scraped from cooking websites. For efficient large-scale retrieval, we utilize dual encoders, but we consider a more complex image-recipe interaction during the training, which we materialize by a novel transformer based module with Image-Text Matching loss that acts as a regularization to better align the encoders representation. We also propose a new adaptive triplet loss with a dynamic margin which changes according to the difficulty of the task. The whole architecture and learning scheme is presented on <ref type="figure" target="#fig_0">Fig 1.</ref> Our contributions are two-fold:</p><p>? Deep architecture design: (a) we propose a new recipe encoder with transformer decoders that captures the interactions between recipe entities, (b) and we leverage VLP models trained on large-scale datasets for the image encoder. (c) We complete this architecture with a multimodal block that consists of inter-connected transformers, especially designed for the training. We keep only the unimodal encoders for efficient crossmodal retrieval at test time.</p><p>? Training framework: (a) We propose a novel Multi-Modal Regularization that consists of an Image-Text Matching loss on top of the multimodal module. (b) In addition, we introduce a new adaptive triplet loss with dynamic margin that adapts to the difficulty of the task.</p><p>We demonstrate the interest of our approach with extensive evaluation and comparison on the Recipe1M dataset.</p><p>The work is organised as follows; we detail some of the related work in section 2. We then present our approach in section 3. The method is validated in the experiments section 4. Finally, the work is summarized in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi Modal Learning</head><p>Vision and Language Pretraining (VLP) has been successful for many downstream tasks such as visual question answering (VQA), image captioning, NLRVR 2 and cross modal retrieval <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b41">41]</ref>. They are mainly based on dual encoders (DE) <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b46">46]</ref>, multimodal encoders (ME) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b72">72]</ref> or a combination of both <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b68">68]</ref>. Dual encoders, such as CLIP <ref type="bibr" target="#b46">[46]</ref> and ALIGN <ref type="bibr" target="#b26">[26]</ref>, use separate encoders for each modality, and a contrastive loss to align the modalities. These models are efficient during inference, however, they are trained on massive amounts of data and rely on global similarity without exploiting the fine-grained modalities interaction.</p><p>Multimodal encoders, on the other hand, employ a heavy transformer that takes the tokens of both modalities and rely on several pretraining tasks such as Masked Language Modeling (MLM) <ref type="bibr" target="#b12">[13]</ref>, Image-Text Matching (ITM) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b38">38]</ref>, Masked Image Modeling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b51">51]</ref> or prefixLM <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b61">61]</ref>. Despite their success in vision-language tasks, they suffer from computational complexity due to self and cross attention between modalities, limiting their adaptability for large scale tasks. To make these models more efficient during inference, some work adopts re-ranking of top k examples after a first ranking using the dual encoders <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b41">41]</ref>. Despite the success of these approaches for cross-modal retrieval, the efficiency of the model at large scale is still a bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Contrastive Learning</head><p>Contrastive learning is a wide area in representation learning, especially self supervised learning (SSL) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">22]</ref>, where the model is forced to be invariant to input transformations. This is done by encoding the positive pairs (e.g. the image and its transformed version) close in the latent space and the negative pairs (e.g. different images) far away, however, while contrastive learning is becoming mostly associated with SSL, the idea goes back to distance metric learning <ref type="bibr" target="#b67">[67]</ref>. Interestingly, the contrastive loss is not applied to the underlying representation, but after projecting it into another space. This is similar to what we propose, although, we differ in; (a) we have 2 projection modules, one linear and another more complex, (b) we work in a multimodal setting. While most of the SSL work use the In-foNCE loss <ref type="bibr" target="#b44">[44]</ref>, here we follow other work on cross modal retrieval and use the triplet loss. Triplet loss <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b62">62]</ref> takes an anchor, positive and negative example, with an ob-jective of making the difference between the positive and negative distances are larger than a predefined margin ?. Several improvements have been proposed to select the best triplets, such as cross batch mining <ref type="bibr" target="#b60">[60]</ref>, semi hard negatives <ref type="bibr" target="#b49">[49]</ref>, batch hard triplets <ref type="bibr" target="#b23">[23]</ref> or truncated triplet <ref type="bibr" target="#b56">[56]</ref>. However, little work have investigated the choice of the margin; in <ref type="bibr" target="#b70">[70]</ref> the authors propose multi stage training where in each stage the margin is increased. In the fashion domain, <ref type="bibr" target="#b71">[71]</ref> propose adaptive margin based on the semantic distance between product descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Cross Modal Recipe/Image Retrieval</head><p>The task consists of retrieving the recipe corresponding to a dish image and vice versa, and it is based on computing the similarity between image and recipe embeddings in a shared latent space. Many datasets have been proposed for food computing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b48">48]</ref>, Recipe1M and Recipe1M+ <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b48">48]</ref> are the largest image/recipe datasets that consist of 800 k/13 M images respectively and 1 M recipes, structured in 3 entities; title, ingredients and instructions. It contains also the class of the recipe (e.g., pizza, green beans...). Despite the large number of images in Recipe1M+, Recipe1M is still only used for comparison.</p><p>Most Cross modal Image/Recipe retrieval approaches encode the recipe and the image using separate encoders <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b59">59]</ref>. The first methods rely on recurrent networks (e.g., LSTMs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b48">48]</ref>) to encode the ingredients and instructions, which are embedded using classical approaches such word2vec <ref type="bibr" target="#b42">[42]</ref> and skip-thoughts <ref type="bibr" target="#b29">[29]</ref>. On the image side, many work use classical image encoders such as ResNet-50 <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b73">73]</ref>. Due to this challenging task, some work add discriminative or generative regularizations. Discriminative regularizations try to leverage the category or the class of the recipe in the shared latent space to obtain more semantic embeddings <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b59">59]</ref>. Generative regularizations include adversarial losses to align the distributions of the 2 modalities <ref type="bibr" target="#b58">[58,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b69">69]</ref> or use Generative Adversarial Networks (GANs) to reconstruct the images from either the recipe or image embedding <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b73">73]</ref>, Though adversarial based approaches show good results, the discriminative ones remain much simpler and more efficient to train.</p><p>Ingredients and instructions do not contribute equally in the alignement, some ingredients can be seen clearly in the image while others does not appear. This motivates some work to include attention modules in the recipe encoder <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b59">59]</ref> or the image encoder <ref type="bibr" target="#b19">[19]</ref> to weight differently several tokens/regions when fusing the 2 modalities. Seeing the success of image transformers in vision <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b54">54]</ref> and text <ref type="bibr" target="#b12">[13]</ref>, some work started recently to use transformers, showing promising results <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b47">47]</ref>.</p><p>Despite being successful for image and language tasks, multimodal transformers are still little investigated in the cooking context. Few work have tried to use cross attention modules between the 2 modalities <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34]</ref>, or a mulimodal transformer encoder <ref type="bibr" target="#b31">[31]</ref>, but their performance lag behind the SOTA work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">T-Food framework</head><p>Given a dataset composed of image and recipe pairs, we propose to align the representations of the two modalities for cross-modal retrieval. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the approach. First, each modality is encoded separately into two sequences of token embeddings. Notably, the recipe encoder fuses the information of title, ingredients and instructions into a single sequence. These two sequences are used in two different manners: (1) we extract two fixedsize embeddings that are used for retrieval at test time, and are brought closer together with a triplet loss at train time.</p><p>(2) we feed the two sequences into the MultiModal Regularization (MMR) module, which computes a fine-grained alignement score (Image-Text Matching loss) between the input image and recipe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dual Encoders (DE)</head><p>In this section, we detail the image and recipe encoders (i.e., Dual Encoders (DE)).</p><p>Image Encoder Inspired by the success of vision transformers <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b54">54]</ref> and particularly for the underlying task of cross-modal retrieval <ref type="bibr" target="#b47">[47]</ref>, we propose to use a vision transformer (i.e., ViT B/16) as an image encoder. In addition, fine-tuning large vision/language models on downstream tasks has been shown to be an effective strategy <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b50">50]</ref>, thus we propose to fine-tune the CLIP ViT B/16 <ref type="bibr" target="#b46">[46]</ref> which was trained on a large-scale dataset scraped from the internet. Specifically, the image encoder E I encodes an input image I into a sequence of image tokens t I = E I (I) along with a [CLS] token output that is linearly projected to form a fixed-size image embedding e I .</p><p>Recipe Encoder Different from other works that use recurrent networks and pretrained embeddings, we follow <ref type="bibr" target="#b47">[47]</ref> using hierarchical transformers to encode the recipes as raw text. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, each tokenized sentence in the list of ingredients (t 1 ing ), instructions (t 1 ins ) and the title t 1 ttl is processed by a transformer T . For the ingredients and instructions, another transformer HT processes the output of the first one (i.e. the sentence embeddings), in a similar way to obtain the embeddings t 2 ins = HT (T (t 1 ins )) and t 2 ing = HT (T (t 1 ing )) for instructions and ingredients respectively. For the title, the embedding is obtained from the first transformer t 2 ttl = T (t 1 ttl ). However, we argue that encoding each recipe entity separately, as done in <ref type="bibr" target="#b47">[47]</ref>, is not effective to capture the interactions between them. To this end, we propose to add another level of hierarchy to capture the interactions between all element tokens that leads to better intra-fusion. Specifically, a transformer decoder with self and cross attention HT D(Q, K, V ) takes the output tokens of each entity as a query Q and the concatenation of the tokens of the other 2 remaining entities as keys K and values V , the final tokens are obtained by concatenating all the tokens:</p><formula xml:id="formula_0">t 3 ttl = HT D(t 2 ttl , [t 2 ing ; t 2 ins ], [t 2 ing ; t 2 ins ]), t 3 ing = HT D(t 2 ing , [t 2 ttl ; t 2 ins ], [t 2 ttl ; t 2 ins ]), t 3 ins = HT D(t 2 ins , [t 2 ttl ; t 2 ing ], [t 2 ttl ; t 2 ing ]), t R = [t 3 ttl ; t 3 ing ; t 3 ins ].<label>(1)</label></formula><p>Finally the output tokens of each recipe entity are averaged, concatenated and linearly projected to obtain the final recipe embedding e R .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">MultiModal Regularization (MMR)</head><p>Inspired by other work on vision and language pretraining <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b51">51]</ref>, we propose a MultiModal Regularization ( <ref type="figure" target="#fig_2">Fig. 3</ref>) that consists of transformer decoders <ref type="bibr" target="#b55">[55]</ref> with an Image-Text Matching loss (ITM) <ref type="bibr" target="#b9">[10]</ref> to further guide the dual encoders during the alignment process. Different from other work that use multimodal encoders during training and testing, here our multimodal modules are used only during training. Compared to other representation learning The Transformer Decoders Block (TDB) consists of self attention, cross attention, feed forward and layer normalization layers with residual connections, which are repeated N times. The cross attention takes the tokens of one modality as Queries (Q) and the tokens of the other modality as Keys (K) and Values (V). The image tokens are enhanced by the ITEM module by attending to the recipe tokens. The recipe tokens are fed to MTD as Q and the enhanced image tokens as K and V. After fusing the two modalities an ITM loss is applied. Note that this module is used only during training approaches, such as self-supervised learning, <ref type="bibr" target="#b7">[8]</ref> where a simple projection layer is added on top of the feature representation to compute the contrastive loss, here, this projection layer is replaced by a more complicated multimodal module, and the contrastive loss is replaced by ITM.</p><p>The main block of MMR consists of a transformer decoder which particularly uses cross attention: the Queries (Q) come from one modality while the Keys (K) and Values (V) come from the other modality (we do not distinguish between K and V, which denoted as KV). In other words, the queries are updated by taking into account the dependencies between the 2 modalities. Specifically, each vector of Q is updated by taking a weighted sum of the vectors of KV of the other modality, where each weight is the similarity of each one of these KV vectors to the underlying Q vector. Our choice for using transformer decoders is based on their computation efficiency compared to that of a transformer encoder, which takes all the tokens as Q, V and K. In addition, we choose the recipe tokens (t R ) as queries, as the number of recipe tokens (N R ? 45) is much less than the number of image tokens (N I = 196), which reduces the complexity to (? N 2 R + N R ? N I ) in contrast to (? (N R + N I ) 2 ) for a transformer encoder.</p><p>The MMR module consists of inter-connected transformers; an Image Tokens Enhancement Module (ITEM) to enhance the image tokens, followed by a multimodal transformer decoder (MTD):</p><p>Image Tokens Enhancement Module (ITEM) We propose to enhance the image tokens before entering MTD, by adding a transformer decoder that takes the image tokens as queries (Q) and the recipe tokens as Keys and Values (KV):</p><formula xml:id="formula_1">t I = IT EM (t R , t I ).<label>(2)</label></formula><p>This module enriches the image tokens by attending to the recipe elements. While it seems natural to use only the ingredients, as they are present in different regions/tokens of the image, we found that other recipe elements, even if they are not explicitly presented in the image, brings additional improvements (see supplementary material).</p><p>Multi Modal Transformer Decoder (MTD) Unimodal encoders encode each modality independently, and lack the ability to capture the interaction between the two modalities. Here we propose a multimodal transformer decoder (MTD) <ref type="bibr" target="#b55">[55]</ref> that predicts the matching score of each pair of samples. We argue that this predicted matching score better captures the interaction between the two modalities, compared to the global cosine similarity applied between the independently extracted embeddings. Specifically, MTD contains self-attention, cross-attention and feedforward modules. For the cross attention, the tokens of one modality are considered as queries (Q) and the tokens of the other modality as Keys and Values (KV). The matching score between a pair of samples can be obtained as follows:</p><formula xml:id="formula_2">s(t R , t I ) = M T D(t R ,t I ) = M T D(t R , IT EM (t I , t R )),<label>(3)</label></formula><p>where t R andt I are the output recipe tokens and the enhanced image tokens respectively.</p><p>Image-Text Matching (ITM) loss: ITM is a binary cross-entropy loss that is optimised to classify whether or not an image-text pair matches. Following the success of batch hard mining for triplet loss, the loss is applied on the hardest negative sample; for each recipe/image anchor we sample a negative recipe/image in the batch that is most similar to the anchor using the fast cosine similarity of the embeddings at the output of the DE. The loss can be written as follows:</p><formula xml:id="formula_3">L itm = ?E t R ,t I ?D [y log(s(t R , t I ))+ (4) (1 ? y) log(1 ? s(t R , t I ))],</formula><p>where y is the label (i.e., 1 for matching pair and 0 otherwise) and D is the set of pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Retrieval loss</head><p>Following other work on cross-modal food retrieval <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b58">58]</ref>, we use the triplet loss in our learning framework. Here we introduce a new variant of triplet loss with dynamic margin.</p><p>Triplet loss: The triplet loss can be written as:</p><formula xml:id="formula_4">l(x a , x p , x n , ?) = [d(x a , x p ) + ? ? d(x a , x n )] + , (5)</formula><p>where x a , x p and x n are the anchor, positive and negative examples respectively, ? is the margin and d(?, ?) is a distance function.</p><p>IncMargin loss: Inspired by the curriculum learning approaches <ref type="bibr" target="#b2">[3]</ref>, where the task becomes harder as the training progresses, we propose to replace the constant margin by a dynamic one. The idea is that the difficulty of the task is also affected by the margin; a small margin is easier to optimize than a large margin. Here we assume that the task is difficult at the beginning, thus we make it simpler by starting with a small margin ? inc and increase it at each epoch until reaching a maximum value:</p><formula xml:id="formula_5">L m (B a , B p , B n ) = xa?Ba xp?Bp xn?Bn l(x a , x p , x n , ? inc )<label>(6)</label></formula><p>We keep the adaptive margin between an acceptable minimum and maximum values. In addition, we follow Adamine <ref type="bibr" target="#b5">[6]</ref> and use the triplet loss with an adaptive weighting strategy, where the triplet is weighted by a similar term as ? to overcome the vanishing update when most of the triplets are inactive. We call the instance loss as imagetext contrastive loss which can be written as:</p><formula xml:id="formula_6">L itc = 1 ? r L m (B r a , B v p , B v n ) + 1 ? v L m (B v a , B r p , B r n )<label>(7)</label></formula><p>Where B r and B v are the sets of recipe and image embeddings, ? r and ? v are the number of triplets that contribute to the loss (l &gt; 0 Eq. 5) when the recipe and image embeddings are anchors respectively. The positive and negative samples are from the other modality; the positive is the one associated with the anchor and other samples in the batch are considered negatives. Similarly, we incorporate the semantic triplet loss of Adamine <ref type="bibr" target="#b5">[6]</ref> as a regularization to obtain more semantically rich embeddings. The semantic loss L sem is the same as the instance loss except for the selection of positive and negative samples. Here the positive samples are those that share the same class with the anchor and the negative samples are the samples with different class.</p><p>Total loss: Thus, the total loss can be written as follows:</p><formula xml:id="formula_7">L = L itc + ? sem L sem + ? itm L itm<label>(8)</label></formula><p>Where ? sem and ? itm are the corresponding losses weights.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We use Recipe1M dataset <ref type="bibr" target="#b48">[48]</ref> with 238,999, 51,119, 51,303 pairs as training, validation and test set. We report median rank (medR) the median of the retrieved samples index for each query, and recall@K (i.e., R@1, R@5, R@10) the percentage of queries for which the correct sample index belongs to the top K retrieved samples. We report the mean over 10 (resp.5) bags of 1k (resp. 10k) pairs of the Recipe1M test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model details:</head><p>We use ViT-B/16 and CLIP-ViT-B/16 as image encoders. For the recipe encoder, similarly to <ref type="bibr" target="#b47">[47]</ref>, we use transformer encoders with 2 layers and 4 heads for hierarchical transformers T and HT . For HT D we use a transformer decoder (wihout masking) with 2 layers and 4 heads. The hidden layer dimension is kept 512 in the recipe encoder. The image and recipe embeddings are obtained with a different linear layers of output dimension 1024. The image and recipe tokens are projected using different linear layers of the same output dimension 1024 before going to the M M R module. To keep the model simpler, the IT EM module consists of a transformer decoder of only 1 layer and 4 heads with hidden size 1024. The M T D consists of a transformer decoder with 4 layers, 4 heads and hidden dimension 1024. All the transformers are trained from scratch, except the ViT, which is initialised by ImageNet weights, and CLIP-ViT with CLIP weights.</p><p>Training details: We mainly follow the implementation details of Adamine <ref type="bibr" target="#b5">[6]</ref>. Specifically, For the alignment loss we use ? sem = 0.1 for the semantic triplet loss and ? itm = 1. For each batch, half of the samples are associated with a class for the semantic loss. To compute the triplet loss, each sample is considered as anchor, the associated sample from the other modality as positive sample and all the others samples from the other modality in the batch as negatives. For the semantic triplet loss, the samples with the same class are considered positive samples and negative samples otherwise. The image encoder is kept frozen for the first 20 epochs, then all the modules are trained with a constant learning rate 1e ? 5 (except CLIP-ViT with learning rate 1e ? 6) and Adam optimiser. The models are trained until reaching 120 epochs with batch size 100. The models are trained with 2 A100 GPUs 40 GB. For IncMargin, we start by a ? inc = 0.05 and increase it by 0.005 each epoch until reaching 0.3. We use a margin ? = 0.3 for all other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with Other Work</head><p>We compare our work to existing approaches in <ref type="table" target="#tab_1">Table  2</ref>. For Adamine, we shifted the values of medR as they assume medR=0 is for exact match, while other approaches adopt medR=1, we also notice that their metrics for recipe to image and image 2 recipe should be swapped. For H-T <ref type="bibr" target="#b47">[47]</ref>, we use their official code to re compute the metrics with the ViT image encoder (H-T (ViT)) and we report these metrics for fair comparison with our approach.</p><p>We can notice that our approach significantly outperforms all other ones by a large margin for medR and R@1 with the 1k setup and for all the metrics with the 10k setup on the Recipe1M test set. Compared to the 2 SOTA H-T <ref type="bibr" target="#b47">[47]</ref> and X-MRS <ref type="bibr" target="#b20">[20]</ref>, we surpass them on the more challenging 10k setup in terms of R@k (+ 6.5 % and + 7.1 % R@1/10k resp.) and medR, and on the smaller setup 1k in terms of R@1 (+3.8 % and + 4.3 % R@1/1k resp.). We are largely better than the adversarial based approaches; +15.5 % R@1/1k compared to ACME <ref type="bibr" target="#b58">[58]</ref> and +27.7 % R@1/1k compared to <ref type="bibr" target="#b73">[73]</ref>. The same hold for the methods that use cross attention modules between the 2 modalities [34] (+8.5% R@1/1k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RE</head><p>Interestingly, our approach with CLIP-ViT outperforms other approaches by a large margin (+8.1 % R@1/1k and +9.9 % R@1/10k compared to H-T).</p><p>In <ref type="figure">Figure 5</ref>, we test our approach on different test sizes more than 10k, we can notice that the performance gap increases further between our and other approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>In this section, we investigate the importance of different design choices. Our baseline (B) consists of the dual encoders; ViT as image encoder, hiererchical transformer as recipe encoder (i.e., with T and HT), and Adamine loss.</p><p>From <ref type="table" target="#tab_2">Table 3</ref> we can notice that the proposed HTD leads to better alignment, seen by the significant improvement of metrics, this validates our hypothesis that the recipe ele-  <ref type="figure">Figure 4</ref>. Image-to-Recipe R@1 with and without multimodal reranking (the top 10 examples for each query are re-ranked using MTD) for different variants of our approach. Both metrics converge with our best models.</p><p>ments are highly entangled and should not be embedded separately. We can also notice that MTD also brings additional improvement, this is consistent with the findings of VLP in general and reveals that multimodal transformers are promising for the underlying task. ITEM brings additional improvements, which in some sense facilitates the role of MTD, by fusing further the image with the recipe tokens. Using ViT-CLIP as image encoder brings significant improvement, this indicates that using models trained on large datasets can cope with the noise in this relatively small dataset.</p><p>From table 4 and 3, our IncMargin loss, based on an increasing margin value ? inc , improves upon the classical triplet loss by more than 3 points (image to recipe R@1). In addition, we compare to AdaMargin, another variant of triplet loss with adaptive margin that changes according to the number of active triplet in the batch (see supplementary material). Despite its simplicity, IncMargin also outperforms AdaMargin. From our experiments, we noticed that the value of the margin increases to its maximum value faster with AdaMargin, while the evolution of the margin is smoother and takes longer time to each its maximum value with IncMargin. In addition, we can notice that the improvement of this variant are smaller with the ITM loss (from 67.0 to 68.2 R@1 image-to-recipe) , this should be expected as the model focuses on the ITM regularization when the triplet loss is small (i.e., the margin is small at the beginning of the training) and we noticed from our experiments that the triplet/retrieval loss is more important than the ITM for retrieval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MMR:</head><p>To perform image/recipe retrieval, we compute the pairwise cosine similarity of each pair of image/recipe in the shared embedding space, which can be done efficiently in parallel. Other work <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b41">41]</ref> propose to use multimodal modules to re-rank the top k retrieval propositions for each query. Since these multimodal modules compute a more fine-grained image/text alignment, it improves the retrieval metrics, but with a much higher computational cost. In <ref type="figure">Figure 4</ref>, we compare those two retrieval methods for our model variants that use a MMR module. We can see that our model MTD w/o Adamine has a low R@1 score without reranking (54.0) but much higher with multimodal re-ranking (63.6), emphasizing the role of multimodal alignment in that case. Adamine helps to partially close this gap by giving more importance to the shared embedding space, which improves greatly the retrieval metrics without re-ranking, with small improvements of the metrics with re-ranking. When we add the HTD module (for recipe encoding), we observe that the role of re-ranking becomes useless. This remains true for our best models T-Food and T-Food (CLIP). Therefore, we conclude that the improvement brought about by re-ranking was due to the lack of expressiveness of the recipe encoder and not the need for a finer image/text alignment. We also conclude that the presumably better alignment capabilities of the MMR module are fully transferred to the shared embedding space at the end of training, given that re-ranking with this module is not better than global embedding similarity. We also investigated how the complexity of MMR affects the performance in <ref type="table" target="#tab_4">Table 5</ref> H-T (ViT) <ref type="bibr" target="#b47">[47]</ref> SEJE <ref type="bibr" target="#b66">[66]</ref> HF-ICMA <ref type="bibr" target="#b33">[33]</ref> IMHF <ref type="bibr" target="#b32">[32]</ref> Figure 5. Scalability analysis: we report the medR (?) of imageto-recipe retrieval on Recipe1M test set with setup more than 10k. we train our model with different number of layers in MTD. We can notice that when MTD becomes more powerful, the metrics become better (nb. of layers = 1 vs 4). Note that, these results are consistent with other contrastive learning work, specifically, in SimCLR <ref type="bibr" target="#b7">[8]</ref>, the authors show better results when the projection head is more powerful (e.g., nonlinear instead of linear), and here we make the analogy between their projection head and our MRR module. However, a too heavy MTD degrades the results (nb. of layers = 4 vs 6), we argue that when the regularization module becomes too powerful, most of the modality fusion and cross modal interaction will be focused in it, instead of the DE. We can notice that each modification brings additional improvement as well as the combination of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a new framework for multimodal alignment in the cooking context and validated the approach by surpassing all existing work by a large margin on the cross modal retrieval task. We introduced a novel MultiModal Regularization and new variant of triplet loss, in addition we showed that transformer encoders/decoders can bring significant improvement by exploiting the intra and inter dependencies between modalities and recipe elements. Finally, we showed that transfer learning from VLP models trained on large datasets can significantly help in this context. While the work is successful in the food domain, we believe that the proposed ideas can be beneficial for cross modal tasks in general, especially cross modal retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. AdaMargin Triplet Loss</head><p>Here we detail another variant that we propose for adaptive triplet with dynamic margin. The number of active triplets (the triplets corresponding to non-zero triplet loss l) in the batch reveals the difficulty of the task; if it is small, that means most of the triplets already satisfy the triplet condition (i.e., the difference between the distance of positive pairs is smaller than the distance of the negative pairs by a margin). Thus, we propose to inversely weigh the margin by the number of active triplets ? in each batch. Thus the loss can be written as: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Ablation Study</head><p>In this section, we investigate the importance of different design choices. Our baseline (B) consists of the dual encoders, ViT for image encoder, hiererchical transformer (HT) for recipe encoder (i.e., with T and HT), and trained with Adamine loss. <ref type="table">Table 6</ref>, we investigate the importance of the title for the instructions and ingredients, thus we compare our approach with another variant that takes only the ingredients (resp. instructions) as K and V with the instructions (resp. ingredients) as queries. We can notice that the title does not bring additional improvement to the instructions and ingredients.  <ref type="table">Table 6</ref>. Ablation study for HTD. medR (?), Recall@k (?) are reported on the Recipe1M test set with 1k setup. v2: in HTD, the cross attention of ingredients takes only the instructions as K and V and the one for instructions takes only the ingredients as K and V, while the title takes both of them as K and V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HTD: In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ITEM:</head><p>We did an ablation study for ITEM in <ref type="table">Table 7</ref>.</p><p>In particular, we test the module with all recipe elements as K and V (ITEM (a)), with the title alone (ITEM (t)) and with ingredients alone (ITEM (n)). The best results are obtained with all recipe elements, which also validate that all recipe elements are important for multimodal fusion.  <ref type="table">Table 7</ref>. Ablation study. medR (?), Recall@k (?) are reported on the Recipe1M test set with 1k setup. B* is with ViT, HT, HTD and MTD. We compare several ways of enhancing the image tokens, the cross attention is with; all recipe elements (ITEM (a)), only the title (ITEM(t) and only the ingredients (ITEM (n)). The cross attention between the image and all recipe elements gives the best results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of our approach; the recipes and images are separately encoded by a Recipe and Image Encoder respectively. The output representation (tokens) are then projected into 2 spaces; the shared latent space where the instance and semantic losses are applied, and the multimodal space where the ITM loss is applied on top of the MMR module. The elements in dashed line are used only during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>'Figure 2 .</head><label>2</label><figDesc>Banana Oatmeal Cookies' '1/2 cups All-purpose Flour' 'In a large bowl, with a mixer...''Preheat oven to 350 degrees F...' Illustration of the Recipe Encoder: the list of ingredients, instructions and the title are fed into transformer encoders T , then the list of output tokens are fed into other transformer encoders HT (except for the title). The tokens of each entity are then processed as Query by transformer decoders HT D that takes the other 2 entities as Keys and Values. The output tokens tR are concatenated and linearly projected, and the recipe embedding eR is obtained by averaging then concatenating and linearly projecting the output tokens of each recipe entity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of our MultiModal Regularization (MMR);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>L</head><label></label><figDesc>m (B a , B p , B n ) = xa?Ba xp?Bp xn?Bn l(x a , x p , x n , ?/?) (9) Where B a , B p and B n are the set of anchors, positive and negative examples in the batch. We keep the ?/? between 0.05 and 0.3, and ? is computed based on ? = 0.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>Comparison with other work. medR (?), Recall@k (?) are reported on the Recipe1M test set. Our approaches significantly outperform all existing work. Best metrics are in bold, and next best metrics are underlined.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study. medR (?), Recall@k (?) are reported on the Recipe1M test set with 1k setup. Each modification brings additional improvement as well as the combination of all of them.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>MMR</cell><cell>L m</cell><cell>CLIP</cell><cell></cell><cell>image-to-recipe</cell><cell></cell><cell></cell><cell>recipe-to-image</cell><cell></cell></row><row><cell cols="12">B HTD MTD ITEM IncMargin ViT-CLIP medR R@1 R@5 R@10 medR R@1 R@5 R@10</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>59.5 83.1</cell><cell>88.1</cell><cell>1</cell><cell>61.1 83.5</cell><cell>88.2</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>65.6 86.7</cell><cell>90.8</cell><cell>1</cell><cell>66.8 86.9</cell><cell>90.8</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>66.9 87.0</cell><cell>91.0</cell><cell>1</cell><cell>67.5 87.2</cell><cell>91.1</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>1</cell><cell>67.0 87.5</cell><cell>90.9</cell><cell>1</cell><cell>68.1 87.4</cell><cell>91.0</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>1</cell><cell>68.2 87.9</cell><cell>91.3</cell><cell>1</cell><cell>68.3 87.8</cell><cell>91.5</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>1</cell><cell>72.3 90.7</cell><cell>93.4</cell><cell>1</cell><cell>72.6 90.6</cell><cell>93.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study for AdaMargin and IncMargin on the Recipe1M test set with 1k setup. We compare our baseline model B (Sec. 4.2) to three variants trained without Adamine. Our new triplet loss IncMargin outperforms significantly AdaMargin and the classical triplet loss with fixed margin. Both AdaMargin and IncMargin outperforms B.</figDesc><table><row><cell></cell><cell>image-to-recipe</cell><cell>recipe-to-image</cell></row><row><cell cols="3">Margin scheme R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell>Baseline B</cell><cell cols="2">59.5 83.1 88.1 61.1 83.5 88.2</cell></row><row><cell>Fixed ?</cell><cell cols="2">59.5 83.4 88.1 60.7 83.5 88.3</cell></row><row><cell cols="3">AdaMargin ?/? 60.8 83.4 88.2 61.5 83.7 88.4</cell></row><row><cell cols="3">IncMargin ?inc 62.9 84.8 89.4 63.8 85.1 89.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study for MMR on the test set with 1k setup. We report the performance w.r.t the number of layers in MTD. All the models are trained with B + HTD + MMR and ?itm = 0.1 .</figDesc><table><row><cell></cell><cell>image-to-recipe</cell><cell>recipe-to-image</cell></row><row><cell cols="3">nb. of layers R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell>w/o MMR</cell><cell cols="2">65.6 86.7 90.8 66.8 86.9 90.8</cell></row><row><cell>1</cell><cell cols="2">66.8 87.0 90.4 67.0 86.9 90.4</cell></row><row><cell>4</cell><cell cols="2">67.5 87.5 91.0 67.8 87.6 91.3</cell></row><row><cell>6</cell><cell cols="2">65.7 86.6 90.3 66.1 86.5 90.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work was partly supported by ANR grant VISA DEEP (ANR-20-CHIA-0022), and HPC resources of IDRIS under the allocation 2021-[AD011013159] made by GENCI.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Association for Computing Machinery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
	<note>Curriculum learning</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Food-101 -mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>ing. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<editor>David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publish</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9912" to="9924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval in the cooking context: Learning semantic text-image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micael</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Cad?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pfid: Pittsburgh fast-food image dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="289" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
	<note>PMLR, 2020. 2, 4, 8</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Virtex: Learning visual representations from textual annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11162" to="11173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dividing and conquering cross-modal recipe retrieval: from nearest neighbours baselines to sota</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Fain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niall</forename><surname>Twomey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ponikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12763</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A benchmark dataset to study the representation of food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Allegra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Stanco</surname></persName>
		</author>
		<editor>Lourdes Agapito, Michael M. Bronstein, and Carsten Rother</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Workshops</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="584" to="599" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Intelligent food planning: Personalized recipe recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Freyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Berkovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Intelligent User Interfaces, IUI &apos;10</title>
		<meeting>the 15th International Conference on Intelligent User Interfaces, IUI &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="321" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mcen: Bridging cross-modal gap between cooking recipes and dish images with latent variable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianling</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="14570" to="14580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval and synthesis (x-mrs): Closing the modality gap in shared subspace learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cookpad image dataset: An image collection as infrastructure for food research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Harashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichiro</forename><surname>Someya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohei</forename><surname>Kikuta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1229" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Foodcam: A realtime mobile food recognition system employing fisher vector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyuki</forename><surname>Kawano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="369" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vilt: Visionand-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprasaath</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven Chu Hong</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross-modal image-recipe retrieval via intra-and intermodality hybrid fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="AssociationforCom-putingMachinery.3" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 International Conference on Multimedia Retrieval, ICMR &apos;21</title>
		<meeting>the 2021 International Conference on Multimedia Retrieval, ICMR &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-modal image-recipe retrieval via intra-and intermodality hybrid fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 International Conference on Multimedia Retrieval, ICMR &apos;21</title>
		<meeting>the 2021 International Conference on Multimedia Retrieval, ICMR &apos;21<address><addrLine>New York, NY, USA, 2021</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
		<title level="m">Hybrid Fusion with Intra-and Cross-Modality Attention for Image-Recipe Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="244" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multi-Subspace Implicit Alignment for Cross-Modal Retrieval on Cooking Recipes and Food Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianquan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recipe1m+: A dataset for learning cross-modal embeddings for cooking recipes and food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritro</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="187" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A structured committee for food recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Piciarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gian</forename><forename type="middle">Luca</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="484" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Thinking fast and slow: Efficient text-to-visual retrieval with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="9826" to="9836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Is saki# delicious? the food perception gap on instagram and its relation to health</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raggi</forename><forename type="middle">Al</forename><surname>Hammouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Revamping cross-modal recipe retrieval with hierarchical transformers and self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhan</forename><surname>Gundogdu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Donoser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="15475" to="15484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning cross-modal embeddings for cooking recipes and food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">How much can clip benefit vision-and-language tasks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian</forename><forename type="middle">Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06383,2021.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Couairon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Galuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04482</idno>
		<title level="m">Flava: A foundational language and vision alignment model</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Cross-Modal Recipe Embeddings by Disentangling Recipe Contents and Dish Styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="2501" to="2509" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning crossmodality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR, 2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Solving inefficiency of self-supervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9505" to="9515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Learning structural representations for recipe generation and food retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01209</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning cross-modal embeddings with adversarial networks for cooking recipes and food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyen</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Crossmodal food retrieval: learning a joint embedding of food images and recipes with semantic consistency and attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyen</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palakorn</forename><surname>Achananuparp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ch Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Cross-batch memory for embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6388" to="6397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10904</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Y. Weiss, B. Sch?lkopf, and J. Platt</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farley</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06706</idno>
		<title level="m">Visual entailment: A novel task for fine-grained image understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning joint embedding with modality alignments for cross-modal retrieval of recipes and food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning tfidf enhanced joint embedding for recipeimage cross-modal retrieval service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Services Computing</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning text-image joint embedding for efficient crossmodal retrieval with deep feature engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Distance metric learning, with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Neural Information Processing Systems, NIPS&apos;02</title>
		<meeting>the 15th International Conference on Neural Information Processing Systems, NIPS&apos;02<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">A new foundation model for computer vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Sentence-Based and Noise-Robust Cross-Modal Retrieval on Cooking Recipes and Food Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning incremental triplet margin for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9243" to="9250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A weakly supervised adaptive triplet loss for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Unified vision-language pretraining for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence, number 07</title>
		<meeting>the AAAI Conference on Artificial Intelligence, number 07</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13041" to="13049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">R2gan: Cross-modal recipe retrieval with generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
