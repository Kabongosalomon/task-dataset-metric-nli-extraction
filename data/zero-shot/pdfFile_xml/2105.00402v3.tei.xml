<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AG-CUResNeSt: A Novel Method for Colon Polyp Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-01">1 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><surname>Dinh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hanoi University of Science and Technology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename><forename type="middle">Quang</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hanoi University of Science and Technology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><surname>Phan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hanoi University of Science and Technology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><surname>Dao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hanoi Medical University</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">The Institute of Gastroenterology and Hepatology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dao</forename><surname>Van Long</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hanoi Medical University</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">The Institute of Gastroenterology and Hepatology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><forename type="middle">Thi</forename><surname>Thuy</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">National University of Agriculture</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country>Vietnam, Vietnam</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AG-CUResNeSt: A Novel Method for Colon Polyp Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-01">1 Mar 2022</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Artificial Intelligence in Medicine March 2, 2022</note>
					<note>(Phan Ngoc Lan), daoviethang@hmu.edu.vn (Dao Viet Hang), bsdaolong@yahoo.com (Dao Van Long), ntthuy@vnua.edu.vn (Nguyen Thi Thuy) 1 These authors contributed equally to this work Colonoscopy</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Attention Mechanism</term>
					<term>Polyp Segmentation</term>
					<term>* Corresponding author</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Colorectal cancer is among the most common malignancies and can develop from high-risk colon polyps. Colonoscopy is an effective procedure to detect and remove polyps, especially in the case of precancerous lesions. However, the missing rate in clinical practice is relatively high due to many factors. The procedure could benefit greatly from using AI models for automatic polyp segmentation, which provides valuable insights for improving colon polyp detection. However, precise segmentation is still challenging due to appearance variations of polyps. This paper proposes a novel neural network architecture called AG-CUResNeSt, which enhances Coupled UNets using the robust ResNeSt backbone and attention mechanisms. The network is capable of effectively combining multi-level features and leveraging semantic information flow to yield high accurate polyp segmentation. Experimental results on five popular benchmark datasets show that our proposed method achieves state-of-the-art accuracy compared to existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Colonoscopy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Colorectal cancer (CRC) is a leading cause of cancer deaths worldwide, with roughly 694,000 fatalities each year <ref type="bibr" target="#b0">[1]</ref>. Most CRC arises from colon polyps, especially the adenomas with high-grade dysplasia <ref type="bibr" target="#b2">[2]</ref>. According to a longitudinal study <ref type="bibr" target="#b3">[3]</ref>, each 1% of adenoma detection rate increase is associated with a 3% decrease in the risk of colon cancer. Therefore, the detection and removal of polyps at the early stage are of great importance to prevent CRC. Nowadays, colonoscopy is considered the gold standard for colon screening and is recommended in many guidelines of different societies <ref type="bibr" target="#b4">[4]</ref>. Nevertheless, the overloaded healthcare systems in many countries, especially in limited-resource settings, might result in shorter endoscopy duration to guarantee the required number of procedures per day. This factor, combined with low-quality endoscopy systems and experience gaps among endoscopists at different levels of the healthcare system, seriously affects the quality of colonoscopy procedures and increases the risk of missing lesions and inaccurate diagnosis <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6]</ref>. A literature review has shown that the colon polyp missing rate in endoscopies could range from 20-47% <ref type="bibr" target="#b7">[7]</ref>. Hence, studies to develop computer-aided systems to support endoscopists in providing accurate polyp regions are much needed in both aspects of training endoscopists and application in clinical practice.</p><p>In recent years, with the advancement of artificial intelligence (AI), particularly deep learning (DL), we can address the limitations of conventional endoscopy systems. Attempts have been made to build computer-aided diagnosis (CAD) systems for the automatic detection and prediction of polyps, which may help clinicians identify lesions and reduce the miss detection rate <ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref><ref type="bibr" target="#b10">[10]</ref>.</p><p>In some retrospective studies, AI has demonstrated promising results in supporting colon polyp detection <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>. The CAD system is expected to support endoscopists in lesion detection, diagnosis, and quality assurance. It can be applied to solve practical problems, including improving the lesion detection rate, supporting doctors in optimizing strategy during endoscopy for high-risk lesions, and serving the increasing number of patients while maintaining diagnostic quality <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref>.</p><p>Despite the growth of many advanced machine learning and computer vision techniques in recent years, automatic polyp segmentation is still a challenging problem. The first challenge is that polyps are often diverse in appearances, such as shape, size, texture, and color. Secondly, as the nature of medical images, the boundary between polyps and their surrounding mucosa, especially in case of flat lesions or unclean bowel preparation, is not always clear during colonoscopy, leading to confusion for segmentation methods.</p><p>There are different approaches to polyp segmentation. Traditional machine learning methods are based on hand-crafted features for image representation <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b17">16]</ref>. These methods rely on color, texture, shape, or edge information as extracted features and train classifiers to distinguish polyps from surrounding normal mucosa. However, hand-crafted features are limited in representing polyps due to their high intra-class diversity and low inter-class variation between them and hard negative mucosa regions. Recently, deep neural networks have proven to be more effective in solving medical image segmentation problems, particularly those related to endoscopic images of the human gastrointestinal (GI) tract. Among various deep models, encoder-decoder based networks like UNet family <ref type="bibr" target="#b18">[17]</ref> have demonstrated impressive performance. In UNets, high-level semantic features in the decoder are gradually up-sampled and fused with corresponding low-level detailed information in the encoder through skip connections. Inspired by the success of UNets, UNet++ <ref type="bibr" target="#b19">[18]</ref> and ResUNet++ <ref type="bibr" target="#b20">[19]</ref> were proposed for polyp segmentation and yielded promising results. However, these methods heavily depend on the dense concatenation of feature maps at multiple levels, resulting in high computational resource requirements and time-consuming procedures.</p><p>Recently, the attention mechanism has been widely used in various deep learning models, allowing them to focus on learning valuable information from the input. In <ref type="bibr" target="#b21">[20]</ref>, Oktay et al. introduce attention gates to UNets in order to suppress irrelevant low-level information from encoders before concatenating them with high-level feature maps in decoders. Fan et al. <ref type="bibr" target="#b22">[21]</ref> enhanced an FCN-like model by a parallel partial decoder and reverse attention module and obtained impressive results. On the other hand, previous works show that stacking multiple UNets allows the networks to learn a better feature representation and considerably improves the accuracy. DoubleUNet <ref type="bibr" target="#b23">[22]</ref> stacks two UNets on top of each other, and it was applied for polyp segmentation. However, DoubleUNet lacks skip connections between the two UNets, which limits the information flow within the network. Another weakness of DoubleUNet is the use of an old VGG-19 backbone, which can be replaced with more efficient models proposed recently, e.g., the ResNet family <ref type="bibr" target="#b24">[23]</ref><ref type="bibr" target="#b25">[24]</ref><ref type="bibr" target="#b26">[25]</ref>. In <ref type="bibr" target="#b27">[26]</ref>, Tang et al.</p><p>introduce the coupled UNets (CUNet) architecture, where coupling connections are utilized to improve the information flow across UNets. In <ref type="bibr" target="#b28">[27]</ref>, Na et al.</p><p>introduce coupled attention residual UNets and use it as a generator for adversarial learning based Facial UV map completion. The model in <ref type="bibr" target="#b28">[27]</ref> is based on the ResNet backbone and utilizes the fast normalized fusion <ref type="bibr" target="#b29">[28]</ref> to combine the information between the two UNets, which can result in potential information loss.</p><p>In this paper, we propose a new deep network, named AG-CUResNeSt, that addresses the above limitations for efficient polyp segmentation. Our main contributions are:</p><p>? A novel network architecture based on coupled UNets with improved mechanisms for integrations of skip connections and attentions. In which, the encoders are strengthened by residual connections and split-attention blocks. Attention gates are integrated into skip connections within each UNet to suppress the redundant low-level information from the encoders.</p><p>Skip connections across the two UNets are leveraged to reduce gradient vanishing and promote feature reuse.</p><p>? An extensive set of experiments on popular benchmark datasets shows that our method yields superior accuracy than other state-of-the-art ap-proaches. Especially, our approach achieves significantly better crossdataset generalization than others when all models are trained on one dataset and tested on another dataset.</p><p>The rest of the paper is structured as follows. Section 2 reviews the literature regarding CNN backbones and semantic segmentation in medical image analysis.</p><p>In Section 3, we describe the proposed network architecture in detail. Section 4 outlines our experiment settings. The results are presented and discussed in Section 5. Finally, we conclude the paper and outline future works in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There have been many methods proposed for semantic image segmentation in general and for the purpose of segmenting the polyps in colonoscopy images in particular. This section briefly reviews prior methods related to our work, focusing on deep neural network models for colorectal polyps segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">CNN Architectures</head><p>Since AlexNet <ref type="bibr" target="#b30">[29]</ref>, Convolutional Neural Networks (CNNs) have dominated in solving computer vision tasks. VGG <ref type="bibr" target="#b31">[30]</ref> proposes a simple yet effective modularized network design exploiting the efficiency of small 3 ? 3 kernels.</p><p>However, plain networks like VGG suffer from degradation when their depth increases. ResNet <ref type="bibr" target="#b25">[24]</ref> introduces identity skip connections to smooth out the objective function's landscape. Skip connections also reduce gradient vanishing and allow very deep networks to learn better feature representations. GoogleNet <ref type="bibr" target="#b32">[31]</ref> demonstrates the success of multi-branch networks, where each branch is carefully designed using different convolutional kernel sizes. ResNeXt <ref type="bibr" target="#b24">[23]</ref> improves ResNet with a unified multi-branch design, where all branches have the same architecture. SE-Net <ref type="bibr" target="#b33">[32]</ref> introduces a channel attention mechanism that adaptively recalibrates channel-wise feature responses. SK-Net <ref type="bibr" target="#b34">[33]</ref> proposes an adaptive selection mechanism to fuse two network branches to adaptively adjust receptive field sizes of neurons according to the input. Recently, ResNeSt <ref type="bibr" target="#b26">[25]</ref> integrates the channel-wise attention on different network branches to exploit their success in capturing cross-feature interactions and learning diverse representations. Besides, with the growth of computing capability, some efficient CNNs such as EfficientNet <ref type="bibr" target="#b35">[34]</ref> are automatically designed by machines thanks to neural architecture search techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semantic Segmentation for Medical Image Analysis</head><p>Semantic image segmentation has been an area of very active research in recent years. Many network architectures and learning techniques have been proposed to improve segmentation accuracy, latency, and throughput. In <ref type="bibr" target="#b36">[35]</ref>, the authors utilize several well-known classification networks (AlexNet <ref type="bibr" target="#b30">[29]</ref>, VGG <ref type="bibr" target="#b31">[30]</ref> and GoogLeNet <ref type="bibr" target="#b32">[31]</ref>) in segmentation networks, coupled with transfer learning techniques. UNet <ref type="bibr" target="#b18">[17]</ref> is among the most famous network architectures for segmentation. The network consists of an encoder and a decoder, with skip connections between corresponding levels. More recently, DeepLabV3 <ref type="bibr" target="#b37">[36]</ref> introduces atrous convolutions to extract denser features for better performance in segmentation tasks.</p><p>Semantic segmentation specifically for medical images has also attracted much attention. UNet is among the first successful applications of neural architecture in medical images and brings several variants in the following years. In <ref type="bibr" target="#b19">[18]</ref>, Zhou et al. introduce UNet++, an ensemble of nested UNets of varying depths, which partially share an encoder and jointly learn using deep supervision. Later, Jha et al. <ref type="bibr" target="#b20">[19]</ref> propose ResUNet++ that takes the advantages of residual blocks, squeeze and excitation units, atrous spatial pyramidal pooling (ASPP), and the attention mechanism.</p><p>DoubleUNet <ref type="bibr" target="#b23">[22]</ref> stacks two UNet blocks with a pre-trained VGG backbone, exploits squeeze and excitation units, and ASPP modules. The performance of this network surpasses previous methods on several different datasets. However, DoubleUNet suffers from using the old VGG backbone and the lack of skip connections across the two UNet blocks, limiting the information flow. CUNet <ref type="bibr" target="#b27">[26]</ref> improves the information flow by adding skip connections across UNet blocks.</p><p>Attention UNet <ref type="bibr" target="#b21">[20]</ref> introduces attention gates, which suppress unimportant regions while highlighting salient features useful for a specific task.</p><p>Non-UNet architectures are also utilized for medical segmentation. Fan et al. <ref type="bibr" target="#b22">[21]</ref> propose PraNet, enhancing an FCN-like model using a parallel partial decoder and reverse attention modules. PraNet achieves state-of-the-art performance on five challenging benchmark medical datasets. HarDNet-MSEG <ref type="bibr" target="#b38">[37]</ref> presented a slightly similar architecture as PraNet and optimized for inference speed. Jha et al. <ref type="bibr" target="#b39">[38]</ref> recently proposed ColonSegNet, a very lightweight architecture that achieved highly competitive performance on the Kvasir-SEG dataset.</p><p>In this paper, we analyze and integrate the three aforementioned architectures <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b27">26]</ref> to propose a novel network that outperforms previous methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>Stacking multiple UNet-like blocks has proven to be efficient in many previous works <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b40">39]</ref>. Inspired by this idea, we design a network that consists of two coupled UNets with similar architectures. The overall architecture of our proposed network is depicted in of attention gates is to suppress the irrelevant information from the encoder before it is concatenated with the decoder. This helps the network focus better on the important regions to yield a good prediction. Next, we use a 1x1 conv layer followed by a sigmoid layer at the end of each UNet to yield its output. The last feature map of the first UNet is combined with the raw input image and then fed to the second UNet for further refinement. Inspired by <ref type="bibr" target="#b27">[26]</ref>, we also use skip connections across the two UNets to enhance the information flow and promote feature reuse in the network. The skip connections between the two encoders help the second UNet maintain the detailed information in the input image, while those between the two decoders amplify the semantic representation of the second decoder. The skip connections are expected to improve the gradient flow during in backpropagation phase during training. They also smoothen the landscape of the loss function <ref type="bibr" target="#b41">[40]</ref> and thus facilitate the optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Backbone: ResNet Family</head><p>Plain networks tend to decrease performance on both training and test datasets as their depth increases. This is a widely observed phenomenon called the degradation problem. ResNet <ref type="bibr" target="#b25">[24]</ref> addresses this problem by introducing skip connections. Suppose that H(x) as an underlying mapping to be fitted by a block of a few nonlinear layers, where x is the input to the first layer.</p><p>Instead of directly approximating H(x), we can let the block approximate the  ResNeXt <ref type="bibr" target="#b24">[23]</ref> introduces a homogeneous multi-branch structure that breaks channel information into K repeated smaller bottleneck branches called cardinal groups.</p><p>One of the latest members in the ResNet family is ResNeSt <ref type="bibr" target="#b26">[25]</ref>, which improves the feature representation to boost the performance across multiple computer vision tasks. ResNeSt proposes to split each cardinal group into R smaller feature groups, where R is called a new radix hyperparameter. Hence, the total number of feature groups is G = K ? R. Each group is associated with a transformation F i , i = 1, 2, . . . , G and outputs an intermediate result</p><formula xml:id="formula_0">U i = F i (X),</formula><p>where U i ? R H?W ?C/K , and H, W, C are the sizes of a cardinal group's output. The output of k-th cardinal group is an element-wise summation of all R splits:? k = R r=1 U R(k?1)+r , k = 1, 2, . . . , K. Inspired by the ideas of SE-Net <ref type="bibr" target="#b33">[32]</ref> and SK-Net <ref type="bibr" target="#b34">[33]</ref>, ResNeSt introduces the channel-wise attention for multi network splits <ref type="figure" target="#fig_3">(Fig. 2)</ref>. Firstly, the global context information across spatial dimensions s k ? R C/K is obtained by applying global average pooling to? k . Then a network G of two consecutive fully connected (FC) layers is added to predict the attention weights over splits in each channel a c k = {a c k1 , a c k2 , ..., a c kR } ? R R as follows:</p><formula xml:id="formula_1">a c kr = ? ? ? ? ? exp(G c r (s k )) R j=1 exp(G c j (s k )) , if R &gt; 1, 1 1+exp(?G c r (s k )) , otherwise.<label>(1)</label></formula><p>The attention weights corresponding to the r-th split can be denoted as</p><formula xml:id="formula_2">a kr = {a 1 kr , a 2 kr , . . . , a C/K kr }.The output of k-th cardinal group V k ? R H?W ?C/K is calculated by a weighted fusion over splits: V c k = R r=1 a c kr U c R(k?1)+r , c = 1, 2, . . . , C/K.<label>(2)</label></formula><p>Next, the representation of all cardinal groups are concatenated along the</p><formula xml:id="formula_3">channel dimension: V = Concat(V 1 , V 2 , . . . , V K ).</formula><p>Finally, a standard skip connection is applied Y = X + T (X), where T (X) is an appropriate transform to align the output shapes if needed.</p><p>The experiments in <ref type="bibr" target="#b26">[25]</ref> show that ResNeSt even outperforms the machine designed architecture EfficientNet <ref type="bibr" target="#b35">[34]</ref> in accuracy and latency trade-off. In this study, we conduct an ablation study on different backbones including ResNet-50, ResNet-101, ResNeSt-50 and ResNeSt-101. Besides, we also compare the ResNet family with Non-ResNet architectures such as VGG and EfficientNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Attention Gate</head><p>Attention gates (AG) <ref type="bibr" target="#b21">[20]</ref> can implicitly learn to suppress the irrelevant information in an input image while strengthening salient features necessary for then down-sampled to align with the shape of g. Next, the two resulting feature maps are combined by passing them to an element-wise summation operation followed by a ReLU function. A 1 ? 1 conv layer with only one kernel is further applied to aggregate the information across all channels. After that, a sigmoid function is used to normalize the information and produce a coarse attention map, which is then resampled to match the shape of x. Finally, the fine-grained attention map ? ? R Hx?Wx is used to scale the feature map x.</p><p>In the first UNet of our proposed network ( <ref type="figure" target="#fig_0">Fig. 1</ref>), the attention gate takes a coarse feature map D U 1 from decoder and a low-level feature map E U 1 from the encoder as input and produces a filtered map? U 1 . The feature map D U 1 is then upsampled and concatenated with? U 1 before fitting them to two successive 3 ? 3 conv layers followed by ReLU and Batch Norm. A similar mechanism is applied in the second UNet. However, in our design,? U 2 is concatenated with not only D U 2 but also the coarse feature map D U 1 passed through the skip connections across the two UNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>It is known that the problem of medical image segmentation poses an issue of imbalanced data, i.e., lesions or polyps are often small regions in an image.</p><p>Therefore, we propose to employ Tversky loss <ref type="bibr" target="#b42">[41]</ref> to address the issue of data imbalance and achieve a much better tradeoff between precision and recall during training the networks. Assume that P and G are the predicted map taken after the softmax layer and the binary ground-truth, respectively, the Tversky loss is defined as follows:</p><formula xml:id="formula_4">T (?, ?, P, G) = N i=1 P i0 G i0 N i=1 P i0 G i0 + ? N i=1 P i0 G i1 + ? N i=1 P i1 G i0 ,<label>(3)</label></formula><p>where N is the number of pixels in the ground-truth G; P i0 is the probability that pixel i belongs to a polyp, P i1 = 1 ? P i0 is the probability that pixel i belongs to a non-polyp region; G i0 = 1 for a polyp pixel, G i0 = 0 for a nonpolyp pixel and vice verse for G i1 ; ? and ? control the magnitude of penalties for false positives and false negatives, respectively.</p><p>An auxiliary Tversky loss is also applied to the first UNet to boost the gradient flow during training. Thus, the final loss function is:</p><formula xml:id="formula_5">L = T (?, ?, P U 2 , G) + T aux (?, ?, P U 1 , G),<label>(4)</label></formula><p>where P U 1 and P U 2 are the output of the first and the second UNet, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Several benchmark datasets are available for evaluating polyp segmentation models. The CVC-ClinincDB <ref type="bibr" target="#b43">[42]</ref> and ETIS-Larib <ref type="bibr" target="#b17">[16]</ref> datasets are provided in the 2015 MICCAI automatic polyp detection sub-challenge. These datasets consist of frames extracted from colonoscopy videos, annotated by expert video endoscopists. CVC-ClinicDB has 612 images (384x288) extracted from 29 different video studies. ETIS-Larib has a total of 196 high-resolution images (1225x966).</p><p>The CVC-ColonDB <ref type="bibr" target="#b44">[43]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment Settings</head><p>We implement the proposed models using the PyTorch framework. A single training run takes approximately 12 hours using an NVIDIA GTX 2080 GPU.</p><p>Weights pre-trained on ImageNet for ResNet and ResNeSt are used to initialize the respective backbones. The training process consists of two phases. The first phase trains the first UNet to convergence, and the second phase trains the entire coupled network model. Both phases use stochastic gradient descent (SGD) with a learning rate of 5.10 ?3 , and a momentum of 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Data Augmentation</head><p>The aforementioned datasets are generally small compared to other computer vision datasets, as annotations require expert endoscopists. Thus, image augmentation is quite often used to help diversify training data. Our experiments follow the Augmentation-II strategy proposed in <ref type="bibr" target="#b46">[45]</ref>. Particularly, we apply the following transformations to every training image:</p><p>? Rotating the images by 90, 180, and 270 degrees, respectively;</p><p>? Flipping the images both horizontally and vertically;</p><p>? Resizing the images with four scale factors of 0.9, 1.1, 1.15, and 1.2, respectively;</p><p>? Blurring the images with a kernel size of 5 ? 5;</p><p>? Brightening the images by using RandomBrightness in <ref type="bibr" target="#b47">[46]</ref> with alpha = 1.5;</p><p>? Darkening the images by using RandomContrast in <ref type="bibr" target="#b47">[46]</ref> with alpha = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation Metrics</head><p>We use the performance metrics listed in the MICCAI 2015 challenge <ref type="bibr">[47]</ref> to evaluate model performance: precision, recall, IoU (Jaccard score), and Dice </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study</head><p>In this section, we measure the impact of each component in the proposed model. For the ablation study, we choose Scenario 1, i.e., CVC-Colon and ETIS-Larib are used for training, and CVC-Clinic is used for testing, due to two following reasons. Firstly, there are a number of options for the model's architecture. Hence, we should choose a combination with a small training dataset to speed up the evaluation process. The CVC-ColonDB dataset, including 380 images, seems insufficient to fit large backbones. The ETIS-Larib may be a good addition to the training dataset. Secondly, the training and test datasets are taken separately from different sources with different image properties and characteristics. This cross-dataset experiment setup is useful to evaluate the generalization capability of the model. <ref type="table" target="#tab_1">Table 1</ref> shows the overall results of ablation study. It can be seen that AG-CUResNeSt-101 architecture obtained the best results over state-of-the-art models in terms of mDice and mIoU scores, the second-best in terms of precision, and is comparable to other models in terms of recall. when AG is added, while ResNeSt101-UNet increases from 0.816 to 0.829. We note that while the overall Dice score increases, adding AG causes a drop in precision score for both models. This is likely due to increased focus on potential polyp regions that had previously been ignored without AG. As attention gates bring more focus to these regions, the network can cover more polyps but is also more likely to make false predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">The Effectiveness of coupled connections</head><p>The Attention CUNet architecture adds one additional UNet, as well as skip connections across the two UNets. We denote the variant with ResNet backbone as Attention ResCUNet, and that with the ResNeSt backbone as AG-CUResNeSt. For each backbone, the mDice score increases by roughly 0.5%.</p><p>AG-CUResNeSt achieves a mDice of 0.833 and a mIOU of 0.754, the best scores among models in <ref type="table" target="#tab_1">Table 1</ref>. Both network size and the enrichment of semantic features play a factor in this improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison to Existing Methods</head><p>This section compares our proposed AG-CUResNeSt to several state-of-theart models for polyp segmentation. From the previous ablation study, we select the best-performing ResNeSt101 backbone as the comparison model for this section. Therefore, the model is briefly called AG-CUResNeSt-101.</p><p>Six existing state-of-the-art models with publicly available source codes are used as baselines in the following evaluations, including: DDANet <ref type="bibr" target="#b48">[48]</ref>, Re-sUNet++ <ref type="bibr" target="#b20">[19]</ref>, DoubleUNet <ref type="bibr" target="#b23">[22]</ref>, HarDNet-MSEG <ref type="bibr" target="#b38">[37]</ref>, ColonSegNet <ref type="bibr" target="#b39">[38]</ref> and PraNet <ref type="bibr" target="#b22">[21]</ref>. We re-use the original authors' reported metrics, and perform our own evaluations based on public codebases for un-tested datasets. Besides these baseline models, depending on each scenario, we also include other published results in literature for comparision where applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Cross-dataset Evaluation</head><p>The following experiments evaluate the performance of AG-CUResNeSt-101</p><p>and previous state-of-the-art models when training and testing across different datasets, i.e., Scenario 1, Scenario 2, and Scenario 3. This setting implies that models need to generalize well to have good performance, as different polyp datasets have different image properties and feature distributions. We first compare the AG-CUResNeSt-101 with the baseline models on the train and test datasets used in our ablation study. Results are shown in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>We find that AG-CUResNest-101 outperforms every baseline by relatively large margins. Against PraNet, the best-performing baseline, AG-CUResNeSt-101 achieves a 3.4% improvement in Dice score and 5.5% improvement in IoU score.</p><p>These numbers are even larger compared to models such as ResUNet++ and ColonSegNet, which yield very low accuracy on the test set. <ref type="figure" target="#fig_6">Fig. 4</ref> shows qualitative results comparison between different methods on challenging images in Scenario 1. Our model can yield accurate segmentation in most of the cases, while all other baseline methods failed.</p><p>We also compare AG-CUResNeSt-101 with Mask-RCNN <ref type="bibr" target="#b49">[49]</ref> alongside the baseline models in Scenario 2, i.e., using CVC-Colon for training and CVC-Clinic for testing. We use the implementation of Mask-RCNN in the detectron2 project and train the model from scratch using the original paper's hyperparameters <ref type="bibr" target="#b49">[49]</ref>. <ref type="table" target="#tab_3">Table 3</ref> shows that AG-CUResNeSt-101 outperforms Mask-RCNN by a large margin (by over 13%). PraNet is still the second-best baseline model, which is outperformed by AG-CUResNeSt-101 by 3.3% in mDice score. <ref type="figure" target="#fig_7">Fig. 5</ref> provides additional references for the output produced by each model. Notably, AG-CUResNeSt-101 seems capable of detecting both tiny and large polyps that occupy the whole image. We also compare the final output taken from the second UNet and the auxiliary output taken from the first UNet. <ref type="figure" target="#fig_8">Fig. 6</ref> shows that the second UNet can correct some regions that the first UNet fails to predict.   Next, we compare AG-CUResNeSt-101 with the baseline models in Scenario 3, i.e., using CVC-ClinicDB for training and ETIS-Larib for testing. <ref type="table" target="#tab_5">Table 4</ref> shows that AG-CUResNeSt-101 outperforms all other models in terms of mDice and mIoU, both by significant margins. HarDNet-MSEG is the second-best baseline in this scenario, achieving a Dice score of 0.659. AG-CUResNeSt-101 outperforms HarDNet-MSEG by 4.2% in Dice score and 3% in IoU score.</p><p>While AG-CUResNeSt-101 has slightly lower precision than HarDNet-MSEG, it achieves significantly higher recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Intra-dataset Evaluation</head><p>The following experiments compare AG-CUResNeSt-101 with several existing models when the training and test set are drawn from the same dataset, i.e., Scenario 4, 5, and 6.  and testing. The proposed AG-CUResNeSt-101 is compared with the aforementioned baseline models along with reported metrics from SFA <ref type="bibr" target="#b50">[50]</ref>, ResUNet-mod <ref type="bibr" target="#b51">[51]</ref>, UNet <ref type="bibr" target="#b18">[17]</ref> and UNet++ <ref type="bibr" target="#b19">[18]</ref>. For the Kvasir-SEG test set, AG-CUResNeSt-101 outperforms the second-best PraNet by 0.4% in mDice and 0.5% in mIoU.</p><p>For the CVC-ClinicDB test set, AG-CUResNeSt-101 is also the best performing model, exceeding PraNet by 1.9% in mDice and 1.8% in mIoU, respectively. We note that models such as DDANet and ColonSegNet perform much better in this scenario than the previous cross-dataset scenarios, implying that they may not be as robust to diverse test images as other models.</p><p>Next, we compare AG-CUResNeSt-101 against the baseline models alongside</p><p>UNet <ref type="bibr" target="#b18">[17]</ref> and MultiResUNet <ref type="bibr" target="#b52">[52]</ref> in Scenario 5, i.e., 5-fold cross-validation on the CVC-Clinic dataset. Results are shown in <ref type="table" target="#tab_7">Table 6</ref>. Note that the authors of UNet and MultiResUNet only reported their IoU scores on this dataset.</p><p>Regardless, we can see AG-CUResNeSt-101 shows significant improvement in We also perform a comparison of AG-CUResNeSt-101 with the baseline models in Scenario 6, i.e., 5-fold cross-validation on the Kvasir-SEG dataset. <ref type="table" target="#tab_8">Table 7</ref> shows that the proposed model achieves the best mDice score, mIoU, recall, and precision. Specifically, AG-CUResNeSt-101 achieves an average Dice score of 0.912, outperforming the second-place PraNet by 2.9%. Besides, metric scores across different folds demonstrate the stability of AG-CUResNeSt-101, with a slightly lower standard deviation than PraNet.</p><p>A qualitative comparison between different models is shown in <ref type="figure" target="#fig_9">Fig. 7</ref>. In many cases, one can see that our model performs significantly better than other state-of-the-art methods. The lesion regions are well-segmented and delineated.</p><p>Nevertheless, in some other case shown in <ref type="figure" target="#fig_10">Fig. 8</ref>, AG-CUResNeSt-101 is  confused in estimating the attention maps, which lead to poor segmentation results. Usually, these imprecise predictions may occur when the input image contains very large polyps or colon mucosa folds, with many similar appearance characteristics as polyps. These are challenging cases for all the segmentation models and even junior endoscopists in practice.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Complexity and inference time comparison</head><p>Finally, we compare the complexity and inference time of our proposed model and the state-of-the-art ones. The model complexity is measured as the number of floating operations (Flops), while the inference time per image is measured in seconds with corresponding Fps. We infer all the models using a machine with an NVIDIA GPU RTX 3090. Results are shown in <ref type="table" target="#tab_9">Table 8</ref>.</p><p>We can see that the high accuracy achieved by our AG-CUResNeSt comes at a tradeoff in terms of floating-point operations. It is the third most complex model, behind ResUNet++ and DoubleUNet. DoubleUNet, in particular, has significantly more Flops than any other model at 431 GFlops. Meanwhile, ColonSegNet and DDANet are the most lightweight models, at fewer This is particularly suitable for clinical practice in healthcare facilities with limited resources, especially in developing countries. The model is also suitable for retrospective studies for comparing the accuracy of an AI system with that of the original endoscopists in clinical practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper has introduced a novel neural network architecture for polyp segmentation called AG-CUResNeSt. The architecture combines several com-ponents, namely ResNeSt, attention gates, and Coupled UNets, to improve performance. The proposed model is verified using extensive experiments and compared against several state-of-the-art methods on public benchmark datasets.</p><p>Results show that AG-CUResNeSt consistently improves against all compared models, with a slight tradeoff in model complexity. Especially, our model achieves significantly better generalization capability in all cross-dataset experiments.</p><p>We hope that our proposal will provide a strong baseline for developing deep neural networks in medical image analysis, especially in colonoscopy. Our future research will focus on reducing the model size without sacrificing accuracy and conducting a deep study on failed cases to understand better the characteristics of the cases for further improvement in terms of segmentation accuracy. In addition, the generalizability issues will be addressed deeper by using more complicated approaches such as domain adaptation and domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head><p>This work is funded by Vingroup Innovation Foundation (VINIF) under project code VINIF.2020.DA17.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of the proposed AG-CUResNeSt. Attention gates within each UNet are used to suppress irrelevant information in the encoder's feature maps. Skip connections across the two UNets are also utilized to boost the information flow and promote feature reuse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 1 .</head><label>1</label><figDesc>Each UNet has an encoder and a decoder with skip connections between them. The encoder takes an image input of size 512 ? 512, then passes through five top-down blocks to produce a high-level semantic feature map of size 16 ? 16. This feature map is gradually up-sampled through five bottom-up blocks of the decoder and fused with low-level information in the encoder via gated skip connections called attention gates. The role</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>corresponding residual mapping F (x) = H(x) ? x. The original mapping can be obtained using a skip connection as H(x) = F (x) + x. By this trick, if the network wants to learn identity mappings, it just simply needs to drive the weights of the nonlinear layers in the block toward zero. Hence, residual connections facilitate the optimization of the network at almost no cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Split attention in the k-th cardinal group with R splits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The Attention Gate (AG) receives two inputs: a low-level feature map x from an encoder and a coarse feature map g from a corresponding decoder. The feature map x is firstly down-sampled and fused with g, then fed to some hidden layers to yield an attention coefficient map ?. Finally, the input features x are scaled with attention coefficients to suppress irrelevant information.a specific task. In the encoder of a UNet, input data is gradually down-sampled and transformed from low-level to high-level semantic feature maps with coarser scales. In the decoder, coarse feature maps are upsampled and fused with lowlevel ones to produce a final segmentation result.Fig. 3describes how the attention gate works. Suppose that g ? R Fg?Hg?Wg is a coarse feature map in the decoder that provides information to suppress the irrelevant content in a low-level feature map x ? R Fx?Hx?Wx from the encoder. Each feature map g and x is fed to a 1 ? 1 convolutional (conv) layer with F i kernels to reduce its number of channels to an intermediate value F i . The low-level feature map is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>score (F1). These are the most well-known measures for segmentation accuracy evaluation. Metrics are measured on the macro level: measurements are made on every image, then averaged on the whole dataset across all images. P + F P + F N IoU (P, G) = T P T P + F P + F N where P represents the model's prediction, G is the ground-truth, TP is true positives, FP is false positives, and FN is false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative result comparison in Scenario 1, i.e., training on CVC-Colon and ETIS-Larib, testing on CVC-Clinic. From left to righ: input image, ground truth, outputs of DDANet, ResUNet++, Double UNet, ColonSegNet, HardNet-MSEG, PraNet, AG-CUResNeSt-101 (ours), and attention map in the last attention gate S9 in Fig. 1. The red color in the attention map indicates the region where the model focus on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative result comparison using CVC-Colon for training and CVC-Clinic for testing. From left to right: input image, ground truth, visualization of ResNet101-MaskR-CNN's output in overlay mode, binary output of ResNet101-MaskR-CNN, visualization of ResNet50-MaskR-CNN's output in overlay mode, binary output of ResNet50-MaskR-CNN, binary outputs of DDANet, ResUNet++, Double UNet, ColonSegNet, HardNet-MSEG, PraNet, our AG-CUResNeSt-101, and attention map in the last attention gate S9 in Fig. 1. The red color in the attention map indicates the region where the model focus on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>The results of AG-CUResNeSt-101 on CVC-Clinic dataset. From left to right: input image, ground truth, output of the first UNet, output of the second UNet, and attention map in the last attention gate S9 in Fig. 1. The red areas in the attention map are high probability where polyps appear.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative result comparison of different models trained in Scenario 6, i.e., 5-fold cross-validation on the Kvasir-SEG dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Some failed cases of our model on the Kvasir-SEG dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9</head><label>9</label><figDesc>shows the ROC curve and PR curve for each model in this experiment.Our AG-CUResNeSt-101 again reports the best AUC value of 0.9585 and the best MAP value of 0.886.From the perspectives of endoscopists, the use of our proposed models is expected to support the training of junior staff in colon polyp detection. The improvements of our proposed model over state-of-the-art methods are especially helpful for inexperienced endoscopists in delineating lesions in challenging cases. Furthermore, it could be considered the possibility of setting up in clinical practice as a second-look tool or an assisting system during the procedure. The high accuracy of our novel models in benchmark datasets proposes a feasible solution to reduce the missing rate in real practice, which may have a significant impact on improving the quality of colorectal cancer screening strategy.(a) ROC curves (b) PR curves</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>ROC curves and PR curves for AG-CUResNeSt-101 and other state-of-the-art models in Scenario 6, i.e., 5-fold cross-validation on the Kvasir-SEG dataset. All curves are averaged over five folds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>dataset is contributed by the Machine Vision Group (MVG) and contains 15 different polyps in 380 images (574x500). Finally, the Kvasir-SEG dataset<ref type="bibr" target="#b45">[44]</ref>, publicized by the Simula Research Laboratory, includes 1000 polyp images with varying sizes.</figDesc><table /><note>To evaluate the effectiveness of each proposed component in our new archi- tecture and compare the performance of our model across datasets over state- of-the-art approaches, we conduct experiments with different scenarios of using training and test data, as follows: ? Scenario 1: CVC-Colon and ETIS-Larib for training, CVC-Clinic for testing; ? Scenario 2: CVC-Colon for training, CVC-Clinic for testing; ? Scenario 3: CVC-ClinicDB for training, ETIS-Larib for testing. This is the combination used in the 2015 MICCAI sub-challenge; ? Scenario 4: The Kvasir-SEG and CVC-Clinic datasets are merged, then split 80/10/10 for training, validation, and testing. The test sets are kept separate for evaluation on each source dataset. This is the combination proposed by PraNet [21]; ? Scenario 5: 5-fold cross-validation on the CVC-Clinic dataset, which is split into five equal folds. Each run uses one fold for testing and four folds for training; ? Scenario 6: 5-fold cross-validation on the Kvasir-SEG dataset, which is split into five equal folds. Each run uses one fold for testing and four folds for training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance metrics of model variants in Scenario 1, i.e., training on CVC-Colon</figDesc><table><row><cell cols="2">and ETIS-Larib, testing on CVC-Clinic</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">mDice ? mIoU ? Recall ? Precision ?</cell></row><row><cell>VGG16-UNet</cell><cell>0.759</cell><cell>0.660</cell><cell>0.831</cell><cell>0.778</cell></row><row><cell>Efficientnet-B0-UNet</cell><cell>0.747</cell><cell>0.650</cell><cell>0.871</cell><cell>0.737</cell></row><row><cell>Efficientnet-B1-UNet</cell><cell>0.754</cell><cell>0.662</cell><cell>0.877</cell><cell>0.747</cell></row><row><cell>Efficientnet-B2-UNet</cell><cell>0.800</cell><cell>0.715</cell><cell>0.806</cell><cell>0.860</cell></row><row><cell>Efficientnet-B3-UNet</cell><cell>0.803</cell><cell>0.716</cell><cell>0.849</cell><cell>0.824</cell></row><row><cell>Efficientnet-B4-UNet</cell><cell>0.813</cell><cell>0.731</cell><cell>0.835</cell><cell>0.860</cell></row><row><cell>ResNet34-UNet</cell><cell>0.783</cell><cell>0.692</cell><cell>0.827</cell><cell>0.821</cell></row><row><cell>ResNet50-UNet</cell><cell>0.805</cell><cell>0.719</cell><cell>0.843</cell><cell>0.827</cell></row><row><cell>ResNet101-UNet</cell><cell>0.811</cell><cell>0.731</cell><cell>0.849</cell><cell>0.838</cell></row><row><cell>ResNeSt50-UNet</cell><cell>0.814</cell><cell>0.725</cell><cell>0.829</cell><cell>0.861</cell></row><row><cell>ResNeSt101-UNet</cell><cell>0.816</cell><cell>0.739</cell><cell>0.813</cell><cell>0.888</cell></row><row><cell>Attention ResNet101-UNet</cell><cell>0.815</cell><cell>0.730</cell><cell>0.863</cell><cell>0.825</cell></row><row><cell>Attention ResNeSt101-UNet</cell><cell>0.829</cell><cell>0.749</cell><cell>0.842</cell><cell>0.877</cell></row><row><cell>Attention ResCUNet-101</cell><cell>0.820</cell><cell>0.736</cell><cell>0.859</cell><cell>0.838</cell></row><row><cell>AG-CUResNeSt-101</cell><cell>0.833</cell><cell>0.754</cell><cell>0.840</cell><cell>0.883</cell></row><row><cell cols="3">5.1.1. The Effectiveness of Backbone Networks</cell><cell></cell><cell></cell></row><row><cell cols="5">We first evaluate the use of different encoder backbones. Several ResNet vari-</cell></row><row><cell cols="5">ants including ResNet34, ResNet50, ResNet101, ResNeSt50, and ResNeSt101</cell></row><row><cell cols="5">have been used. Besides, we also try other CNN architectures such as VGG16</cell></row><row><cell cols="5">and EfficientNet family from B0 to B4. The ResNeSt architecture uses channel-</cell></row><row><cell cols="5">wise attention on separate branches to enrich their features. Table 1 shows that</cell></row><row><cell cols="5">ResNeSt101 gives the best overall performance. ResNet backbones generally</cell></row><row><cell cols="5">perform better as size increases. ResNeSt101 also improves over ResNest50,</cell></row><row><cell cols="5">but the improvement is quite marginal. ResNest101 achieves lower recall (to</cell></row><row><cell cols="5">0.813 from 0.829), suggesting that a larger ResNeSt such as ResNeSt152 would</cell></row><row><cell cols="5">likely not yield significant improvements. ResNeSt101 backbone significantly</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>ResUNet++ [19]</cell><cell>0.406</cell><cell>0.302</cell><cell>0.481</cell><cell>0.496</cell></row><row><cell>ColonSegNet [38]</cell><cell>0.427</cell><cell>0.321</cell><cell>0.529</cell><cell>0.552</cell></row><row><cell>DDANet [48]</cell><cell>0.624</cell><cell>0.515</cell><cell>0.697</cell><cell>0.692</cell></row><row><cell>DoubleUNet [22]</cell><cell>0.738</cell><cell>0.651</cell><cell>0.758</cell><cell>0.824</cell></row><row><cell>HarDNet-MSEG [37]</cell><cell>0.765</cell><cell>0.681</cell><cell>0.774</cell><cell>0.863</cell></row><row><cell>PraNet [21]</cell><cell>0.779</cell><cell>0.689</cell><cell>0.832</cell><cell>0.812</cell></row><row><cell>AG-CUResNeSt-101 (Ours)</cell><cell>0.833</cell><cell>0.754</cell><cell>0.840</cell><cell>0.883</cell></row><row><cell cols="5">indicates a model retrained with the original reported configurations.</cell></row></table><note>Performance metrics for ResUNet++, DoubleUNet, DDANet, ColonSegNet, HarDNet-MSEG, PraNet and AG-CUResNeSt-101 in Scenario 1, i.e., training on CVC-Colon and ETIS-Larib, testing on CVC-Clinic Method mDice ? mIoU ? Recall ? Precision ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">Performance metrics for Mask-RCNN, ResUNet++, DoubleUNet, DDANet, Colon-</cell></row><row><cell cols="5">SegNet, HarDNet-MSEG, PraNet and AG-CUResNeSt in Scenario 2, i.e., using CVC-Colon</cell></row><row><cell>for training, CVC-Clinic for testing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">mDice ? mIoU ? Recall ? Precision ?</cell></row><row><cell>ResNet50-Mask-RCNN [49]</cell><cell>0.639</cell><cell>0.560</cell><cell>0.648</cell><cell>0.710</cell></row><row><cell>ResNet101-Mask-RCNN [49]</cell><cell>0.641</cell><cell>0.565</cell><cell>0.646</cell><cell>0.725</cell></row><row><cell>ResUNet++ [19]</cell><cell>0.339</cell><cell>0.247</cell><cell>0.380</cell><cell>0.484</cell></row><row><cell>DoubleUNet [22]</cell><cell>0.441</cell><cell>0.375</cell><cell>0.423</cell><cell>0.639</cell></row><row><cell>DDANet [48]</cell><cell>0.476</cell><cell>0.370</cell><cell>0.501</cell><cell>0.644</cell></row><row><cell>ColonSegNet [38]</cell><cell>0.582</cell><cell>0.268</cell><cell>0.511</cell><cell>0.460</cell></row><row><cell>HarDNet-MSEG [37]</cell><cell>0.721</cell><cell>0.633</cell><cell>0.744</cell><cell>0.818</cell></row><row><cell>PraNet [21]</cell><cell>0.738</cell><cell>0.647</cell><cell>0.751</cell><cell>0.832</cell></row><row><cell>AG-CUResNeSt-101 (Ours)</cell><cell>0.771</cell><cell>0.686</cell><cell>0.793</cell><cell>0.830</cell></row><row><cell cols="5">indicates a model retrained with the original reported configurations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>shows our evaluation results in Scenario 4, i.e., Kvasir-SEG and</cell></row><row><cell>CVC-Clinic datasets are merged, then split 80/10/10 for training, validation,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance metrics for Mask-RCNN, Double UNet, ResUNet++, ColonSegNet,</figDesc><table><row><cell cols="5">DDANet, PraNet, HarDNet-MSEG and AG-CUResNeSt in Scenario 3, i.e., using CVC-</cell></row><row><cell cols="2">ClinicDB for training, ETIS-Larib for testing</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">mDice ? mIoU ? Recall ? Precision ?</cell></row><row><cell>ResNet50-Mask-RCNN [49]</cell><cell>0.501</cell><cell>0.412</cell><cell>0.546</cell><cell>0.573</cell></row><row><cell>ResNet101-Mask-RCNN [49]</cell><cell>0.565</cell><cell>0.469</cell><cell>0.565</cell><cell>0.639</cell></row><row><cell>DoubleUNet (BCE loss) [22]</cell><cell>0.482</cell><cell>0.400</cell><cell>0.713</cell><cell>0.475</cell></row><row><cell>DoubleUNet (Dice loss) [22]</cell><cell>0.588</cell><cell>0.500</cell><cell>0.689</cell><cell>0.599</cell></row><row><cell>ResUNet++ [19]</cell><cell>0.211</cell><cell>0.155</cell><cell>0.309</cell><cell>0.203</cell></row><row><cell>ColonSegNet [38]</cell><cell>0.217</cell><cell>0.110</cell><cell>0.654</cell><cell>0.144</cell></row><row><cell>DDANet [48]</cell><cell>0.400</cell><cell>0.313</cell><cell>0.507</cell><cell>0.464</cell></row><row><cell>PraNet [21]</cell><cell>0.631</cell><cell>0.555</cell><cell>0.762</cell><cell>0.597</cell></row><row><cell>HarDNet-MSEG [37]</cell><cell>0.659</cell><cell>0.583</cell><cell>0.676</cell><cell>0.705</cell></row><row><cell>AG-CUResNeSt-101 (Ours)</cell><cell>0.701</cell><cell>0.613</cell><cell>0.755</cell><cell>0.693</cell></row><row><cell cols="5">indicates a model retrained with the original reported configurations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>mDice and mIoU scores for models trained in Scenario 4 on the Kvasir-SEG and</figDesc><table><row><cell>CVC-ClinicDB test sets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Kvasir-SEG</cell><cell cols="2">CVC-ClinicDB</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">mDice ? mIoU ? mDice ? mIoU ?</cell></row><row><cell>UNet [17]</cell><cell>0.818</cell><cell>0.746</cell><cell>0.823</cell><cell>0.755</cell></row><row><cell>UNet++ [18]</cell><cell>0.821</cell><cell>0.743</cell><cell>0.794</cell><cell>0.729</cell></row><row><cell>ResUNet-mod [51]</cell><cell>0.791</cell><cell>n/a</cell><cell>0.779</cell><cell>n/a</cell></row><row><cell>ResUNet++ [19]</cell><cell>0.813</cell><cell>0.793</cell><cell>0.796</cell><cell>0.796</cell></row><row><cell>SFA [50]</cell><cell>0.723</cell><cell>0.611</cell><cell>0.700</cell><cell>0.607</cell></row><row><cell>DDANet [48]</cell><cell>0.758</cell><cell>0.658</cell><cell>0.761</cell><cell>0.668</cell></row><row><cell>DoubleUNet [22]</cell><cell>0.781</cell><cell>0.700</cell><cell>0.791</cell><cell>0.730</cell></row><row><cell>ColonSegNet [38]</cell><cell>0.753</cell><cell>0.643</cell><cell>0.803</cell><cell>0.709</cell></row><row><cell>HarDNet-MSEG [37]</cell><cell>0.877</cell><cell>0.807</cell><cell>0.907</cell><cell>0.853</cell></row><row><cell>PraNet [21]</cell><cell>0.898</cell><cell>0.840</cell><cell>0.899</cell><cell>0.849</cell></row><row><cell>AG-CUResNeSt-101 (Ours)</cell><cell>0.902</cell><cell>0.845</cell><cell>0.917</cell><cell>0.867</cell></row><row><cell cols="5">indicates a model retrained with the original reported configurations.</cell></row><row><cell cols="5">this metric, outperforming the second-best PraNet by 1.3% in mDice and 1.8%</cell></row><row><cell>in mIoU.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Performance metrics for UNet, MultiResUNet, ResUNet++, DoubleUNet, DDANet,</figDesc><table><row><cell cols="5">ColonSegNet, HarDNet-MSEG, PraNet and AG-CUResNeSt-101 in Scenario 5, i.e., 5-fold</cell></row><row><cell cols="2">cross-validation on the CVC-Clinic dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>mDice ?</cell><cell>mIoU ?</cell><cell>Recall ?</cell><cell>Precision ?</cell></row><row><cell>UNet [17]</cell><cell>-</cell><cell>0.792</cell><cell>-</cell><cell>-</cell></row><row><cell>MultiResUNet [52]</cell><cell>-</cell><cell>0.849</cell><cell>-</cell><cell>-</cell></row><row><cell>ResUNet++ [19]</cell><cell cols="4">0.815 ? 0.018 0.736 ? 0.017 0.832 ? 0.018 0.830 ? 0.020</cell></row><row><cell>DoubleUNet [22]</cell><cell cols="4">0.920 ? 0.018 0.866 ? 0.025 0.922 ? 0.027 0.928 ? 0.017</cell></row><row><cell>DDANet [48]</cell><cell cols="4">0.860 ? 0.014 0.786 ? 0.017 0.858 ? 0.023 0.892 ? 0.014</cell></row><row><cell>ColonSegNet [38]</cell><cell cols="4">0.817 ? 0.020 0.873 ? 0.024 0.926 ? 0.025 0.933 ? 0.014</cell></row><row><cell>HarDNet-MSEG [37]</cell><cell cols="4">0.923 ? 0.020 0.873 ? 0.024 0.926 ? 0.025 0.933 ? 0.014</cell></row><row><cell>PraNet [21]</cell><cell cols="4">0.933 ? 0.012 0.884 ? 0.015 0.940 ? 0.005 0.937 ? 0.016</cell></row><row><cell cols="5">AG-CUResNeSt-101 (Ours) 0.946?0.01 0.902?0.015 0.953?0.013 0.944?0.009</cell></row><row><cell cols="4">indicates a model retrained with the original reported configurations.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Performance metrics of UNet, ResUNet++, PraNet, DoubleUNet, DDANet, Colon-SegNet, HarDNet-MSEG and AG-CUResNeSt-101 in Scenario 6, i.e., 5-fold cross-validation on the Kvasir-SEG dataset</figDesc><table><row><cell>Method</cell><cell>mDice ?</cell><cell>mIoU ?</cell><cell>Recall ?</cell><cell>Precision ?</cell></row><row><cell>UNet [17]</cell><cell>0.708?0.017</cell><cell>0.602?0.01</cell><cell>0.805?0.014</cell><cell>0.716?0.02</cell></row><row><cell>ResUNet++ [19]</cell><cell>0.780?0.01</cell><cell>0.681?0.008</cell><cell>0.834?0.01</cell><cell>0.799?0.01</cell></row><row><cell>DoubleUNet [22]</cell><cell>0.879 ? 0.018</cell><cell>0.816 ? 0.026</cell><cell cols="2">0.902 ? 0.027 0.894 ? 0.039</cell></row><row><cell>DDANet [48]</cell><cell>0.860 ? 0.005</cell><cell>0.791 ? 0.004</cell><cell cols="2">0.876 ? 0.015 0.892 ? 0.018</cell></row><row><cell>ColonSegNet [38]</cell><cell>0.676 ? 0.037</cell><cell>0.557 ? 0.040</cell><cell cols="2">0.731 ? 0.088 0.730 ? 0.080</cell></row><row><cell>HarDNet-MSEG [37]</cell><cell>0.889 ? 0.011</cell><cell>0.831 ? 0.011</cell><cell cols="2">0.892 ? 0.015 0.926 ? 0.014</cell></row><row><cell>PraNet [21]</cell><cell>0.883?0.02</cell><cell>0.822?0.02</cell><cell>0.897?0.02</cell><cell>0.906?0.01</cell></row><row><cell cols="3">AG-CUResNeSt-101 (Ours) 0.912?0.01 0.860?0.011</cell><cell cols="2">0.923?0.009 0.927?0.014</cell></row><row><cell cols="4">indicates a model retrained with the original reported configurations.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Complexity and inference time comparison between our AG-CUResNeSt and other existing methods million parameters each. Nevertheless, these models also perform relatively poorly on cross-dataset testing experiments. Finally, HarDNet-MSEG and PraNet require the fewest floating operations despite their size, but their accuracy metrics still trails AG-CUResNeSt-101 significantly. The complexity of AG-CUResNeSt-101 is relatively high since it doubles the UNet structure, but in return, our model outperforms other state-of-the-art methods in terms of accuracy. Note that, in clinical practice, each 1% of adenoma detection rateincrease is associated with a 3% decrease in the risk of colon cancer. Therefore, it is worth it to trade off the model's complexity with its improvement of detection rate. Furthermore, our model still achieves 19.2 fps that is feasible for deployment on decent dedicated computing devices for real-time applications.</figDesc><table><row><cell>Model</cell><cell cols="3">GFlops Inference time per image (sec) Fps</cell></row><row><cell>ResUNet++ [19]</cell><cell>283.42</cell><cell>0.056</cell><cell>17.9</cell></row><row><cell>ColonSegNet [38]</cell><cell>64.84</cell><cell>0.024</cell><cell>41.7</cell></row><row><cell>DDANet [48]</cell><cell>83.46</cell><cell>0.019</cell><cell>52.6</cell></row><row><cell>DoubleUNet [22]</cell><cell>431.00</cell><cell>0.1</cell><cell>10</cell></row><row><cell>HarDNet-MSEG [37]</cell><cell>11.38</cell><cell>0.01</cell><cell>100</cell></row><row><cell>PraNet [21]</cell><cell>13.11</cell><cell>0.014</cell><cell>71.4</cell></row><row><cell>AG-CUResNeSt-101</cell><cell>273.40</cell><cell>0.052</cell><cell>19.2</cell></row><row><cell>than 7</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajkbaksh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Matuszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Angermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rustad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Balasingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pogorelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Debard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brandao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>C?rdova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>S?nchez-Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fern?ndez-Esparrach</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparative validation of polyp detection methods in video colonoscopy: Results from the miccai 2015 endoscopic vision challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2017.2664042</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1231" to="1249" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High-grade dysplasia and invasive carcinoma in colorectal adenomas: a multivariate analysis of the impact of adenoma and patient characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gschwantler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kriwanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>G?ritzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schrutka-K?lbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brownstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feichtinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European journal of gastroenterology &amp; hepatology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="188" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adenoma detection rate and risk of colorectal cancer and death</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Doubeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Zauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Fireman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Schottinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New england journal of medicine</title>
		<imprint>
			<biblScope unit="volume">370</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1298" to="1306" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Colorectal cancer screening: An updated review of the available options</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noureddine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World journal of gastroenterology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page">5086</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An adequate level of training for technical competence in screening and diagnostic colonoscopy: a prospective multicenter evaluation of the learning curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-K</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-O</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-M</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hwangbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointestinal endoscopy</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="683" to="689" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visibility map: a new method in evaluation quality of optical colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Armin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Conlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grimpen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Salvado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Factors influencing the miss rate of polyps in a back-to-back colonoscopy study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leufkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Oijen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vleggaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siersema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endoscopy</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="470" to="475" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computer-aided classification of gastrointestinal lesions in regular colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mesejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abergel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rouquette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beorchia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Poincloux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2051" to="2063" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Berzin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R G</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">951e-a real-time automatic deep learning polyp detection system increases polyp and adenoma detection during colonoscopy: a prospective double-blind randomized study</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="2019" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Artificial intelligence and colonoscopy: Current status and future perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digestive Endoscopy</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="363" to="371" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning localizes and identifies polyps in real time with 96% accuracy in screening colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alkayali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Karnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterology</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1069" to="1078" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Machine learning for computer-aided polyp detection using wavelets and content-based image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Viscaino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Cheein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="961" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate classification of diminutive colorectal polyps using computer-aided analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterology</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="568" to="575" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Advanced imaging for detection and differentiation of colorectal neoplasia: European society of gastrointestinal endoscopy (esge) guideline-update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bisschops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>East</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hazewinkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kami?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pellis?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Antonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Balen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Coron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endoscopy</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1155" to="1179" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwahori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Woodham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatic polyp detection in endoscope images using a hessian filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhuyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kasugai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>MVA</publisher>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Granado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unet++: Redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Resunet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">De</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Attention u-net: Learning where to look for the pancreas</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pranet: Parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Doubleunet: A deep convolutional neural network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CBMS49503.2020.00111</idno>
	</analytic>
	<monogr>
		<title level="m">33rd IEEE International Symposium on Computer-Based Medical Systems</title>
		<editor>A. G. S. de Herrera, A. R. Gonz?lez, K. C. Santosh, Z. Temesgen, B. Kane, P. Soda</editor>
		<meeting><address><addrLine>Rochester, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="558" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Resnest: Split-attention networks, CoRR abs/2004.08955</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<title level="m">Proceedings of 29th British Machine Vision Conference</title>
		<meeting>29th British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cu-net: Coupled u-nets</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facial uv map completion for poseinvariant face recognition: a novel adversarial approach based on coupled attention residual unets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dinh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human-centric Computing and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efficientdet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10778" to="10787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Y. Bengio, Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
	<note>Selective kernel networks</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efficientnet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07172</idno>
		<title level="m">Hardnet-mseg: A simple encoderdecoder polyp segmentation neural network that achieves over 0.9 mean dice and 86 fps</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Real-time polyp detection, localization and segmentation in colonoscopy using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ieee Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="40496" to="40510" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03" />
			<biblScope unit="page" from="6391" to="6401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tversky loss function for image segmentation using 3d fully convolutional deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on machine learning in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fern?ndez-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vilari?o</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vilarino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Towards automatic polyp detection with a polyp appearance model</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="3166" to="3182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Kvasir-seg: A segmented polyp dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automatic colon polyp detection using region based deep cnn and post learning approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Aabakken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergsland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Balasingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="40950" to="40962" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albumentations</forename></persName>
		</author>
		<ptr target="https://github.com/albumentations-team/albumentations" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Online; accessed 10</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ddanet: Dual decoder attention network for automatic polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR International Workshop and Challenges</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Balasingham, Polyp detection and segmentation using mask r-cnn: Does a deeper feature extractor cnn always perform better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solhusvik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergsland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Aabakken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International Symposium on Medical Information and Communication Technology (IS-MICT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Selective feature aggregation network with area-boundary constraints for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Road extraction by deep residual u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multiresunet: Rethinking the u-net architecture for multimodal biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ibtehaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
