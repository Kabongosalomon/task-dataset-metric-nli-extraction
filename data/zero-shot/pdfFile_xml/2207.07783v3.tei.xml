<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Long-Term Spatial-Temporal Graphs for Active Speaker Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Min</surname></persName>
							<email>kyle.min@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><surname>Roy</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">UC Riverside</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subarna</forename><surname>Tripathi</surname></persName>
							<email>subarna.tripathi@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanaya</forename><surname>Guha</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Glasgow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somdeb</forename><surname>Majumdar</surname></persName>
							<email>somdeb.majumdar@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Long-Term Spatial-Temporal Graphs for Active Speaker Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Active speaker detection (ASD) in videos with multiple speakers is a challenging task as it requires learning effective audiovisual features and spatial-temporal correlations over long temporal windows. In this paper, we present SPELL, a novel spatial-temporal graph learning framework that can solve complex tasks such as ASD. To this end, each person in a video frame is first encoded in a unique node for that frame. Nodes corresponding to a single person across frames are connected to encode their temporal dynamics. Nodes within a frame are also connected to encode inter-person relationships. Thus, SPELL reduces ASD to a node classification task. Importantly, SPELL is able to reason over long temporal contexts for all nodes without relying on computationally expensive fully connected graph neural networks. Through extensive experiments on the AVA-ActiveSpeaker dataset, we demonstrate that learning graph-based representations can significantly improve the active speaker detection performance owing to its explicit spatial and temporal structure. SPELL outperforms all previous state-of-the-art approaches while requiring significantly lower memory and computational resources. Our code is publicly available: https://github.com/SRA2/SPELL * Authors contributed equally ? Work partially done during an internship at Intel Labs</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Holistic scene understanding in the wild is still a challenge in computer vision despite recent breakthroughs in several other areas. A scene represents reallife events spanning complex visual and auditory information, which are often intertwined. Active speaker detection (ASD) is a key component in scene understanding and is an inherently multimodal (audio-visual) task. The objective here is, given a video input, to identify which persons are speaking in each frame. This has numerous practical applications ranging from speech enhancement systems <ref type="bibr" target="#b0">[1]</ref> to human-robot interaction <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Earlier efforts on ASD had limited success due to the unavailability of large datasets, powerful learning models, or computing resources <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. With the release of AVA-ActiveSpeaker <ref type="bibr" target="#b27">[28]</ref>, a large and diverse ASD dataset, a number of promising approaches have been developed including both visual-only <ref type="figure">Fig. 1</ref>. SPELL converts a video into a canonical graph from the audio-visual input data, where each node corresponds to a person in a frame, and an edge represents a spatial or temporal interaction between the nodes. The constructed graph is dense enough for modeling long-term dependencies through message passing across the temporallydistant but relevant nodes, yet sparse enough to be processed within low memory and computation budget. The Active Speaker Detection (ASD) task is posed as a binary node classification in this long-range spatial-temporal graph. and audio-visual methods. As visual-only methods <ref type="bibr" target="#b7">[8]</ref> are unable to distinguish between verbal and non-verbal lip movements, more recent approaches have focused on joint modeling of the audio-visual information. Audio-visual approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18]</ref> address the task by first encoding visual (primarily facial) and audio features from videos, and then by classifying the fused multimodal features. Such models generally have multi-stage frameworks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b17">18]</ref> and show good detection performance. However, state-of-the-art methods have relied on complex architectures for processing the audio-visual features with high computation and memory overheads. For example, TalkNet <ref type="bibr" target="#b34">[35]</ref> suggests using a transformerstyle architecture <ref type="bibr" target="#b35">[36]</ref> to model the cross-modal information from the audiovisual input. ASDNet <ref type="bibr" target="#b17">[18]</ref>, which is the leading state-of-the-art method, uses a complex 3D convolutional neural network (CNN) to extract more powerful features. These approaches are not scalable and may not be suitable for real-world situations with limited memory and computation budgets.</p><p>In this paper, we propose an efficient graph-based framework, which we call SPELL (Spatial-Temporal Graph Learning). <ref type="figure">Figure 1</ref> illustrates an overview of our framework. We construct a multimodal graph from the audio-visual data and cast the active speaker detection as a graph node classification task. First, we create a graph where each node corresponds to each person at each frame and the edges represent spatial or temporal relationships among them. The initial node features are constructed using simple and lightweight 2D CNNs instead of a complex 3D CNN or a transformer. Next, we perform binary node classi-fication -active or inactive speaker -on this graph by learning a three-layer graph neural network (GNN) model each with a small number of parameters. In our framework, graphs are constructed specifically for encoding the spatial and temporal dependencies among the different facial identities. Therefore, the GNN can leverage this graph structure and model the temporal continuity in speech as well as the long-term spatial-temporal context, while requiring low memory and computation.</p><p>Although the proposed graph structure can model the long-term spatialtemporal information from the audio-visual features, it is likely that some of the short-term information may be lost in the process of feature encoding. This is because we use 2D CNNs that are not well-suited for processing the spatialtemporal information when compared to the transformer or the 3D CNNs. To encode the short-term information, we adopt TSM [20] -a generic module for 2D CNNs that is capable of modeling temporal information without introducing any additional parameters or computation. We empirically verify that SPELL can benefit both from the supplementary short-term information provided by TSM and the long-term information modeled by our graph structure.</p><p>We show the effectiveness of SPELL by performing extensive experiments on the AVA-ActiveSpeaker dataset <ref type="bibr" target="#b27">[28]</ref>. Using our spatial-temporal graph framework on top of the TSM-inspired feature encoders, SPELL outperforms all previous state-of-the-art approaches. Critically, SPELL requires significantly less hardware resources for the visual feature encoding (0.7 GFLOPs, 11.2M #Params) compared to ASDNet <ref type="bibr" target="#b17">[18]</ref> (13.2 GFLOPs, 48.6M params), which is the leading state-of-the-art method. In addition, SPELL achieved 2nd place in the AVA-ActiveSpeaker challenge at ActivityNet 2022 1 , which also demonstrates the effectiveness of our method (please refer to the technical report <ref type="bibr" target="#b22">[23]</ref>).</p><p>There are three main contributions in this paper: ? We present a graph-based approach for solving the task of active speaker detection over long time supports by casting it as a node classification problem. ? Our model, SPELL, learns from videos to model the short-term and longterm spatial-temporal information. Specifically, we propose to construct graphs on the TSM-inspired audio-visual features. The graphs are dense enough for message passing across temporally-distant nodes, yet sparse enough to model their interactions within tight memory and compute constraints. ? SPELL notably outperforms existing methods with lower memory and computation complexity on the active speaker detection benchmark dataset, AVA-ActiveSpeaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We discuss related works in two relevant areas: application of GNNs in video scene understanding and active speaker detection.</p><p>GNNs for scene understanding. CNNs, Long Short Term Memory (LSTM), and their variants have long dominated the field of video understanding. In recent times, two new types of models are gaining traction in many areas of visual information processing: Transformers <ref type="bibr" target="#b35">[36]</ref> and GNNs. They are not necessarily in competition with the former models, but it has been shown that they can augment the performance of CNN/LSTM based models. Applications of specialized GNN models in video understanding include visual relationship forecasting <ref type="bibr" target="#b21">[22]</ref>, dialog modeling <ref type="bibr" target="#b10">[11]</ref>, video retrieval <ref type="bibr" target="#b33">[34]</ref>, emotion recognition <ref type="bibr" target="#b29">[30]</ref>, and action detection <ref type="bibr" target="#b39">[40]</ref>. GNN-based generalized video representation frameworks have also been proposed <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref> that can be used for multiple downstream tasks. For example, in Arnab et al. <ref type="bibr" target="#b2">[3]</ref>, a fully connected graph is constructed over the foreground nodes from video frames in a sliding window fashion, and a foreground node is connected to other context nodes from its neighboring frames. The message passing over the fully connected spatial-temporal graph is expensive in terms of the computational time and memory. Thus in practice such models end up using a small sliding window, making them unable to process longerterm sequences. SPELL also operates on foreground nodes -particularly, faces. However, the graph structure is not fully connected. We construct the graph such that it enables interactions only between relevant nodes over space and time. The graph remains sparse enough such that the longer-term context can be accommodated within a comparatively smaller memory and compute budget.</p><p>Active speaker detection (ASD). Earlier work on active speaker detection by Cutler et al. <ref type="bibr" target="#b6">[7]</ref> detects correlated audio-visual signals using a timedelayed neural network. Subsequent works depend only on visual information and considers a simpler set-up focusing on lip and facial gestures <ref type="bibr" target="#b7">[8]</ref>. More recently, high-performing ASD models rely on large networks -developed for capturing the spatial-temporal variations in audio-visual signals, often relying on ensemble networks or complex 3D CNN features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref>. Sharma et al. <ref type="bibr" target="#b28">[29]</ref> and Zhang et al. <ref type="bibr" target="#b37">[38]</ref> both used large 3D CNN architectures for audio-visual learning. The Active Speaker in Context (ASC) model <ref type="bibr" target="#b1">[2]</ref> uses non-local attention modules with an LSTM to model the temporal interactions between audio and visual features encoded by two-stream ResNet-18 networks <ref type="bibr" target="#b12">[13]</ref>. TalkNet <ref type="bibr" target="#b34">[35]</ref> achieves superior performance through the use of a 3D CNN and a couple of Transformers <ref type="bibr" target="#b35">[36]</ref> resulting in an effectively large model. Another recent work, the ASDNet <ref type="bibr" target="#b17">[18]</ref>, uses 3D-ResNet101 for encoding visual data and SincNet <ref type="bibr" target="#b26">[27]</ref> for audio. The Unified Context Network (UniCon) <ref type="bibr" target="#b38">[39]</ref> proposes relational context modules to capture visual (spatial) and audio-visual context based on convolutional layers. Much of these advances are due to the availability of the AVA-ActiveSpeaker dataset <ref type="bibr" target="#b27">[28]</ref>. Previously available multimodal datasets (e.g. <ref type="bibr" target="#b3">[4]</ref>) were either smaller or constrained or lacked variability in data. The work by Roth et al. <ref type="bibr" target="#b27">[28]</ref> also introduced a competitive baseline along with the large dataset. Their baseline involves jointly learning an audio-visual model that is end-to-end trainable. The audio and visual branches in this model are CNN-based which uses a depth-wise separable technique.</p><p>MAAS <ref type="bibr" target="#b18">[19]</ref> presents a different multimodal graph approach. Our work differs from MAAS in several ways, where the main difference is in the handling of temporal context. While MAAS focuses on short-term temporal windows to construct their graphs, we focus on constructing longer-term audio-visual graphs. More specifically, in MAAS, different faces are connected only between consecutive frames. In contrast, SPELL directly connects faces in a longer-term neighborhood controlled by the time threshold hyperparameter, ? (defined in Sec 3.2). In addition, SPELL exploits the temporal ordering patterns of the face tracks by using all the forward/backward/undirected edges in the time domain. In SPELL, each graph can span from 13 to 55 seconds (refer to Sec 4.2) of a video depending on the number of nodes. This is significantly larger than MAAS where the time window size is fixed at 1.59 seconds. During inference, SPELL performs single forward pass, whereas MAAS performs multiple forward passes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we describe our approach in detail. <ref type="figure">Figure 2</ref> illustrates how SPELL constructs a graph from an input video where each node corresponds to a face within a temporal window of the video. SPELL is unique in terms of its canonical way of constructing the graph from a video. The graph is able to reason over long temporal contexts for all nodes without being fully-connected. This is an important design choice to reduce memory and computation overheads. The edges in the graph are only between relevant nodes needed for message passing, leading to a sparse graph that can be accommodated within a small memory and computation budget. After converting the video into a graph, we train a lightweight GNN to perform binary node classification on this graph. The model architecture is illustrated in <ref type="figure">Figure 3</ref>. The model utilizes three separate GNN modules for the forward, backward, and undirected graph, respectively. Each module has three layers where the weight of the second layer is shared across all the above three modules. More details and the intuition behind the design choice are described in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>Let G = (V, E) be a graph with the node set V and edge set E. For any v ? V , we define N v to be the set of neighbors of v in G. We will assume the graph has self-loops, i.e., v ? N v . In addition, let X denote the set of given node features {x v } v?V where x v ? R d is the feature vector associated with the node v. Given this setup, we can define a k-layer GNN as a set of functions</p><formula xml:id="formula_0">F = {f i } i?[k] for i ? 1 where each f i : V ? R m ( m will depend on layer index i). All f i is parameterized by a set of learnable parameters. Furthermore, X i V = {x v } v?V is the set of features at layer i where x v = f i (v).</formula><p>Here, we assume that f i has access to the graph G and the feature set from the last layer X i?1 V . ? SAGE-CONV aggregation: This aggregation was proposed by <ref type="bibr" target="#b11">[12]</ref> and has a computationally efficient form. Given a d-dimensional feature set</p><formula xml:id="formula_1">X i?1 V , the (a) (b) Fig. 2. (a):</formula><p>An illustration of our graph construction process. The frames above are temporally ordered from left to right. The three colors of blue, red, and yellow denote three identities that are present in the frames. Each node in the graph corresponds to each face in the frames. SPELL connects all the inter-identity faces from the same frame with the undirected edges. SPELL also connects the same identities by the forward/backward/undirected edges across the frames. In this example, the same identities are connected across the frames by the forward edges, which are directed and only go in the temporally forward direction. (b): The process for creating the backward and undirected graph is identical, except in the former case the edges for the same identities go in the opposite direction and the latter has no directed edge. Each node also contains the audio information which is not shown here. </p><formula xml:id="formula_2">function f i : V ? R m is defined for i ? 1 as follows: f (v) = ? w?Nv M i x w where x w ? X i?1 V , M i ? R m?d</formula><formula xml:id="formula_3">f i (v) = ? w?Nv g i x v ? x w</formula><p>where ? denotes concatenation and g i : R 2d ? R m is a learnable transformation. Often g i is implemented by MLPs. The number of parameters for EDGE-CONV is larger than SAGE-CONV. This gives the EDGE-CONV layer more expressive power at a cost of higher complexity and possible risk of overfitting. For our model, we set g i to be an MLP with two layers of linear transformation and a non-linearity. We describe the details in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Video as a multimodal graph</head><p>We represent a video as a multimodal graph that is suitable for the task of active speaker detection. We assume that the bounding-box information of every face region in each frame is given as per the problem set up. For simplicity, we assume that the entire video is represented by a single graph -if the video has n faces in it, the graph will have n nodes. In our actual implementation, we temporally order the set of all faces in a video, divide them in contiguous sets, and then construct one graph for each such set. Let B be the set of all face images cropped from an input video (i.e. facecrops). Then, each element b ? B can be represented by a tuple (Box, Time, Id), where Box is the normalized bounding-box coordinates of a face-crop in its frame, Time is the time-stamp of its frame, and Id is a unique string that is common to all the face-crops that shares the same identity.</p><p>In other words, B can be represented by a set of nodes [n] where n = |B| is the total number of faces that appear in the video. Box is treated as a map such that Box(i) is defined by the bounding-box coordinates of the i-th face for any i ? [n]. Similarly, Time(i) and Id(i) correspond to the time and identity of the ith face, respectively. With this setup, the node set of G = (V, E) is V = [n] ? = B, and for any (i, j) ? [n] ? [n], we have (i, j) ? E if either of the following two conditions are satisfied:</p><formula xml:id="formula_4">? Id(i) = Id(j) and |Time(i)-Time(j)| ? ? ? Time(i) = Time(j)</formula><p>where ? is a hyperparameter for the maximum time difference between the nodes having the same identities. In essence, we connect two nodes (faces) if they share the same identity and are temporally close or if they belong to the same frame. Thus, the interactions between different speakers and the temporal variations of the same speaker can jointly be modeled.</p><p>To pose the active speaker detection task as a node classification problem, we also need to specify the feature vectors for each node v ? V . We use a twostream 2D ResNet <ref type="bibr" target="#b12">[13]</ref> architecture as in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2]</ref> for extracting the visual features of each face-crop and the audio features of each frame. Then, a feature vector of node v is defined to be</p><formula xml:id="formula_5">x v = [v visual ? v audio ]</formula><p>where v visual is the visual feature of face-crop v and v audio is the audio feature of v's frame where ? denotes the concatenation. Finally, we can write G = (V, E, X) where X is the set of the node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ASD as a node classification task</head><p>In the previous section, we described our graph construction procedure that converts a video into a graph G = (V, E, X) where each node has its own audiovisual feature vector. During the training process, we have access to the groundtruth labels of all face-crops indicating if each of the face-crop is active speaker or not. Therefore, the task of active speaker detection can be naturally posed <ref type="figure">Fig. 3</ref>. An illustration of our proposed Bi-directional (a.k.a. Bi-dir ) GNN model for active speaker detection. Here, we have three separate GNN modules for the forward, backward, and undirected graph, respectively. Each module has three layers where the weight of the second layer is shared by all three graph modules. The second layer is placed inside a solid-lined box to indicate the weight sharing while for the first and the third layer we use dotted-lines. E-CONV and S-CONV are shorthand for EDGE-CONV and SAGE-CONV, respectively. We use the color coding: blue and red to denote different identities in input frames. The output of the third layers are added together and then passed to the prediction layer. It applies the sigmoid function to the summed features of every node and produces node classification probabilities.</p><p>as a binary node classification problem in the constructed graph G, whether a node is speaking or not speaking. Specifically, we train a three-layer GNN for this classification task. The first layer in the network uses EDGE-CONV aggregation to learn pair-wise interactions between the nodes. For the last two layers, we observe that using SAGE-CONV aggregation provides better performance than EDGE-CONV, possibly due to EDGE-CONV's tendency to overfit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SPELL</head><p>We now describe how our graph construction and embedding strategy takes temporal ordering into consideration. Specifically, as we use the criterion: |Time(i) ? Time(j)| ? ? for connecting the nodes having the same identities across the frames, the resultant graph becomes undirected. In this process, we lose the information of the temporal ordering of the nodes. To address this issue, we explicitly incorporate temporal direction as shown in <ref type="figure">Figure 2(b)</ref>. The undirected GNN is augmented with two other parallel networks; one for going forward in time and another for going backward in time.</p><p>More precisely, in addition to the undirected graph, we create a forward graph where we connect (i, j) if and only if 0 ? Time(i) ? Time(j) ? ?? . Similarly, (i, j) is connected in a backward graph if and only if 0 ? Time(i) ? Time(j) ? ? . This gives us three separate graphs where each of the graphs can model different spatial-temporal relationships between the nodes. Furthermore, the weights of the second layer of each graph is shared across the three graphs. This weight sharing technique can enforce the temporal consistencies among the different information modeled by the three graphs as well as reduce the number of parameters. For the remaining parts of this paper, we will refer to this network that is augmented with the foward/backward graphs as Bi-directional or Bi-dir for short.</p><p>The proposed three-layer Bi-dir is illustrated in <ref type="figure">Figure 3</ref>. We note that right before the Bi-dir is applied, the audio and the visual features are further encoded by two learnable MLP layers (linear transformation with ReLU activation) separately and then added to form the fused features for the graph nodes. After the fused features are processed by the first and the second layers, the third layer aggregates all the information and reduce the feature dimension to 1. These 1D features coming from the three separate graphs are added and applied to the sigmoid function to get the final prediction score for each node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Feature learning</head><p>Similar to ASC <ref type="bibr" target="#b1">[2]</ref>, we use a two-stream 2D ResNet <ref type="bibr" target="#b12">[13]</ref> architecture for the audio-visual feature encoding. The networks take as visual input k consecutive face-crops and take as audio input the Mel-spectrogram of the audio wave sliced along the time duration of the face-crops for the visual stream. Although the 2D ResNet requires significantly lower hardware resources than 3D CNN counterparts or a transformer-style architecture <ref type="bibr" target="#b35">[36]</ref>, it is not specifically designed for processing spatial-temporal information that is crucial in understanding video contents. To better encode the spatial-temporal information, we augment the visual feature encoder with TSM <ref type="bibr" target="#b19">[20]</ref>, which provides 2D CNNs with a capability to model the short-term temporal information without introducing any additional parameters or computation. This additional use of TSM can greatly improve the quality of the visual features, and we empirically establish that SPELL benefits from the supplementary short-term information. The audio-visual features from the two stream are concatenated to be node features {x v }.</p><p>Data augmentation. Reliable ASD models should be able to detect speaking signals even if there is a noise in the audio. To make our method robust to noise, we make use of data augmentation methods while training the feature extractor. Inspired by TalkNet <ref type="bibr" target="#b34">[35]</ref>, we augment the audio data by negative sampling. For each audio signal in a batch, we randomly select another audio sample from the whole training dataset and add it after decreasing its volume by a random factor. This technique can effectively increase the amount of training samples for the feature extractor by selecting negative samples from the whole training dataset.</p><p>Spatial feature. The visual features encoded by the 2D ResNet do not have any information about where each face is spatially located in each frame because we only use the cropped face regions in the visual feature encoding. Here, we argue that the spatial locations of speakers can be another type of inductive bias. In order to exploit the spatial information of each face-crop, we incorporate the spatial features corresponding to each face as additional input to the node feature as follows: We project the 4-D spatial feature of each face region parameterized by the normalized center location, height and width (x, y, <ref type="table">Table 1</ref>. Performance comparisons with other state-of-the-art methods on the validation set of AVA-ActiveSpeaker datset <ref type="bibr" target="#b27">[28]</ref>. We report mAP (mean average precision). SPELL outperforms all the previous approaches. 3D Conv denotes an additional use of one or more 3D convolutional layers. Note that TSM <ref type="bibr" target="#b19">[20]</ref> does not increase memory usage nor the computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Feature encoding network mAP(%)</p><p>Roth et al. <ref type="bibr" target="#b27">[28]</ref> MobileNet <ref type="bibr" target="#b13">[14]</ref> 79.2 Zhang et al. <ref type="bibr" target="#b37">[38]</ref> 3D ResNet-18 <ref type="bibr" target="#b30">[31]</ref> + VGG-M <ref type="bibr" target="#b4">[5]</ref> 84.0 MAAS-LAN <ref type="bibr" target="#b18">[19]</ref> 2D ResNet-18 <ref type="bibr" target="#b12">[13]</ref> 85.1 Chung et al. <ref type="bibr" target="#b5">[6]</ref> VGG-M [5] + 3D Conv 85.5 ASC <ref type="bibr" target="#b1">[2]</ref> 2D ResNet-18 <ref type="bibr" target="#b12">[13]</ref> 87.1 MAAS-TAN <ref type="bibr" target="#b18">[19]</ref> 2D ResNet-18 <ref type="bibr" target="#b12">[13]</ref> 88.8 UniCon <ref type="bibr" target="#b38">[39]</ref> 2D ResNet-18 <ref type="bibr" target="#b12">[13]</ref> 92.0 TalkNet <ref type="bibr" target="#b34">[35]</ref> 2D h, w) to a 64-D feature vector using a single fully-connected layer. The resulting spatial feature vector is then concatenated to the visual feature at each node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We perform experiments on the large-scale AVA-ActiveSpeaker dataset <ref type="bibr" target="#b27">[28]</ref>. Derived from Hollywood movies, this dataset comes with a number of face tracks for active and inactive speakers and their audio signals. Its extensive annotations of the face tracks is a key feature that was missing in its predecessors. Implementation details. Following ASC <ref type="bibr" target="#b27">[28]</ref>, we utilize a two-stream network with a ResNet <ref type="bibr" target="#b12">[13]</ref> backbone for the audio-visual feature encoder. In the training process, we perform visual augmentation including horizontal flipping, color jittering, and scaling and audio augmentation as described in Section 3.5. We extract the encoded audio, visual, and spatial features for each face-crop to make the node feature. For the visual features, we use a stack of 11 consecutive face-crops (resolution: 144?144). We implement SPELL using PyTorch Geometric library <ref type="bibr" target="#b9">[10]</ref>. Our model consists of three GCN layers, each with 64 dimensional filters. The first layer is implemented by an EDGE-CONV layer that uses a two-layer MLP for feature projection. The second and third GCN layers are of type SAGE-CONV and each of them uses a single MLP layer. We set the number of nodes n to 2000 and ? parameter to 0.9, which ensures that each graph fully spans each of the face tracks. We train SPELL with a batch size of 16 using the Adam optimizer <ref type="bibr" target="#b15">[16]</ref>. The learning rate starts at 5 ? 10 ?3 and decays following the cosine annealing schedule <ref type="bibr" target="#b20">[21]</ref>. The whole training process of 70 epochs takes less than an hour using a single GPU (TITAN V). <ref type="table">Table 2</ref>. Performance comparison of context-reasoning with state-of-the-art methods. SPELL without TSM <ref type="bibr" target="#b19">[20]</ref> demonstrates the higher context-reasoning capacity of our method when compared to the other 2D CNN-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Stage </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with the state-of-the-art</head><p>We summarize the performance comparisons of SPELL with other state-of-theart approaches on the validation set of the AVA-ActiveSpeaker dataset <ref type="bibr" target="#b27">[28]</ref> in <ref type="table">Table 1</ref>. We want to point out that SPELL significantly outperforms all the previous approaches using the two-stream 2D ResNet-18 <ref type="bibr" target="#b12">[13]</ref>. Critically, SPELL's visual feature encoding has significantly lower computational and memory overhead (0.7 GFLOPs and 11.2M parameters) compared to ASDNet <ref type="bibr" target="#b17">[18]</ref> (13.2 GFLOPs, 48.6M #Params), the leading state-of-the-art method. A concurrent and closely related work MAAS <ref type="bibr" target="#b18">[19]</ref> also uses a GNN-based framework. MAAS-LAN uses a graph that is generated on a short video clip. To improve the detection performance, MAAS-TAN extends MAAS-LAN by connecting the graphs over time, which makes 13 temporally-linked graph spanning about 1.59 seconds. This time span is relatively shorter than SPELL since the SPELL graph spans around 13-55 seconds, as explained in the next subsection. In addition, SPELL requires a single forward pass when MAAS performs multiple forward passes for each inference process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Context-reasoning capacity</head><p>Most of the previous approaches have multi-stage frameworks, which includes a feature-encoding stage for audio-visual feature extraction that is followed by one or more context-reasoning stages for modeling long-term interactions and the context information. For example, SPELL has a single context-reasoning stage that uses a three-layer Bi-dir GNN for modeling long-term spatial-temporal information. In <ref type="table">Table 2</ref>, we compare the performance of context-reasoning stages with previous methods. Specifically, we analyze the detection performance when using only the feature-encoding stage (Stage-1 mAP) and the final performance. The difference between the two scores can provide a good insight on the capacity of the context-reasoning modules. Because ASDNet <ref type="bibr" target="#b17">[18]</ref> uses 3D CNNs, it is likely that some degree of the temporal context is already incorporated in the feature-encoding stage, which leads to a low context-reasoning performance. Sim- ilarly, using TSM <ref type="bibr" target="#b19">[20]</ref> provides the short-term context information in the featureencoding stage, which leads to a smaller score difference between the Stage-1 and Final mAP and thus underestimates the context-reasoning capacity. Therefore, we also estimate the performance of SPELL without TSM. In this case, the context-reasoning performance of SPELL outperforms all the other methods, which shows the higher context-reasoning capacity of our method, thanks to the longer-term context modeling. Note that although ASC <ref type="bibr" target="#b1">[2]</ref>, MAAS <ref type="bibr" target="#b18">[19]</ref>, Unicon <ref type="bibr" target="#b38">[39]</ref>, and SPELL use the same 2D ResNet-18 <ref type="bibr" target="#b12">[13]</ref>, their Stage-1 mAP can be different due to the inconsistency of input resolution, number of face-crops, and training scheme. Long-term temporal context. Note that ? (= 0.9 second in our experiments) in SPELL imposes additional constraint on direct connectivity across temporally distant nodes. The face identities across consecutive time-stamps are always connected. Below is the estimate of the effective temporal context size of SPELL. AVA-ActiveSpeaker dataset contains 3.65 million frames and 5.3 million annotated faces, resulting into 1.45 faces per frame. With an average of 1.45 faces per frame, a graph with 500 to 2000 faces in sorted temporal order spans over 345 to 1379 frames which correspond to 13 to 55 seconds for a 25-fps video. In other words, the nodes in the graph might have a time-difference of about 1 minute, and SPELL is able to reason over that long-term temporal window within a limited memory and compute budget, thanks to the effectiveness of the proposed graph structure. It is note worthy that the temporal window size in MAAS [19] is 1.9 seconds and TalkNet <ref type="bibr" target="#b34">[35]</ref> uses up to 4 seconds as long-term sequence-level temporal context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Efficiency of the context-reasoning stage</head><p>In <ref type="table" target="#tab_2">Table 3</ref>, we compare the complexity of the context-reasoning stage of SPELL with ASC <ref type="bibr" target="#b1">[2]</ref>, MAAS-TAN <ref type="bibr" target="#b18">[19]</ref>, and ASDNet <ref type="bibr" target="#b17">[18]</ref>. These methods release the source code for their models, so we use the official code to compute the number of parameters and the model size of the context-reasoning stage. ASC has about 10 times more parameters and model size than ours. Nevertheless, SPELL achieves 7.1% higher mAP than ASC. SPELL has fewer number of parameters than MAAS-TAN even while achieving 5.4% higher mAP. When compared to the leading state-of-the-art method, ASDNet, SPELL is one order of magnitude more computationally efficient. <ref type="table">Table 4</ref>. Performance comparisons of different ablative settings: TSM <ref type="bibr" target="#b19">[20]</ref>, Cx-reason (context-reasoning only with an undirected graph), Bi-dir (augmenting with forward/backward graphs), Audio-aug (audio data augmentation), Sp-feat (spatial features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TSM</head><p>Cx-reason Bi-dir Audio-aug Sp-feat mAP(%) <ref type="figure">Fig. 4</ref>. Study on the impact of two hyperparameters, which are ? (when n is set to 2000) and n (when ? is fixed at 0.9).</p><formula xml:id="formula_6">- - - - - 80.2 - - - ? - 82.6 ? - - ? - 88.0 ? ? - ? - 92.4 ? ? ? ? - 93.9 ? ? ? ? ? 94.2 (a) (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>We perform an ablative study to validate the contributions of individual components, namely TSM, Cx-reason (context-reasoning only with an undirected graph), Bi-dir (augmenting context-reasoning with the forward/backward graphs), Audio-aug (audio data augmentation), and Sp-feat (spatial features). We summarize the main contributions in <ref type="table">Table 4</ref>. We can observe that TSM, Bi-dir graph structure, and audio data augmentation play significant roles in boosting the detection performance. This implies that 1) retaining short-term information in the feature-encoding stage is important, 2) processing the spatial-temporal information using our graph structure is effective, and 3) the negatively sampled audio makes our model more robust to the noise. Additionally, the spatial features also bring meaningful performance gain.</p><p>In addition, we analyze the impact of two hyperparameters: ? (Section 3.2) and the number of nodes in a graph embedding process. ? controls the connectivity or the edge density in the graph construction. Specifically, larger values for ? allow us to model longer temporal correlations but increases the average degree of the nodes, thus making the system more computationally expensive. In <ref type="figure">Figure 4</ref>, we can observe that connecting too distant face-crops deteriorates the detection performance. One potential reason behind this could be that the aggregation procedure becomes too smooth due to the high degree of connectivity. Interestingly, we also found that a larger number of nodes does not always lead to higher performance. This might be because after a certain point, larger number of nodes leads to over-fitting. We perform additional experiments with different filter dimensions of the EDGE-CONV and SAGE-CONV. In <ref type="table" target="#tab_3">Table 5</ref>, we show how the detection performance and the model size change depending on the filter dimension. We can observe that increasing the filter dimension above 64 does not bring any performance gain when the model size increases significantly.</p><p>We also perform an ablation study of the input modalities. When using only the visual features, the detection performance drops significantly from 94.2% to 84.9% mAP (when using only the audio: 55.6%), which shows that both the audio and video modalities are important for this application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative analysis</head><p>In the supplementary material, we show several detection examples to provide a qualitative analysis. The selected frames have multiple faces and have a long time-span about 5-10 seconds. In all of the provided examples in the supplementary material, SPELL correctly classifies all the speakers when the counterpart fails to do. The qualitative analysis demonstrates that SPELL is effective and that it is good at modeling spatial-temporal long-term information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed SPELL -an effective graph-based approach to active speaker detection in videos. The main idea is to capture the long-term spatial and temporal relationships among the cropped faces through a graph structure that is aware of the temporal order of the faces. SPELL outperforms all the previous approaches and requires significantly less hardware resources when compared to the leading state-of-the-art method. The model we propose is also generic -it can be used to address other video understanding tasks such as action localization and audio source localization. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>is a learnable linear transformation, and ? : R ? R is a non-linear activation function applied point-wise. ? EDGE-CONV aggregation: EDGE-CONV [37] models global and local-structures by applying channel-wise symmetric aggregation operation on the edge features associated with all the edges emanating from each node. The aggregation function f i : V ? R m can be defined as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results: The green boxes indicate the active speakers and yellow indicates non-active speaker. The selected frames have a long time-span about 5-10 seconds. GT: Ground Truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Complexity comparisons of the context-reasoning stage. SPELL achieves the best performance while requiring the lowest memory and computation consumption.</figDesc><table><row><cell>Method</cell><cell>#Params(M)</cell><cell>Size(MB)</cell><cell>mAP(%)</cell></row><row><cell>ASC [2]</cell><cell>1.13</cell><cell>4.32</cell><cell>87.1</cell></row><row><cell>MAAS-TAN [19]</cell><cell>0.16</cell><cell>0.63</cell><cell>88.8</cell></row><row><cell>ASDNet [18]</cell><cell>2.56</cell><cell>9.77</cell><cell>93.5</cell></row><row><cell>SPELL (Ours)</cell><cell>0.11</cell><cell>0.45</cell><cell>94.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Comparisons of the detection performance and the model size with different filter dimensions.</figDesc><table><row><cell>Filter Dim</cell><cell>#Params(M)</cell><cell>Size(MB)</cell><cell>mAP(%)</cell></row><row><cell>16</cell><cell>0.02</cell><cell>0.10</cell><cell>93.5</cell></row><row><cell>32</cell><cell>0.05</cell><cell>0.21</cell><cell>93.9</cell></row><row><cell>64</cell><cell>0.11</cell><cell>0.45</cell><cell>94.2</cell></row><row><cell>128</cell><cell>0.29</cell><cell>1.14</cell><cell>94.1</cell></row><row><cell>256</cell><cell>0.88</cell><cell>3.38</cell><cell>94.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://research.google.com/ava/challenge.html</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Network architecture details</head><p>For clarification, network architecture of SPELL is shown in <ref type="table">Table 6</ref>. <ref type="table">Table 6</ref>. Detailed architecture of SPELL. Batch normalization <ref type="bibr" target="#b14">[15]</ref> and ReLU <ref type="bibr" target="#b24">[25]</ref> follow after each of (6), <ref type="bibr" target="#b6">(7)</ref>, and <ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs Description Dimension</head><p>( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Qualitative results</head><p>In <ref type="figure">Figure 5</ref>, we provide qualitative results of SPELL. For additional comparison, we also show the results of ASC <ref type="bibr" target="#b1">[2]</ref>, which performs the best among the approaches that have released their model weights as well as the source code. In the first example, ASC has both false positive and false negative results while SPELL is able to correctly classify all the speakers. In the second and the third examples, SPELL finds every active speaker when ASC has many false negatives. The results show that SPELL is effective and is good at modeling spatialtemporal long-term information. For more examples, please refer to the videos that are included in the subfolder.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The conversation: Deep audio-visual speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04121</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Active Speakers in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Alcazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12465" to="12474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unified graph structured models for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15662</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cross-modal supervision for learning active speaker detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="285" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10555</idno>
		<title level="m">Naver at activitynet challenge 2019-task b active speaker detection (ava)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Look who&apos;s talking: Speaker detection using video and audio correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No. 00TH8532)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1589" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hello! my name is... buffy&quot;-automatic naming of characters in tv video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: BMVC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Taking the bite out of automated naming of characters in tv video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="545" to="559" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<title level="m">Fast graph representation learning with pytorch geometric</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<title level="m">Spatio-temporal scene graphs for video dialog. arXiv e-prints pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2007</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Resource efficient 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kopuklu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunduz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>K?p?kl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taseska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Internal Conference on Computer Vision</title>
		<meeting>Internal Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MAAS: Multi-modal Assignation for Active Speaker Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le?n-Alc?zar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internal Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.01181</idno>
		<title level="m">Visual relationship forecasting in videos</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Intel labs at activitynet challenge 2022: Spell for long-term active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<ptr target="https://research.google.com/ava/2022/S2_SPELL_ActivityNet_Challenge_2022.pdf" />
	</analytic>
	<monogr>
		<title level="m">The ActivityNet Large-Scale Activity Recognition Challenge</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ego-topo: Environment affordances from egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10211</idno>
		<title level="m">Space-time crop &amp; attend: Improving cross-modal video representation learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Speaker recognition from raw waveform with sincnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1021" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ava active speaker: An audio-visual dataset for active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stopczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4492" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Crossmodal learning for audio-visual speech event localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Somandepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04358</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learnable graph inception network for emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shirian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04105</idno>
		<title level="m">Combining residual networks with lstms for lipreading</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Vision-based active speaker detection in multiparty interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stefanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Grounding Language Understanding</title>
		<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-25" />
		</imprint>
		<respStmt>
			<orgName>Royal Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Look who&apos;s talking: visual identification of the active speaker in multi-party human-robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stefanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Advancements in Social Signal Processing for Multimodal Interaction</title>
		<meeting>the 2nd Workshop on Advancements in Social Signal Processing for Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="22" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Logan: Latent graph co-attention network for weakly-supervised video moment retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3927" to="3935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-task learning for audio-visual active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The ActivityNet Large-Scale Activity Recognition Challenge</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unicon: Unified context network for robust active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3964" to="3972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A structured model for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9975" to="9984" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
