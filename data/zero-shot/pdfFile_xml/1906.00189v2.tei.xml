<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Are Anchor Points Really Indispensable in Label-Noise Learning?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">RIKEN</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">RIKEN</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">RIKEN</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Are Anchor Points Really Indispensable in Label-Noise Learning?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In label-noise learning, the noise transition matrix, denoting the probabilities that clean labels flip into noisy labels, plays a central role in building statistically consistent classifiers. Existing theories have shown that the transition matrix can be learned by exploiting anchor points (i.e., data points that belong to a specific class almost surely). However, when there are no anchor points, the transition matrix will be poorly learned, and those previously consistent classifiers will significantly degenerate. In this paper, without employing anchor points, we propose a transitionrevision (T -Revision) method to effectively learn transition matrices, leading to better classifiers. Specifically, to learn a transition matrix, we first initialize it by exploiting data points that are similar to anchor points, having high noisy class posterior probabilities. Then, we modify the initialized matrix by adding a slack variable, which can be learned and validated together with the classifier by using noisy data. Empirical results on benchmark-simulated and real-world label-noise datasets demonstrate that without using exact anchor points, the proposed method is superior to state-of-the-art label-noise learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Label-noise learning can be dated back to <ref type="bibr" target="#b0">[1]</ref> but becomes a more and more important topic recently. The reason is that, in this era, datasets are becoming bigger and bigger. Often, large-scale datasets are infeasible to be annotated accurately due to the expensive cost, which naturally brings us cheap datasets with noisy labels.</p><p>Existing methods for label-noise learning can be generally divided into two categories: algorithms that result in statistically inconsistent/consistent classifiers. Methods in the first category usually employ heuristics to reduce the side-effect of noisy labels. For example, many state-of-the-art approaches in this category are specifically designed to, e.g., select reliable examples <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref>, reweight examples <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b14">15]</ref>, correct labels <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b31">32]</ref>, employ side information <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b20">21]</ref>, and (implicitly) add regularization <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b20">21]</ref>. All those methods were reported to work empirically very well. However, the differences between the learned classifiers and the optimal ones for clean data are not guaranteed to vanish, i.e., no statistical consistency has been guaranteed.</p><p>The above issue motivates researchers to explore algorithms in the second category: risk-/classifierconsistent algorithms. In general, risk-consistent methods possess statistically consistent estimators to the clean risk (i.e., risk w.r.t. the clean data), while classifier-consistent methods guarantee the classifier learned from the noisy data is consistent to the optimal classifier (i.e., the minimizer of the clean risk) <ref type="bibr" target="#b41">[42]</ref>. Methods in this category utilize the noise transition matrix, denoting the probabilities that clean labels flip into noisy labels, to build consistent algorithms. Let Y denote the variable for the clean label,? the noisy label, and X the instance/feature. The basic idea is that given the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. arXiv:1906.00189v2 <ref type="bibr">[cs.</ref>LG] 17 Dec 2019 noisy class posterior probability P (?|X = x) = [P (? = 1|X = x), . . . , P (? = C|X = x)] (which can be learned using noisy data) and the transition matrix T (X = x) where T ij (X = x) = P (? = j|Y = i, X = x), the clean class posterior probability P (Y|X = x) can be inferred, i.e., P (Y|X = x) = (T (X = x) ) ?1 P (?|X = x). For example, loss functions are modified to ensure risk consistency, e.g., <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b25">26]</ref>; a noise adaptation layer is added to deep neural networks to design classifier-consistent deep learning algorithms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47]</ref>. Those algorithms are strongly theoretically grounded but heavily rely on the success of learning transition matrices.</p><p>Given risk-consistent estimators, one stream to learn the transition matrix is the cross-validation method (using only noisy data) for binary classification <ref type="bibr" target="#b25">[26]</ref>. However, it is prohibited for multi-class problems as its computational complexity grows exponentially to the number of classes. Besides, the current risk-consistent estimators involve the inverse of the transition matrix, making tuning the transition matrix inefficient and also leading to performance degeneration <ref type="bibr" target="#b29">[30]</ref>, especially when the transition matrix is non-invertible. Independent of risk-consistent estimators, another stream to learn the transition matrix is closely related to mixture proportion estimation <ref type="bibr" target="#b39">[40]</ref>. A series of assumptions <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31]</ref> were proposed to efficiently learn transition matrices (or mixture parameters) by only exploiting the noisy data. All those assumptions require anchor points, i.e., instances belonging to a specific class with probability exactly one or close to one. Nonetheless, without anchor points, the transition matrix could be poorly learned, which will degenerate the accuracies of existing consistent algorithms.</p><p>Therefore, in this paper, to handle the applications where the anchor-point assumptions are violated <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b40">41]</ref>, we propose a transition-revision (T -Revision) method to effectively learn transition matrices, leading to better classifiers. In a high level, we design a deep-learning-based risk-consistent estimator to tune the transition matrix accurately. Specifically, we first initialize the transition matrix by exploiting examples that are similar to anchor points, namely, those having high estimated noisy class posterior probabilities. Then, we modify the initial matrix by adding a slack variable, which will be learned and validated together with the classifier by using noisy data only. Note that given true transition matrix, the proposed estimator will converge to the classification risk w.r.t. clean data by increasing the size of noisy training examples. Our heuristic for tuning the transition matrix is that a favorable transition matrix would make the classification risk w.r.t. clean data small. We empirically show that the proposed T -Revision method will enable tuned transition matrices to be closer to the ground truths, which explains why T -Revision is much superior to state-of-the-art algorithms in classification.</p><p>The rest of the paper is organized as follows. In Section 2 we review label-noise learning with anchor points. In Section 3, we discuss how to learn the transition matrix and classifier without anchor points. Experimental results are provided in Section 4. Finally, we conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Label-Noise Learning with Anchor Points</head><p>In this section, we briefly review label-noise learning when there are anchor points.</p><p>Preliminaries Let D be the distribution of a pair of random variables (X, Y ) ? X ? {1, 2, . . . , C}, where the feature space X ? R d and C is the size of label classes. Our goal is to predict a label y for any given instance x ? X . However, in many real-world classification problems, training examples drawn independently from distribution D are unavailable. Before being observed, their true labels are independently flipped and what we can obtain is a noisy training sample {(X i ,? i )} n i=1 , where? denotes the noisy label. LetD be the distribution of the noisy random variables (X,? ) ? X ? {1, 2, . . . , C}.</p><p>Transition matrix The random variables? and Y are related through a noise transition matrix T ? [0, 1] C?C <ref type="bibr" target="#b7">[8]</ref>. Generally, the transition matrix depends on instances, i.e., T ij (X = x) = P (? = j|Y = i, X = x). Given only noisy examples, the instance-dependent transition matrix is nonidentifiable without any additional assumption. For example,</p><formula xml:id="formula_0">P (? = j|X = x) = C i=1 T ij (X = x)P (Y = i|X = x) = C i=1 T ij (X = x)P (Y = i|X = x) are both valid, when T ij (X = x) = T ij (X = x)P (Y = i|X = x)/P (? = i|X = x).</formula><p>In this paper, we study the class-dependent and instance-independent transition matrix, i.e., P (? = j|Y = i, X = x) = P (? = j|Y = i), which is identifiable under mild conditions and on which the vast majority of current methods focus <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistent algorithms</head><p>The transition matrix bridges the class posterior probabilities for noisy and clean data, i.e., P (? = j|X = x) = C i=1 T ij P (Y = i|X = x). Thus, it has been exploited to build consistent algorithms. Specifically, it has been used to modify loss functions to build risk-consistent estimators, e.g., <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b29">30]</ref>, and has been used to correct hypotheses to build classifier-consistent algorithms, e.g., <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47]</ref>. Note that an estimator is risk-consistent if, by increasing the size of noisy examples, the empirical risk calculated by noisy examples and the modified loss function will converge to the expected risk calculated by clean examples and the original loss function. Similarly, an algorithm is classifier-consistent if, by increasing the size of noisy examples, the learned classifier will converge to the optimal classifier learned by clean examples. Definitions of the expected and empirical risks can be found in Appendix B, where we further discuss how consistent algorithms work.</p><p>Anchor points The successes of consistent algorithms rely on firm bridges, i.e., accurately learned transition matrices. To learn transition matrices, the concept of anchor point was proposed <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref>. Anchor points are defined in the clean data domain, i.e., an instance x is an anchor point for the class i if P (Y = i|X = x) is equal to one or close to one 1 . Given an x, if P (Y = i|X = x) = 1, we have that for k = i, P (Y = k|X = x) = 0. Then, we have</p><formula xml:id="formula_1">P (? = j|X = x) = C k=1 T kj P (Y = k|X = x) = T ij .<label>(1)</label></formula><p>Namely, T can be obtained via estimating the noisy class posterior probabilities for anchor points <ref type="bibr" target="#b46">[47]</ref>. However, the requirement of given anchor points is a bit strong. Thus, anchor points are assumed to exist but unknown in datasets, which can be identified either theoretically <ref type="bibr" target="#b21">[22]</ref> or heuristically <ref type="bibr" target="#b29">[30]</ref>. Transition matrix learning is also closely related to mixture proportion estimation <ref type="bibr" target="#b39">[40]</ref>, which is independent of classification. By giving only noisy data, to ensure the learnability and efficiency of learning transition matrices (or mixture parameters), a series of assumptions were proposed, e.g., irreducibility <ref type="bibr" target="#b35">[36]</ref>, anchor point <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref>, and separability <ref type="bibr" target="#b30">[31]</ref>. All those assumptions require anchor points or instances belonging to a specific class with probability one or approaching one.</p><p>When there are no anchor points in datasets/data distributions, all the above mentioned methods will lead to inaccurate transition matrices, which will degenerate the performances of current consistent algorithms. This motivates us to investigate how to maintain the efficacy of those consistent algorithms without using exact anchor points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Label-Noise Learning without Anchor Points</head><p>This section presents a deep-learning-based riskconsistent estimator for the classification risk w.r.t. clean data. We employ this estimator to tune the transition matrix effectively without using anchor points, which finally leads to better classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>According to Eq. (1), to learn the transition matrix, P (?|X = x) needs to be estimated and anchor points need to be given. Note that learning P (?|X = x) may introduce error. Even worse, when there are no anchor points, it will be problematic if we use existing methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31]</ref> to learn transition matrices. For example, let P (Y|X = x i ) be the i-th column of a matrix L, i = 1, . . . , C. If x i is an anchor point for the i-th class, then L is an identity matrix. According to Eq. (1), if we use x i as an anchor point for the i-th class while P (Y = i|X = x i ) = 1 (e.g., the identified instances in <ref type="bibr" target="#b29">[30]</ref> are not guaranteed to be anchor points), the learned transition matrix would be T L, where L is a non-identity matrix. This means that transition matrices will be inaccurately estimated.</p><p>Based on inaccurate transition matrices, the accuracy of current consistent algorithms will significantly degenerate. To demonstrate this, <ref type="figure" target="#fig_0">Figure 1</ref> shows that given a noisy class posterior probability P (?|X = x), even if the transition matrix changes slightly by two entries, e.g., T ?T 1 / T 1 = 0.02 where T andT are defined in <ref type="figure" target="#fig_0">Figure 1</ref> and T 1 = ij |T ij |, the inferred class posterior probability for the clean data may lead to an incorrect classification. Since anchor points require clean class posterior probabilities to be or approach one, which is quite strong to some real-world applications <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b40">41]</ref>, we would like to study how to maintain the performances of current consistent algorithms when there are no anchor points and then transition matrices are inaccurately learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Risk-consistent estimator</head><p>Intuitively, the entries of transition matrix can be tuned by minimizing the risk-consistent estimator, since the estimator is asymptotically identical to the expected risk for the clean data and that a favorable transition matrix should make the clean expected risk small. However, existing riskconsistent estimators involve the inverse of transition matrix (more details are provided in Appendix B), which degenerates classification performances <ref type="bibr" target="#b29">[30]</ref> and makes tuning the transition matrix ineffectively. To address this, we propose a risk-consistent estimator that does not involve the inverse of the transition matrix.</p><p>The inverse of transition matrix is involved in risk-consistent estimators, since the noisy class posterior probability P (?|X = x) and the transition matrix are explicitly or implicitly used to infer the clean class posterior probability P (Y|X = x), i.e., P (Y|X = x) = (T ) ?1 P (?|X = x). To avoid the inverse in building risk-consistent estimators, we directly estimate P (Y|X = x) instead of inferring it through P (?|X = x). Thanks to the equation T P (Y|X = x) = P (?|X = x), P (Y|X = x) and P (?|X = x) could be estimated at the same time by adding the true transition matrix to modify the output of the softmax function, e.g., <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b29">30]</ref>. Specifically, P (?|X = x) can be learned by exploiting the noisy data, as shown in <ref type="figure" target="#fig_1">Figure 2</ref> by minimizing the unweighted loss</p><formula xml:id="formula_2">R n (f ) = 1/n n i=1 (f (X i ),? i ), where (f (X),? )</formula><p>is a loss function <ref type="bibr" target="#b24">[25]</ref>. LetT + ?T be the true transition matrix, i.e.,T + ?T = T . Due to P (?|X = x) = T P (Y|X = x), the output of the softmax function g(x) =P (Y|X = x) before the transition matrix is an approximation for P (Y|X = x). However, the learned g(x) =P (Y|X = x) by minimizing the unweighted loss may perform poorly if the true transition matrix is inaccurately learned as explained in the motivation.</p><p>If having P (Y|X = x) and P (?|X = x), we could employ the importance reweighting technique <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref> to rewrite the expected risk w.r.t. clean data without involving the inverse of transition matrix. Specifically,</p><formula xml:id="formula_3">R(f ) = E (X,Y )?D [ (f (X), Y )] = x i P D (X = x, Y = i) (f (x), i)dx = x i PD(X = x,? = i) P D (X = x,? = i) PD(X = x,? = i) (f (x), i)dx = x i PD(X = x,? = i) P D (? = i|X = x) PD(? = i|X = x) (f (x), i)dx (2) = E (X,Y )?D [? (f (X), Y )],</formula><p>where D denotes the distribution for clean data,D for noisy data,? (f (x), i) = independent of instances. In the rest of the paper, we have omitted the subscript for P when no confusion is caused. Since P (?|X = x) = T P (Y|X = x) and that the diagonal entries of (learned) transition matrices for label-noise learning are all much larger than zero, P D (? = i|X = x) = 0 implies PD(? = i|X = x) = 0, which also makes the proposed importance reweighting method stable without truncating the importance ratios.</p><formula xml:id="formula_4">P D (? =i|X=x) PD(? =i|X=x) (f (x), i),</formula><p>Eq. (2) shows that the expected risk w.r.t. clean data and the loss (f (x), i) is equivalent to an expected risk w.r.t. noisy data and a reweighted loss, i.e., P D (? =i|X=x) PD(? =i|X=x) (f (x), i). The empirical counterpart of the risk in the rightmost-hand side of Eq. (2) is therefore a risk-consistent estimator for label-noise learning. We exploit a deep neural network to build this counterpart. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we use the output of the softmax function g(x) to approximate P (Y|X = x), i.e., g(x) =P (Y|X = x) ? P (Y|X = x). Then, T g(x) (or (T + ?T ) g(x) in the figure) is an approximation for P (?|X = x), i.e., T g(x) =P (?|X = x) ? P (?|X = x). By employin? P (Y = y|X = x)/P (? = y|X = x) as weight, we build the risk-consistent estimator as</p><formula xml:id="formula_5">R n,w (T, f ) = 1 n n i=1 g? i (X i ) (T g)? i (X i ) (f (X i ),? i ),<label>(3)</label></formula><p>where f (X) = arg max j?{1,...,C} g j (X), g j (X) is an estimate for P (Y = j|X), and the subscript w denotes that the loss function is weighted. Note that if the true transition matrix T is given, R n,w (T, f ) only has one argument g to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation and the T -revision method</head><p>When the true transition matrix T is unavailable, we propose to useR n,w (T + ?T, f ) to approximate R(f ), as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. To minimizeR n,w (T + ?T, f ), a two-stage training procedure is proposed. Stage 1: first learn P (?|X = x) by minimizing the unweighted loss without a noise adaption layer and initializeT by exploiting examples that have the highest learnedP (?|X = x); Stage 2: modify the initializationT by adding a slack variable ?T and learn the classifier and ?T by minimizing the weighted loss. The procedure is called the Weighted T -Revision method and is summarized in Algorithm 1. It is worthwhile to mention that all anchor points based consistent estimators for label-noise learning have a similar two-stage training procedure. Specifically, with one stage to learn P (?|X = x) and the transition matrix and a second stage to learn the classifier for the clean data.</p><p>The proposed T -revision method works because we learn ?T by minimizing the risk-consistent estimator, which is asymptotically equal to the expected risk w.r.t. clean data. The learned slack variable can also be validated on the noisy validation set, i.e., to check ifP (?|X = x) fits the validation set. The philosophy of our approach is similar to that of the cross-validation method. However, the proposed method does not need to try different combinations of parameters (?T is learned) and thus is much more computationally efficient. Note that the proposed method will also boost the performances of consistent algorithms even there are anchor points as the transition matrices and classifiers are jointly learned. Note also that if a clean validation set is available, it can be used to better initialize the transition matrix, to better validate the slack variable ?T , and to fine-tune the deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Generalization error</head><p>While we have discussed the use of the proposed estimator for evaluating the risk w.r.t clean data, we theoretically justify how it generalizes for learning classifiers. Assume the neural network has d layers, parameter matrices W 1 , . . . , W d , and activation functions ? 1 , . . . , ? d?1 for each layer. Let denote the mapping of the neural network by h :</p><formula xml:id="formula_6">x ? W d ? d?1 (W d?1 ? d?2 (. . . ? 1 (W 1 x))) ? R C .</formula><p>Then, the output of the softmax is defined by</p><formula xml:id="formula_7">g i (x) = exp (h i (x))/ C k=1 exp (h k (x)), i = 1, . . . , C. Letf = arg max i?{1,.</formula><p>..,C}?i be the classifier learned from the hypothesis space F determined by the real-valued parameters of the neural network, i.e.,f = arg min f ?FRn,w (f ).</p><p>To derive a generalization bound, as the common practice <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>, we assume that instances are upper bounded by B, i.e., x ? B for all x ? X , and that the loss function is L-Lipschitz continuous w.r.t. f (x) and upper bounded by M , i.e., for any f 1 , f 2 ? F and any (x,?),</p><formula xml:id="formula_8">| (f 1 (x),?) ? (f 2 (x),?)| ? L|f 1 (x) ? f 2 (x)|, and for any (x,?), (f (x),?) ? M .</formula><p>Theorem 1 Assume the Frobenius norm of the weight matrices W 1 , . . . , W d are at most M 1 , . . . , M d . Let the activation functions be 1-Lipschitz, positive-homogeneous, and applied element-wise (such as the ReLU). Let the loss function be the cross-entropy loss, i.e., (f (x),?) = ? C i=1 1 {?=i} log(g i (x)). Letf and ?T be the learned classifier and slack variable. Assume ?T is searched from a space of ?T constituting valid transition matrices 2 , i.e., ??T and ?i = j, T ij + ?T ij ? 0 andT ii + ?T ii &gt;T ij + ?T ij . Then, for any ? &gt; 0, with probability at least 1 ? ?,</p><formula xml:id="formula_9">E[R n,w (T + ?T ,f )] ?R n,w (T + ?T ,f ) ? 2BCL( ? 2d log 2 + 1)? d i=1 M i ? n + CM log 1/? 2n .</formula><p>A detailed proof is provided in Appendix C. The factor ( ? 2d log 2 + 1)? d i=1 M i is induced by the hypothesis complexity of the deep neural network <ref type="bibr" target="#b9">[10]</ref> (see Theorem 1 therein), which could be improved <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b15">16]</ref>. Although the proposed reweighted loss is more complex than the traditional unweighted loss function, we have derived a generalization error bound not larger than those derived for the algorithms employing the traditional loss <ref type="bibr" target="#b24">[25]</ref> (can be seen by Lemma 2 in the proof of the theorem). This shows that the proposed Algorithm 1 does not need a larger training sample to achieve a small difference between training error (R n,w (T + ?T ,f )) and test error (E[R n,w (T + ?T ,f )]). Also note that deep learning is powerful in yielding a small training error. If the training sample size n is large, then the upper bound in Theorem 1 is small, which implies a small E[R n,w (T + ?T ,f )] and justifies why the proposed method will have small test errors in the experiment section. Meanwhile, in the experiment section, we show that the proposed method is much superior to the state-of-the-art methods in classification accuracy, implying that the small generalization error is not obtained at the cost of enlarging the approximation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets We verify the effectiveness of the proposed method on three synthetic noisy datasets, i.e., MNIST <ref type="bibr" target="#b18">[19]</ref>, CIFAR-10 <ref type="bibr" target="#b17">[18]</ref>, and CIFAR-100 <ref type="bibr" target="#b17">[18]</ref>, and one real-world noisy dataset, i.e., clothing1M <ref type="table">Table 1</ref>: Means and standard deviations (percentage) of classification accuracy. Methods with "-A" means that they run on the intact datasets without removing possible anchor points; Methods with "-R" means that the transition matrix used is revised by a revision ?T .   <ref type="bibr" target="#b43">[44]</ref>. MNIST has 10 classes of images including 60,000 training images and 10,000 test images. CIFAR-10 has 10 classes of images including 50,000 training images and 10,000 test images. CIFAR-100 also has 50,000 training images and 10,000 test images, but 100 classes. For all the datasets, we leave out 10% of the training examples as a validation set. The three datasets contain clean data. We corrupted the training and validation sets manually according to true transition matrices T . Specifically, we employ the symmetry flipping setting defined in Appendix D. Sym-50 generates heavy label noise and leads almost half of the instances to have noisy labels, while Sym-20 generates light label noise and leads around 20% of instances to have label noise. Note that the pair flipping setting <ref type="bibr" target="#b13">[14]</ref>, where each row of the transition matrix only has two non-zero entries, has also been widely studied. However, for simplicity, we do not pose any constraint on the slack variable ?T to achieve specific speculation of the transition matrix, e.g., sparsity <ref type="bibr" target="#b12">[13]</ref>. We leave this for future work.</p><p>Besides reporting the classification accuracy on test set, we also report the discrepancy between the learned transition matrixT + ?T and the true one T . All experiments are repeated five times on those three datasets. Clothing1M consists of 1M images with real-world noisy labels, and additional 50k, 14k, 10k images with clean labels for training, validation, and testing. We use the 50k clean data to help initialize the transition matrix as did in the baseline <ref type="bibr" target="#b29">[30]</ref>.</p><p>Network structure and optimization For fair comparison, we implement all methods with default parameters by PyTorch on NVIDIA Tesla V100. We use a LeNet-5 network for MNIST, a ResNet-18 network for CIFAR-10, a ResNet-34 network for CIFAR-100. For learning the transition matrix T in the first stage, we follow the optimization method in <ref type="bibr" target="#b29">[30]</ref>. During the second stage, we first use SGD with momentum 0.9, weight decay 10 ?4 , batch size 128, and an initial learning rate of 10 ?2 to initialize the network. The learning rate is divided by 10 after the 40th epoch and 80th epoch. 200 epochs are set in total. Then, the optimizer and learning rate are changed to Adam and 5 ? 10 ?7 to learn the classifier and slack variable. For CIFAR-10 and CIFAR-100, we perform data augmentation by horizontal random flips and 32?32 random crops after padding 4 pixels on each side. For clothing1M, we use a ResNet-50 pre-trained on ImageNet. Follow <ref type="bibr" target="#b29">[30]</ref>, we also exploit the 1M noisy data and 50k clean data to initialize the transition matrix. In the second stage, for initialization, we use SGD with momentum 0.9, weight decay 10 ?3 , batch size 32, and run with learning rates 10 ?3 and 10 ?4 for 5 epochs each. For learning the classifier and slack variable, Adam is used and the learning rate is changed to 5 ? 10 ?7 . <ref type="table">Table 3</ref>: Means and standard deviations (percentage) of classification accuracy on MNIST with different label noise levels. Methods with "-A" means that they run on the intact datasets without removing possible anchor points; Methods with "-R" means that the transition matrix used is revised by a revision ?T ; Methods with "-N/A" means instances with high estimated P (Y |X) are removed from the dataset.  Baselines We compare the proposed method with state-of-the-art approaches. Specifically, we compare with the following three inconsistent but well-designed algorithms: Decoupling <ref type="bibr" target="#b23">[24]</ref>, MentorNet <ref type="bibr" target="#b14">[15]</ref>, and Co-teaching <ref type="bibr" target="#b13">[14]</ref>, which free the learning of transition matrices. To compare with consistent estimators, we set Forward <ref type="bibr" target="#b29">[30]</ref>, a classifier-consistent algorithm, and the importance reweighting method (Reweight), a risk-consistent algorithm, as baselines. The risk-consistent estimator involving the inverse of transition matrix, e.g., Backward in <ref type="bibr" target="#b29">[30]</ref>, has not been included in the comparison, because it has been reported to perform worse than the Forward method <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison for classification accuracy</head><p>The importance of anchor points To show the importance of anchor points, we modify the datasets by moving possible anchor points, i.e., instances with large estimated class posterior probability P (Y |X), before corrupting the training and validation sets. As the MNIST dataset is simple, we removed 40% of the instances with the largest estimated class posterior probabilities in each class. For CIFAR-10 and CIFAR-100, we removed 20% of the instances with the largest estimated class posterior probabilities in each class. To make it easy for distinguishing, we mark a "-A" in the algorithm's name if it runs on the original intact datasets, and mark a "-N/A" in the algorithm's name if it runs on those modified datasets. <ref type="table">Table 1</ref> with Decoupling-N/A, MentorNet-N/A, and Co-teaching-N/A in <ref type="table" target="#tab_1">Table 2</ref>, we can find that on MNIST, the methods with "-N/A" work better; while on CIFAR-10 and CIFAR-100, the methods with "-A" work better. This is because those methods are independent of transition matrices but dependent of dataset properties. Removing possible anchors points may not always lead to performance degeneration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing Decoupling-A, MentorNet-A, and Co-teaching-A in</head><p>Comparing Forward-A and Reweight-A with Forward-N/A and Reweight-N/A, we can find that the methods without anchor points, i.e., with "-N/A", degenerate clearly. The degeneration on MNIST is slight because the dataset can be well separated and many instances have high class posterior probability even in the modify dataset. Those results show that, without anchor points, the consistent algorithms will have performance degeneration. Specifically, on CIFAR-100, the methods with "-N/A" have much worse performance than the ones with "-A", with accuracy dropping at least 4%.</p><p>To discuss the model performances on MNIST with more label noise, we raise the noise rates to 60%, 70%, 80%. Other experiment settings are unchanged. The results are presented in <ref type="table">Table 3</ref>. We can see that the proposed model outperforms the baselines more significantly as the noise rate grows. <ref type="table">Table 1</ref> and comparing Forward-N/A with Reweight-N/A in <ref type="table" target="#tab_1">Table 2</ref>, it can be seen that the proposed Reweight method, a risk-consistent estimator not involving the inverse of transition matrix, works slightly better than or is comparable to Forward, a classifier-consistent algorithm. Note that in <ref type="bibr" target="#b29">[30]</ref>, it is reported that Backward, a risk-consistent estimator which involves the inverse of the transition matrix, works worse than Forward, the classifier-consistent algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Risk-consistent estimator vs. classifier-consistent estimator Comparing Forward-A with Reweight-A in</head><p>The importance of T -revision Note that for fair comparison, we also set it as a baseline to modify the transition matrix in Forward. As shown in Tables 1 and 2, methods with "-R" means that they use the proposed T -revision method, i.e., modify the learnedT by adding ?T . Comparing the results in Tables 1 and 2, we can find that the T -revision method significantly outperforms the others. Among them, the proposed Reweight-R works significantly better than the baseline Forward-R. We can find that the T -Revision method boosts the classification performance even without removing possible anchor points. The rationale behind this may be that the network, transition matrix, and classifier are jointly learned and validated and that the identified anchor points are not reliable.</p><p>Comparison on real-world dataset The proposed T -revision method significantly outperforms the baselines as shown in <ref type="table" target="#tab_3">Table 4</ref>, where the highest accuracy is bold faced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison for estimating transition matrices</head><p>To show that the proposed risk-consistent estimator is more effective in modifying the transition matrix, we plot the estimation error for the transition matrix, i.e., T ?T ? ?T 1 / T 1 . In <ref type="figure" target="#fig_5">Figure  4</ref>, we can see that for all cases, the proposed risk-consistent-estimator-based revision leads to smaller estimator errors than the classifier-consistent algorithm based method (Forward-R), showing that the risk-consistent estimator is more powerful in modifying the transition matrix. This also explains why the proposed method works better. We provide more discussions about <ref type="figure" target="#fig_5">Figure 4</ref> in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents a risk-consistent estimator for label-noise learning without involving the inverse of transition matrix and a simple but effective learning paradigm called T -revision, which trains deep neural networks robustly under noisy supervision. The aim is to maintain effectiveness and efficiency of current consistent algorithms when there are no anchor points and then the transition matrices are poorly learned. The key idea is to revise the learned transition matrix and validate the revision by exploiting a noisy validation set. We conduct experiments on both synthetic and real-world label noise data to demonstrate that the proposed T -revision can significantly help boost the performance of label-noise learning. In the future, we will extend the work in the following aspects. First, how to incorporate some prior knowledge of the transition matrix, e.g., sparsity, into the end-to-end learning system. Second, how to recursively learn the transition matrix and classifier as our experiments show that transition matrices can be refined.</p><p>A Label-noise learning, or noisy-label learning, that is the point Note that the title of this paper is a question "are anchor points really indispensable in label-noise learning" but not "are anchor points really indispensable in noisy-label learning". Here, we explain why in order to make sense it must be label-noise learning. At first glance, label-noise learning may sound like we are learning the label noise, but this is exactly what we have implied in the title.</p><p>Generally speaking, the two names are synonyms of learning with noisy labels-this is the title of <ref type="bibr" target="#b25">[26]</ref> where the first statistically consistent learning method was proposed for training classifiers with noisy labels. For the family of consistent learning methods, the estimation of the transition matrix T is always necessary. In fact, any such method requires three components:</p><p>? a label corruption process parameterized by T ,</p><p>? an estimator of T , and ? a statistical or algorithmic correction using the estimated T .</p><p>As a result, T is also a target of learning, that is, the label noise is both a target to be learned and a source from which we learn. This is learning with noisy labels, more than learning from noisy labels.</p><p>For the first component, we assume the class-conditional noise model <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>; for the third component, we rely on importance reweighting for learning with noisy labels <ref type="bibr" target="#b21">[22]</ref>. Our novelty and major contribution is the second component, where we relax a requirement in existing consistent learning methods, namely we should have a certain amount of anchor points for estimating T accurately, so that the correction using the estimated T can be performed well.</p><p>While it is necessary to estimate T for consistent learning methods, it is not the case for inconsistent learning methods. For instance, sample selection methods try to remove mislabeled data, and label correction methods try to fix the wrong labels of mislabeled data. None of them estimate T so that none of them ever need the existence of anchor points.</p><p>Therefore, if we ask "are anchor points really indispensable in label-noise learning" where we are learning, modeling or estimating the label noise, the answer was yes previously and is no currently. Nevertheless, if we ask "are anchor points really indispensable in noisy-label learning" where some inconsistent learning method is employed without estimating the label noise, the answer has already been known to be no. That is the point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B How consistent algorithms work</head><p>The aim of multi-class classification is to learn a hypothesis f that predicts labels for given instances. Typically, the hypothesis is of the following form: f (x) = arg max i?{1,2,...,C} g i (x), where g i (x) is an estimate of P (Y = i|X = x). Let define the expected risk of employing f as</p><formula xml:id="formula_10">R(f ) = E (X,Y )?D [ (f (X), Y )].<label>(4)</label></formula><p>The optimal hypothesis to learn is the one that minimizes the risk R(f ). Usually, the distribution D is unknown. The optimal hypothesis is approximated by the minimizer of an empirical counterpart of R(f ), i.e., the empirical risk</p><formula xml:id="formula_11">R n (f ) = 1 n n i=1 (f (x i ), y i ).<label>(5)</label></formula><p>The empirical risk R n (f ) is risk-consistent w.r.t. all loss functions, i.e., R n (f ) ? R(f ) as n ? ?.</p><p>Note that in the main paper, we have treated the training sample {X i ,? i } n i=1 as iid variables to derive the generalization bound.</p><p>If the loss function is zero-one loss, i.e., (f (x), y) = 1 {f (x) =y} where 1 {?} is the indicator function and that the predefined hypothesis class <ref type="bibr" target="#b24">[25]</ref> is large enough, the optimal hypothesis that minimizing R(f ) is identical to the Bayes classifier <ref type="bibr" target="#b2">[3]</ref>, i.e., f ? (x) = arg max i?{1,2,...,C}</p><formula xml:id="formula_12">P (Y = i|X = x).<label>(6)</label></formula><p>Many frequently used loss functions are proven to be classification-calibrated <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34]</ref>, which means they will lead to classifiers having the same predictions as the classifier learned by using zero-one loss if the training sample size is sufficiently large <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b24">25]</ref>. In other words, the approximation, i.e., arg min R n (f ), could converge to the optimal hypothesis by increasing the sample size n and the corresponding estimator is therefore classifier-consistent. Note that risk-consistent algorithm is also classifier-consistent. However, a classifier-consistent algorithm may not be risk-consistent.</p><p>Given only the noisy training sample {(X i ,? i )} n i=1 , we have a noisy version of the empirical risk as</p><formula xml:id="formula_13">R n (f ) = 1 n n i=1 (f (X i ),? i ).<label>(7)</label></formula><p>The learned g(X) can be used to approximate P (?|X). According to the definition of transition matrix, we have that P (?|X) = T P (Y|X), implying that if we let h(X) = arg max i?{1,2,...,C}</p><formula xml:id="formula_14">(T g) i (X),<label>(8)</label></formula><p>minimizingR</p><formula xml:id="formula_15">n (h) = 1 n n i=1 (h(X i ),? i )<label>(9)</label></formula><p>by using only noisy data will lead to a classifier-consistent algorithm. In other words, arg max i?{1,2,...,C} g i (x) in the algorithm will converge to the optimal classifier for clean data by increasing the noisy sample size. That's why noise adaption layer has been widely used in deep learning to modify the softmax function (i.e., g(x)) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>If the transition matrix is invertable, the equation P (Y|X) = (T ) ?1 P (?|X) has been explored to design risk-consistent estimator for R(f ), e.g., <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>. The basic idea is to modify the loss function (f (X),? ) to be? (f (X),? ) such that for X and Y ,</p><formula xml:id="formula_16">E? [? (f (X),? )] = (f (X), Y )<label>(10)</label></formula><p>and thus</p><formula xml:id="formula_17">E (X,Y,? )? (f (X),? ) = R(f ).<label>(11)</label></formula><p>Specifically, let </p><formula xml:id="formula_18">L(f (X), Y) = [ (f (X), Y = 1), . . . , (f (X), Y = C)]<label>(12)</label></formula><p>The losses? will lead to risk-consistent estimator because</p><formula xml:id="formula_20">E? |Y [L(f (X),?)] = T L (f (X),?) = L(f (X), Y).<label>(14)</label></formula><p>Risk-consistent algorithms are also classifier-consistent, but have some unique properties than classifier-consistent algorithms, e.g., can be used to tune hyper-parameter. However, the current risk-consistent estimators contain the inverse of transition matrix, making parameter tuning inefficient and leading to performance degeneration. Our proposed risk-consistent estimator overcome the aforementioned issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof of Theorem 1</head><p>We have definedR</p><formula xml:id="formula_21">n,w (T + ?T, f ) = 1 n n i=1 g? i (X i ) ((T + ?T ) g)? i (X i ) (f (X i ),? i ),<label>(15)</label></formula><p>where f (X) = arg max i?{1,...,C} g i (X). Let S = {(X 1 ,? 1 ), . . . , (X n ,? n )}, S i = {(X 1 ,? 1 ), . . . , (X i?1 ,? i?1 ), (X i ,? i ), (X i+1 ,? i+1 ), . . . , (X n ,? n )}, and</p><formula xml:id="formula_22">?(S) = sup ?T,f (R n,w (T + ?T, f ) ? E S [R n,w (T + ?T, f )]).<label>(16)</label></formula><p>Lemma 1 Let ?T andf be the learned slack variable and classifier respectively. Assume the learned transition matrix is valid, i.e.,T ij + ?T ij ? 0 for all i, j andT ii + ?T ii &gt;T ij + ?T ij for all j = i. For any ? &gt; 0, with probability at least 1 ? ?, we have</p><formula xml:id="formula_23">E[R n,w (T + ?T ,f )] ?R n,w (T + ?T ,f ) ? E[?(S)] + CM log 1/? 2n .<label>(17)</label></formula><p>Detailed proof of Lemma 1 is provided in Section C.1.</p><p>Using the same trick to derive Rademacher complexity <ref type="bibr" target="#b3">[4]</ref>, we have</p><formula xml:id="formula_24">E[?(S)] ? 2E sup ?T,f 1 n n i=1 ? i g? i (X i ) ((T + ?T ) g)? i (X i ) (f (X i ),? i ) ,<label>(18)</label></formula><p>where ? 1 , . . . , ? n are i.i.d. Rademacher random variables.</p><p>We can upper bound the right hand part of the above inequality by the following lemma.</p><formula xml:id="formula_25">Lemma 2 E sup ?T,f 1 n n i=1 ? i g? i (X i ) ((T + ?T ) g)? i (X i ) (f (X i ),? i ) ? E sup f 1 n n i=1 ? i (f (X i ),? i ) . (19)</formula><p>Note that Lemma 2 is not an application of Talagrand Contraction Lemma <ref type="bibr" target="#b19">[20]</ref>. Detailed proof of Lemma 2 is provided in Section C.2.</p><p>Recall that f = arg max i?{1,...,C} g i is the classifier, where g is the output of the softmax function, i.e., g i (X) = exp (h i (X))/ C k=1 exp (h k (X), i = 1, . . . , C, and h(X) is defined by a d-layer neural network, i.e., h :</p><formula xml:id="formula_26">X ? W d ? d?1 (W d?1 ? d?2 (. . . ? 1 (W 1 X))) ? R C , W 1 , .</formula><p>. . , W d are the parameter matrices, and ? 1 , . . . , ? d?1 are activation functions. To further upper bound the Rademacher complexity, we need to consider the Lipschitz continuous property of the loss function w.r.t. to h(X). To avoid more assumption, We discuss the widely used cross-entropy loss, i.e.,</p><formula xml:id="formula_27">(f (X),? ) = ? C i=1 1 {? =i} log(g i (X)).<label>(20)</label></formula><p>We can further upper bound the Rademacher complexity by the following lemma.</p><formula xml:id="formula_28">Lemma 3 E sup f 1 n n i=1 ? i (f (X i ),? i ) ? CLE sup h?H 1 n n i=1 ? i h(X i ) ,<label>(21)</label></formula><p>where H is the function class induced by the deep neural network.</p><p>Detailed proof of Lemma 3 is provided in Section C.3.</p><p>Note that E sup h?H 1 n n i=1 ? i h(X i ) measures the hypothesis complexity of deep neural networks, which has been widely studied recently <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28]</ref>. Specifically, <ref type="bibr" target="#b9">[10]</ref> proved the following theorem (Theorem 1 therein). </p><formula xml:id="formula_29">E sup h?H 1 n n i=1 ? i h(X i ) ? B( ? 2d log 2 + 1)? d i=1 M i ? n .<label>(22)</label></formula><p>Theorem 1 follows by combining Lemmas 1, 2, 3, and Theorem 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Proof of Lemma 1</head><p>We employ McDiarmid's concentration inequality <ref type="bibr" target="#b6">[7]</ref> to prove the lemma. We first check the bounded difference property of ?(S), e.g.,</p><formula xml:id="formula_30">?(S) ? ?(S i ) ? sup ?T,f 1 n g? i (X i ) (f (X i ),? i ) ((T + ?T ) g)? i (X i ) ? g? i (X i ) (f (X i ),? i ) ((T + ?T ) g)? i (X i ) .<label>(23)</label></formula><p>Before further upper bounding the above difference, we show that the weighted loss is upper bounded by CM . Specifically, we have assume the learned transition matrix is valid, i.e.,T ij +?T ij ? 0 for all i, j andT ii + ?T ii &gt;T ij + ?T ij for all j = i. Thus g? (X) ((T +?T ) g)? (X) ? 1/ min i (T ii + ?T ii ) ? C for any (X,? ) and?. Then, we can conclude that the weighted loss is upper bounded by CM and that</p><formula xml:id="formula_31">?(S) ? ?(S i ) ? CM n .<label>(24)</label></formula><p>Similarly, we could prove that ?(S i ) ? ?(S) ? CM n . By employing McDiarmid's concentration inequality, for any ? &gt; 0, with probability at least 1 ? ?, we have</p><formula xml:id="formula_32">?(S) ? E[?(S)] ? CM log(1/?) 2n .<label>(25)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Lemma 2</head><p>Given the learned transition matrix is valid, we have shown that g? (X)</p><formula xml:id="formula_33">((T +?T ) g)? (X) ? 1/ min i (T ii + ?T ii ) ? C for all (X,? ) in the proof of Lemma 1.</formula><p>Lemma 2 holds of we could prove the following inequality</p><formula xml:id="formula_34">E ? sup ?T,f 1 n n i=1 ? i g? i (X i ) ((T + ?T ) g)? i (X i ) (f (X i ),? i ) ? E ? sup f 1 n n i=1 ? i (f (X i ),? i ) .<label>(26)</label></formula><p>Note that</p><formula xml:id="formula_35">E ? sup ?T,f 1 n n i=1 ? i g? i (X i ) ((T + ?T ) g)? i (X i ) (f (X i ),? i ) = E ?1,...,?n?1 E ?n sup ?T,f 1 n n i=1 ? i g? i (X i ) ((T + ?T ) g)? i (X i ) (f (X i ),? i ) .<label>(27)</label></formula><p>Let</p><formula xml:id="formula_36">s n?1 (?T, f ) = n?1 i=1 ? i g? i (Xi) ((T +?T ) g)? i (Xi) (f (X i ),? i ).</formula><p>By definition of the supremum, for any &gt; 0, there exist (?T, f 1 ) and (?T, f 2 ) such that g? n (X n )</p><formula xml:id="formula_37">((T + ?T ) g)? n (X n ) (f 1 (X n ),? n ) + s n?1 (?T, f 1 ) ? (1 ? ) sup ?T,f g? n (X n ) ((T + ?T ) g)? n (X n ) (f (X n ),? n ) + s n?1 (?T, f )<label>(28)</label></formula><p>and ? g? n (X n )</p><formula xml:id="formula_38">((T + ?T ) g)? n (X n ) (f 2 (X n ),? n ) + s n?1 (?T, f 2 ) ? (1 ? ) sup ?T,f ? g? n (X n ) ((T + ?T ) g)? n (X n ) (f (X n ),? n ) + s n?1 (?T, f ) .<label>(29)</label></formula><p>Thus, for any , we have</p><formula xml:id="formula_39">(1 ? )E ?n sup ?T,f ? n g? n (X n ) ((T + ?T ) g)? n (X n ) (f (X n ),? n ) + s n?1 (?T, f ) = (1 ? ) 2 sup ?T,f g? n (X n ) ((T + ?T ) g)? n (X n ) (f 1 (X n ),? n ) + s n?1 (?T, f 1 ) + (1 ? ) 2 sup ?T,f ? g? n (X n ) ((T + ?T ) g)? n (X n ) (f 2 (X n ),? n ) + s n?1 (?T, f 2 ) ? 1 2 g? n (X n ) ((T + ?T ) g)? n (X n ) (f 1 (X n ),? n ) + s n?1 (?T, f 1 ) +(s n?1 (?T, f 2 ) ? g? n (X n ) ((T + ?T ) g)? n (X n ) (f 2 (X n ),? n ) ? 1 2 s n?1 (?T, f 1 ) + s n?1 (?T, f 2 ) + C| (f 1 (X n ),? n ) ? (f 2 (X n ),? n )| ,<label>(30)</label></formula><p>where the last inequality holds because g? (X) ((T +?T ) g)? (X) ? C for any (X,? ), g, and validT + ?T . Let s = sgn( (f 1 (X n ),? n ) ? (f 2 (X n ),? n )). We have</p><formula xml:id="formula_40">(1 ? )E ?n sup ?T,f ? n g? n (X n ) ((T + ?T ) g)? n (X n ) (f (X n ),? n ) + s n?1 (?T, f ) ? 1 2 s n?1 (?T, f 1 ) + s n?1 (?T, f 2 ) + sC( (f 1 (X n ),? n ) ? (f 2 (X n ),? n )) = 1 2 s n?1 (?T, f 1 ) + sC (f 1 (X n ),? n ) + 1 2 s n?1 (?T, f 2 ) ? sC (f 2 (X n ),? n ) ? 1 2 sup f ?F s n?1 (?T, f ) + sC (f (X n ),? n ) + 1 2 sup f ?F s n?1 (?T, f ) ? sC (f (X n ),? n ) = E ?n sup ?T,f ? n (f (X n ),? n ) + s n?1 (?T, f ) .<label>(31)</label></formula><p>Since the above inequality holds for any &gt; 0, we have</p><formula xml:id="formula_41">E ?n sup ?T,f ? n g? n (X n ) ((T + ?T ) g)? n (X n ) (f (X n ),? n ) + s n?1 (?T, f ) ? E ?n sup ?T,f ? n (f (X n ),? n ) + s n?1 (?T, f ) .<label>(32)</label></formula><p>Proceeding in the same way for all other ?, we have</p><formula xml:id="formula_42">E ? sup ?T,f n i=1 ? i g? i (X i ) ((T + ?T ) g)? i (X i ) (f (X i ),? i ) ? E ? sup f ?F n i=1 ? i (f (X i ),? i ) .<label>(33)</label></formula><p>and thus E sup</p><formula xml:id="formula_43">?T,f n i=1 ? i g? i (X i ) ((T + ?T ) g)? i (X i ) (f (X i ),? i ) ? E sup f ?F n i=1 ? i (f (X i ),? i ) .<label>(34)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof of Lemma 3</head><p>Before proving Lemma 3, we show that the loss function (f (X),? ) is 1-Lipschitz-continuous w.r.t. h i (X), i = {1, . . . , C}.</p><p>Recall that</p><formula xml:id="formula_44">(f (X),? ) = ? C i=1 1 {? =i} log(g i (X)) = ? log exp(h? (X)) C i=1 exp(h i (X)) .<label>(35)</label></formula><p>Take the derivative of (f (X),? ) w.r.t. h i (X). If i =? , we have</p><formula xml:id="formula_45">? (f (X),? ) ?h i (X) = exp(h i (X)) c i=1 exp(h i (X)) .<label>(36)</label></formula><formula xml:id="formula_46">If i =? , we have ? (f (X),? ) ?h i (X) = ?1 + exp(h i (X)) c i=1 exp(h i (X))</formula><p>.</p><p>According to Eqs. <ref type="bibr" target="#b35">(36)</ref> and <ref type="formula" target="#formula_5">(37)</ref>, it is easy to conclude that ?1 ? ? (f (X),? ) ?hi(X) ? 1, which also indicates that the loss function is 1-Lipschitz with respect to h i (X), ?i ? {1, . . . , C}. Now we are ready to prove Lemma 3. We have</p><formula xml:id="formula_48">E sup f 1 n n i=1 ? i (f (X i ),? i ) = E sup f =arg max{h1,...,hc} 1 n n i=1 ? i (f (X i ),? i ) = E sup max{h1,...,hc} 1 n n i=1 ? i (f (X i ),? i ) ? E C k=1 sup h k ?H 1 n n i=1 ? i (f (X i ),? i ) = C k=1 E sup h k ?H 1 n n i=1 ? i (f (X i ),? i ) ? CLE sup h k ?H 1 n n i=1 ? i h k (X i ) = CLE sup h?H 1 n n i=1 ? i h(X i ) ,<label>(38)</label></formula><p>where the first equation holds because the softmax function preserves the rank of its inputs, i.e., f (X) = arg max i?{1,...,C} g i (X) = arg max i?{1,...,C} h i (X); the second equation holds because arg max{h 1 , ? ? ? , h c } and max{h 1 , ? ? ? , h c } give the same constraint on h i , ?i ? {1, . . . , C};the fifth inequality holds because of the Talagrand Contraction Lemma <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Definition of transition matrix</head><p>The definition of symmetry flipping transition matrix is as follows, where C is number of the class. E More discussions about <ref type="figure" target="#fig_2">Figure 3</ref> We represent <ref type="figure" target="#fig_2">Figure 3</ref> in the main paper as <ref type="figure" target="#fig_0">Figure 1</ref> in this appendix.</p><p>From the figure, we can compare the transition matrices learned by the proposed T-revision method and the traditional anchor point based method. Specifically, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, at epoch 0, the estimation error corresponds to the estimation error of transition matrix learned by identifying anchor points <ref type="bibr" target="#b37">[38]</ref> (the traditional method to learn transition matrix). Note that the method with "-N/A" in its name means it runs on the modified datasets where instances with large clean class posterior probobilities are removed (anchor points are removed); while the method with "-A" in its name means it runs the original intact dataset (may contain anchor points). Clearly, we can see that the estimation error will increase by removing possible anchor points, meaning that anchor points is crucial in the traditional transition matrix learning. Moreover, as the number of epochs grows, the figures show how the estimation error varies by running the proposed revision methods. We can see that the proposed Reweight method always leads to smaller estimation errors, showing that the proposed method works well in find a better transition matrix. <ref type="figure" target="#fig_0">Figure 1</ref> also shows the comparison of learning transition matrices between the risk-consistent estimator based method and the classifier-consistent method based method. For classifier-consistent algorithms, we can also modify the transition matrix by adding a slack variable and learning it jointly with the classifier, e.g., Forward-A-R and Forward-N/A-R. However, we can find that the classifierconsistent algorithm based method Forward-N/A-R may fail in learning a good transition matrix, e.g., <ref type="figure" target="#fig_0">Figure 1(a)</ref>. This is because there is no reason to learn the transition matrix by minimizing the classifier-consistent objective function. It is reasonable to learn the transition matrix by minimizing the risk-consistent estimator because a favorable transition matrix should make the classification risk w.r.t. clean data small. This is verified by comparing Forward-A-R and Forward-N/A-R with the proposed Reweight-A-R and Reweight-N/A-R, we can find that the risk-consistent estimator Reweight always leads to smaller estimation errors for learning transition matrix.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustrative experimental results (using a 5-class classification problem as an example). The noisy class posterior probability P (?|X = x) can be estimated by exploiting noisy data. Let an example have P (?|X = x) = [0.141; 0.189; 0.239; 0.281; 0.15]. If the true transition matrix T is given, we can infer the clean class posterior probability as P (Y|X = x) = (T ) ?1 P (?|X = x) = [0.15; 0.28; 0.25; 0.3; 0.02] and that the instance belongs to the fourth class. However, if the transition matrix is not accurately learned asT (only slightly differing from T with two entries in the second row), the clean class posterior probability can be inferred as P (Y|X = x) = (T ) ?1 P (?|X = x) = [0.1587; 0.2697; 0.2796; 0.2593; 0.0325] and the instance could be mistakenly classified into the third class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>and the second last equation holds because label noise is assumed to be An overview of the proposed method. The proposed method will learn a more accurate classifier because the transition matrix is renovated.Algorithm 1 Reweight T -Revision (Reweight-R) Algorithm. Input: Noisy training sample D t ; Noisy validation set D v . Stage 1: LearnT 1: Minimize the unweighted loss to learnP (?|X = x) without a noise adaption layer; 2: InitializeT according to Eq. (1) by using instances with the highestP (? = i|X = x) as anchor points for the i-th class; Stage 2: Learn the classifier f and ?T 3: Initialize the neural network by minimizing the weighted loss with a noisy adaption layerT ; 4: Minimize the weighted loss to learn f and ?T with a noisy adaption layer (T + ?T ) ; //Stopping criterion for learningP (?|X = x), f and ?T : whenP (?|X = x) yields the minimum classification error on the noisy validation set D v Output:T , ?T , and f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The estimation error of the transition matrix by employing classifier-consistent and riskconsistent estimators. The first row is about sym-20 label noise while the second row is about sym-50 label noise. The error bar for standard deviation in each figure has been shaded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>andL (f (X),?) = [? (f (X),? = 1), . . . ,? (f (X),? = C)] = (T ) ?1 L(f (X),?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 2</head><label>2</label><figDesc>Assume the Frobenius norm of the weight matrices W 1 , . . . , W d are at most M 1 , . . . , M d . Let the activation functions be 1-Lipschitz, positive-homogeneous, and applied element-wise (such as the ReLU). Let x is upper bounded by B, i.e., for any x ? X , x ? B. Then,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Comparing the estimation error of the transition matrix by employing classifier-consistent and risk-consistent estimators. The first row is about sym-20 label noise while the second row is about sym-50 label noise. The error bar for STD in each figure has been highlighted as a shade.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>39?0.29 81.52?0.29 79.85?0.30 52.22?0.45 42.75?0.49 29.24?0.54 MentorNet-A 96.57?0.18 90.13?0.09 80.49?0.52 70.71?0.24 52.11?0.10 38.45?0.25 Co-teaching-A 97.22?0.18 91.68?0.21 82.38?0.11 72.80?0.45 54.23?0.08 41.37?0.08 Forward-A 98.75?0.08 97.86?0.22 85.63?0.52 77.92?0.66 57.75?0.37 44.66?1.01 Reweight-A 98.71?0.11 98.13?0.19 86.77?0.40 80.16?0.46 58.35?0.64 43.97?0.67 Forward-A-R 98.84?0.09 98.12?0.22 88.10?0.21 81.11?0.74 62.13?2.09 50.46?0.52 Reweight-A-R 98.91?0.04 98.38?0.21 89.63?0.13 83.40?0.65 65.40?1.07 50.24?1.45</figDesc><table><row><cell cols="2">MNIST</cell><cell cols="2">CIFAR-10</cell><cell cols="2">CIFAR-100</cell></row><row><cell>Sym-20%</cell><cell>Sym-50%</cell><cell>Sym-20%</cell><cell>Sym-50%</cell><cell>Sym-20%</cell><cell>Sym-50%</cell></row><row><cell>Decoupling-A 95.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Means and standard deviations (percentage) of classification accuracy. Methods with "-N/A" means instances with high estimated P (Y |X) are removed from the dataset; Methods with "-R" means that the transition matrix used is revised by a revision ?T .</figDesc><table><row><cell cols="2">MNIST</cell><cell cols="2">CIFAR-10</cell><cell cols="2">CIFAR-100</cell></row><row><cell>Sym-20%</cell><cell>Sym-50%</cell><cell>Sym-20%</cell><cell>Sym-50%</cell><cell>Sym-20%</cell><cell>Sym-50%</cell></row></table><note>Decoupling-N/A 95.93?0.21 82.55?0.39 75.37?1.24 47.19?0.19 39.59?0.42 24.04?1.19 MentorNet-N/A 97.11?0.09 91.44?0.25 78.51?0.31 67.37?0.30 48.62?0.43 33.53?0.31 Co-teaching-N/A 97.69?0.23 93.58?0.49 81.72?0.14 70.44?1.01 53.21?0.54 40.06?0.83 Forward-N/A 98.64?0.12 97.74?0.13 84.75?0.81 74.32?0.69 56.23?0.34 39.28?0.59 Reweight-N/A 98.69?0.08 98.05?0.22 85.53?0.26 77.70?1.00 56.60?0.71 39.28?0.71 Forward-N/A-R 98.80?0.06 97.96?0.13 86.93?0.39 77.14?0.65 58.72?0.45 44.60?0.79 Reweight-N/A-R 98.85?0.02 98.37?0.17 88.90?0.22 81.55?0.94 62.00?1.78 44.75?2.10</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Classification accuracy (percentage) on Clothing1M.</figDesc><table><row><cell cols="7">Decoupling MentorNet Co-teaching Forward Reweight Forward-R Reweight-R</cell></row><row><cell>53.98</cell><cell>56.77</cell><cell>58.68</cell><cell>71.79</cell><cell>70.95</cell><cell>72.25</cell><cell>74.18</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In the literature, the assumption infx P (Y = i|X = x) ? 1 was introduced as irreducibility<ref type="bibr" target="#b4">[5]</ref> to ensure the transition matrix is identifiable; an anchor point x for class i is defined by P (Y = i|X = x) = 1<ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b21">22]</ref> to ensure a fast convergence rate. In this paper, we generalize the definition for the anchor point family, including instances whose class posterior probability P (Y = i|X = x) is equal to or close to one.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">During the training, T + ?T can be ensured to be a valid transition matrix by first projecting their negative entries to be zero and then performing row normalization. In the experiments, ?T is initialized to be a zero matrix and we haven't pushed T + ?T to be a valid matrix when tuning ?T .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning from noisy examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Angluin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="343" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spectrally-normalized margin bounds for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><forename type="middle">J</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matus J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6240" to="6249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convexity, classification, and risk bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">473</biblScope>
			<biblScope unit="page" from="138" to="156" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyemin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2973" to="3009" />
			<date type="published" when="2010-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Theory of classification: A survey of some recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?bor</forename><surname>Lugosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESAIM: probability and statistics</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="323" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Concentration inequalities: A nonasymptotic theory of independence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?bor</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Massart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning with bounded instance-and label-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotagiri</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03768</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Size-independent sample complexity of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="297" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Covariate shift by kernel mean matching. Dataset shift in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Schmittfull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="131" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Curriculumnet: Weakly supervised learning from large-scale web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinglong</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5836" to="5846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2309" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">Pack</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05468</idno>
		<title level="m">Generalization in deep learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust active label correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="308" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Probability in Banach Spaces: isoperimetry and processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Ledoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Talagrand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1910" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3361" to="3370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Foundations of Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nati</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5947" to="5956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A PAC-Bayesian approach to spectrally-normalized margin bounds for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning with confident examples: Rank pruning for robust classification with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tailin</forename><surname>Curtis G Northcutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><forename type="middle">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mixture proportion estimation via kernel embeddings of distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2052" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Scott E Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4331" to="4340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Calibrated asymmetric surrogate losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="958" to="992" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A rate of convergence for mixture proportion estimation, with application to learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="838" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Classification with asymmetric label noise: Consistency and maximal denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Handy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="489" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robustness of conditional gans to noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Thekumparampil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinan</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewoong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10271" to="10282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5596" to="5605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">An operator theoretic approach to nonparametric mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton D</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00071</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">An operator theoretic approach to nonparametric mixture models. accepted to The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton D</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The nature of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer science &amp; business media</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">How does disagreement benefit co-teaching?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An efficient and provable approach for mixture proportion estimation using linear independence assumption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4480" to="4489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning with biased complementary labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
