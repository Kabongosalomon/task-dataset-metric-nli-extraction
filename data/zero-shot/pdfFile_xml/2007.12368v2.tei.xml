<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Learning Across Domains</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bucci</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>D&amp;apos;innocente</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Liao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
						</author>
						<title level="a" type="main">Self-Supervised Learning Across Domains</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, VOL. ..., NO. ..., MONTH ... 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Self-Supervision</term>
					<term>Domain Generalization</term>
					<term>Domain Adaptation</term>
					<term>Multi-Task Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human adaptability relies crucially on learning and merging knowledge from both supervised and unsupervised tasks: the parents point out few important concepts, but then the children fill in the gaps on their own. This is particularly effective, because supervised learning can never be exhaustive and thus learning autonomously allows to discover invariances and regularities that help to generalize. In this paper we propose to apply a similar approach to the problem of object recognition across domains: our model learns the semantic labels in a supervised fashion, and broadens its understanding of the data by learning from self-supervised signals on the same images. This secondary task helps the network to focus on object shapes, learning concepts like spatial orientation and part correlation, while acting as a regularizer for the classification task over multiple visual domains. Extensive experiments confirm our intuition and show that our multi-task method combining supervised and self-supervised knowledge shows competitive results with respect to more complex domain generalization and adaptation solutions. It also proves its potential in the novel and challenging predictive and partial domain adaptation scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>M Any definitions of intelligence have been formulated by psychologists and learning researches along the years. Despite the differences, they all indicate the ability to adapt and achieve goals under a wide range of conditions as a key component <ref type="bibr">[1]</ref>. Artificial intelligence inherits these definitions, with the most recent research demonstrating the importance of knowledge transfer and domain generalization <ref type="bibr" target="#b17">[18]</ref>. Indeed, in many practical applications the underlying distributions of training (i.e. source) and test (i.e. target) data are inevitably different, asking for robust and adaptable solutions. When dealing with visual domains, most of the current strategies are based on supervised learning. These processes search for semantic spaces able to capture basic data knowledge regardless of the specific appearance of input images: some decouple image style from the shared object content <ref type="bibr" target="#b6">[7]</ref>, others generate new samples <ref type="bibr" target="#b74">[75]</ref>, or impose adversarial conditions to reduce feature discrepancy <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b47">[48]</ref>. With the analogous aim of getting general purpose feature embeddings, an alternative research direction is pursued by self-supervised learning that captures visual invariances and regularities solving tasks that do not need data annotation, like image orientation recognition <ref type="bibr" target="#b29">[30]</ref> or image coloring <ref type="bibr" target="#b83">[84]</ref>. Unlabeled data are largely available and by their very nature are less prone to bias (no labeling bias issue <ref type="bibr" target="#b71">[72]</ref>), thus they seem the perfect candidate to provide visual information independent from specific domain styles. However their potential has not been fully exploited: the existing selfsupervised approaches often come with tailored architectures that need dedicated fine-tuning strategies to re-engineer the acquired knowledge <ref type="bibr" target="#b59">[60]</ref>. Moreover, they are mainly applied on real-world photos without considering cross-domains scenarios with images of paintings or sketches.</p><p>? S. <ref type="bibr">Bucci</ref>  Recognizing objects across visual domains is a challenging task that requires high generalization abilities. Self-supervisory image signals allow to capture natural invariances and regularities that can help to bridge across large style gaps. With our multi-task approach we learn jointly to classify objects and solve jigsaw puzzles or recognize image orientation, showing that this supports generalization to new domains.</p><p>This clear separation between learning intrinsic regularities from images (self-supervised knowledge) and robust classification across domains (supervised knowledge) is in contrast with the visual learning strategies of biological systems, and in particular of the human visual system. Indeed, numerous studies highlight that infants and toddlers learn both to categorize objects and about regularities at the same time <ref type="bibr" target="#b5">[6]</ref>. For instance, popular toys for infants teach to recognize different categories by fitting them into shape sorters; jigsaw puzzles of animals or vehicles to encourage learning of object parts' spatial relations are equally widespread among 12-18 months old. This joint learning is certainly a key ingredient in the ability of humans to reach sophisticated visual generalization abilities at an early age <ref type="bibr" target="#b25">[26]</ref>.</p><p>Inspired by this, our original paper <ref type="bibr" target="#b11">[12]</ref> was the first to introduce a multi-task approach that learns simultaneously how to recognize objects by exploiting supervised data, and how to generalize to new domains by leveraging intrinsic self-supervised information about spatial co-location of image parts ( <ref type="figure">Fig. 1 and  2)</ref>. Specifically, we proposed to recover an original image from . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convnet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shuffled Images</head><p>Feature Extractor G f G c G p <ref type="figure">Fig. 2</ref>. Illustration of the proposed multi-task approach when using jigsaw puzzle as self-supervised task. We start from images of multiple domains and use a 3 ? 3 grid to decompose them in 9 patches which are then randomly shuffled and recomposed into images of the same dimension of the original ones. Through the maximal Hamming distance algorithm in <ref type="bibr" target="#b57">[58]</ref> we define a set of P patch permutations and assign an index to each of them. Both the original ordered and the shuffled images are fed to a convolutional network that is optimized to satisfy two objectives: object classification on the ordered images and jigsaw classification (i.e. permutation index recognition) on the shuffled images. An analogous scheme holds when using rotation recognition as self-supervision. The names assigned to each network part refer to the notation adopted in Sec. <ref type="bibr">3.</ref> its shuffled parts, re-purposing the popular game of solving jigsaw puzzles. Differently from previous approaches that deal with feature extraction from separate image patches <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b59">[60]</ref>, we moved the patch re-assembly at the image level and we formalized the jigsaw task as a classification problem over recomposed images with the same dimension of the original one. In this way object recognition and patch reordering can share the same network backbone and we can seamlessly leverage over any convolutional learning structure as well as several pretrained models without the need of specific architectural changes.</p><p>Here we extend our previous work providing a wider overview on self-supervised learning across domains. (1) We consider rotation recognition and jigsaw puzzle as self-supervised tasks showing their effect both as pretext and in the multi-task model together with supervised learning for domain generalization; <ref type="bibr" target="#b1">(2)</ref> we delve into the details of the multi-task method with an extensive ablation analysis and visualizing successful as well as failure cases; (3) we consider both single source and multi-source domain adaptation experiments with a thorough analysis against the most recent state-of-the art methods; (4) we discuss the effect of our multi-task model in the challenging predictive and partial domain adaptation scenarios also extending <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Self-Supervised Learning. Self-Supervised Learning is a paradigm developed to learn visual features from large-scale unlabeled data <ref type="bibr" target="#b39">[40]</ref>. Its first step is a pretext task that exploits inherent data attributes to automatically generate surrogate labels: part of the existing knowledge about the images is manually removed (e.g. the color, the orientation, the patch order) and the task consists in recovering it. It has been shown that the first layers of a network trained in this way capture useful semantic knowledge <ref type="bibr" target="#b2">[3]</ref>. The second step of the learning process consists in transferring the selfsupervised learned model of those initial layers to a supervised downstream task (e.g. classification, detection), while the ending part of the network is newly trained.</p><p>The possible pretext tasks can be organized in three main groups. One group relies only on original visual cues and involves either the whole image with geometric transformations (e.g. translation, scaling, rotation <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b29">[30]</ref>), clustering <ref type="bibr" target="#b14">[15]</ref>, inpainting <ref type="bibr" target="#b61">[62]</ref> and colorization <ref type="bibr" target="#b83">[84]</ref>, or considers image patches focusing on their equivariance (learning to count <ref type="bibr" target="#b58">[59]</ref>) and relative position (solving jigsaw puzzles <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b59">[60]</ref>). A second group uses external sensory information either real or synthetic: this solution is often applied for multi-cue (visual-to-audio <ref type="bibr" target="#b60">[61]</ref>, RGB-to-depth <ref type="bibr" target="#b62">[63]</ref>) and robotic data <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Finally, the third group relies on video and on the regularities introduced by the temporal dimension <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b76">[77]</ref>. The most recent self-supervised learning research focuses on proposing novel pretext tasks or combining several of them together, to then compare their initialization performance for a downstream task with respect to using supervised models as in standard transfer learning <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b62">[63]</ref>.</p><p>Our work investigates a new research direction: we combine supervised and self-supervised knowledge in a multi-task framework, studying its effect on domain generalization and adaptation.</p><p>Domain Generalization and Adaptation. Several algorithms have been developed to cope with domain shift, mainly in two different settings: Domain Generalization (DG) and Domain Adaptation (DA). In DG the target is unknown at training time: the learning process can usually leverage multiple labeled sources to define a model robust to any new, previously unseen domain <ref type="bibr" target="#b55">[56]</ref>. In DA the learning process has access to the labeled source data and to the unlabeled target data, so the aim is to generalize to the given specific target set <ref type="bibr" target="#b17">[18]</ref>. In multi-source DA the source domain label may be unknown <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b51">[52]</ref>, while for most of the DG methods it remains a crucial information to leverage on.</p><p>There are three main families of solutions for both DG and DA. Feature-level strategies focus on learning domain invariant data representations mainly by minimizing different domain shift measures <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b70">[71]</ref>. The domain shift can also be reduced by training a domain classifier and inverting the optimization to guide the features towards maximal domain confusion <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b72">[73]</ref>. This adversarial approach has several variants, some of which also exploit class-specific domain recognition modules <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b66">[67]</ref>. Metric learning <ref type="bibr" target="#b54">[55]</ref> and deep autoencoders <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b45">[46]</ref> have also been used to search for domain-shared embedding spaces. In DG, these approaches leverage on the availability of multiple sources and on the access to the domain label for each sample. Model-level strategies either change how the data are loaded with ad-hoc episodes <ref type="bibr" target="#b44">[45]</ref>, or modify conventional learning algorithms to search for more robust minima of the objective function <ref type="bibr" target="#b42">[43]</ref>. Besides these main approaches, other solutions consists in introducing domain alignment layers <ref type="bibr" target="#b12">[13]</ref>, aggregation layers <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b44">[45]</ref>, or using low-rank network parameter decomposition <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b43">[44]</ref> with the goal of identifying and neglecting domain-specific signatures. Finally, data-level techniques exploit variants of the Generative Adversarial Networks (GANs, <ref type="bibr" target="#b30">[31]</ref>) to synthesize new images. Indeed, producing source-like target images or/and targetlike source images <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b64">[65]</ref> help to reduce the domain gap.</p><p>Some recent works have started investigating intermediate settings between DA and DG. In Predictive Domain Adaptation (PrDA) a labeled source and several auxiliary unlabeled domains are available at training time together with meta-data that describe their relation <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b81">[82]</ref>. The target data are not available, but their meta-data are provided and used to compose an adapted model directly from the sources.</p><p>In both DA and DG, the main assumption is that source and target share the same label set, with few works studying exceptions to this basic condition <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b78">[79]</ref>. In particular, in Partial Domain Adaptation (PDA) the target to covers only a subset of the source class set. In this case it is important to adjust the adaptation process so that the samples with not-shared labels would not influence the learned model. The more commonly used techniques consist in adding a re-weight source sample strategy to a standard DA approach <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b82">[83]</ref>. Alternative solutions leverage on two separate deep classifiers and their prediction inconsistency on the target <ref type="bibr" target="#b53">[54]</ref> or on feature norm matching <ref type="bibr" target="#b79">[80]</ref>.</p><p>As indicated by this brief overview, previous literature did not investigate self-supervision for DA or DG. In this work we present a thorough study of self-supervised learning across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We introduce here the technical notation for our multi-task approach across domains and specify the objectives in each of the considered settings. Let us assume to observe data {(x s i , y s i )} n s i=1 from one or more source distributions. Here x s i represent the i-th image while y s i is the corresponding one-hot vector label of dimension |Y s |. Starting from these images we can always apply different procedures to generate self-supervised variants. One simple choice is that of applying rotation to produce 4 copies of each sample with {0 ? , 90 ? , 180 ? , 270 ? } orientation. The related self-supervised task consists in choosing the correct image rotation. A more structured alternative is that of decomposing the original images according to a 3 ? 3 grid: this produces 9 squared patches from every sample, which are then moved from their original locations and re-positioned to form a set of 9! shuffled images. This task is reminiscent of the jigsaw puzzle game, where the tiles have to be rearranged to get back the original image. For both the described cases, {(z s k , p s k )} K s k=1 refer to the newly obtained images. The dimension of the one-hot vector label p is 4 when applying rotation, while for patch shuffling we choose a subset P of the 9! possible permutations selected by following the Hamming distance based algorithm in <ref type="bibr" target="#b57">[58]</ref>. The total number of images changes depending on the self-supervised task: K s = 4 ? n s for rotation and K s = P ? n s for patch shuffling. Regardless of the specific chosen self-supervised objective we can combine it with supervised learning through a standard hardparameter sharing multi-task model realized with a multi-branch ending network <ref type="bibr" target="#b15">[16]</ref>. One output branch will be dedicated to the supervised task exploiting the labels of the source data, while the other will solve the self-supervised problem: rotation or jigsaw puzzle permutation recognition (see <ref type="figure">Figure 2</ref>). The auxiliary selfsupervised objective contributes in extracting relevant semantic features from the data, with a final beneficial effect on the object recognition performance. Since the self-supervised objective is label agnostic it can run both over supervised and unsupervised domains, supporting generalization and adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Domain Generalization</head><p>For our network we indicate the convolutional feature extraction backbone with G f , parametrized by ? f . The parameters of the object classifier G c and of the self-supervised task G p are respectively ? c and ? p . Overall we train the network to obtain the optimal model through arg min</p><formula xml:id="formula_0">? f ,?c,?p 1 n s n s i=1 L c (G c (G f (x s i )), y s i )+ ? s 1 K s K s k=1 L p (G p (G f (z s k )), p s k )<label>(1)</label></formula><p>where L c and L p are cross entropy losses for both the object and self-supervised classifiers. We underline that the self-supervised loss is also calculated on the original images. Indeed, the 0 ? orientation as well as the correct patch sorting correspond also to one of the possible self-supervised image transformation variants. Differently, the supervised classification loss is not influenced by the shuffled or rotated images, as this would make object recognition tougher. At test time we use the object classifier G c to predict on the new target images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Domain Adaptation</head><p>By its nature self-supervised learning does not need manual annotation and it can exploit the unlabeled target data {x t j } n t j=1 when available in the DA setting. The target samples are transformed (rotated, shuffled) so that each newly produced instance {z t k } K t k=1 gets its own self-supervised label p t k . An alternative and widely used way to involve the target data in the learning process consists in applying the source supervised knowledge on them to evaluate the pseudo-label? y t = G c (G f (x t )), and minimize the prediction uncertainty measured by the entropy H = ? |Y s | l=1? t l log? t l <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b79">[80]</ref>. This is a semi-supervised technique which guides the class decision boundary to pass through low-density target areas, but its success across domains depends on moderate levels of domain shift to avoid wrong pseudo-labels. Given their orthogonal and possibly complementary nature, in our DA analysis we combine the entropy term with the supervised and self-supervised loss. The overall learning objective is formalized as</p><formula xml:id="formula_1">arg min ? f ,?c,?p 1 n s n s i=1 L c (G c (G f (x s i )), y s i )+ ? s 1 K s K s k=1 L p (G p (G f (z s k )), p s k )+ ? 1 n t n t j=1 H(G c (G f (x t j ))) + ? t 1 K t K t k=1 L p (G p (G f (z t k )), p t k ).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Partial Domain Adaptation</head><p>In PDA the label space of the target domain is contained in that of the source domain Y t ? Y s . This further shift in the label space makes the problem even more challenging: if the matching between the whole source and target data is forced, any adaptive method may incur in a degenerate case producing worse performance than its plain non-adaptive version due to negative transfer <ref type="bibr" target="#b63">[64]</ref>.</p><p>The two L p terms in (2) help domain shift reduction, however their co-presence may be redundant: the features are already chosen to minimize the source classification loss and the selfsupervised task on the target back-propagates inducing a crossdomain adjustment on the learned features. Thus, for PDA we can drop the source self-supervised term, which corresponds to setting ? s = 0. This choice has a double positive effect: on one side it reduces the number of hyper-parameters in the learning process, leaving space for the introduction of other complementary learning conditions, on the other we let the self-supervised module focus only on the target without involving the extra classes of the source.</p><p>To further enforce the focus on the shared classes, we extend our approach by integrating a weighting mechanism analogous to that presented in <ref type="bibr" target="#b9">[10]</ref>. The source classification output on the target data are accumulated with ? = 1 n t n t j=1? t j and normalized as ? ? ?/ max(?), obtaining a |Y s |-dimensional vector that quantifies the contribution of each source class. Moreover, we can easily integrate a source vs target domain discriminator G d as in <ref type="bibr" target="#b26">[27]</ref> and adversarially maximize the related binary cross-entropy to increase the domain confusion, taking also into consideration the defined class weighting procedure for the source samples. In more formal terms, the final objective of our multi-task problem in the PDA setting is arg min</p><formula xml:id="formula_2">? f ,?c,?p max ? d 1 n s n s i=1 ?y i Lc(Gc(G f (x s i ), y s i )+ ? log(G d (G f (x s i ))) + 1 n t n t j=1 ? H(G c (G f (x t j ))) + ? log(1 ? G d (G f (x t j ))) + ? t 1 K t K t k=1 L p (G p (G f (z t k )), p t k )<label>(3)</label></formula><p>where ? yi is the class weight for the ground truth label of the source point x s i and ? is a hyper-parameter that adjusts the importance of the introduced domain discriminator. When ? = 0 and ? y = 1/|Y s | we fall back to the standard DA case. A schematic illustration of the method is presented in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation details</head><p>We designed our multi-task network to leverage over different convolutional deep architectures: the backbone G f may inherit the structure of standard networks as AlexNet or ResNet. The specific object and self-supervised classifiers heads G c , G p are respectively implemented by an ending fully connected layer. When including multiple self-supervised tasks in the model (i.e. Jigsaw+Rotation), a G p head is assigned to each self-supervised objective. Specifically, shuffled images are directed to the Jigsaw final head G J p , while the rotated images to the Rotation recognition head G R p . In the PDA setting we introduced the domain classifier G d by adding three fully connected layers after the last pooling layer of the main backbone, and using a sigmoid function for the last activation as in <ref type="bibr" target="#b26">[27]</ref>. For all our experiments we trained the network end-to-end by fine-tuning all the feature layers from Imagenet pre-trained models <ref type="bibr" target="#b18">[19]</ref>, while G c , G p and G d are learned from scratch.</p><p>Overall the network for DG has two main hyper-parameters: ? that weights the self-supervised loss, and the data bias parameter ? which regulates the data input process. The self-supervised variants of the images enter the network together with the original ones, hence each image batch contains both of them with ? specifying their relative ratio. For instance ? = 0.6 means that for each batch, 60% of the images are standard, while the remaining 40% are rotated or composed of shuffled patches. In our experiments we chose ? and ? by keeping a source validation set (10% of the training data) and performing model selection on it by following <ref type="bibr" target="#b32">[33]</ref>. When combining Jigsaw+Rotation we have respectively ? J and ? R , while the fraction of transformed images regulated by ? are rotated or shuffled with equal probability. In the DA setting ? decouples in ? s and ? t respectively for source and target data. While discussing the experimental results we will see the outcome of cross-validating ? on the source and then setting ? = ? s = ? t or fixing ? s = 0 as well as the effect on model robustness when manually tuning ? t . Further parameters in DA and PDA are ? and ?. The first is the weight assigned to the entropy loss which we safely fixed to small values: 0.1 for DA and 0.2 for PDA. Finally, ? balances the importance of the gradient reversal layer when included in PDA and we adopted the same scheduling of <ref type="bibr" target="#b26">[27]</ref> to update its value, so that the importance of the domain discriminator increases with the training epochs.</p><p>In designing the jigsaw puzzle task we need to choose the image patch grid size n ? n, and the cardinality of the patch permutation subset P . As we will detail in the following section, our multi-task approach is robust to these values and for all our experiments we kept them fixed (3 ? 3 grid, P = 30).</p><p>We used a simple data augmentation protocol by randomly cropping the images to retain between 80 ? 100% and randomly applied horizontal flipping. By following <ref type="bibr" target="#b59">[60]</ref>, we also randomly (10% probability) convert an image tile to grayscale. Our DG/DA model is trained with an SGD solver, 30 epochs, batch size 128, learning rate set to 0.001 and stepped down to 0.0001 after 80% of the training epochs. Our PDA model is trained with SGD with momentum set at 0.9, weight decay 0.0005 and 24 epochs. We used batch size of 64 and initial learning rate 0.0005. Some specific training details are used in the PrDA setting and will be described in Sec. 4.1.8. We implemented our deep methods in PyTorch and the code is available at https://github.com/silvia1993/ Self-Supervised Learning Across Domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section we present an extensive evaluation of using selfsupervised knowledge across visual domains. First of all we focus on DG (Sec. 4.1). We test both the rotation and jigsaw puzzle self-supervised pretexts before using them extensively as auxiliary tasks together with supervised learning in our multi-task model. The second part of our analysis is dedicated to the DA scenario (Sec. 4.2) and its more challenging PDA setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Self-Supervision for Domain Generalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Data and Setup</head><p>For our DG analysis we used as main testbed the PACS dataset <ref type="bibr" target="#b43">[44]</ref> that covers 7 object categories and 4 domains (Photo, Art Paintings, Cartoon and Sketches). We followed the experimental protocol in <ref type="bibr" target="#b43">[44]</ref> and finetuned the Imagenet-pretrained models with three domains as source datasets and the remaining one as target test. We also considered other two data collections. The VLCS <ref type="bibr" target="#b71">[72]</ref> aggregates images of 5 object categories shared by the PASCAL VOC 2007, LabelMe, Caltech and Sun datasets. We followed the standard protocol of <ref type="bibr" target="#b27">[28]</ref> dividing each domain into a training set (70%) and a test set (30%) by random selection from the overall dataset. The Office-Home dataset <ref type="bibr" target="#b73">[74]</ref> contains 65 categories of daily objects from 4 domains: Art, Clipart, Product and Real-World. For this dataset we used the same experimental protocol of <ref type="bibr" target="#b21">[22]</ref>. Note that Office-Home and PACS are related in terms of domain types and it is useful to consider both as testbeds to check if our multi-task self-supervised approach scales when the number of categories changes from 7 to 65. Instead VLCS offers different challenges by combining object categories from Caltech with scene images of the other domains. The evaluation is based on three repetitions of each run: we report the average ? standard deviation of the obtained class recognition accuracy.</p><p>Only for the single-source DG analysis we focused on digits datasets to compare the sensitivity of our approach against a competitor method. For PrDA we considered a fine-grained car dataset. All the details for these last two settings are described respectively in Sec. 4.1.5 and 4.1.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Self-Supervised Pretraining</head><p>We test here the robustness of image orientation and patch colocation knowledge across domains by using both rotation and jigsaw puzzle as pretext tasks for domain generalization. Baselines. As first step we considered three jigsaw puzzles and one rotation model trained on Imagenet (ILSVRC12, <ref type="bibr" target="#b18">[19]</ref>) data without original labels. For the jigsaw puzzle, we used the two Context-Free-Network (CFN) models provided by the authors of <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b59">[60]</ref>. The CFN has 9 AlexNet-based siamese branches that extract features separately from each image patch and then recompose them before entering the final classification layer. We indicate these models respectively as J-CFN <ref type="bibr" target="#b57">[58]</ref> and J-CFN+ <ref type="bibr" target="#b59">[60]</ref>. The third puzzle-based model is obtained by training an AlexNet on whole images recomposed from disordered patches, which we call J-AlexNet. Inspired by <ref type="bibr" target="#b29">[30]</ref>, we also trained an AlexNet model for rotation recognition that we dub R-AlexNet. Results. The obtained results are collected in the top part of <ref type="table" target="#tab_1">Table  1</ref> and show that using a patch-based (p) jigsaw method provides on average a more reliable pretext model than dealing with the whole (w) recomposed image. The rotation pretext model shows the best results with a small advantage over the patch based jigsaw approaches. In summary, we find that moving the jigsaw puzzle task from the feature to the image level when training a pretext model does not appear as a good choice and that the rotation task is the simplest and most effective solution. In designing our multi-task approach which combines supervised and self-supervised learning we have several options, both in terms of the architecture to use and of the best self-supervised task.</p><p>Baselines. We compare the CFN multi-branch architecture with a plain AlexNet backbone. To differentiate the classificationaware CFN model with respect to the self-supervised pretraining discussed in the previous section we name it C-CFN. Regardless of the specific architecture used, we indicate with DeepAll the single-task supervised model trained on all the original source images (i.e. ? = 0), while we use Jigsaw (Puzzle) or Rotation to specify the multi-task case where each of those self-supervised tasks was trained jointly with the object classification.</p><p>Results. From the results in the bottom part of <ref type="table" target="#tab_1">Table 1</ref> we can draw two conclusions. First, combining supervised and selfsupervised learning provides better results than a single-task supervised model across domains. This is true regardless of the chosen architecture, as indicated by the comparison between the DeepAll and Jigsaw/Rotation variants. Second, a single branch architecture is better suited for the multi-task problem at hand. In this case, moving the jigsaw puzzle task from feature to image level simplifies the self-supervised task and its combination with the supervised objective. The whole-image Rotation auxiliary task supports generalization even slightly better than Jigsaw.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Multi-Source Domain Generalization</head><p>Here we provide an extensive evaluation of our multi-task approach against state-of-the-art multi-source DG methods. Baselines: We consider different families of DG approaches 1 . The first is based on low-rank constraints applied on network parameters: TF <ref type="bibr" target="#b43">[44]</ref>, SLRC <ref type="bibr" target="#b19">[20]</ref>. The second exploits domainspecific component aggregation: Epi-FCR <ref type="bibr" target="#b44">[45]</ref>, D-SAM <ref type="bibr" target="#b21">[22]</ref>. The third builds on meta-learning strategies: MLDG <ref type="bibr" target="#b42">[43]</ref>, MetaReg <ref type="bibr" target="#b3">[4]</ref>, MASF <ref type="bibr" target="#b24">[25]</ref>. Finally, the fourth family leverages adversarial classifiers in different ways: DDAIG <ref type="bibr" target="#b86">[87]</ref>, PAR <ref type="bibr" target="#b75">[76]</ref>, MMLD <ref type="bibr" target="#b52">[53]</ref>.</p><p>1. We are aware of recent DG solutions based on data augmentation. In <ref type="bibr" target="#b84">[85]</ref>, MSCOCO (http://cocodataset.or) and WikiArt (https://www.kaggle.com/ c/painter-by-number) are used for style transfer. None of the other considered references exploit those extra data collections so we do not include this method.  We carefully report the DeepAll reference for each method to have an overview on their relative advantage 2 .</p><p>Results: <ref type="table" target="#tab_2">Table 2</ref> shows the results of our multi-task approach on 2. The differences between the DeepAll results, are likely due to small undocumented inconsistencies and/or different library implementations of these baseline methods. Reporting them all is the only fair way of showing the relative improvement brought by each approach and highlighting possible inconsistencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 4</head><p>Comparison with DG-sota methods on Office-Home. Refer to <ref type="table" target="#tab_2">Table 2</ref> for notation details. the dataset PACS. We tested Jigsaw, Rotation and their combination. On average our approach produces results equal or better than all the competitors with the only exception of DDAIG which got the top results on Resnet-18. We highlight that DDAIG needs domain annotation for each source sample. In many practical conditions this information might not be available <ref type="bibr" target="#b51">[52]</ref>, and our multi-task method does not rely on it. Moreover, DDAIG benefits from a tailored per-domain model parameter selection, different from our approach for which the parameters are fixed and shared by all the domain pairs of each dataset. Analogous observations hold for the VLCS results <ref type="table" target="#tab_3">(Table 3</ref>). For Office-Home <ref type="table">(Table 4</ref>), Rotation appears more suitable than Jigsaw as auxiliary task with a gain larger than three percentage points over the DeepAll baseline and with even higher advantage in the Jigsaw+Rotation case. DDAIG, although producing apparently the top average result, improves slightly more than one percentage point over its DeepAll reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Single-Source Domain Generalization</head><p>The generalization ability of a model depends both on the learning process and on the used training data. To better evaluate the regularization effect provided by the self-supervised tasks, we investigate the case of training data from a single source domain. Baseline and Datasets: For these experiments we compare against the generalization method based on adversarial data augmentation (Adv.DA) presented in <ref type="bibr" target="#b74">[75]</ref>. We based our model on their same backbone (conv-pool-conv-pool-fc-fc-softmax), we reproduced their experimental setting and adopted a similar result display style with bar plots. We trained a model on 10k digit samples of the MNIST dataset <ref type="bibr" target="#b40">[41]</ref> and evaluated on the respective test sets of MNIST-M <ref type="bibr" target="#b26">[27]</ref> and SVHN <ref type="bibr" target="#b56">[57]</ref>. The digits are handwritten on black background for MNIST and on colorful background for MNIST-M. In SVHN the images are house numbers from Google Street View. To work with comparable datasets, all the images were resized to 32 ? 32 and treated as RGB.</p><p>Results: In <ref type="figure">Figure 4</ref> we show the performance of Jigsaw and Rotation when varying the data bias ? and the self-supervised task weight ?. With the red background shadow we indicate the overall range covered by Adv.DA results when changing its parameters, while the horizontal line is the reference Adv.DA results around which the authors of <ref type="bibr" target="#b74">[75]</ref> ran their ablation analysis. The bar plots indicates that, although Adv.DA can reach high peak values, it is also very sensitive to the chosen hyper-parameters. On the other hand, our multi-task approach is much more stable and usually performs better than Adv.DA. One exception arises on SVHN, with Jigsaw when the data bias is 0.5, and with Rotation when the self-supervised task weight is 0.9: both correspond to limit cases for the proper combination of object classification and selfsupervised learning as will be discussed in the next section. More-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=0.4</head><p>DeepAll Rotation <ref type="figure">Fig. 6</ref>. Ablation results on the Alexnet-PACS DG setting when using Rotation. We report the average accuracy over all target domains with three repetitions for each run. The red line is our DeepAll from <ref type="table" target="#tab_2">Table 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.6">Ablation and hyper-parameter tuning</head><p>As mentioned in Sec. 3.4, the parameters ? and ? of our multitask approach regulates respectively the importance of the selfsupervised auxiliary loss, and the amount of samples out of each input data batch that reaches the self-supervised branch.</p><p>By considering extreme cases for those parameters we obtain an ablation study on the respective roles of the self-supervised and of the supervised task of the learning model. Furthermore, we test the robustness of our method to the number of Jigsaw classes (patch permutations) P , and to the dimension of the patch grid n ? n. Baseline: for these experiments we focus on the Alexnet-PACS DG setting. We keep the Jigsaw hyper-parameters fixed with a 3?3 patch grid and P = 30 when studying ablation. Setting {? = 0, ? = 1} means that the self-supervised task is off, and the data batches contain only original ordered images, which corresponds to our DeepAll baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results -Jigsaw ablation:</head><p>The value assigned to the data bias ? drives the training: it moves the focus from the self-supervised task when using low values (? &lt; 0.5), to object classification when using high values (? ? 0.5). We set the data bias to ? = 0.6 which means that we fed the network with more ordered than shuffled images, thus keeping the classification as the primary goal of the network. In this case, when changing the loss weight ? in {0.1, 1}, we observe results which are always either statistically equal or better than the DeepAll baseline as shown in the first plot on the left of <ref type="figure">Figure 5</ref>. The second plot indicates that, for high values of ?, tuning ? has a significant effect on the overall performance. Indeed {? ? 1, ? = 1} means that Jigsaw is on and highly relevant in the learning process, but we are feeding the network only with ordered images: in this case the puzzle task is trivial and forces the network to recognize always the same permutation class which, instead of regularizing the learning process, may increase the risk of data memorization and overfitting. Further experiments confirm that, for ? = 1 but lower ? values, our multi-task method based on  <ref type="figure">Fig. 8</ref>. CAM activation maps: yellow corresponds to high values, while dark blue corresponds to low values. The jigsaw puzzle task is able to localize the most informative part of the image, useful for object class prediction regardless of the visual domain. Rotation recognition has a similar effect but tend to be less precise in localization especially for sketches, cartoon and paintings.</p><p>Jigsaw and DeepAll perform equally well. Setting ? = 0 means feeding the network only with shuffled images. For each image we have P variants, only one of which has the patches in the correct order and is allowed to enter the object classifier, resulting in a drastic reduction of the real batch size. In this condition the object classifier is unable to converge, regardless of Jigsaw being active (? &gt; 0) or not (? = 0). In those cases the accuracy is very low (&lt; 20%), so we do not show it in the plots to ease the visualization.</p><p>Results -Jigsaw hyper-parameter tuning: By using the same experimental setting of the previous paragraph, the third plot in <ref type="figure">Figure 5</ref> shows the change in performance when the number of Jigsaw classes P varies between 5 and 1000. We started from a low number, with the same order of magnitude of the number of object classes in PACS, and we grew till 1000 which is the value used for the experiments in <ref type="bibr" target="#b57">[58]</ref>. We observe an overall variation of 1.5 percentage points in the accuracy which still remains almost always higher than the DeepAll baseline. Finally, we ran a test to check the accuracy when changing the grid size and consequently the patch number. Even in this case, the range of variation is limited when passing from a 2 ? 2 to a 4 ? 4 grid, confirming the conclusions of robustness already obtained for this parameter in <ref type="bibr" target="#b57">[58]</ref> and <ref type="bibr" target="#b16">[17]</ref>. Moreover all the results are better than DeepAll.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results -Rotation ablation:</head><p>Changing the orientation has a milder effect on the global appearance of the image with respect to patch decomposition and puzzle reordering. One significant further difference between the Rotation and Jigsaw tasks is in the number of self-supervised classes which is P ? 10 ? 50 for Jigsaw and just 4 for Rotation, which actually reduces to 3 if we consider that one of the classes matches with the original image orientation. In this conditions, even using a low ? = 0.4 does not distract the network focus from the main object classification task and, combined with ? = 0.4 produces the results reported in <ref type="table" target="#tab_2">Table 2</ref>. For the ablation analysis we keep each of the two parameters fixed while varying the other: the results are always above the DeepAll baseline and on average the performance variation is limited (around 1 percentage point) indicating low sensitivity to the specific parameter settings.</p><p>Results -self-supervised performance: We have seen how the selfsupervised tasks support the main supervised classifier for domain generalization, but it is also interesting to check their own internal functioning and whether those tasks get meaningful results. We show their performance when testing on the same target images used to evaluate the object classifier but with shuffled patches for Jigsaw and randomly changed orientation for the Rotation task. In <ref type="figure" target="#fig_4">Figure 7</ref>, the first plot shows the accuracy over the learning epochs for the Object, Rotation and Jigsaw classifiers indicating that it grows for all of them simultaneously (on different scales). The second plot shows the Jigsaw recognition accuracy when changing the number of permutation classes P : of course the performance decreases when the task becomes more difficult, but overall the obtained results indicate that the Jigsaw model is always effective in reordering the shuffled patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.7">Visual Explanation and Failure Cases</head><p>As highlighted in <ref type="bibr" target="#b37">[38]</ref>, supervised deep models tend to focus too much on local image statistics, which limits the generalization and robustness properties of the learned representation. The jigsaw puzzle and the rotation recognition task, by forcing the network to use the whole image, allow to capture global information and to identify domain agnostic object shapes. By combining the supervised and self-supervised objectives we aim at learning a representation better able to capture discriminative cues, helpful in recognizing the image object content across domains. To analyse this behaviour we used the Class Activation Mapping (CAM, <ref type="bibr" target="#b85">[86]</ref>) method on ResNet-18 DG experiments, with which we produced the activation maps in <ref type="figure">Fig. 8</ref> for the PACS dataset. The first two rows show that our multi-task approach with Jigsaw or Rotation self-supervision is better at localizing the object class with respect to DeepAll. Rotation seems slightly less precise than Jigsaw in capturing the object shapes especially when dealing with sketches (see the dog on the second and sixth row), cartoon and paintings (fourth and fifth row), while works reasonably well with photos. The last two rows indicate that for both Jigsaw and Rotation the recognition mistakes are related to some flaw in data interpretation, while the localization remains meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.8">Predictive Domain Adaptation</head><p>Recent works have investigated intermediate settings between DG and DA. In PrDA <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b81">[82]</ref> one labeled and several unlabeled source domains are available at training time, together with their descriptive meta-data which are a very specific kind of domain labels (see <ref type="figure">Fig. 9</ref>). The meta-data of the target is also available: they can be used to relate the target domain to the known sources and compose a target model. Since this model is obtained without having access to the target images we are still in the DG scenario. However, the task is clearly simplified because the level of domain similarity between the sources and the target is known a priori. We believe it is worthwhile to evaluate our multi-task method in this scenario for two main reasons. (1) Most of the existing DG methods urge both domain and class labels of training data to work. PrDA techniques can be considered as DG methods with reduced needs in terms of class supervision, but strongly dependent on the availability of domain labels. By leveraging on self-supervision, our multi-task approach can work with a limited set of annotated source samples, as in AdaGraph <ref type="bibr" target="#b50">[51]</ref>, but it is ... <ref type="bibr">(rear, 2012)</ref> source unlabeled <ref type="bibr">(front, 2009)</ref> target <ref type="bibr">(rear-side, 2014)</ref> ? <ref type="figure">Fig. 9</ref>. Scheme of the Predictive DA setting. The goal is to recognize the four types of car, while the view point and the year are the meta-data.</p><p>also completely free from the need of source (and target) domain labels. Thus, it is much cheaper in terms of manual annotations and would still be reliable in case of missing or noisy domain labels.</p><p>(2) The existing PrDA testbeds focus on fine-grained classification tasks, thus allowing us to evaluate our method on a recognition problem significantly different with respect to that offered by the standard DG datasets. Baseline and Dataset: We use as baseline the source-only case which learns from the single labeled source and cannot exploit unlabeled data. We also consider a state of the art semi-supervised method based on Label Propagation (LP, <ref type="bibr" target="#b35">[36]</ref>) and the Minimum Class Confusion multi-target approach (MCC, <ref type="bibr" target="#b38">[39]</ref>). LP uses the unlabeled images in the learning process via pseudo-labeling, while they are considered as a temporary target data for MCC that adapts on them and finally uses the obtained model on the real target. Finally, AdaGraph <ref type="bibr" target="#b50">[51]</ref> is our main PrDA reference. It is a very recent approach that exploits domain-specific batchnormalization layers to learn models for each source domain in a graph, where the graph is provided on the basis of the source auxiliary meta-data. We follow the experimental protocol described in <ref type="bibr" target="#b50">[51]</ref> on the Comprehensive Cars (CompCars) dataset <ref type="bibr" target="#b80">[81]</ref>. We used a subset of 24,151 images with 4 categories (MPV, SUV, sedan and hatchback) which are type of cars produced between 2009 and 2014 and taken under 5 different view points (front, front-side, side, rear, rear-side). Each view point and each manufacturing year defines a separate domain and specifies its meta-data, leading to a total of 30 domains. We selected a pair of domains as source and target and use the remaining 28 as auxiliary unlabeled sources. Considering all possible domain pairs, we got 870 experiments and observe the average accuracy results over all of them. More in details, we started from an Imagenet pretrained model and trained for 6 epochs on source domain using Adam as optimizer with weight decay of 10 6 . The batch size used is 16 and the learning rate is 10 ?3 for the classifier and 10 ?4 for the rest of the network; the learning rate is decayed by a factor of 10 after 4 epochs. We tried both Jigsaw and Rotation with loss weight parameter set to ? = 0.5. Results: <ref type="table" target="#tab_7">Table 5</ref> collects the obtained results and shows that our multi-task approach significantly improves over the source-only baseline, as well as over LP and MCC. AdaGraph, which leverages on both the meta-information and the unlabeled data, shows the top result. Considering the limited gap between AdaGraph and our Jigsaw based result, we claim that when the meta-data information is noisy or missing, our approach can be used as reliable and inexpensive fallback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Self-Supervised Domain Adaptation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Single-and Multi-Source Domain Adaptation</head><p>When unlabeled target samples are available at training time we can use any self-supervised task on them. Indeed we can run patch reordering and orientation recognition on both source and target data to support adaptation of the source classification model. Baselines and Datasets: We consider as reference four families of DA approaches. The first is based on measuring the Maximum Mean Discrepancy (MMD, <ref type="bibr" target="#b68">[69]</ref>) across domains and minimizing it to reduce the domain shift: DAN <ref type="bibr" target="#b48">[49]</ref>, JAN <ref type="bibr" target="#b49">[50]</ref>. The second adopts adversarial approaches as DANN <ref type="bibr" target="#b26">[27]</ref> which is based on reverse gradient backpropagation from the auxiliary domain classification network branch. A third family is that based on batch normalization: Dial <ref type="bibr" target="#b13">[14]</ref> introduced adaptive layers to match source and target distribution to a standard gaussian. In DDiscovery <ref type="bibr" target="#b51">[52]</ref> the same idea is revisited to first discover the existence of multiple latent domains in the source and then differently adapt their knowledge to the target. Finally the fourth family focuses on increasing the feature norms of the two domains with the Hard Adaptive Feature Norm (HAFN, <ref type="bibr" target="#b79">[80]</ref>) method and its step-wise variant SAFN. Several DA approaches minimize the entropy loss as an extra domain alignment condition (e.g. SAFN+ENT). For a fair comparison we also turned on the entropy loss for our method. Moreover we solve the self-supervised task either involving both source and target or considering only the latter. We weight the source and target self-supervised loss equally on the basis of the source cross-validation. As datasets we considered Office-Home for the single-source experiments and PACS for the multi-source setting. As in the DG case, all the reported results are average over three runs. Results: Tables 6 shows the single source results on Office-Home. Our multi-task approach improves over its baseline and over DAN, JAN, DANN but has worse performance than HAFN, SAFN and SAFN+ENT. Although not usually presented, we show the specific baseline (ResNet-50) results of the HAFN/SAFN methods to better evaluate their relative gain. Indeed their basic architecture has an extra fully connected layer with respect to a standard ResNet which appears particularly helpful in this cross-domain setting. We performed also a stability analysis by turning off the self-supervised task on the source ? s = 0: the minimal results variation indicates that most of the adaptive effect originates from running the self-supervised task on the target.</p><p>The multi-source experiments in <ref type="table" target="#tab_9">Table 7</ref> shed further light on the adaptive abilities of the auxiliary self-supervised objective included in our multi-task approach. When the source domain is rich and covers large style variability, our method is able to outperform not only the batch-normalization based techniques Dial and DDiscovery, but also the state-of-the-art DA approaches HAFN and SAFN which have more difficulties in aligning the norms between the multiple sources and a single target domain. Among Jigsaw and Rotation, the second appears more suitable for domain adaptation, with higher performance and better stability to hyper-parameter tuning. When the two self-supervised tasks are combined we get on average a small accuracy improvement. The bottom part of the table also shows the effect of changing the ? value which appears more relevant for Jigsaw than for Rotation. On the Jigsaw+Rotation model we also report the DG result which corresponds to setting ? t = 0 and ? = 0, while keeping all the other chosen parameters. We further show the separate effect of turning off only the self-supervised tasks on the target (? t = 0) or  the entropy loss (? = 0). This ablation highlights how the major adaptive effect originates from the self-supervised tasks running on the target rather than from the entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Partial Domain Adaptation</head><p>The setting with source and target domains sharing exactly the same classes may be too restrictive. Here we discuss experimental results on the more realistic PDA setting where the target domain contains only a subset of the source classes. Baselines: We consider as reference five PDA methods all based on down-weighting the importance of source classes which are absent in the target. The methods SAN <ref type="bibr" target="#b8">[9]</ref>, PADA <ref type="bibr" target="#b9">[10]</ref>, and DRCN <ref type="bibr" target="#b46">[47]</ref>, exploit the source model prediction to evaluate the target class distribution. A different solution is proposed by IWAN <ref type="bibr" target="#b82">[83]</ref>, where each domain has its own feature extractor and the source sample weight is obtained from the domain recognition model rather than from the source classifier. The most recent ETN <ref type="bibr" target="#b10">[11]</ref> uses only the relevant source examples to train both the label classifier and the domain discriminator. The relevance (weight) of each source example is computed through an auxiliary domain discriminator, not directly involved in the adaptation phase, which quantifies the source example transferability. The methods HAFN and SAFN leverage only the sample norms rather than the whole domain distributions and are quite robust to negative transfer also in the PDA setting, without the  <ref type="figure">Fig. 10</ref>. Histogram showing the elements of the ? vector, corresponding to the class weight learned by PADA, SSPDA-? and SSPDA-PADA for the A?W experiment. need of any weighting mechanism. Thus, we also considered them as reference. Finally, we report the results of DAN and DANN as basic adaptive baselines, to show the effect of methods not originally designed to deal with PDA. Datasets: We follow previous literature in choosing two datasets and their related setting for the PDA experiments. We use Office-31 <ref type="bibr" target="#b65">[66]</ref> which contains 4652 images of 31 object categories common in office environments. Samples are drawn from three annotated distributions: Amazon (A), Webcam (W) and DSLR (D) which correspond respectively to online vendor website, digital SLR camera and web camera images. Similarly to <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, 10 classes are used as target for this dataset (the same classes shared by this dataset with Caltech-256 <ref type="bibr" target="#b31">[32]</ref>). The second testbed is VisDA2017, originally used in the 2017 Visual Domain Adaptation challenge (classification track): with respect to the other datasets, it allows us to investigate the proposed multi-task approach on a very large-scale sample size scenario. It has two domains, synthetic 2D object renderings and real images with a total of 208k images organized in 12 categories. We focus on the synthetic-to-real shift, the same considered in the challenge, but keeping only the first 6 categories of the target in alphabetic order. For all the experiments we use ResNet-50 as backbone. Results: <ref type="table" target="#tab_10">Tables 8 and 9</ref> show the obtained results respectively on Office-31 and VisDA2017 datasets. Each table is organized in four horizontal blocks: the first one shows the results obtained without adaptation or with standard DA methods, the second block illustrates the performance with algorithms designed to deal with PDA, the third one includes the performance of the norm-based adaptation approaches HAFN/SAFN together with their corresponding ResNet-50 baseline. Finally, the fourth part contains the results of our method. We remind that, as described in Sec. 3.3, our approach in the PDA setting does not involve the source data in the auxiliary self-supervised task: indeed the results obtained in the single source DA setting confirmed that it is possible to set ? s = 0 without any performance drop (see <ref type="table" target="#tab_8">Table  6</ref>). Moreover, we set ? t = 1.0 for all the experiments.</p><p>All the tables show that both Jigsaw and Rotation outperform the first group of adaptive references. With respect of the PDA techniques in the second group, our method shows better results on VisDA2017 even if many of these competitors take advantage by a ten-crop image evaluation procedure (indicated by the star * ). The top result on Office-31 is obtained by ETN which however, has a dedicated parameter selection procedure for each domain pair, different from our approach for which the parameters are fixed and shared by all the domain pairs of a dataset. Finally the HAFN/SAFN variants in the third group confirm the effectiveness of the norm-based methods also for PDA. Their results are comparable or worse than ours.</p><p>Despite not being tailored for the PDA setting, the obtained performance show that the auxiliary self-supervised task supports adaptation also in this scenario. Given that our solution is orthogonal to the sample selection strategies, we further tried to combine them together to evaluate if they complement each other. Specifically, we focused on Office-31 and the Jigsaw: we estimated the target class statistics through the weight ? and included also a domain discriminator weighted by the parameter ?, following <ref type="bibr" target="#b9">[10]</ref> as discussed in Sec. 3.3. To allow a fair comparison we also adopted the ten-crop evaluation. The results in the last two rows of <ref type="table" target="#tab_10">Table 8</ref> indicate that estimating the target statistics helps the network to focus only on the shared categories, with an average accuracy improvement of two percentage points over the plain Jigsaw method, getting up to a result comparable with that of ETN considering the standard deviation. We can state that the advantage comes from a better alignment of the domain features: by comparing the ? values on the A?W domain shift we observe that Jigsaw-? is more precise in identifying the missing classes of the target (see <ref type="figure">Figure 10</ref>). We indicate with Jigsaw-?, ? the case that includes the domain classifier: since the produced features are already well aligned across domains, we fixed ?-max to 0.1 and observed a further small average improvement. From the last bar plot on the right of <ref type="figure">Figure 10</ref> we also observe a better identification of the target classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This work provides an extensive study on the use of selfsupervised learning across domains. In particular we focused on solving jigsaw puzzles and recognizing image orientation, showing that they can be easily integrated in a multi-task approach with supervised learning. The results show an improvement in cross-domain robustness and an advantage on generalization performance: the obtained results are competitive with that of more elaborate domain adaptation and domain generalization methods. Our work paves the way for many other adaptive methods exploiting the invariances captured by the most recent self-supervised solutions <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b37">[38]</ref>, also beyond object classification towards other challenging tasks like semantic segmentation <ref type="bibr" target="#b77">[78]</ref>, detection <ref type="bibr" target="#b20">[21]</ref> or 3D visual learning <ref type="bibr" target="#b1">[2]</ref> where the domain shift effect strongly impacts the deployment of methods in the wild.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1.</head><label></label><figDesc>Fig. 1. Recognizing objects across visual domains is a challenging task that requires high generalization abilities. Self-supervisory image signals allow to capture natural invariances and regularities that can help to bridge across large style gaps. With our multi-task approach we learn jointly to classify objects and solve jigsaw puzzles or recognize image orientation, showing that this supports generalization to new domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>arXiv:2007.12368v2 [cs.CV] 31 Mar 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Our PDA approach with jigsaw puzzle self-supervision. The main blocks of the network are in gray. The solid line arrows indicate the contribution of each group of training samples to the corresponding final tasks. The related optimization goals appear at the end of the black/green/ocher arrows. The red blocks illustrate the domain adversarial classifier and source sample weighting procedure (weight ?). An analogous scheme holds with self-supervised rotation recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Single Source DG experiments. We analyze the performance of our multi-task Jigsaw (top row) and Rotation (bottom row) approaches in comparison with Adv.DA<ref type="bibr" target="#b74">[75]</ref>. The shaded background area covers the overall range of results of Adv.DA obtained when changing the hyperparameters of the method. The reference result of Adv.DA (? = 1, K = 2) together with its standard deviation is indicated here by the horizontal red line. The blue histogram bars show the performance of Jigsaw and Rotation when changing the self-supervised task weight ? and data bias ? . Ablation results and hyper-parameter analysis on the Alexnet-PACS DG setting when using Jigsaw. The reported accuracy is the global average over all the target domains with three repetitions for each run. The red line represents our DeepAll average fromTable 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Analysis of the Jigsaw classifier on Alexnet-PACS DG setting. In the left plot each axes refers to the color matching curve in the graph.over, Jigsaw and Rotation have similar performance to Adv.DA on MNIST-M and significantly outperform it on SVHN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, B. Caputo, T. Tommasi are with Politecnico di Torino, Italy, Italian Institute of Technology. E-mail {name.surname}@polito.it ? A. D'Innocente is with University of Rome Sapienza, Italy, Italian Institute of Technology. E-mail: dinnocente@diag.uniroma1.it ? Y. Liao is with Politecnico di Torino, Italy. E-mail s274673@studenti.polito.it</figDesc><table /><note>? F.M. Carlucci is with Huawei AI Theory, London. Work done while at University of Rome Sapienza, Italy. E-mail: fabio.maria.carlucci@huawei.com</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Test on different tasks and architectures: DG classification accuracy. The target is indicated as column title. Best results are in bold. Top: self-supervised pretraining on Imagenet, followed by fine-tuning on the source. (p) indicates the methods that use patch-based networks, while (w) the ones that use whole-images networks. Bottom: supervised pretraining on Imagenet followed by the multi-task combination of self-supervised objective and supervised fine-tuning.PACS art paint. cartoon sketches photo Avg.</figDesc><table><row><cell></cell><cell cols="3">Self-Supervised Pretraining</cell></row><row><cell>J-CFN (p)</cell><cell>47.23</cell><cell>62.18</cell><cell>58.03 70.18 59.41</cell></row><row><cell>J-CFN+ (p)</cell><cell>51.14</cell><cell>58.83</cell><cell>54.85 73.44 59.57</cell></row><row><cell>J-AlexNet (w)</cell><cell>38.93</cell><cell>53.75</cell><cell>49.00 64.23 51.48</cell></row><row><cell>R-AlexNet (w)</cell><cell>52.08</cell><cell>59.24</cell><cell>56.54 72.91 60.19</cell></row><row><cell cols="4">Supervised Pretraining and Multitask</cell></row><row><cell cols="2">C-CFN-DeepAll (p) 59.69</cell><cell>59.88</cell><cell>45.66 85.42 62.66</cell></row><row><cell>C-CFN-Jigsaw (p)</cell><cell>60.68</cell><cell>60.55</cell><cell>55.66 82.68 64.89</cell></row><row><cell cols="2">AlexNet-DeepAll (w) 66.50</cell><cell>69.65</cell><cell>61.42 89.68 71.81</cell></row><row><cell cols="2">AlexNet-Jigsaw (w) 67.79</cell><cell>70.79</cell><cell>64.01 89.64 73.05</cell></row><row><cell cols="2">AlexNet-Rotation (w) 69.43</cell><cell>69.40</cell><cell>65.20 89.17 73.30</cell></row><row><cell cols="4">4.1.3 Supervised Pretraining and Multi-task Learning</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Comparison with DG-sota methods on PACS. The target is indicated as column title. We report the used hyper-parameters, obtained through source cross-validation. The top result is highlighted in bold. Only to get fair comparison with MASF we computed the max target accuracy over the training period: the results are indicated with . The top result in this case is underlined.</figDesc><table><row><cell></cell><cell>PACS</cell><cell cols="4">art paint. cartoon sketches photo</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell>Alexnet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[44]</cell><cell>DeepAll TF</cell><cell>63.30 62.86</cell><cell>63.13 66.97</cell><cell>54.07 57.51</cell><cell>87.70 89.50</cell><cell>67.05 69.21</cell></row><row><cell>[22]</cell><cell>DeepAll D-SAM</cell><cell>64.44 63.87</cell><cell>72.07 70.70</cell><cell>58.07 64.66</cell><cell>87.50 85.55</cell><cell>70.52 71.20</cell></row><row><cell>[45]</cell><cell>DeepAll Epi-FCR</cell><cell>63.40 64.70</cell><cell>66.10 72.30</cell><cell>56.60 65.00</cell><cell>88.50 86.10</cell><cell>68.70 72.00</cell></row><row><cell>[43]</cell><cell>DeepAll MLDG</cell><cell>64.91 66.23</cell><cell>64.28 66.88</cell><cell>53.08 58.96</cell><cell>86.67 88.00</cell><cell>67.24 70.01</cell></row><row><cell>[4]</cell><cell>DeepAll MetaReg</cell><cell>67.21 69.82</cell><cell>66.12 70.35</cell><cell>55.32 59.26</cell><cell>88.47 91.07</cell><cell>69.28 72.62</cell></row><row><cell>[76]</cell><cell>DeepAll PAR</cell><cell>63.30 68.70</cell><cell>63.10 70.50</cell><cell>54.00 64.60</cell><cell>87.70 90.40</cell><cell>67.03 73.54</cell></row><row><cell>[53]</cell><cell>DeepAll MMLD</cell><cell>68.09 66.99</cell><cell>70.23 70.64</cell><cell>61.80 67.78</cell><cell>88.86 89.35</cell><cell>72.25 73.69</cell></row><row><cell></cell><cell>DeepAll</cell><cell>66.50</cell><cell>69.65</cell><cell>61.42</cell><cell>89.68</cell><cell>71.81?0.26</cell></row><row><cell></cell><cell>Jigsaw?=0.9,?=0.6</cell><cell>67.76</cell><cell>70.79</cell><cell>64.01</cell><cell>89.64</cell><cell>73.05?0.20</cell></row><row><cell></cell><cell>Rotation?=0.4,?=0.4</cell><cell>69.43</cell><cell>69.40</cell><cell>65.20</cell><cell>89.17</cell><cell>73.30?0.47</cell></row><row><cell cols="2">Jigsaw+Rotation? J =0.9,?R=0.9,?=0.4</cell><cell>69.70</cell><cell>71.00</cell><cell>66.00</cell><cell>89.60</cell><cell>74.08?0.32</cell></row><row><cell>[25]</cell><cell>DeepAll MASF</cell><cell>67.60 70.35</cell><cell>68.87 72.49</cell><cell>61.13 67.33</cell><cell>89.20 90.58</cell><cell>71.70 75.21</cell></row><row><cell></cell><cell>Jigsaw</cell><cell>69.76</cell><cell>72.27</cell><cell>66.41</cell><cell>90.97</cell><cell>74.86?0.64</cell></row><row><cell></cell><cell>Rotation</cell><cell>69.80</cell><cell>71.10</cell><cell>66.57</cell><cell>90.13</cell><cell>74.40?0.67</cell></row><row><cell></cell><cell>Jigsaw+Rotation</cell><cell>70.23</cell><cell>73.33</cell><cell>67.23</cell><cell>90.40</cell><cell>75.30?0.50</cell></row><row><cell></cell><cell></cell><cell>Resnet-18</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[22]</cell><cell>DeepAll D-SAM</cell><cell>77.87 77.33</cell><cell>75.89 72.43</cell><cell>69.27 77.83</cell><cell>95.19 95.30</cell><cell>79.55 80.72</cell></row><row><cell>[45]</cell><cell>DeepAll Epi-FCR</cell><cell>77.60 82.10</cell><cell>73.90 77.00</cell><cell>70.30 73.00</cell><cell>94.40 93.90</cell><cell>79.10 81.50</cell></row><row><cell>[4]</cell><cell>DeepAll MetaReg</cell><cell>79.90 83.70</cell><cell>75.10 77.20</cell><cell>69.50 70.30</cell><cell>95.20 95.50</cell><cell>79.90 81.70</cell></row><row><cell>[87]</cell><cell>DeepAll DDAIG</cell><cell>77.00 84.20</cell><cell>75.90 78.10</cell><cell>69.20 74.70</cell><cell>96.00 95.30</cell><cell>79.50 83.10</cell></row><row><cell>[53]</cell><cell>DeepAll MMLD</cell><cell>78.34 81.28</cell><cell>75.02 77.16</cell><cell>65.24 72.29</cell><cell>96.21 96.09</cell><cell>78.70 81.83</cell></row><row><cell></cell><cell>DeepAll</cell><cell>77.83</cell><cell>74.26</cell><cell>65.81</cell><cell>95.71</cell><cell>78.40?0.28</cell></row><row><cell></cell><cell>Jigsaw?=0.7,?=0.9</cell><cell>79.28</cell><cell>75.74</cell><cell>68.31</cell><cell>95.71</cell><cell>79.80?0.55</cell></row><row><cell></cell><cell>Rotation?=0.8,?=0.4</cell><cell>81.07</cell><cell>74.13</cell><cell>76.17</cell><cell>96.10</cell><cell>81.87?0.49</cell></row><row><cell cols="2">Jigsaw+Rotation? J =0.7,?R=0.7,?=0.8</cell><cell>81.07</cell><cell>73.97</cell><cell>74.67</cell><cell>95.93</cell><cell>81.41?0.50</cell></row><row><cell>[25]</cell><cell>DeepAll MASF</cell><cell>77.38 80.29</cell><cell>75.68 77.17</cell><cell>69.64 71.69</cell><cell>94.35 94.99</cell><cell>79.26 81.04</cell></row><row><cell></cell><cell>Jigsaw</cell><cell>80.00</cell><cell>76.52</cell><cell>70.70</cell><cell>96.03</cell><cell>80.81?0.31</cell></row><row><cell></cell><cell>Rotation</cell><cell>82.40</cell><cell>75.27</cell><cell>77.20</cell><cell>96.53</cell><cell>82.85?0.55</cell></row><row><cell></cell><cell>Jigsaw+Rotation</cell><cell>81.40</cell><cell>75.03</cell><cell>76.47</cell><cell>96.40</cell><cell>82.33?0.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Comparison with DG-sota methods on VLCS. Refer toTable 2for notation details.</figDesc><table><row><cell></cell><cell>VLCS</cell><cell cols="3">Caltech Labelme Pascal Sun</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell>Alexnet</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[44]</cell><cell>DeepAll TF</cell><cell>93.40 93.63</cell><cell>62.11 63.49</cell><cell>68.41 64.16 69.99 61.32</cell><cell>72.02 72.11</cell></row><row><cell>[20]</cell><cell>DeepAll SLRC</cell><cell>86.67 92.76</cell><cell>58.20 62.34</cell><cell>59.10 57.86 65.25 63.54</cell><cell>65.46 70.97</cell></row><row><cell>[22]</cell><cell>DeepAll D-SAM</cell><cell>94.95 91.75</cell><cell>57.45 56.95</cell><cell>66.06 65.87 58.59 60.84</cell><cell>71.08 67.03</cell></row><row><cell>[45]</cell><cell>DeepAll Epi-FCR</cell><cell>93.10 94.10</cell><cell>60.60 64.30</cell><cell>65.40 65.80 67.10 65.90</cell><cell>71.20 72.90</cell></row><row><cell>[53]</cell><cell>DeepAll MMLD</cell><cell>95.89 96.66</cell><cell>57.88 58.77</cell><cell>72.01 67.76 71.96 68.13</cell><cell>73.39 73.88</cell></row><row><cell></cell><cell>DeepAll</cell><cell>96.15</cell><cell>59.05</cell><cell cols="2">70.84 63.92 72.49?0.21</cell></row><row><cell></cell><cell>Jigsaw?=0.5,?=0.8</cell><cell>96.46</cell><cell>59.51</cell><cell cols="2">72.95 64.40 73.33?0.16</cell></row><row><cell></cell><cell>Rotation?=0.9,?=0.6</cell><cell>97.30</cell><cell>60.30</cell><cell cols="2">71.93 65.97 73.88?0.62</cell></row><row><cell cols="3">Jigsaw+Rotation? J =0.9,?R=0.5,?=0.7 96.30</cell><cell>59.20</cell><cell cols="2">70.73 66.37 73.15?0.36</cell></row><row><cell>[25]</cell><cell>DeepAll MASF</cell><cell>92.86 94.78</cell><cell>63.10 64.90</cell><cell>68.67 64.11 69.14 67.64</cell><cell>72.19 74.11</cell></row><row><cell></cell><cell>Jigsaw</cell><cell>98.27</cell><cell>61.44</cell><cell cols="2">73.61 66.53 74.96?0.21</cell></row><row><cell></cell><cell>Rotation</cell><cell>98.40</cell><cell>62.80</cell><cell cols="2">73.03 67.40 75.41?0.63</cell></row><row><cell></cell><cell>Jigsaw+Rotation</cell><cell>98.10</cell><cell>60.20</cell><cell cols="2">72.60 68.87 74.94?0.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>Predictive DA results. The top result is highlighted in bold.</figDesc><table><row><cell></cell><cell></cell><cell>Resnet-18</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Baseline Souce-Only [36] [39] LP MCC AdaGraph Jigsaw Rotation [51]</cell></row><row><cell>CompCars</cell><cell>56.80</cell><cell>57.91 59.00</cell><cell>65.10</cell><cell>63.00</cell><cell>61.77</cell></row><row><cell>source labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">meta-data: (side, 2010)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sedan</cell><cell>SUV</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hatchback</cell><cell>MPV</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6</head><label>6</label><figDesc>Accuracy on Office-Home under single-source DA setting. The top result is highlighted in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7</head><label>7</label><figDesc>Multi-source Domain Adaptation results on PACS. Jigsaw ? s =0.7,? t =0.1,?=0.8 85.40 81.49 76.93 98.35 85.54?1.63 Jigsaw ? s =0.7,? t =0.3,?=0.8 85.92 81.61 79.74 98.04 86.33?0.58 Jigsaw ? s =0.7,? t =0.5,?=0.8 87.01 81.25 78.87 98.00 86.28?0.67 Jigsaw ? s =0.7,? t =0.9,?=0.8 84.21 80.38 76.64 97.86 84.77?0.76 Rotation ? s =0.8,? t =0.1,?=0.4 89.27 81.30 82.23 89.73 87.71?0.13 Rotation ? s =0.8,? t =0.3,?=0.4 88.73 82.20 81.47 98.27 87.67?0.07 Rotation ? s =0.8,? t =0.5,?=0.4</figDesc><table><row><cell></cell><cell cols="2">PACS-DA</cell><cell cols="3">art paint. cartoon sketches photo</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Resnet-18</cell><cell></cell></row><row><cell></cell><cell cols="2">DeepAll</cell><cell>74.70</cell><cell>72.40</cell><cell>60.10 92.90</cell><cell>75.03</cell></row><row><cell>[52]</cell><cell></cell><cell>Dial</cell><cell>87.30</cell><cell>85.50</cell><cell>66.80 97.00</cell><cell>84.15</cell></row><row><cell></cell><cell cols="2">DDiscovery</cell><cell>87.70</cell><cell>86.90</cell><cell>69.60 97.00</cell><cell>85.30</cell></row><row><cell></cell><cell cols="2">DeepAll</cell><cell>76.17</cell><cell>73.58</cell><cell cols="2">55.65 96.07 75.37?0.42</cell></row><row><cell>[80]</cell><cell cols="2">HAFN SAFN</cell><cell>84.95 86.78</cell><cell>79.64 82.72</cell><cell cols="2">64.24 97.70 81.63?0.50 60.26 98.26 82.01?0.32</cell></row><row><cell></cell><cell cols="2">SAFN+ENT</cell><cell>89.22</cell><cell>87.39</cell><cell cols="2">60.02 98.14 83.69?0.17</cell></row><row><cell></cell><cell cols="2">DeepAll</cell><cell>77.83</cell><cell>74.26</cell><cell cols="2">65.81 95.71 78.40?0.28</cell></row><row><cell></cell><cell cols="2">Jigsaw ? s =? t =0.7,?=0.8</cell><cell>84.49</cell><cell>82.07</cell><cell cols="2">79.86 97.98 86.10?0.26</cell></row><row><cell></cell><cell cols="2">Rotation ? s =? t =0.8,?=0.4</cell><cell>89.97</cell><cell>82.60</cell><cell cols="2">82.00 98.07 88.16?0.51</cell></row><row><cell cols="2">Jigsaw+Rotation</cell><cell>? s J =? t R =0.8,?=0.8 R =? t ? s J =0.2,</cell><cell>89.67</cell><cell>82.87</cell><cell cols="2">83.93 98.17 88.66?0.36</cell></row><row><cell></cell><cell></cell><cell></cell><cell>89.83</cell><cell>80.10</cell><cell cols="2">81.13 98.00 87.27?0.94</cell></row><row><cell></cell><cell cols="2">Rotation ? s =0.8,? t =0.9,?=0.4</cell><cell>89.17</cell><cell>81.47</cell><cell cols="2">82.73 97.87 87.81?0.21</cell></row><row><cell></cell><cell cols="2">Jigsaw+Rotation ? t =0,?=0</cell><cell>81.07</cell><cell>73.97</cell><cell cols="2">74.67 95.93 81.41?0.50</cell></row><row><cell></cell><cell cols="2">Jigsaw+Rotation ? t =0</cell><cell>82.80</cell><cell>77.23</cell><cell cols="2">77.70 97.17 83.73?0.39</cell></row><row><cell></cell><cell cols="2">Jigsaw+Rotation ?=0</cell><cell>84.67</cell><cell>78.63</cell><cell cols="2">80.37 97.27 85.23?0.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8</head><label>8</label><figDesc>Classification accuracy in the PDA setting on Office-31 (source: 31 classes, target: 10 classes). The * indicates ten-crop testing. ] 87.57 98.08 99.36 88.11 93.95 93.77 93.47 Resnet-50 74.35 93.90 96.81 78.13 78.46 86.81 84.74?0.71 Jigsaw 91.75 94.12 98.93 90.87 89.95 93.42 93.18?0.46 Rotation 87.91 95.14 99.57 86.84 88.73 93.98 92.03?1.29 Jigsaw*-? 99.32 94.69 99.36 96.39 86.36 94.22 95.06?1.86 Jigsaw*-?, ? 99.66 94.46 99.57 97.67 87.33 94.26 95.49?1.19</figDesc><table><row><cell cols="4">Office-31-PDA A?W D?W W?D A ?D D?A W?A</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell>Resnet-50</cell></row><row><cell>Resnet-50</cell><cell cols="3">75.37 94.13 98.84 79.19 81.28 85.49</cell><cell>85.73</cell></row><row><cell>DAN [49]</cell><cell cols="3">59.32 73.90 90.45 61.78 74.95 67.64</cell><cell>71.34</cell></row><row><cell>DANN [27]</cell><cell cols="3">75.56 96.27 98.73 81.53 82.78 86.12</cell><cell>86.50</cell></row><row><cell>IWAN [83]</cell><cell cols="3">89.15 99.32 99.36 90.45 95.62 94.26</cell><cell>94.69</cell></row><row><cell>SAN* [9]</cell><cell cols="3">93.90 99.32 99.36 94.27 94.15 88.73</cell><cell>94.96</cell></row><row><cell>PADA* [10]</cell><cell cols="3">86.54 99.32 100 82.17 92.69 95.41</cell><cell>92.69</cell></row><row><cell>DRCN* [47]</cell><cell cols="3">86.00 88.05 95.60 100.0 95.80 100.0</cell><cell>94.30</cell></row><row><cell>ETN [11]</cell><cell cols="3">94.52 100.0 100.0 95.03 96.21 94.64</cell><cell>96.73</cell></row><row><cell>Resnet-50</cell><cell cols="3">76.05 97.52 99.36 83.23 83.89 86.18</cell><cell>87.71</cell></row><row><cell>HAFN [80]</cell><cell cols="3">79.89 97.63 99.57 84.93 89.59 90.08</cell><cell>90.28</cell></row><row><cell>SAFN [80]</cell><cell cols="3">84.52 97.40 98.94 84.50 92.07 92.90</cell><cell>91.72</cell></row><row><cell>6 SAFN+ENT [801 11 16 0 0.2 0.4 0.6 0.8 1 Weight</cell><cell>21 Shared Classes 26 Outlier Classes</cell><cell>31</cell></row><row><cell>Class Id</cell><cell></cell><cell></cell></row><row><cell>PADA</cell><cell></cell><cell>Jigsaw-?</cell><cell>Jigsaw-?, ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 9</head><label>9</label><figDesc>Classification accuracy in the PDA setting on VisDA2017 (source: 12 classes, target: 6 classes).</figDesc><table><row><cell cols="2">VisDA2017-PDA Synthetic?Real</cell></row><row><cell>Resnet-50</cell><cell></cell></row><row><cell>Resnet-50</cell><cell>45.26</cell></row><row><cell>DAN [49]</cell><cell>47.60</cell></row><row><cell>DANN [27]</cell><cell>51.01</cell></row><row><cell>PADA* [10]</cell><cell>53.53</cell></row><row><cell>DRCN* [47]</cell><cell>58.20</cell></row><row><cell>Resnet-50</cell><cell>49.89</cell></row><row><cell>HAFN [80]</cell><cell>65.06</cell></row><row><cell>SAFN [80]</cell><cell>67.65</cell></row><row><cell>SAFN+ENT* [80]</cell><cell>70.40</cell></row><row><cell>Resnet-50</cell><cell>58.65?0.66</cell></row><row><cell>Jigsaw</cell><cell>68.18?1.36</cell></row><row><cell>Rotation</cell><cell>71.95?0.39</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially founded by the ERC grant 637076 RoboExNovo (BC, SB, AD) and took advantage of the GPU donated by NVIDIA (Academic Hardware Grant, TT).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ar?Cl Ar?Pr Ar?Rw Cl ?Ar Cl?Pr Cl?Rw Pr?Ar Pr?Cl Pr?Rw Rw?Ar Rw?Cl Rw?Pr Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Joint supervised and selfsupervised learning for 3d real-world challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alliegro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07392</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A critical analysis of selfsupervision, or what we can learn from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bisanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Bisanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kail</surname></persName>
		</author>
		<title level="m">Learning in Children: Progress in Cognitive Development Research</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Domain Separation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tackling partial domain adaptation with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIAP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Partial transfer learning with selective adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Partial adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to transfer examples for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Autodial: Automatic domain alignment layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Just dial: domain alignment layers for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual permutation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Domain Adaptation in Computer Vision Applications. Advances in Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep domain generalization with structured low-rank constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="304" to="313" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">One-shot unsupervised cross-domain detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Borlino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11610</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domain generalization with domainspecific aggregation modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Domain generalization via model-agnostic learning of semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>De Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very young infants learn abstract rules in the visual modality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Franconeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Waxman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning representations by predicting bags of visual words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>7694</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">In search of lost domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Discovering latent domains for multisource domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Label propagation for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Grasp2vec: Learning object representations from self-supervised grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CoRL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Steering self-supervised feature learning beyond local pixel statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Minimum class confusion for versatile domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep residual correction network for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adagraph: Unifying predictive and continuous domain adaptation through graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Boosting domain adaptation by discovering latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Domain generalization using a mixture of multiple latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsuura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Twins: Two weighted inconsistency-reduced networks for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsuura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07405</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Boosting selfsupervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with selfsupervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Cross-domain self-supervised multi-task feature learning using synthetic imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">To transfer or not to transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Rosenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-Workshop</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">From source to target and back: symmetric bi-directional adaptive gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Open set domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Equivalence of distance-based and rkhs-based statistics in hypothesis testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2263" to="2291" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of the European Conference on Computer Vision (ECCV-Workshop)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Deep cocktail network: Multi-source unsupervised domain adaptation with category shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for fine-grained categorization and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Multivariate regression on the grassmannian for predicting novel domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Importance weighted adversarial nets for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Learning robust shape-based features for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="63748" to="63756" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Deep domain-adversarial image generation for domain generalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
