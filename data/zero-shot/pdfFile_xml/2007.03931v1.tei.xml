<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detection and Classification of Acoustic Scenes and Events</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20202-11-03">2020 2-3 November 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Turpault</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit? de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>F-54000</postCode>
									<settlement>Nancy</settlement>
									<region>Inria, Loria</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Serizel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit? de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>F-54000</postCode>
									<settlement>Nancy</settlement>
									<region>Inria, Loria</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detection and Classification of Acoustic Scenes and Events</title>
					</analytic>
					<monogr>
						<meeting> <address><addrLine>Tokyo, Japan</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="20202-11-03">2020 2-3 November 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Sound event detection</term>
					<term>weakly labeled data</term>
					<term>semi-supervised learning</term>
					<term>synthetic soundscapes</term>
					<term>ablation study</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training a sound event detection algorithm on a heterogeneous dataset including both recorded and synthetic soundscapes that can have various labeling granularity is a non-trivial task that can lead to systems requiring several technical choices. These technical choices are often passed from one system to another without being questioned. We propose to perform a detailed analysis of DCASE 2020 task 4 sound event detection baseline with regards to several aspects such as the type of data used for training, the parameters of the mean-teacher or the transformations applied while generating the synthetic soundscapes. Some of the parameters that are usually used as default are shown to be sub-optimal.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Ambient sound analysis aims at extracting information from the soundscapes that constantly surround us <ref type="bibr">[1]</ref>. Many ambient sound analysis applications, ranging from urban planning to home assisted living, have surfaced within the past first years <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7]</ref>. Most of these were inspired by the fact that we, humans constantly rely on the soundscapes around us to decide on how to act or react.</p><p>Sound event detection (SED) is a task of the ambient sound analysis that consists not only in predicting what sound event did occur in a recording but also to detect when did it happen. Intuitively, the simplest way to solve this problem with a system relying on supervised training would be to use a training dataset composed of so-called strongly labeled soundscapes (with onset and offset timestamps). However strongly labeling a sufficiently large dataset is prohibitive. Strong label annotations are also very likely to contain human errors/disagreement given the ambiguity in the perception of some sound event onsets and offsets. One alternative is too rely on so-called weakly labeled soundscapes (without timestamp) that are considerably cheaper to obtain <ref type="bibr" target="#b6">[8]</ref>. In the case of weakly labeled data, we only have information about whether an event is present in a recording or not. We have no information about how many times the event occurs nor the temporal locations of the occurrences within the audio clip. However, there are quite a few shortcoming in exploiting weakly labels soundscapes <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b9">11</ref>]. Another, cheaper, option is to generate realistic soundscapes that can then easily be strongly labeled <ref type="bibr" target="#b10">[12]</ref>. The problem in this latter case is that is that there might be some mismatch between the synthetic This work was made with the support of the French National Research Agency, in the framework of the project LEAUDS Learning to understand audio scenes (ANR-18-CE23-0020) and the French region Grand-Est. Experiments presented in this paper were carried out using the Grid5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several Universities as well as other organizations (see https://www.grid5000). soundscapes generated for training and the recorded soundscapes fed to the SED at runtime.</p><p>In DCASE 2019 task 4 <ref type="bibr" target="#b11">[13]</ref> we proposed to try solving the above problems by designing DESED, a dataset composed of weakly-labeled and unlabeled recorded soundscapes and strongly labeled synthetic soundscapes generated with Scaper <ref type="bibr" target="#b10">[12]</ref>. Exploiting such a heterogeneous dataset all together is not necessarily trivial. The baseline was inspired by previous submission to the challenge <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15]</ref> and developed incrementally (and so are quite a few systems trying to solve this task). These two aspects led to a solution that is involving a lot of different technical choices that were rarely clearly motivated in the literature. These choices are frequently passed from one system to another without being questioned.</p><p>The aim of this paper is to propose a detailed analysis of DCASE 2019 baseline <ref type="bibr" target="#b11">[13]</ref>. The ablation study includes the analysis of aspects such as the kind of data used for training, the parameters for the mean-teacher or the transformation applied on the synthetic soundscapes dataset. The conclusion of this study leads to a system that is close to the baseline for DCASE 2020 task 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM STATEMENT AND BASELINE DESCRIPTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Problem description</head><p>The problem we try to solve here is similar to that of DCASE 2019 Task 4 <ref type="bibr" target="#b11">[13]</ref> and focuses on the same 10 classes of sound events. Systems are expected to produce strongly labeled output (i.e. detect sound events with a start time, end time, and sound class label), but are provided with weakly labeled data (i.e. sound recordings with only the presence/absence of a sound event included in the labels without any timing information) for training. Multiple events can be present in each audio recording, including overlapping events. As in the previous iteration of this task, the challenge entails exploiting a large amount of unbalanced and unlabeled training data together with a small weakly annotated training set to improve system performance and an additional training set with strongly annotated synthetic soundscapes which can be created in many ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Sound event detection baseline</head><p>The SED baseline system is inspired by the best performing system from DCASE 2018 Task 4 <ref type="bibr" target="#b12">[14]</ref> and the improvement provided by the second best performing system in DCASE 2019 task 4 <ref type="bibr" target="#b13">[15]</ref> . It uses a mean-teacher model which is a combination of two models: a student model and a teacher model (both have the same architecture). The student model is the final model used at inference time, while the teacher model aims at helping the student model during training and its weights are an exponential moving average of the arXiv:2007.03931v1 [cs.SD] 8 Jul 2020 Our implementation of the mean-teacher model is based on the work of Tarvainen and Valpola <ref type="bibr" target="#b14">[16]</ref>. The models are a combination of convolutional neural network (CNN) and recurrent neural network (RNN) called CRNN. The model architecture is inspired by DCASE 2019 task4 second to best system.</p><p>The student model is trained on the strongly and weakly labeled data. The loss (binary cross-entropy) is computed at the frame level for the strongly labeled synthetic data and at the clip level for the weakly labeled data. The teacher model is not trained, rather, its weights are a moving average of the student model (at each epoch).</p><p>During training, the teacher model receives the same input as the student model but with added Gaussian noise, and helps train the student model via a consistency loss (mean-squared error) for both strong (frame-level) and weak (clip-level) predictions for all the data in the batch. Every batch contains a combination of unlabeled, weakly and strongly labeled samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BASELINE SETUP AND DATASET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">DESED dataset</head><p>The dataset used for SED experiments is DESED 1 , a flexible dataset for SED composed of 10-sec audio clips recorded in a domestic environment or synthesized to simulate a domestic environment <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b15">17]</ref>. The recorded soundscapes are taken from Au-dioSet <ref type="bibr" target="#b16">[18]</ref>. The synthetic soundscapes are generated using Scaper <ref type="bibr" target="#b10">[12]</ref>, a python library for soundscape synthesis and augmentation. The foreground events are obtained from the Freesound Dataset (FSD50k) <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b18">20]</ref>. The background textures are obtained from the SINS dataset (activity class "other") <ref type="bibr" target="#b19">[21]</ref> and TUT scenes 2016 development dataset <ref type="bibr" target="#b20">[22]</ref>.</p><p>The dataset includes a validation set and a public evaluation set composed of recorded clips (VAL and EVAL) that are used to adjust the hyper-parameters and evaluate the SED, respectively. All the experiments reported in Tables 1-5 are performed on VAL.</p><p>In order to monitor the SED model convergence, we further split the DESED training set into an training set and a crossvalidation set (refereed to as x-valid in the tables). In the public 1 https://project.inria.fr/desed/ DCASE 2020 task 4 baseline, the cross-validation set was composed of 10% on the total amount of weakly labeled soundscapes and 10% of the total amount of synthetic soundscapes generated in the training set 2 .</p><p>The difference here is that the cross-validation synthetic soundscapes are generated using a separate set different isolated foreground events that are different from those used to generate the training set (isolated events are split into 90%/10% for the train/cross-validation respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sound event detection baseline</head><p>The input features for the SED baseline 3 are mel-spectrograms with 128 mel bands. The signals are sampled at <ref type="bibr" target="#b14">16</ref>  The RNN block is composed of 2 layers of 128 bidirectional gated recurrent units. The RNN block is followed by an attention pooling layer that is multiplication between a linear layer with softmax activation and linear layer with sigmoid activation.</p><p>The model is trained with Adam optimizer, we apply 50 % dropout. The model is trained for 200 epochs and the best epoch on the cross-validation set is kept.</p><p>When they are used, the SED detection thresholds are fixed to 0.5 for every classes. The post-processing is a median filtering on ?0.45 s (27 frames at 16 kHz).     <ref type="table">Table 4</ref>: SED performance depending on the SNR parameter for the noise applied to the mean-teacher input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation metrics</head><p>overall F1-score is the unweighted average of the class-wise F1scores (macro-average). F1-scores are computed on a single operating point (decision thresholds=0.5) using the sed eval library <ref type="bibr" target="#b21">[23]</ref>. SED systems are also evaluated with poly-phonic sound event detection scores(PSDS) <ref type="bibr" target="#b22">[24]</ref>. PSDS are computed using 50 operating points (linearly distributed from 0.01 to 0.99) with the following parameters: detection tolerance parameter (?DTC = 0.5), ground truth intersection parameter (?GTC = 0.5), cross-trigger tolerance parameter (?CTTC = 0.3), maximum False Positive rate (emax = 100). The weight on the cost trigger cost is set to ?CT = 1 and the weight on the class instability cost is set to ?ST = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>The SED baseline has been built on previous year's baseline <ref type="bibr" target="#b11">[13]</ref> and challenge submissions <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15]</ref>. As a consequence from this, the SED baseline results from the combination of several technical solutions. The impact of each of these solution has barely been investigated until now. In this part we propose a detailed analysis of the SED baseline system. We propose to study the following aspects:</p><p>? the type of data used for training;</p><p>? the transformations applied to the isolated sound events and to the synthetic soundscapes while generating the soundscapes; ? the amount of noise added to the mean-teacher input;</p><p>? the use of ramp-up to balance between the classification loss and the consistency loss; ? the use of ramp-up on the learning rate.</p><p>In the subsequent tables, the column where the performance is 34.14% corresponds to the original SED baseline setup, trained with the new training/cross-validation split (see also Section 3.1). For simplicity sake we report here only the largest confidence intervals obtained during the experiments to provide insight about the significance of the performance difference. For F1-score, performance difference below 1.2% on the VAL set and below 1.3% on the EVAL set are generally not to be considered as statistically significant. For the PSDS the intervals are 0.015 and 0.018 on the VAL and EVAL sets, respectively.</p><p>One of the key challenge in the task is the way the heterogeneous training data composition is handled. In a first set of experiments we analyse the impact of each subset (recorded clips without labels or weakly labeled, strongly labeled synthetic soundscapes) on the SED performance <ref type="table" target="#tab_1">(Table 1)</ref>. We did not include the case when using only the unlabeled data as the SED baseline requires some supervision.</p><p>When using only weakly labeled recorded soundscapes, the performance degrades severely. This is due to the lack of strong labels that provide information on how to perform the sound event segmentation at test time. When using only synthetic soundscapes with strong labels, the performance degrades also. In this case, the degradation is probably caused by the acoustic mismatch between the training data (synthetic soundscapes) and the test data (recorded soundscapes).</p><p>When combining several subsets, the ratio of soundscapes used per subset has been optimized during preliminary experiments and we present here only the performance obtained with the optimal ratio. When combining only two subset, the combinations including unlabeled data fail to improve the performance. The only combination of two datasets that improves the performance is the combination of the weakly labeled subset and the synthetic soundscapes subset. This combination probably allows for overcoming the weak labels problem and the domain mismatch problem mentioned above. Finally, the best performance is obtained by combining all the subsets (as in the original SED baseline). The diversity provided by the unlabeled subset seems to be beneficial to the SED system.</p><p>The second experiment aims at investigating the impact of the transformation used while generating the synthetic soundscapes. The number of isolated sound events in DESED is limited. To overcome this problem, Scaper offers the possibility to apply pitch shifting on the isolated sound events before including them in the soundscapes. <ref type="table" target="#tab_2">Table 2</ref> presents the SED performance depending on pitch-   shifting being applied on the isolated sound events before generating the soundscapes or not. The best combination in terms of F1score is not to apply pitch-shifting on the training set but to apply it on the cross-validation set. In terms of PSDS the best combination is to apply pitch-shifting on both the training set and the validation (as in the original baseline). This indicates that even if the impact of the pitch shifting on the SED performance is not always significant, using pitch-shifting is beneficial for the SED systems as it increases the diversity of the isolated sound events.</p><p>In Scaper there is the possibility to add reverberation to the soundscapes in order to blend isolated sound events in and to avoid having just a juxtaposition of isolated sound events. By default, the reverberation is added with Sox 4 . In order to have a more realistic reverberation applied to the synthetic soundscapes we are replacing the Sox-based reverberation applied on the soundscapes by room impulse responses (RIR) from the FUSS dataset <ref type="bibr" target="#b23">[25]</ref> applied on each isolated sound events. <ref type="table" target="#tab_3">Table 3</ref> presents the SED performance depending on if reverberation is applied on the synthetic soundscapes or not. The original baseline was not using FUSS reverberation. Applying RIR from FUSS on the validation set improves the performance, possibly because the synthetic soundscapes are then more realistic. Surprisingly, when applying RIR from FUSS on both the training set and the validation set, the performance degrades severely. This can be due to the fact that the RIR in FUSS do not match to the acoustic condition observed in the recorded soundscapes.</p><p>The next set of experiments is related to the SED model. In the mean-teacher model, Gaussian white noise is added to the original soundscapes before feeding them to the mean-teacher branch. This is supposed to improve the robustness of the SED model. In the original SED baseline, the SNR between the soundscape and the Gaussian noise was 30 dB. In <ref type="table">Table 4</ref> we present the SED performance depending on the SNR applied on the mean-teacher branch. Performance quickly degrades when the SNR decreases. Performing SED at 0dB SNR is really challenging <ref type="bibr" target="#b15">[17]</ref>, this could explain why feeding 0dB soundscapes to the mean-teacher branch is actually degrading performance. On the other hand, totally removing the noise from the teacher branch allows for improved performance. An explanation for this could be that the different dropout in the branches is already adding noise between the models.</p><p>Optimizing the mean-teacher model involves the combination of several losses (two classification losses and two consistency costs). One frequent problem when optimizing several losses at once is that of balancing between the losses. A solution that have proven to be effective in the case of the mean-teacher is the use of the so-called ramp-up where the weight attributed to one loss is gradually increased across time. Delphin-Poulat et al. proposed to apply the same approach to increase gradually the learning rate (LR) <ref type="bibr" target="#b13">[15]</ref>. This is commonly called LR warm-up <ref type="bibr" target="#b24">[26]</ref>. <ref type="table" target="#tab_5">Table 5</ref> presents the SED performance depending on the weight applied on the consistency costs and depending on whether ramp-up is applied on not. Applying ramp-up on both the consistency costs and the LR (as in the baseline) allows for obtaining the best performance. Additionally, increasing the weight on the consistency cost from 1 to 2 also improves the performance.</p><p>Experiments on the SNR in the mean-teacher branch, on pitchshifting and on reverberation showed that there is room for improvement compared to the original baseline. In table 6 we propose to perform an additional set of experiments on pitch-shifting and reverberation in the case where there is no additive noise in the mean-teacher branch. In this experiment, we also provide the performance on DESED public evaluation set (EVAL). Here pitchshifting and reverberation are applied only on the cross-validation set. The best performance (on the VAL set) is obtained when applying pitch-shifting but not reverberation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper we provided an detailed analysis of several technical aspects implemented in DCASE task 4 baseline since 2018 (as well as in other SED systems). Some of the default settings that are passed from one system to another without being questioned were shown to be sub-optimal for the task at hand. Through the detailed ablation study, this paper provides insights on how to configure a SED system to be trained on a heterogeneous dataset. The resulting SED system, even-though really similar to DCASE 2020 task 4 official baseline, actually outperforms this baseline by up to 3% on both the validation and set the public evaluation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENTS</head><p>We would like to thank all the other organizers of DCASE 2020 task 4 <ref type="bibr" target="#b3">5</ref> . In particular, we would like to thank Justin Salamon and Prem Seetharaman for their help with Scaper and Hakan Erdogan, John R. Hershey and Scott Wisdom for their help with the FUSS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">REFERENCES</head><p>[1] T. Virtanen, M. D. Plumbley, and D. Ellis, Computational analysis of sound scenes and events. Springer, 2018.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Mean-teacher model. ? and ? represent noise applied to the different models (in this case dropout). student model's weights. A depiction of the baseline model is provided inFigure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>SED systems are evaluated according to an event-based F1-score with a 200 ms collar on the onsets and a collar on the offsets that is the greatest of 200 ms and 20% of the sound event's length. The 2 https://zenodo.org/record/3745475 3 https://github.com/turpaultn/dcase20_task4/ tree/public_branch/baseline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>]] per layer, respectively. The convolution operations are followed by gated liner unit activation.</figDesc><table><row><cell>kHz, the mel-</cell></row><row><cell>spectrogram features are obtained from Short-term Fourier trans-</cell></row><row><cell>form coefficient (STFT) computed on 2048 sample windows with</cell></row><row><cell>255 hop size.</cell></row><row><cell>The CNN block is composed of 7 layers with [16, 32, 64, 128,</cell></row><row><cell>128, 128, 128] filters per layer, respectively. We use a kernel size</cell></row><row><cell>of 3x3 and the max-pooling is [[2, 2], [2, 2], [1, 2], [1, 2], [1, 2],</cell></row><row><cell>[1, 2], [1, 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>SED performance on the evaluation set depending on the kind of data used for training.</figDesc><table><row><cell></cell><cell cols="2">Training set</cell><cell></cell><cell></cell><cell cols="2">Ratio between training sets</cell></row><row><cell></cell><cell cols="2">Synthetic</cell><cell>1/3</cell><cell>1</cell><cell>1/4</cell><cell></cell><cell>1/2</cell></row><row><cell></cell><cell>Weak</cell><cell></cell><cell>1/3</cell><cell></cell><cell></cell><cell>1</cell><cell>1/4</cell><cell>1/2</cell></row><row><cell></cell><cell cols="2">Unlabeled</cell><cell>1/3</cell><cell></cell><cell>3/4</cell><cell></cell><cell>3/4</cell></row><row><cell></cell><cell cols="2">F1-score</cell><cell cols="5">34.14% 20.41% 11.56% 16.46% 17.97% 31.76%</cell></row><row><cell></cell><cell cols="2">PSDS</cell><cell>0.502</cell><cell>0.250</cell><cell>0.140</cell><cell>0.287</cell><cell>0.328</cell><cell>0.435</cell></row><row><cell></cell><cell cols="3">Pitch-shifting</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>x-valid</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">F1-score 35.15% 35.91% 34.14%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSDS</cell><cell>0.487</cell><cell>0.495</cell><cell>0.502</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>SED performance depending on pitch shifting being applied or not on isolated events during synthetic soundscapes generation.</figDesc><table><row><cell></cell><cell></cell><cell>Reverberation</cell><cell></cell></row><row><cell>Training</cell><cell></cell><cell></cell><cell></cell></row><row><cell>x-valid</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">F1-score 18.30% 35.54% 34.14%</cell></row><row><cell>PSDS</cell><cell>0.435</cell><cell>0.508</cell><cell>0.502</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Mean-teacher SNR</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>15</cell><cell>30</cell><cell>?</cell></row><row><cell cols="5">F1-score 12.56% 26.29% 34.14% 37.80%</cell></row><row><cell>PSDS</cell><cell>0.261</cell><cell>0.437</cell><cell>0.502</cell><cell>0.540</cell></row></table><note>SED performance depending on reverberation being ap- plied or not on during synthetic soundscapes generation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Impact Of the ramp-up</figDesc><table><row><cell cols="2">Reverb</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pitch-shifting</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VAL</cell><cell cols="5">F1-score 31.13% 36.27% 37.80% 34.46% PSDS 0.482 0.521 0.540 0.520</cell></row><row><cell>EVAL</cell><cell>F1-score PSDS</cell><cell>33.7% 0.515</cell><cell>39.9% 0.568</cell><cell>39.0% 0.552</cell><cell>36.8% 0.566</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Parameter combination for the new baseline definition</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://sox.sourceforge.net/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://dcase.community/challenge2020/task-sound-event-detectionand-separation-in-domestic-environments</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SONYC: A system for the monitoring, analysis and mitigation of urban noise pollution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mydlarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doraiswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Audio analysis for surveillance applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA. IEEE</title>
		<meeting>WASPAA. IEEE</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="158" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Machine listening techniques as a complement to video image analysis in forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Essid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
		<ptr target="https://hal.archives-ouvertes.fr/hal-01393959" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Phoenix, AZ, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="page" from="948" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Event-based video retrieval using audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schulam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monitoring activities of daily living in smart homes: Understanding human behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Debes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merentitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frangiadakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="81" to="94" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A method for automatic fall detection of elderly people using floor vibrations and soundproof of concept on human mimicking doll falls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zigel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gannot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2858" to="2867" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-Scale Weakly Labeled Semi-Supervised Sound Event Detection in Domestic Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eghbal-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Parag</forename><surname>Shah</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-01850270" />
	</analytic>
	<monogr>
		<title level="m">Workshop on Detection and Classification of Acoustic Scenes and Events</title>
		<meeting><address><addrLine>Woking, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
	<note>submitted to DCASE2018 Workshop</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A closer look at weak label learning for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09288</idno>
		<ptr target="http://arxiv.org/abs/1804.09288" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Limitations of weak labels for embedding and tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-02467401" />
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -45th International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive pooling operators for weakly labeled sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2180" to="2193" />
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scaper: A library for soundscape synthesis and augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Macconnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA</title>
		<meeting>WASPAA</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="344" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Parag</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DCASE Workshop</title>
		<meeting>DCASE Workshop</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mean teacher convolution system for dcase 2018 task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2018 Challenge</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mean teacher with data augmentation for dcase 2019 task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Delphin-Poulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plapous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Orange Labs Lannion, France, Tech. Rep</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semisupervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sound event detection in synthetic domestic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Freesound technical demo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM</title>
		<meeting>ACM</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="411" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">FSD50k: an open dataset of human-labeled sound events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The SINS database for detection of daily activities in a home environment using an acoustic sensor network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lauwereins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thoen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Adhana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Brouckxon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Waterschoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vanrumste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verhelst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Karsmakers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DCASE Workshop</title>
		<meeting>DCASE Workshop</meeting>
		<imprint>
			<date type="published" when="2017-11" />
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">TUT database for acoustic scene classification and sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/document/7760424/" />
	</analytic>
	<monogr>
		<title level="m">2016 24th European Signal Processing Conference (EUSIPCO). IEEE</title>
		<imprint>
			<biblScope unit="page" from="1128" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Metrics for polyphonic sound event detection</title>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A framework for the robust evaluation of sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ferroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tuveri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Azcarreta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krstulovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hershey, Free Universal Sound Separation (FUSS) dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename></persName>
		</author>
		<ptr target="https://github.com/google-research/sound-separation/tree/master/datasets/fuss" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
