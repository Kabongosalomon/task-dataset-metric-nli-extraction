<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2018 BREAKING THE SOFTMAX BOTTLENECK: A HIGH-RANK RNN LANGUAGE MODEL</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
							<email>zhiliny@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>wcohen@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2018 BREAKING THE SOFTMAX BOTTLENECK: A HIGH-RANK RNN LANGUAGE MODEL</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity. 1 * Equal contribution. Ordering determined by dice rolling. 1 Code is available at https://github.com/zihangdai/mos.</p><p>Published as a conference paper at ICLR 2018 matrices that have much larger normalized singular values and thus much higher rank than Softmax and other baselines on real-world datasets.</p><p>We evaluate our proposed approach on standard language modeling benchmarks. MoS substantially improves over the current state-of-the-art results on benchmarks, by up to 3.6 points in terms of perplexity, reaching perplexities 47.69 on Penn Treebank and 40.68 on WikiText-2. We further apply MoS to a dialog dataset and show improved performance over Softmax and other baselines.</p><p>Our contribution is two-fold. First, we identify the Softmax bottleneck by formulating language modeling as a matrix factorization problem. Second, we propose a simple and effective method that substantially improves over the current state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As a fundamental task in natural language processing, statistical language modeling has gone through significant development from traditional Ngram language models to neural language models in the last decade <ref type="bibr" target="#b2">(Bengio et al., 2003;</ref><ref type="bibr" target="#b31">Mnih &amp; Hinton, 2007;</ref><ref type="bibr" target="#b29">Mikolov et al., 2010)</ref>. Despite the huge variety of models, as a density estimation problem, language modeling mostly relies on a universal auto-regressive factorization of the joint probability and then models each conditional factor using different approaches. Specifically, given a corpus of tokens X = (X 1 , . . . , X T ), the joint probability P (X) factorizes as P (X) = t P (X t | X &lt;t ) = t P (X t | C t ), where C t = X &lt;t is referred to as the context of the conditional probability hereafter.</p><p>Based on the factorization, recurrent neural networks (RNN) based language models achieve stateof-the-art results on various benchmarks <ref type="bibr" target="#b27">(Merity et al., 2017;</ref><ref type="bibr" target="#b25">Melis et al., 2017;</ref><ref type="bibr" target="#b22">Krause et al., 2017)</ref>. A standard approach is to use a recurrent network to encode the context into a fixed size vector, which is then multiplied by the word embeddings <ref type="bibr" target="#b17">(Inan et al., 2016;</ref><ref type="bibr" target="#b34">Press &amp; Wolf, 2017)</ref> using dot product to obtain the logits. The logits are consumed by the Softmax function to give a categorical probability distribution over the next token. In spite of the expressiveness of RNNs as universal approximators <ref type="bibr" target="#b36">(Sch?fer &amp; Zimmermann, 2006)</ref>, an unclear question is whether the combination of dot product and Softmax is capable of modeling the conditional probability, which can vary dramatically with the change of the context.</p><p>In this work, we study the expressiveness of the aforementioned Softmax-based recurrent language models from a perspective of matrix factorization. We show that learning a Softmax-based recurrent language model with the standard formulation is essentially equivalent to solving a matrix factorization problem. More importantly, due to the fact that natural language is highly context-dependent, the matrix to be factorized can be high-rank. This further implies that standard Softmax-based language models with distributed (output) word embeddings do not have enough capacity to model natural language. We call this the Softmax bottleneck.</p><p>We propose a simple and effective method to address the Softmax bottleneck. Specifically, we introduce discrete latent variables into a recurrent language model, and formulate the next-token probability distribution as a Mixture of Softmaxes (MoS). Mixture of Softmaxes is more expressive than Softmax and other surrogates considered in prior work. Moreover, we show that MoS learns 2 LANGUAGE MODELING AS MATRIX FACTORIZATION As discussed in Section 1, with the autoregressive factorization, language modeling can be reduced to modeling the conditional distribution of the next token x given the context c. Though one might argue that a natural language allows an infinite number of contexts due to its compositionality <ref type="bibr" target="#b33">(Pinker, 1994)</ref>, we proceed with our analysis by considering a finite set of possible contexts. The unboundedness of natural language does not affect our conclusions, which will be discussed later.</p><p>We consider a natural language as a finite set of pairs of a context and its conditional next-token distribution 2 L = {(c 1 , P * (X|c 1 )), ? ? ? , (c N , P * (X|c N ))}, where N is the number of possible contexts. We assume P * &gt; 0 everywhere to account for errors and flexibility in natural language. Let {x 1 , x 2 , ? ? ? , x M } denote a set of M possible tokens in the language L. The objective of a language model is to learn a model distribution P ? (X|C) parameterized by ? to match the true data distribution P * (X|C).</p><p>In this work, we study the expressiveness of the parametric model class P ? (X|C). In other words, we are asking the following question: given a natural language L, does there exist a parameter ? such that P ? (X|c) = P * (X|c) for all c in L?</p><p>We start by looking at a Softmax-based model class since it is widely used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SOFTMAX</head><p>The majority of parametric language models use a Softmax function operating on a context vector (or hidden state) h c and a word embedding w x to define the conditional distribution P ? (x|c). More specifically, the model distribution is usually written as</p><formula xml:id="formula_0">P ? (x|c) = exp h c w x x exp h c w x<label>(1)</label></formula><p>where h c is a function of c, and w x is a function of x. Both functions are parameterized by ?. Both the context vector h c and the word embedding w x have the same dimension d. The dot product h c w x is called a logit.</p><p>To help discuss the expressiveness of Softmax, we define three matrices: We further specify a set of matrices formed by applying row-wise shift to A</p><formula xml:id="formula_1">H ? = ? ? ? ? h c1 h c2 ? ? ? h c N ? ? ? ? ; W ? = ? ? ? ? w x1 w x2 ? ? ? w x M ? ? ? ? ; A = ? ? ? ? log P * (x 1 |c 1 ), log P * (x 2 |c 1 ) ? ? ? log P * (x M |c 1 ) log P * (x 1 |c 2 ), log P * (x 2 |c 2 ) ? ? ? log P * (x M |c 2 ) . . . . . . . . . . . . log P * (x 1 |c N ), log P * (x 2 |c N ) ? ? ? log P * (x M |c N ) ? ? ? ? where H ? ? R N ?d , W ? ? R M ?d , A ? R N ?M ,</formula><formula xml:id="formula_2">F (A) = {A + ?J N,M |? is diagonal and ? ? R N ?N },</formula><p>where J N,M is an all-ones matrix with size N ? M . Essentially, the row-wise shift operation adds an arbitrary real number to each row of A. Thus, F (A) is an infinite set. Notably, the set F (A) has two important properties (see Appendix A for the proof), which are key to our analysis. Property 1. For any matrix A , A ? F (A) if and only if Softmax(A ) = P * . In other words, F (A) defines the set of all possible logits that correspond to the true data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Property 2. For any</head><formula xml:id="formula_3">A 1 = A 2 ? F (A), |rank(A 1 ) ? rank(A 2 )| ? 1.</formula><p>In other words, all matrices in F (A) have similar ranks, with the maximum rank difference being 1.</p><p>Based on the Property 1 of F (A), we immediately have the following Lemma. Lemma 1. Given a model parameter ?,</p><formula xml:id="formula_4">H ? W ? ? F (A) if and only if P ? (X|c) = P * (X|c) for all c in L.</formula><p>Now the expressiveness question becomes: does there exist a parameter ? and A ? F (A) such that</p><formula xml:id="formula_5">H ? W ? = A .</formula><p>This is essentially a matrix factorization problem. We want the model to learn matrices H ? and W ? that are able to factorize some matrix A ? F (A). First, note that for a valid factorization to exist, the rank of H ? W ? has to be at least as large as the rank of A . Further, since H ? ? R N ?d and W ? ? R M ?d , the rank of H ? W ? is strictly upper bounded by the embedding size d. As a result, if d ? rank(A ), a universal approximator can theoretically recover A . However, if d &lt; rank(A ), no matter how expressive the function family U is, no (H ? , W ? ) can even theoretically recover A .</p><p>We summarize the reasoning above as follows (see Appendix A for the proof). Proposition 1. Given that the function family U is a universal approximator, there exists a parameter ? such that P ? (X|c) = P * (X|c) for all c in L if and only if d ? min A ?F (A) rank(A ).</p><p>Combining Proposition 1 with the Property 2 of F (A), we are now able to state the Softmax Bottleneck problem formally. Corollary 1. (Softmax Bottleneck) If d &lt; rank(A) ? 1, for any function family U and any model parameter ?, there exists a context c in L such that P ? (X|c) = P * (X|c).</p><p>The above corollary indicates that when the dimension d is too small, Softmax does not have the capacity to express the true data distribution. Clearly, this conclusion is not restricted to a finite language L. When L is infinite, one can always take a finite subset and the Softmax bottleneck still exists. Next, we discuss why the Softmax bottleneck is an issue by presenting our hypothesis that A is high-rank for natural language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">HYPOTHESIS: NATURAL LANGUAGE IS HIGH-RANK</head><p>We hypothesize that for a natural language L, the log probability matrix A is a high-rank matrix. It is difficult (if possible) to rigorously prove this hypothesis since we do not have access to the true data distribution of a natural language. However, it is suggested by the following intuitive reasoning and empirical observations:</p><p>? Natural language is highly context-dependent <ref type="bibr" target="#b28">(Mikolov &amp; Zweig, 2012)</ref>. For example, the token "north" is likely to be followed by "korea" or "korean" in a news article on international politics, which however is unlikely in a textbook on U.S. domestic history. We hypothesize that such subtle context dependency should result in a high-rank matrix A.</p><p>? If A is low-rank, it means humans only need a limited number (e.g. a few hundred) of bases, and all semantic meanings can be created by (potentially) negating and (weighted) averaging these bases. However, it is hard to find a natural concept in linguistics and cognitive science that corresponds to such bases, which questions the existence of such bases. For example, semantic meanings might not be those bases since a few hundred meanings may not be enough to cover everyday meanings, not to mention niche meanings in specialized domains.</p><p>? Empirically, our high-rank language model outperforms conventional low-rank language models on several benchmarks, as shown in Section 3. We also provide evidences in Section 3.3 to support our hypothesis that learning a high-rank language model is important.</p><p>Given the hypothesis that natural language is high-rank, it is clear that the Softmax bottleneck limits the expressiveness of the models. In practice, the embedding dimension d is usually set at the scale of 10 2 , while the rank of A can possibly be as high as M (at the scale of 10 5 ), which is orders of magnitude larger than d. Softmax is effectively learning a low-rank approximation to A, and our experiments suggest that such approximation loses the ability to model context dependency, both qualitatively and quantitatively (Cf. Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">EASY FIXES?</head><p>Identifying the Softmax bottleneck immediately suggests some possible "easy fixes". First, as considered by a lot of prior work, one can employ a non-parametric model, namely an Ngram model <ref type="bibr" target="#b21">(Kneser &amp; Ney, 1995)</ref>. Ngram models are not constrained by any parametric forms so it can universally approximate any natural language, given enough parameters. Second, it is possible to increase the dimension d (e.g., to match M ) so that the model can express a high-rank matrix A.</p><p>However, these two methods increase the number of parameters dramatically, compared to using a low-dimensional Softmax. More specifically, an Ngram needs (N ? M ) parameters in order to express A, where N is potentially unbounded. Similarly, a high-dimensional Softmax requires (M ? M ) parameters for the word embeddings. Increasing the number of model parameters easily leads to overfitting. In past work, <ref type="bibr" target="#b21">Kneser &amp; Ney (1995)</ref> used back-off to alleviate overfitting. Moreover, as deep learning models were tuned by extensive hyper-parameter search, increasing the dimension d beyond several hundred is not helpful 3 <ref type="bibr" target="#b27">(Merity et al., 2017;</ref><ref type="bibr" target="#b25">Melis et al., 2017;</ref><ref type="bibr" target="#b22">Krause et al., 2017)</ref>.</p><p>Clearly there is a tradeoff between expressiveness and generalization on language modeling. Naively increasing the expressiveness hurts generalization. Below, we introduce an alternative approach that increases the expressiveness without exploding the parametric space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">MIXTURE OF SOFTMAXES: A HIGH-RANK LANGUAGE MODEL</head><p>We propose a high-rank language model called Mixture of Softmaxes (MoS) to alleviate the Softmax bottleneck issue. MoS formulates the conditional distribution as</p><formula xml:id="formula_6">P ? (x|c) = K k=1 ? c,k exp h c,k w x x exp h c,k w x ; s.t. K k=1 ? c,k = 1</formula><p>where ? c,k is the prior or mixture weight of the k-th component, and h c,k is the k-th context vector associated with context c. In other words, MoS computes K Softmax distributions and uses a weighted average of them as the next-token probability distribution. Similar to prior work on recurrent language modeling <ref type="bibr" target="#b27">(Merity et al., 2017;</ref><ref type="bibr" target="#b25">Melis et al., 2017;</ref><ref type="bibr" target="#b22">Krause et al., 2017)</ref>, we first apply a stack of recurrent layers on top of X to obtain a sequence of hidden states (g 1 , ? ? ? , g T ).</p><p>The prior and the context vector for context c t are parameterized as ? ct,k = exp w ?,k gt K k =1 exp w ?,k gt and h ct,k = tanh(W h,k g t ) where w ?,k and W h,k are model parameters.</p><p>Our method is simple and easy to implement, and has the following advantages:</p><p>? Improved expressiveness (compared to Softmax). MoS is theoretically more (or at least equally) expressive compared to Softmax given the same dimension d. This can be seen by the fact that MoS with K = 1 is reduced to Softmax. More importantly, MoS effectively approximates A b?</p><formula xml:id="formula_7">A MoS = log K k=1 ? k exp(H ?,k W ? )</formula><p>where ? k is an (N ? N ) diagonal matrix with elements being the prior ? c,k . Because? MoS is a nonlinear function (log_sum_exp) of the context vectors and the word embeddings,? MoS can be arbitrarily high-rank. As a result, MoS does not suffer from the rank limitation, compared to Softmax.</p><p>? Improved generalization (compared to Ngram). Ngram models and high-dimensional Softmax (Cf. Section 2.3) improve the expressiveness but do not generalize well. In contrast, MoS does not have a generalization issue due to the following reasons. First, MoS defines the following generative process: a discrete latent variable k is first sampled from {1, ? ? ? , K}, and then the next token is sampled based on the k-th Softmax component. By doing so we introduce an inductive bias that the next token is generated based on a latent discrete decision (e.g., a topic), which is often safe in language modeling <ref type="bibr" target="#b3">(Blei et al., 2003)</ref>. Second, since? MoS is defined by a nonlinear function and not restricted by the rank bottleneck, in practice it is possible to reduce d to compensate for the increase of model parameters brought by the mixture structure. As a result, MoS has a similar model size compared to Softmax and thus is not prone to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">MIXTURE OF CONTEXTS: A LOW-RANK BASELINE</head><p>Another possible approach is to directly mix the context vectors (or logits) before taking the Softmax, rather than mixing the probabilities afterwards as in MoS. Specifically, the conditional distribution is parameterized as</p><formula xml:id="formula_8">P ? (x|c) = exp K k=1 ? c,k h c,k w x x exp K k=1 ? c,k h c,k w x = exp K k=1 ? c,k h c,k w x x exp K k=1 ? c,k h c,k w x ,<label>(2)</label></formula><p>where h c,k and ? c,k share the same parameterization as in MoS. Despite its superficial similarity to MoS, this model, which we refer to as mixture of contexts (MoC), actually suffers from the same rank limitation problem as Softmax. This can be easily seen by defining</p><formula xml:id="formula_9">h c = K k=1 ? c,k h c,k , which turns the MoC parameterization (2) into P ? (x|c) = exp h c wx x exp h c w x .</formula><p>Note that this is equivalent to the Softmax parameterization (1). Thus, performing mixture in the feature space can only make the function family U more expressive, but does not change the fact that the rank of H ? W ? is upper bounded by the embedding dimension d. In our experiments, we implement MoC as a baseline and compare it experimentally to MoS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MAIN RESULTS</head><p>We conduct a series of experiments with the following settings:</p><p>? Following previous work <ref type="bibr" target="#b22">(Krause et al., 2017;</ref><ref type="bibr" target="#b27">Merity et al., 2017;</ref><ref type="bibr" target="#b25">Melis et al., 2017)</ref>, we evaluate the proposed MoS model on two widely used language modeling datasets, namely Penn Treebank (PTB) <ref type="bibr" target="#b29">(Mikolov et al., 2010)</ref>    <ref type="formula" target="#formula_0">(2017)</ref> and <ref type="bibr" target="#b22">Krause et al. (2017)</ref>. ? indicates using dynamic evaluation.  are used as baselines. For evaluation, we include both the perplexity and the precision/recall of Smoothed Sentence-level BLEU, as suggested by <ref type="bibr" target="#b41">Zhao et al. (2017)</ref>. When generating responses, we use beam search with beam size 10, restrict the maximum length to 30, and retain the top-5 responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model #Param Validation Test</head><p>The language modeling results on PTB and WT2 are presented in <ref type="table" target="#tab_2">Table 1</ref> and <ref type="table" target="#tab_3">Table 2</ref>   The improvement on the large-scale dataset is even more significant. As shown in <ref type="table" target="#tab_5">Table 3</ref>, MoS outperforms Softmax by over 5.6 points in perplexity. It suggests the effectiveness of MoS is not limited to small datasets where many regularization techniques are used. Note that with limited computational resources, we didn't tune the hyper-parameters for MoS.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ABLATION STUDY</head><p>To further verify the improvement shown above does come from the MoS structure rather than adding another hidden layer or finding a particular set of hyper-parameters, we conduct an ablation study on both PTB and WT2. Firstly, we compare MoS with an MoC architecture with the same number of layers, hidden sizes, and embedding sizes, which thus has the same number of parameters. In addition, we adopt the hyper-parameters used to obtain the best MoS model (denoted as MoS hyper-parameters), and train a baseline AWD-LSTM. To avoid distractive factors and save computational resources, all ablative experiments excluded the use of finetuing and dynamic evaluation.</p><p>The results are shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">VERIFY THE ROLE OF RANK</head><p>While the study above verifies that MoS is the key to achieving the state-of-the-art performance, it is still not clear whether the superiority of MoS comes from its potential high rank, as suggested by our theoretical analysis in Section 2. In the sequel, we take steps to verify this hypothesis.</p><p>? Firstly, we verify that MoS does induce a high-rank log-probability matrix empirically, while MoC and Softmax fail. On the validation or test set of PTB with tokens X = {X 1 , . . . , X T }, we compute the log probabilities {log P (X i | X &lt;i ) ? R M } T t=1 for each token using all three models. Then, for each model, we stack all T log-probability vectors into a T ? M matrix, resulting in A MoS ,? MoC and? Softmax . Theoretically, the number of non-zero singular values of a matrix is equal to its rank. However, performing singular value decomposition of real valued matrices using numerical approaches often encounter roundoff errors. Hence, we adopt the expected roundoff error suggested by <ref type="bibr" target="#b35">Press (2007)</ref> when estimating the ranks of? MoS ,? MoC and? Softmax . The estimated ranks are shown in <ref type="table" target="#tab_11">Table 6</ref>. As predicted by our theoretical analysis, the matrix ranks induced by Softmax and MoC are both limited by the corresponding embedding sizes. By contrast, the matrix rank obtained from MoS does not suffer from this constraint, almost reaching full rank (M = 10000). In appendix C.1, we give additional evidences for the higher rank of MoS.   ? Secondly, we show that, before reaching full rank, increasing the number of mixture components in MoS also increases the rank of the log-probability matrix, which in turn leads to improved performance (lower perplexity). Specifically, on PTB, with other hyper-parameters fixed as used in section 3.1, we vary the number of mixtures used in MoS and compare the corresponding empirical rank and test perplexity without finetuning. <ref type="table" target="#tab_12">Table 7</ref> summarizes the results. This clear positive correlation between rank and performance strongly supports the our theoretical analysis in section 2. Moreover, note that after reaching almost full rank (i.e., using 15 mixture components), further increasing the number of components degrades the performance due to overfitting (as we inspected the training and test perplexities).</p><p>? In addition, as performance improvement can often come from better regularization, we investigate whether MoS has a better, though unexpected, regularization effect compared to Softmax. We consider the 1B word dataset where overfitting is unlikely and no explicit regularization technique (e.g., dropout) is employed. As we can see from the left part of <ref type="table" target="#tab_5">Table 3</ref>, MoS and Softmax achieve a similar generalization gap, i.e., the performance gap between the test set and the training set. It suggests both models have similar regularization effects. Meanwhile, MoS has a lower training perplexity compared to Softmax, indicating that the improvement of MoS results from improved expressiveness.</p><p>? The last evidence we provide is based on an inverse experiment. Empirically, we find that when Softmax does not suffer from a rank limitation, e.g., in character-level language modeling, using MoS will not improve the performance. Due to lack of space, we refer readers to Appendix C.2 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">ADDITIONAL ANALYSIS</head><p>MoS computational time The expressiveness of MoS does come with a computational costcomputing a K-times larger Softmax. To give readers a concrete idea of the influence on training time, we perform detailed analysis in Appendix C.3. As we will see, computational wall time of MoS is actually sub-linear w.r.t. the number of Softmaxes K. In most settings, we observe a two to three times slowdown when using MoS with up to 15 mixture components.</p><p>Qualitative analysis Finally, we conduct a case study on PTB to see how MoS improves the next-token prediction in detail. Due to lack of space, we refer readers to Appendix C.4 for details.</p><p>The key insight from the case study is that MoS is better at making context-dependent predictions. Specifically, given the same immediate preceding word, MoS will produce distinct next-step prediction based on long-term context in history. By contrast, the baseline often yields similar next-step prediction, independent of the long-term context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>In language modeling, <ref type="bibr" target="#b15">Hutchinson et al. (2011;</ref> have previously considered the problem from a matrix rank perspective. However, their focus was to improve the generalization of Ngram language models via a sparse plus low-rank approximation. By contrast, as neural language models already generalize well, we focus on a high-rank neural language model that improves expressiveness without sacrificing generalization. <ref type="bibr" target="#b32">Neubig &amp; Dyer (2016)</ref> proposed to mix Ngram and neural language models to unify and benefit from both. However, this mixture might not generalize well since an Ngram model, which has poor generalization, is included. Moreover, the fact that the two components are separately trained can limit its expressiveness. <ref type="bibr" target="#b23">Levy &amp; Goldberg (2014)</ref> also considered the matrix factorization perspective, but in the context of learning word embeddings.</p><p>In a general sense, Mixture of Softmaxes proposed in this work can be seen as a particular instantiation of the long-existing idea called Mixture of Experts (MoE) <ref type="bibr" target="#b18">(Jacobs et al., 1991)</ref>. However, there are two core differences. Firstly, MoE has usually been instantiated as mixture of Gaussians to model data in continuous domains <ref type="bibr" target="#b18">(Jacobs et al., 1991;</ref><ref type="bibr" target="#b13">Graves, 2013;</ref><ref type="bibr" target="#b1">Bazzani et al., 2016)</ref>. More importantly, the motivation of using the mixture structure is distinct. For Gaussian mixture models, the mixture structure is employed to allow for a parameterized multi-modal distribution. By contrast, Softmax by itself can parameterize a multi-modal distribution, and MoS is introduced to break the Softmax bottleneck as discussed in Section 2.</p><p>There has been previous work <ref type="bibr" target="#b7">(Eigen et al., 2013;</ref><ref type="bibr" target="#b37">Shazeer et al., 2017)</ref> proposing architectures that can be categorized as instantiations of MoC, since the mixture structure is employed in the feature space. <ref type="bibr">6</ref> The target of <ref type="bibr" target="#b7">Eigen et al. (2013)</ref> is to create a more expressive feed-forward layer through the mixture structure. In comparison, <ref type="bibr" target="#b37">Shazeer et al. (2017)</ref> focuses on a sparse gating mechanism also on the feature level, which enables efficient conditional computation and allows the training of a very large neural architecture. In addition to having different motivations from our work, all these MoC variants suffer from the same rank limitation problem as discussed in Section 2.</p><p>Finally, several previous works have tried to introduce latent variables into sequence modeling <ref type="bibr" target="#b0">(Bayer &amp; Osendorfer, 2014;</ref><ref type="bibr" target="#b14">Gregor et al., 2015;</ref><ref type="bibr" target="#b5">Chung et al., 2015;</ref><ref type="bibr" target="#b10">Gan et al., 2015;</ref><ref type="bibr" target="#b8">Fraccaro et al., 2016;</ref><ref type="bibr" target="#b6">Chung et al., 2016)</ref>. Except for <ref type="bibr" target="#b6">(Chung et al., 2016)</ref>, these structures all define a continuous latent variable for each step of the RNN computation, and rely on the SGVB estimator <ref type="bibr" target="#b20">(Kingma &amp; Welling, 2013)</ref> to optimize a variational lower bound of the log-likelihood. Since exact integration is infeasible, these models cannot estimate the likelihood (perplexity) exactly at test time. Moreover, for discrete data, the variational lower bound is usually too loose to yield a competitive approximation compared to standard auto-regressive models. As an exception, <ref type="bibr" target="#b6">Chung et al. (2016)</ref> utilizes Bernoulli latent variables to model the hierarchical structure in language, where the Bernoulli sampling is replaced by a thresholding operation at test time to give perplexity estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>Under the matrix factorization framework, the expressiveness of Softmax-based language models is limited by the dimension of the word embeddings, which is termed as the Softmax bottleneck. Our proposed MoS model improves the expressiveness over Softmax, and at the same time avoids overfitting compared to non-parametric models and naively increasing the word embedding dimensions.</p><p>Our method improves the current state-of-the-art results on standard benchmarks by a large margin, which in turn justifies our theoretical reasoning: it is important to have a high-rank model for natural language.</p><p>A PROOFS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Property 1</head><p>Proof. For any A ? F (A), let P A (X|C) denote the distribution defined by applying Softmax on the logits given by A . Consider row i column j, by definition any entry in A can be expressed as</p><formula xml:id="formula_10">A ij = A ij + ? ii . It follows P A (x j |c i ) = exp A ij k exp A ik = exp(A ij + ? ii ) k exp(A ik + ? ii ) = exp A ij k exp A ik = P * (x j |c i )</formula><p>For any A ? {A | Softmax(A ) = P * }, for any i and j, we have</p><formula xml:id="formula_11">P A (x j |c i ) = P A (x j |c i )</formula><p>It follows that for any i, j, and k,</p><formula xml:id="formula_12">P A (x j |c i ) P A (x k |c i ) = exp A ij exp A ik = exp A ij exp A ik = P A (x j |c i ) P A (x k |c i )</formula><p>As a result,</p><formula xml:id="formula_13">A ij ? A ij = A ik ? A ik</formula><p>This means each row in A can be obtained by adding a real number to the corresponding row in A. Therefore, there exists a diagonal matrix ? ? R N ?N such that</p><formula xml:id="formula_14">A = A + ?J N,M</formula><p>It follows that A ? F (A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Property 2</head><p>Proof. For any A 1 and A 2 in F (A), by definition we have A 1 = A + ? 1 J N,M , and A 2 = A + ? 2 J N,M where ? 1 and ? 2 are two diagonal matrices. It can be rewritten as</p><formula xml:id="formula_15">A 1 = A 2 + (? 1 ? ? 2 )J N,M</formula><p>Let S be a maximum set of linearly independent rows in A 2 . Let e N be an all-ones vector with dimension N . The i-th row vector a 1,i in A 1 can be written as a 1,i = a 2,i + (? 1,ii ? ? 2,ii )e N Because a 2,i is a linear combination of vectors in S, a 1,i is a linear combination of vectors in S ? {e N }. It follows that rank(A 1 ) ? rank(A 2 ) + 1</p><p>Similarly, we can derive rank(A 2 ) ? rank(A 1 ) + 1 Therefore, |rank(A 1 ) ? rank(A 2 )| ? 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition 1</head><p>Proof. If there exists a parameter ? such that P ? (X|c) = P * (X|c) for all c in L, by Lemma 1, we have H ? W ? ? F (A). As a result, there exists a matrix A ? F (A) such that H ? W ? = A . Because The hyper-parameters used for MoS in language modeling experiment is summarized below.   <ref type="bibr" target="#b9">(Gal &amp; Ghahramani, 2016)</ref>. See <ref type="bibr" target="#b27">(Merity et al., 2017)</ref> for more detailed descriptions.</p><p>The hyper-parameters used for dynamic evaluation of MoS is summarized below.</p><p>Hyper-parameter PTB WT2 Batch size 100 100 learning rate (?) 0.002 0.002 0.001 0.002 ? 0.075 0.02  In section 3, we compute the rank of different models based on the non-zero singular values of the empirical log-likelihood matrix. Since there can be roundoff mistakes, a less error-prone approach is to directly study the distribution of singular values. Specifically, if more singular values have relatively larger magnitude, the rank of the matrix tends to be higher. Motivated from this intuition, we visualize the distribution of the singular values. To account for the different magnitudes of singular values from different models, we first normalize all singular values to [0, 1]. Then, we plot the cumulative percentage of normalized singular values, i.e., percentage of normalized singular values below a threshold, in <ref type="figure" target="#fig_2">Figure 1</ref>. As we can see, most of the singular values of Softmax and MoC concentrate on an area with very low values. In comparison, the concentration area of the MoS singular values is not only several orders larger, but also spans a much wider region. Intuitively, MoS utilizes the corresponding singular vectors to capture a larger and more diverse set of contexts.  What's more, another indicator of high rank is that the model can precisely capture the nuance of difference contexts. If a model can better capture the distinctions among contexts, we expect the nextstep conditional distributions to be less similar to each on average. Based on this intuition, we use the expected pairwise Kullback-Leibler divergence (KLD), i.e., E c,c ?C [KLD(P (X | c) P (X | c ))] where C denotes all possible contexts, as another metric to evaluate the ranks of the three models (MoS, MoC and Softmax). Practically, we sample c, c from validation or test data of PTB to get the empirical estimations for the three models, which are shown in the right half of <ref type="table" target="#tab_2">Table 11</ref>. As we expected, MoS achieves higher expected pairwise KLD, indicating its superiority in covering more contexts of the next-token distribution.  <ref type="table" target="#tab_2">Table 12</ref>: BPC comparison on text8. For MoS, "-n" indicates using n mixtures. "hid" and "emb" denote the hidden size and embedding size respectively.</p><p>Here, we detail the inverse experiment, which shows that when Softmax does not suffer from a rank limitation, using MoS will not improve the performance. Notice that character-level language modeling (CharLM) is exactly such a problem, because the rank of the log-likelihood matrix is upper bounded by the vocabulary size, and CharLM usually has a very limited vocabulary (tens of characters). In this case, with the embedding size being hundreds in practice, Softmax is no longer a bottleneck in this task. Hence, we expect MoS to yield similar performance to Softmax on CharLM.</p><p>We conduct experiments of CharLM using the text8 dataset <ref type="bibr" target="#b24">(Mahoney, 2011)</ref>, which consists of 100M characters including only alphabetical characters and spaces derived from Wikipedia. We follow <ref type="bibr" target="#b28">Mikolov et al. (2012)</ref> and use the first 90M characters for training, the next 5M for validation and the final 5M for testing. The standard evaluation metric bit-per-character (BPC) is employed. We employ a 1-layer 1024-unit LSTM followed by Softmax as the baseline. For MoS, we consider 7 or 10 mixtures and reduce the hidden and/or embedding size to match the baseline capacity. When decreasing the hidden and/or embedding size, we either keep both the same, or make the hidden size relatively larger. The results are summarized in  indicates Softmax and MoS use the same batch sizes on one GPU. "best-1" and "best-3" refer to the settings where Softmax and MoS obtain their own best perplexity, with 1 and 3 GPUs respectively.</p><p>We evaluate the additional computational cost introduced by MoS. We consider two sets of controlled experiments. In the first set, we compare the training time of MoS and Softmax using the same batch sizes. In the second set, we compare the training time of two methods using the hyperparameter settings that achieve the best performance for each model (i.e., the settings in Tables 1, 2, and 3). In both sets, we control two models to have comparable model sizes.</p><p>The results on the three datasets are shown in <ref type="table" target="#tab_2">Table 13</ref>. Thanks to the efficiency of matrix multiplication on GPU, the computational wall time of MoS is actually sub-linear w.r.t. the number of Softmaxes K. In most settings, we observe a two to three times slowdown when using MoS. Specifically, the "bs" setting measures the computational cost introduced by MoS given enough memory, which is 1.9x, 2.5x, and 3.8x slowdown on PTB, WT2, and 1B respectively. The "best-1" setting is usually slower compared to "bs", because a single batch does not fit into the memory of a single GPU using MoS, in which case we have to split one batch into multiple small ones, resulting in further slowdown. In this sense, the gap between "best-1" and "bs" measures the computational cost introduced due to the increase of memory consumed by MoS. The "best-3" alleviates this issue by using three GPUs, which allows larger-batch training for MoS. In this case, we reduce the computational cost to 2.9x on WT2 and 2.1x on 1B with our best performing model.</p><p>Note that the computational cost is closely related to the batch size, which is interleaved with optimization. Though how batch sizes affect optimization remains an open question and might be task dependent, we believe the "best-1" and "best-3" settings well reflect the actual computational cost brought by MoS on language modeling tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 QUALITATIVE ANALYSIS</head><p>Since MoC shows a stronger performance than Softmax on PTB, the qualitative study focuses on the comparison between MoC and MoS. Concretely, given the same context (previous tokens), we search for prediction steps where MoS achieves lower negative log loss than MoC by a margin. We show some representative cases in <ref type="table" target="#tab_2">Table 14</ref> with the following observations:</p><p>? Comparing the first two cases, given the same preceding word "N", MoS flexibly adjusts its top predictions based on the different topic quantities being discussed in the context. In comparison, MoC emits quite similar top choices regardless of the context, suggesting its inferiority in make context-dependent predictions. ? In the 3rd case, the context is about international politics, where country/region names are likely to appear. MoS captures this nuance well, and yields top choices that can be used to complete a country name given the immediate preceding word "south". Similarly, in the 4th case, MoS is able to include "ual", a core entity of discussion in the context, in its top predictions. In contrast, MoC gives rather generic predictions irrieselevant to the context in both cases. ? For the 5th and the 6th example, we see MoS is able to exploit less common words accurately according to the context, while MoC fails to yield such choices. This well matches our analysis that MoS has the capacity of modeling context-dependent language.</p><p>#1 Context managed properly and with a long-term outlook these can become investment-grade quality properties &lt;eos&gt; canadian &lt;unk&gt; production totaled N metric tons in the week ended oct. N up N N from the preceding week 's total of N __?__ MoS top-5 million 0.38 tons 0.24 billion 0.09 barrels 0.06 ounces 0.04</p><p>MoC top-5 billion 0.39 million 0.36 trillion 0.05 &lt;eos&gt; 0.04 N 0.03</p><p>Reference canadian &lt;unk&gt; production totaled N metric tons in the week ended oct. N up N N from the preceding week 's total of N tons statistics canada a federal agency said &lt;eos&gt; #2 Context the thriving &lt;unk&gt; street area offers &lt;unk&gt; of about $ N a square foot as do &lt;unk&gt; locations along lower fifth avenue &lt;eos&gt; by contrast &lt;unk&gt; in the best retail locations in boston san francisco and chicago rarely top $ N __?__ Reference as the days go by the south african government will be ever more hard pressed to justify the continued &lt;unk&gt; of mr. &lt;unk&gt; as well as the continued banning of the anc and enforcement of the state of emergency &lt;eos&gt; #4 Context shares of ual the parent of united airlines were extremely active all day friday reacting to news and rumors about the proposed $ N billion buy-out of the airline by an &lt;unk&gt; group &lt;eos&gt; wall street 's takeover-stock speculators or risk arbitragers had placed unusually large bets that a takeover would succeed and __?__ MoS top-5 the 0.14 that 0.07 ual 0.07 &lt;unk&gt; 0.03 it 0.02</p><p>MoC top-5 the 0.10 &lt;unk&gt; 0.06 that 0.05 in 0.02 it 0.02</p><p>Reference wall street 's takeover-stock speculators or risk arbitragers had placed unusually large bets that a takeover would succeed and ual stock would rise &lt;eos&gt; #5 Context the government is watching closely to see if their presence in the &lt;unk&gt; leads to increased &lt;unk&gt; protests and violence if it does pretoria will use this as a reason to keep mr. &lt;unk&gt; behind bars &lt;eos&gt; pretoria has n't forgotten why they were all sentenced to life &lt;unk&gt; in the first place for sabotage and __?__ MoS top-5 &lt;unk&gt; 0.47 violence 0.11 conspiracy 0.03 incest 0.03 civil 0.03</p><p>MoC top-5 &lt;unk&gt; 0.41 the 0.03 a 0.02 other 0.02 in 0.01</p><p>Reference pretoria has n't forgotten why they were all sentenced to life &lt;unk&gt; in the first place for sabotage and conspiracy to &lt;unk&gt; the government &lt;eos&gt; #6 Context china 's &lt;unk&gt; &lt;unk&gt; program has achieved some successes in &lt;unk&gt; runaway economic growth and stabilizing prices but has failed to eliminate serious defects in state planning and an &lt;unk&gt; drain on state budgets &lt;eos&gt; the official china daily said retail prices of &lt;unk&gt; foods have n't risen since last december but acknowledged that huge government __?__ Reference the official china daily said retail prices of &lt;unk&gt; foods have n't risen since last december but acknowledged that huge government subsidies were a main factor in keeping prices down &lt;eos&gt;  <ref type="bibr" target="#b29">(Mikolov et al., 2010)</ref>. The context shown only includes the previous sentence and the current sentence the prediction step resides in.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Inan et al. (2016) -Variational LSTM + augmented loss 28M 91.5 87.0 Grave et al. (2016) -LSTM + continuous cache pointer ? Merity et al. (2017) -AWD-LSTM + continuous cache pointer ? 33M 53.8 52.0 Krause et al. (2017) -AWD-LSTM + dynamic evaluation ? 33M 46.4 44.3 Ours -AWD-LSTM-MoS + dynamical evaluation ? 35M 42.41 40.68</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>H ? and W ? are of dimensions (N ? d) and (M ? d) respectively, we have d ? rank(A ) ? min A ?F (A) rank(A ) If d ? min A ?F (A) rank(A ), there exist matrices A ? F (A), H ? R N ?d and W ? R M ?d , such that A can be factorized as A = H W . Because U is a universal approximator, there exists ? such that H ? = H and W ? = W . By Lemma 1, P ? (X|c) = P * (X|c) for all c in L. B EXPERIMENT SETTING AND HYPER-PARAMETERS B.1 PTB AND WT2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Cumulative percentage of normalized singulars given a value in [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and the rows of H ? , W ? , and A correspond to context vectors, word embeddings, and log probabilities of the true data distribution respectively. We use the subscript ? because (H ? , W</figDesc><table /><note>? ) is effectively a function indexed by the parameter ?, from the joint function family U. Concretely, H ? is implemented as deep neural networks, such as a recurrent network, while W ? is instantiated as an embedding lookup.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and WikiText-2 (WT2)<ref type="bibr" target="#b26">(Merity et al., 2016)</ref> based on perplexity. For fair comparison, we closely follow the regularization and optimization techniques introduced by<ref type="bibr" target="#b27">Merity et al. (2017)</ref>. We heuristically and manually search hyper-parameters for MoS based on the validation performance while limiting the model size (see Appendix B.1 for our hyper-parameters).? To investigate whether the effectiveness of MoS can be extended to even larger datasets, we conduct an additional language modeling experiment on the 1B Word dataset<ref type="bibr" target="#b4">(Chelba et al., 2013)</ref>. Specifically, we lower-case the text and choose the top 100K tokens as the vocabulary. A standard neural language model with 2 layers of LSTMs followed by a Softmax output layer is used as the baseline. Again, the network size of MoS is adjusted to ensure a comparable number of parameters. Notably, dropout was not used, since we found it not helpful to either model (see Appendix B.2 for more details). ? To show that the MoS is a generic structure that can be used to model other context-dependent distributions, we additionally conduct experiments in the dialog domain. We use the Switchboard dataset (Godfrey &amp; Holliman, 1997) preprocessed by<ref type="bibr" target="#b41">Zhao et al. (2017)</ref> 4 to train a Seq2Seq (Sutskever et al., 2014) model with MoS added to the decoder RNN. Then, a Seq2Seq model using Softmax and another one augmented by MoC with comparable parameter sizes</figDesc><table><row><cell>Model</cell><cell cols="2">#Param Validation</cell><cell>Test</cell></row><row><cell>Mikolov &amp; Zweig (2012) -RNN-LDA + KN-5 + cache</cell><cell>9M  ?</cell><cell>-</cell><cell>92.0</cell></row><row><cell>Zaremba et al. (2014) -LSTM</cell><cell>20M</cell><cell>86.2</cell><cell>82.7</cell></row><row><cell>Gal &amp; Ghahramani (2016) -Variational LSTM (MC)</cell><cell>20M</cell><cell>-</cell><cell>78.6</cell></row><row><cell>Kim et al. (2016) -CharCNN</cell><cell>19M</cell><cell>-</cell><cell>78.9</cell></row><row><cell>Merity et al. (2016) -Pointer Sentinel-LSTM</cell><cell>21M</cell><cell>72.4</cell><cell>70.9</cell></row><row><cell>Grave et al. (2016) -LSTM + continuous cache pointer  ?</cell><cell>-</cell><cell>-</cell><cell>72.1</cell></row><row><cell>Inan et al. (2016) -Tied Variational LSTM + augmented loss</cell><cell>24M</cell><cell>75.7</cell><cell>73.2</cell></row><row><cell>Zilly et al. (2016) -Variational RHN</cell><cell>23M</cell><cell>67.9</cell><cell>65.4</cell></row><row><cell>Zoph &amp; Le (2016) -NAS Cell</cell><cell>25M</cell><cell>-</cell><cell>64.0</cell></row><row><cell>Melis et al. (2017) -2-layer skip connection LSTM</cell><cell>24M</cell><cell>60.9</cell><cell>58.3</cell></row><row><cell>Merity et al. (2017) -AWD-LSTM w/o finetune</cell><cell>24M</cell><cell>60.7</cell><cell>58.8</cell></row><row><cell>Merity et al. (2017) -AWD-LSTM</cell><cell>24M</cell><cell>60.0</cell><cell>57.3</cell></row><row><cell>Ours -AWD-LSTM-MoS w/o finetune</cell><cell>22M</cell><cell>58.08</cell><cell>55.97</cell></row><row><cell>Ours -AWD-LSTM-MoS</cell><cell>22M</cell><cell>56.54</cell><cell>54.44</cell></row><row><cell>Merity et al. (2017) -AWD-LSTM + continuous cache pointer  ?</cell><cell>24M</cell><cell>53.9</cell><cell>52.8</cell></row><row><cell>Krause et al. (2017) -AWD-LSTM + dynamic evaluation  ?</cell><cell>24M</cell><cell>51.6</cell><cell>51.1</cell></row><row><cell>Ours -AWD-LSTM-MoS + dynamic evaluation  ?</cell><cell>22M</cell><cell>48.33</cell><cell>47.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained from Merity et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and Krause et al. (2017). ? indicates using dynamic evaluation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>respectively. With a comparable number of parameters, MoS outperforms all baselines with or without dynamic evaluation, and substantially improves over the current state of the art, by up to 3.6 points in perplexity.</figDesc><table><row><cell>Model</cell><cell cols="3">#Param Train Validation</cell><cell>Test</cell></row><row><cell>Softmax</cell><cell>119M</cell><cell>41.47</cell><cell>43.86</cell><cell>42.77</cell></row><row><cell>MoS</cell><cell>113M</cell><cell>36.39</cell><cell>38.01</cell><cell>37.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Perplexity comparison on 1B word dataset. Train perplexity is the average of the last 4,000 updates.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Evaluation scores on Switchboard. Further, the experimental results on Switchboard are summarized in Table 4 5 . Clearly, on all metrics, MoS outperforms MoC and Softmax, showing its general effectiveness.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>PTB</cell><cell>WT2</cell></row></table><note>. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the performance, which rules out hyper-parameters as the main source of improvement.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Rank comparison on PTB. To ensure comparable model sizes, the embedding sizes of Softmax, MoC and MoS are 400, 280, 280 respectively. The vocabulary size, i.e., M , is 10,000 for all models.</figDesc><table><row><cell cols="3">#Softmax Rank Perplexity</cell></row><row><cell>3</cell><cell>6467</cell><cell>58.62</cell></row><row><cell>5</cell><cell>8930</cell><cell>57.36</cell></row><row><cell>10</cell><cell>9973</cell><cell>56.33</cell></row><row><cell>15</cell><cell>9981</cell><cell>55.97</cell></row><row><cell>20</cell><cell>9981</cell><cell>56.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Empirical rank and test perplexity on PTB with different number of Softmaxes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table /><note>Hyper-parameters used for MoS. V-dropout abbreviates variational dropout</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Hyper-parameters used for dynamic evaluation of MoS. See<ref type="bibr" target="#b22">(Krause et al., 2017)</ref> for more detailed</figDesc><table><row><cell>descriptions.</cell><cell></cell><cell></cell></row><row><cell>B.2 1B WORD DATASET</cell><cell></cell><cell></cell></row><row><cell cols="3">For training, we use all of the 100 training shards. For validation, we use two shards from the</cell></row><row><cell cols="3">heldout set, namely [heldout-00, heldout-10]. For test, we use another three shards from</cell></row><row><cell cols="3">the heldout set, namely [heldout-20, heldout-30, heldout-40].</cell></row><row><cell>The hyper-parameters are listed below.</cell><cell></cell><cell></cell></row><row><cell>Hyper-parameter</cell><cell>Softmax</cell><cell>MoS-7</cell></row><row><cell>Learning rate</cell><cell>20</cell><cell>20</cell></row><row><cell>Batch size</cell><cell>60</cell><cell>60</cell></row><row><cell>BPTT langth</cell><cell>35</cell><cell>35</cell></row><row><cell>Embedding size</cell><cell>1024</cell><cell>900</cell></row><row><cell>RNN hidden sizes</cell><cell cols="2">[1024, 1024] [1024,1024]</cell></row><row><cell>Dropout rate</cell><cell>0</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Hyper-parameters used for Softmax and MoS in experiment on 1B word dataset.</figDesc><table /><note>C ADDITIONAL EXPERIMENTS C.1 HIGHER EMPIRICAL RANK OF MOS COMPARED TO MOC AND SOFTMAX</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Empirical expected pairwise KLD on PTB.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 12 .</head><label>12</label><figDesc>Clearly, the Softmax and MoS obtain the same BPC on the test set and comparable BPC on the validation set, which well match our hypothesis. Since the only difference in word-level language modeling is the existence of the Softmax bottleneck, the distinct behavior of MoS again supports our hypothesis that it is solving the Softmax bottleneck problem.</figDesc><table><row><cell cols="3">C.3 MOS COMPUTATIONAL TIME</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="8">PTB/bs PTB/best-1 WT2/bs WT2/best-1 WT2/best-3 1B/bs 1B/best-1 1B/best-3</cell></row><row><cell>Softmax</cell><cell>1x</cell><cell>1x</cell><cell>1x</cell><cell>1x</cell><cell>1x</cell><cell>1x</cell><cell>1x</cell><cell>1x</cell></row><row><cell>MoS-5</cell><cell>1.2x</cell><cell>-</cell><cell>1.3x</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MoS-7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.8x</cell><cell>5.7x</cell><cell>2.1x</cell></row><row><cell>MoS-10</cell><cell>1.6x</cell><cell>-</cell><cell>1.9x</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MoS-15</cell><cell>1.9x</cell><cell>2.8x</cell><cell>2.5x</cell><cell>6.4x</cell><cell>2.9x</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 13 :</head><label>13</label><figDesc>Training time slowdown compared to Softmax. MoS-K means using K mixture components. "bs"</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>Referenceby contrast &lt;unk&gt; in the best retail locations in boston san francisco and chicago rarely top $ N a square foot &lt;eos&gt; #3 Context as other &lt;unk&gt; governments particularly poland and the soviet union have recently discovered initial steps to open up society can create a momentum for radical change that becomes difficult if not impossible to control &lt;eos&gt; as the days go by the south __?__</figDesc><table><row><cell>MoS top-5</cell><cell>&lt;eos&gt; 0.36</cell><cell>a 0.13</cell><cell>to 0.07</cell><cell>for 0.07</cell><cell>and 0.06</cell></row><row><cell>MoC top-5</cell><cell>million 0.39</cell><cell>billion 0.36</cell><cell>&lt;eos&gt; 0.05</cell><cell>to 0.04</cell><cell>of 0.03</cell></row><row><cell>MoS top-5</cell><cell>africa 0.15</cell><cell>african 0.15</cell><cell>&lt;eos&gt; 0.14</cell><cell>korea 0.08</cell><cell>korean 0.05</cell></row><row><cell>MoC top-5</cell><cell>&lt;eos&gt; 0.38</cell><cell>and 0.08</cell><cell>of 0.06</cell><cell>or 0.05</cell><cell>&lt;unk&gt; 0.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 14 :</head><label>14</label><figDesc>Compaison of next-token prediction on Penn Treebank test data. N stands for a number as the result of preprocessing</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use capital letters for variables and small letters for constants.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This is also confirmed by our preliminary experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/snakeztc/NeuralDialog-CVAE/tree/master/data</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The numbers are not directly comparable to<ref type="bibr" target="#b41">Zhao et al. (2017)</ref> since their Seq2Seq implementation and evaluation scripts are not publicly available.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6"> Although Shazeer et al. (2017)  name their architecture as MoE, it is not a standard MoE<ref type="bibr" target="#b18">(Jacobs et al., 1991)</ref> and should be classified as MoC under our terminology.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the DARPA award D17AP00001, the Google focused award, and the Nvidia NVAIL award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning stochastic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Osendorfer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7610</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recurrent mixture density network for spatiotemporal visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08199</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01704</idno>
		<title level="m">Hierarchical multiscale recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning factored representations in a deep mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4314</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequential neural models with stochastic layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>S?ren Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2199" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep temporal sigmoid belief networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2467" to="2475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Switchboard-1 release 2. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holliman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04426</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Low rank language models for small training sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Fazel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="489" to="492" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A sparse plus low rank maximum entropy language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Fazel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1676" to="1679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01462</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
	<note>1995 International Conference on</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dynamic evaluation of neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Kahembwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07432</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Large text compression benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?bor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05589</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing lstm language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02182</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="234" to="239" />
		</imprint>
		<respStmt>
			<orgName>SLT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafi?t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Son</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernocky</surname></persName>
		</author>
		<ptr target="http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf" />
		<title level="m">Subword language modeling with neural networks. preprint</title>
		<imprint>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Generalizing and hybridizing count-based and neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00499</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The language instinct</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Pinker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Numerical recipes 3rd edition: The art of scientific computing. Cambridge university press</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Press</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recurrent neural networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><forename type="middle">Maximilian</forename><surname>Sch?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Georg</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="632" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Yann L Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th international conference on machine learning (ICML-13)</title>
		<meeting>the 30th international conference on machine learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10960</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.03474</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Recurrent highway networks</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
