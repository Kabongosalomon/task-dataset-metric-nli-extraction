<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2020 VARIATIONAL TEMPLATE MACHINE FOR DATA-TO- TEXT GENERATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxian</forename><surname>Shi</surname></persName>
							<email>shiwenxian@bytedance.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
							<email>zywei@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2020 VARIATIONAL TEMPLATE MACHINE FOR DATA-TO- TEXT GENERATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations. Learning such templates is prohibitive since it often requires a large paired &lt;table,description&gt; corpus, which is seldom available. This paper explores the problem of automatically learning reusable "templates" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include: a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able to generate more diversely while keeping a good fluency and quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Generating text descriptions from structured data (data-to-text) is an important task with many practical applications. Data-to-text has been used to generate different kinds of texts, such as weather reports <ref type="bibr" target="#b0">(Angeli et al., 2010)</ref>, sports news <ref type="bibr" target="#b22">(Mei et al., 2016;</ref><ref type="bibr" target="#b29">Wiseman et al., 2017)</ref> and biographies <ref type="bibr" target="#b19">(Lebret et al., 2016;</ref><ref type="bibr" target="#b28">Wang et al., 2018b;</ref><ref type="bibr" target="#b7">Chisholm et al., 2017)</ref>. <ref type="figure">Figure 1</ref> gives an example of data-to-text task, which takes an infobox 1 as the input and outputs a brief description of the information in the table. There are several recent methods utilizing neural encoder-decoder frameworks to generate text description from data tables <ref type="bibr" target="#b19">(Lebret et al., 2016;</ref><ref type="bibr" target="#b2">Bao et al., 2018;</ref><ref type="bibr" target="#b7">Chisholm et al., 2017;</ref><ref type="bibr" target="#b20">Liu et al., 2018)</ref>.</p><p>Although current table-to-text models could generate high quality sentences, the diversity of these output sentences are not satisfactory. We find that templates are crucial in increasing the variations of sentence structure. For example, <ref type="table" target="#tab_1">Table 1</ref> gives three descriptions with their templates for the given table input. Different templates control the sentence arrangement, thus vary the generation. Some related work <ref type="bibr" target="#b30">(Wiseman et al., 2018;</ref><ref type="bibr" target="#b8">Dou et al., 2018)</ref> employs hidden semi-Markov hidden model to extract templates from table-text pairs.</p><p>We argue that templates can be better considered for generating more diverse outputs. First, it is non-trivial to sample different templates for obtaining different output utterances. Directly adopting variational auto-encoders <ref type="bibr">(VAEs, Kingma &amp; Welling (2014)</ref>) in table-to-text only enables to sample in the latent space. However, VAEs always generate irrelevant outputs, which may change the table content instead of sampling templates. This may harm the quality of output sentences. To address the above problem, if we can directly sample in the template space, we may get more diverse outputs while keeping the good quality of output sentences. nameVariable is a pub with a low rating and average cost, it is a Japanese restaurant and nameVariable is in riverside. In this paper, to address the above two problems, we propose the variational template machine (VTM) for data-to-text generation, which enables to generate sentences with diverse templates while preserving the high quality. Particularly, we introduce two latent variables, representing template and content, to control the generation. The two latent variables are disentangled, and thus we can generate diverse outputs by directly sampling in the latent space for template. Moreover, we propose a novel approach for semi-supervised learning in the VAE framework, which could fully exploit the raw sentences for enriching the template space. Inspired by back-translation <ref type="bibr" target="#b24">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b5">Burlot &amp; Yvon, 2018;</ref><ref type="bibr" target="#b1">Artetxe et al., 2018)</ref>, we design a variational back-translation process. Instead of training a sentence-to-table backward generation model directly, we take the variational posterior of the content latent variable as the backward model to help to train the forward generative model. Auxiliary losses are introduced to ensure the learning of meaningful and disentangled latent variables.</p><p>Experimental results on Wikipedia biography dataset <ref type="bibr" target="#b19">(Lebret et al., 2016)</ref> and sentence planning NLG dataset <ref type="bibr" target="#b23">(Reed et al., 2018)</ref> show that our model can generate texts with more diversity while keeping a good fluency. Training together with a large amount of raw text, VTM can further improve the generation performance. Besides, VTM is more predominant in the case where sentence-to-table backward model is hard to train. Ablation studies also demonstrate the effects of the auxiliary losses on the disentanglement of template and content spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM FORMULATION AND NOTATIONS</head><p>As a data-to-text task, we have table-text pairs</p><formula xml:id="formula_0">D p = {(x i , y i )} N i=1</formula><p>, where x i is the table, and y i is the output sentence.</p><p>Following the description scheme of <ref type="bibr" target="#b19">Lebret et al. (2016)</ref>, a table x can be viewed as a set of K records of field-position-value triples, i.e.,</p><formula xml:id="formula_1">x = {(f, p, v) i } K i=1 ,</formula><p>where f is the field and p is the index of value v in the field f . For example, an item "Name: John Lennon" is denoted as two corresponding records: (Name, 1, John) and (Name, 2, Lennon). For each triple, we first embed field, position and value as d-dim vectors e p , e f , e v ? R d . Then, the d t -dim representation of the record is obtained by h i = tanh(W [e f , e p , e v ] T + b), i = 1...K, where W ? R dt?3d and b ? R dt are parameters. The final representation of the table, denoted as f enc (x), is obtained by max-pooling over all field-position-value triple records,</p><formula xml:id="formula_2">f enc (x) = h = MaxPool i {h i ; i = 1...K}.</formula><p>In addition to the table-text pairs, we also have raw texts without table input, denoted as D r = {y i } M i=1 . It usually has M N . <ref type="figure">Figure 1</ref>: Two types of data in the data-to-text task: Row 2 presents an example of table-text pairs; Row 3 shows a sample of raw text, whose table input is missing and only sentence is provided. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VARIATIONAL TEMPLATE MACHINE</head><p>As shown in the graphical model in <ref type="figure">Figure 2</ref>, our VTM modifies the vanilla VAE model by introducing two independent latent variables z and c, representing template latent variable and content latent variable respectively. c models the content information in the table, while z models the sentence template information. Target sentence y is generated by both content and template variables. The two latent variables are disentangled, which makes it possible to generate diverse and relevant sentences by sampling template variable and retraining the content variable. Considering pairwise and raw data presented in <ref type="figure">Figure 1</ref>, their generation process for the content latent variable c is different.</p><p>? For a given table-text pair (x, y) ? D p , the content is observable from table x. As a result, c is assumed to be deterministic given table x, whose prior is defined as a delta distribution p(c|x) = ?(c = f enc (x)). The marginal log-likelihood is:</p><formula xml:id="formula_3">log p ? (y|x) = log z c p ? (y|x, z, c)p(z)p(c|x)dcdz = log z p ? (y|x, z, c = f enc (x))p(z)dz, (x, y) ? D p .</formula><p>(1)</p><p>? For raw text y ? D n , the content is unobservable with the absence of table x. As a result, the content latent variable c should be sampled from prior of Gaussian distribution N (0, I). The marginal log-likelihood is:</p><formula xml:id="formula_4">log p ? (y) = log z c p ? (y|z, c)p(z)p(c)dcdz, y ? D r .<label>(2)</label></formula><p>In order to make full use of both table-text pair data and raw text data, the above marginal loglikelihood should be optimized jointly:</p><formula xml:id="formula_5">L(?) = E (x,y)?Dp [log p ? (y|x)] + E y?Dr [log p ? (y)].<label>(3)</label></formula><p>Directly optimizing Equation 3 is intractable. Following the idea of variational inference (Kingma &amp; Welling, 2014), a variational posterior q ? (?) is constructed as an inference model (dashed lines in <ref type="figure">Figure 2</ref>) to approximate the true posterior. Instead of optimizing the marginal log-likelihood in Equation 3, we maximize the evidence lower bound (ELBO). In Section 3.1 and 3.2, the ELBO of table-text pairwise data and raw text data are discussed, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LEARNING FROM TABLE-TEXT PAIR DATA</head><p>In this section, we will show the learning loss of table-text pair data. According to the aforementioned assumption, the content variable c is observable and follows a delta distribution centred in the hidden representation of the table x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ELBO objective.</head><p>Assuming that the template variable z only relies on the template of target sentence, we introduce q ? (z|y) as an approximation of the true posterior p(z|y, c, x),</p><p>The ELBO loss of Equation 1 is written as</p><formula xml:id="formula_6">L ELBOp (x, y) = ?E q ?z (z|y) log p ? (y|z, c = f enc (x), x) + D KL (q ?z (z|y) p(z)), (x, y) ? D p .</formula><p>The variational posterior q ?z (z|y) is assumed as a multivariate Gaussian distribution N (? ?z (y), ? ?z (y)), while the prior p(z) is taken as a normal distribution N (0, I).</p><p>Preserving-Template Loss. Without any supervision, the ELBO loss alone does not guarantee to learn a good template representation space. Inspired by the work in style-transfer <ref type="bibr" target="#b13">(Hu et al., 2017b;</ref><ref type="bibr" target="#b25">Shen et al., 2017;</ref><ref type="bibr" target="#b3">Bao et al., 2019;</ref><ref type="bibr" target="#b16">John et al., 2018)</ref>, an auxiliary loss is introduced to embed the template information of sentences into template variable z.</p><p>With table, we are able to roughly align the tokens in sentence with the records in the table. By replacing these tokens with a special token &lt;ent&gt;, we can remove the content information from sentences and get the sketchy sentence template, denote as?. We introduce the preserving-template loss L pt to ensure that the latent variable z only contains the information of the template.</p><formula xml:id="formula_7">L pt (x, y,?) = ?E q ?z (z|y) log p ? (?|z) = ?E q ?z (z|y) m t=1 log p ? (? t |z,? &lt;t )</formula><p>where m is the length of the?, and ? denotes the parameters of the extra template generator. L pt is trained via parallel data. In practice, due to the insufficient amount of parallel data, template generator p ? may not be well-learned. However, experimental results show that this loss is sufficient to provide a guidance for learning a template space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LEARNING FROM RAW TEXT DATA</head><p>Our model is able to make use of a large number of raw data without table since the content information of table could be obtained by the content latent variable.</p><p>ELBO objective. According to the definition of generative model in Equation 2, the ELBO of raw text data is</p><formula xml:id="formula_8">log p ? (y) = E q ? (z,c|y) log p ? (y, z, c) q ? (z, c|y) , y ? D r .</formula><p>With the mean field approximation <ref type="bibr" target="#b31">(Xing et al., 2003)</ref>, q ? (z, c|x) can be factorized as: q ? (z, c|y) = q ?z (z|y)q ?c (c|y). We have:</p><formula xml:id="formula_9">L ELBOr (y) = ? E q ?z (z|y)q ?c (c|y) log p ? (y|z, c) + D KL (q ?z (z|y)||p(z)) + D KL (q ?c (c|y)||p(c)), y ? D r .</formula><p>In order to make use of template information contained in raw text data effectively, the parameters of generation network p ? (y|z, c) and posterior network q ?z (z|y) are shared for pairwise and raw data. In decoding process, for raw text data, we use content variable c as the table embedding for the missing of table x. Variational posterior for c is deployed as another multivariate Guassian q ?c (c|y) = N (? ?c (y), ? ?c (y)). Both p(z) and p(c) are taken as normal distribution N (0, I).</p><p>Preserving-Content Loss. In order to make the posterior q ?c (c|y) correctly infers the content information, the table-text pairs are used as the supervision to train the recognition network of q ?c (c|y). To this end, we add a preserving-content loss</p><formula xml:id="formula_10">L pc (x, y) = ?E q ?c (c|y) c ? h 2 + D KL (q ?c (c|y)||p(c)), (x, y) ? D p ,</formula><p>where h = f enc (x) is the embedding of table obtained by the table encoder. Minimizing L pc is also helpful to bridge the gap of c between pairwise (taking c = h) and raw training data (sampling from q ? (c|y)). Moreover, we find that the first term of L pc is equivalent to (1) make the mean of q ? (c|y) closer to h; (2) minimize the trace of co-variance of q ? (c|y). The second term serves as a regularization. Detailed explanations and proof are referred in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Training procedure</head><p>Input: Model parameters ?z, ?c, ?, ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MUTUAL INFORMATION LOSS</head><p>As introduced by previous works <ref type="bibr" target="#b6">(Chen et al., 2016;</ref><ref type="bibr" target="#b32">Zhao et al., 2017;</ref>, adding mutual information term to ELBO could alleviate KL collapse effectively and improve the quality of variational posterior. Adding mutual information terms directly imposes the association of content and template latent variables with target sentences. Besides, theoretical proof 2 and experimental results show that introducing mutual information bias is necessary in the presence of preserving-template loss L pt (x p , y p ).</p><p>As a result, in our work, the following mutual information term is added to objective L MI (y) = ?I(z, y) ? I(c, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">TRAINING PROCESS</head><p>The final loss of VTM is made up of the ELBO losses and extra losses:</p><formula xml:id="formula_11">L tot (x p , y p , y r ) = L ELBOp (x p , y p ) + L ELBOr (y r ) + ? MI (L MI (y p ) + L MI (y r )) + ? pt L pt (x p , y p ) + ? pc L pc (x p , y p ), (x p , y p ) ? D p , y r ? D r .</formula><p>? MI , ? pt and ? pc are hyperparameters with respect to auxiliary losses.</p><p>The training procedure is shown in Algorithm 1. The parameters of generation network ? and posterior network ? z,c could be trained jointly by both table-text pair data and raw text data. In this way, a large number of raw text data can be used to enrich the generation diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS AND BASELINE MODELS</head><p>Dataset. We perform the experiment on SPNLG <ref type="bibr" target="#b23">(Reed et al., 2018)</ref> 3 and WIKI <ref type="bibr" target="#b19">(Lebret et al., 2016;</ref><ref type="bibr" target="#b28">Wang et al., 2018b)</ref>. Two datasets come from two different domains. The former is a collection of restaurant descriptions, which expands the E2E dataset 4 into a total of 204, 955 utterances with more varied sentence structures and instances. The latter contains 728, 321 sentences of biographies from Wikipedia. To simulate the environment that a large number of raw texts provided, we just use part of the table-text pairs from two datasets, leaving most of the instances as raw texts. Concretely, for two datasets, we initially keep the ratio of table-text pairs to raw texts as 1:10. For WIKI dataset, in addition to the data from WikiBio <ref type="bibr" target="#b19">(Lebret et al., 2016)</ref>, the raw text data is further extended by the biographical descriptions of people 5 from external Wikipedia Person and Animal Dataset <ref type="bibr" target="#b27">(Wang et al., 2018a)</ref>. The statistics for the number of table-text pairs and raw texts in the training, validation and test sets are shown in <ref type="table" target="#tab_5">Table 2</ref>.</p><p>Evaluation Metrics. For WIKI dataset, we evaluate the generation quality based on BLEU-4, NIST, ROUGE-L (F-score). For SPNLG, we use BLEU-4, NIST, METEOR, ROUGE-L (F-score), and CIDEr. We use the same automatic evaluation script from E2E NLG Challenge 6 . The diversity of generation is evaluated by self-BLEU <ref type="bibr" target="#b35">(Zhu et al., 2018)</ref>. The lower self-BLEU, the more diversely the model generates.  Baseline models. We implement the following models as baselines:</p><p>? Table2seq: Table2seq model first encodes the table into hidden representations then generates the sentence in a sequence-to-sequence architecture <ref type="bibr" target="#b26">(Sutskever et al., 2014)</ref>. For a fair comparison, we apply the same table-encoder architecture as in Section 2 and the same LSTM decoder with attention mechanism as our model. The model is only trained on pair-wise data. During the testing, we generate five sentences with beam size ranging from one to five to increase some variations. We denote the model as Table2seq-beam. We also implement the decoding with forward sampling strategy (namely Table2seq-sample). Moreover, to incorporate raw data, we first pretrain the decoder using raw text as a language model, then train Table2seq on the table-text pairs, which is noted as Table2seq-pretrain. Table2seq-pretrain has the same decoding strategy as Table2seq-beam.</p><p>? Temp-KN: Template-KN model <ref type="bibr" target="#b19">(Lebret et al., 2016)</ref> first generates a template according to the interpolated 5-gram Kneser-Ney (KN) language modeled over sentence templates, then replaces the special token for the field with the corresponding words from the table.</p><p>The hype-parameters of the VTM are chosen based on the lowest L ELBOp on the validation set of SPNLG and L ELBOp + L ELBOr on the validation set of WIKI. Word embeddings are randomly initialized with 300-dimension. During training, we use Adam optimizer <ref type="bibr" target="#b17">(Kingma &amp; Ba, 2015)</ref> with the initial learning rate as 0.001. Details on hyperparameters are listed in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPERIMENTAL RESULTS ON SPNLG DATASET</head><p>Quantitative analysis. According to the results in <ref type="table" target="#tab_7">Table 3</ref>, we find that our variational template machine (VTM) can generally produce sentences with more diversity under a promising performance in terms of BLEU metrics. Table2seq with beam search algorithm (Table2seq-beam), which is only trained on parallel data, generates the most fluent sentences, but its diversity is rather poor. Although the sampling decoder (Table2seq-sample) gets the lowest self-BLEU, it sacrifices the fluency at the cost. Table2seq performs even worse when the decoder is pre-trained by raw data as a language model. Because there is still a gap between the language model and data-to-text task, the decoder fails to learn how to use raw text in the generation of data-to-text stage. On the contrary, VTM can make full use of the raw data with the help of content variables. As a template-based model, Temp-KN receives the lowest self-BLEU score, but it fails to generate fluent sentences.</p><p>Ablation study. To study the effectiveness of the auxiliary loses and the augmented raw texts, we progressively remove the auxiliary losses and raw data in the ablation study. We reach the conclusions as follows.</p><p>? Without the preserving-content loss L pc , the model has a relative decline in generation quality. This implies that, by training the same inference model of content variable in pairwise data, preserving-content loss provides an effective instruction for learning the content space.</p><p>? VTM-noraw is the model trained without using raw data, where only the loss functions in Section 3.1 are optimized. Comparing with VTM-noraw, VTM gets a substantial improvement in generation quality. More importantly, without extra raw text data, there is also a decline in diversity (self-BLEU). Experimental results show that raw data plays a valuable role in improving both generation quality and diversity, which is often neglected by previous studies.</p><p>? We further remove the mutual information loss and preserving-template loss from VTM-noraw model. Both generation quality and diversity continuously decline, which verifies the effectiveness of the two losses. Moreover, the automatic evaluation results of VTM-noraw-L MI -L pt empirically show that preserving-template loss may be a hinder if we only add it during the training, as illustrated in Section 3.3.   Experiment on quality and diversity trade-off. The quality and diversity trade-off is further analyzed to illustrate the superiority of VTM. In order to evaluate the quality and diversity under different sampling methods, we conduct experiment on sampling from the softmax with different temperatures. Sampling from the softmax with temperature is commonly applied to shape the distribution <ref type="bibr" target="#b9">(Ficler &amp; Goldberg, 2017;</ref><ref type="bibr" target="#b11">Holtzman et al., 2019)</ref>. Given the logits u 1:|V | and temperature ?, we sample from the distribution:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU NIST METEOR ROUGE CIDEr Self-BLEU</head><formula xml:id="formula_12">p(y t = V l |y &lt;t , x, z, ? ) = exp (u l /?) l exp (u l /?)</formula><p>When ? ? 0, it approaches greedy decoding. When ? = 1.0, it is the same as forward sampling. In the experiment, we gradually adjust temperature from 0 to 1, taking ? = 0.1, 0.2, 0.3, 0.5, 0.6, 0.9, 1.0. BLEU and self-BLEU under different temperatures are evaluated for both Table2seq and VTM. The self-BLEU in different temperatures and BLEU and self-BLEU curves are plotted in <ref type="figure" target="#fig_2">Figure 3</ref>. It empirically demonstrates the trade-off between the generation quality and diversity. By sampling from different temperatures, we can plot the portfolios of (Self-BLEU,BLEU) pairs of Table2seq and VTM. The closer the curve is to the upper left, the better the performance of the model. VTM generally gets lower self-BLEU with more diverse outputs under the comparable level of BLEU score.</p><p>Human evaluation In addition to the quantitative experiments, human evaluation is conducted as well. We randomly select 120 generated samples (each has five sentences) and ask three annotators to rate them on a 1-5 Likert scale in terms of the following features:</p><p>? Accuracy: whether the generated sentences are consistent with the content in the table.</p><p>? Coherence: whether the generated sentences are coherent.</p><p>? Diversity: whether the sentences have as many patterns/structures as possible.</p><p>Based on the qualitative results in <ref type="table" target="#tab_9">Table 4</ref>, VTM generates the best sentences with the highest accuracy and coherence. Besides, VTM is able to obtain the comparable diversity with Table2seqsample and Temp-KN. Compared with the model without using raw data (VTM-no raw), there is a significant improvement in diversity, which indicates that raw data essentially enriches the latent    Experiment on the diversity under different proportions of raw. In order to show how much raw data may contribute to the VTM model, we train the model under different proportions of raw data to pairwise data in training. Specifically, we control the ratio of raw sentences to the table-text pairs under 0.5:1, 1:1, 2:1, 3:1, 5:1, 7:1 and 10:1. As shown in <ref type="figure" target="#fig_0">Figure 4</ref>, the self-BLEU rapidly decreases even adding a small number of raw data, and continuously decreases until the ratio equals 5:1. The improvement is marginal after adding more than 5 times of raw data.</p><p>Case study. According to ." (Sentence 4, one sentence, "and" aggregation). <ref type="table" target="#tab_10">Table 5</ref> shows the results for WIKI dataset, the same conclusions can be drawn as in the results in SPNLG dataset for both the quantitative analysis and ablation study. VTM is able to generate sentences with the comparable quality as Table2seq-beam but more diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EXPERIMENTAL RESULTS ON WIKI DATASET</head><p>Comparison with the pseudo-table-based method. Another way to incorporate raw data is to construct pseudo-table from the given sentence by applying a sentence-to-table backward model via name entity recognition (NER). However, when the type of entities is complicated, such as in product introduction, or the raw data comes from the different domains as pairwise data, the commonlyused model for NER cannot provide accurate pseudo-tables. In this experiment, we replace 841,507 biography raw sentences with 101,807 sentences that describe the animals <ref type="bibr" target="#b28">(Wang et al., 2018b)</ref> to test the generalization of our model in raw data of different domains. NER+Table2seq is the twostep model that first constructs the pseudo-table by a Bi-LSTM-CRF <ref type="bibr" target="#b14">(Huang et al., 2015)</ref> model <ref type="table" target="#tab_5">trained from the table-text pairs, then trains Table2seq from both table-text pairs and pseudo-table-</ref>text pairs. We control the temperature in decoding method as previous, and results are plotted in <ref type="figure" target="#fig_1">Figure 5</ref>. We find that compared with NER+Table2seq, the curve of VTM is closer to the upper left, Table2seq VTM-noraw VTM Train ?30min / 6 epochs ?30min / 6 epochs ?160min / 15 epochs Test ?80min ?80min ?80min    Computational cost. We further compare the computational cost of VTM with other models, for both training and testing phases. We train and test the models on a single Tesla V100 GPU. The time spent to reach the lowest ELBO in the validation set is listed in <ref type="table" target="#tab_12">Table 6</ref>. VTM is trained about five times longer than the baseline Table2seq model (160 minutes, 15 epochs in total) because of the training of an extra large number of raw data (84k pairwise data and 841k raw texts). In the testing phase, VTM enjoys the same speed as other competitor models, approximately 80 minutes to generate 72k wiki sentences in the test set.</p><p>Case study. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Data-to-text Generation. Data-to-text generation aims to produce summary for the factual structured data, such as numerical Semi-supervised Learning From Raw Data. It is easier to acquire raw text than to get structured data, and most neural generators cannot make the best use of raw text, universally. <ref type="bibr" target="#b21">Ma et al. (2019)</ref> proposed that encoder-decoder framework may fail when not enough parallel corpus is provided.</p><p>In the area of machine translation, back-translation have been proved to be an effective method to utilize monolingual data <ref type="bibr" target="#b24">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b5">Burlot &amp; Yvon, 2018)</ref>.  <ref type="bibr" target="#b12">(Hu et al., 2017a;</ref><ref type="bibr" target="#b34">Zhou &amp; Neubig, 2017;</ref><ref type="bibr" target="#b3">Bao et al., 2019)</ref>. For instance, <ref type="bibr" target="#b3">Bao et al. (2019)</ref> devised multi-task losses adversarial losses to disentangle the latent space into syntactic space and semantic space. Motivated by the idea of back-translation and variational autoencoders, VTM model proposed in this work can not only fully utilize the non-parallel text corpus, but also learn a disentangled representation for template and content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose the Variational Template Machine (VTM) based on a semi-supervised learning approach in the VAE framework. Our method not only builds independent latent spaces for template and content for diverse generation, but also exploits raw texts without tables to further expand the template diversity. Experimental results on two datasets show that VTM outperforms the model without using raw data in terms of both generation quality and diversity, and it can achieve a comparable quality in generation with Table2seq, as well as promote the diversity by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXPLANATION FOR PRESERVING-CONTENT LOSS</head><p>The first term of ?L pc (x, y) is equivalent to:</p><formula xml:id="formula_13">E qc(c|x) ||c ? h|| 2 = E qc(c|x) K i=1 (c i ? h i ) 2 = K i=1 E qc(c|x) (c i ? h i ) 2 = K i=1 [(E(c i ? h i )) 2 + var(c i )] = K i=1 [(E(c i ) ? h i ) 2 + var(c i )] = K i=1 [(? i ? h i ) 2 + ? ii ] = ||? ? h|| 2 + tr(?)</formula><p>When we minimize it, we jointly minimize the distance between mean of approximated posterior distribution, and the trace of the co-variance matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOF FOR ANTI-INFORMATION PROPERTY OF ELBO</head><p>Consider the KL divergence over the whole dataset (or a mini-batch of data), we have where q(z) = E x?D (q(z|x)) and I(z, x) = H(z) ? H(z|x). Since KL divergence can be viewed as a regularization term in ELBO loss, When ELBO is maximized, the KL term is minimized, and mutual information between x and latent z, I(z, x) is minimized. This implies that z and x eventually become more independent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C PROOF FOR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D IMPLEMENTATION DETAILS</head><p>For the model trained on WIKI dataset, the the dimension of latent template variable is set as 100, and the dimension of latent content variable is set as 200. The dimension of the hidden for table is 300. For the hyperparameters of total loss L tot , we set ? MI = 0.5, ? pt = 1.0 and ? pc = 0.5.</p><p>For the model trained on SPNLG dataset, the dimension of latent template variable is set as 64, and the dimension of latent content variable is set as 100. The dimension of the hidden for table is also 300. For the hyperparameters of total loss L tot , we set ? MI = ? pt = ? pc = 1.0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E CASE STUDY ON SPNLG EXPERIMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FigureFigure 4 :</head><label>4</label><figDesc>Self-BLEU and the proportion of raw texts to table-sentence pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Quality-diversity trade-off curve compared with NER+Table2seq. template space. Although obtaining the highest scores in diversity for Table2seq-sample and Temp-KN, their generation qualities are much inferior to the VTM, and comparable generation quality is the prerequisite when comparing the diversity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 :</head><label>3</label><figDesc>John Ryder (21 August 1889 -4 April 1977) was an Australian cricketer. 4: Jack Ryder (8 March 1889 -3 April 1977) was an Australian cricketer. 5: John Ryder (August 1889 -April 1977) was an Australian cricketer. VTM 1: John Ryder (8 August 1889 -4 April 1977) was an Australian cricketer. 2: John Ryder (born 8 August 1889) was an Australian cricketer. 3: Jack Ryder (born August 9, 1889 in Victoria, Australia) was an Australian cricketer. 4: John Ryder (August 8, 1889 -April 4, 1977) was an Australian rules footballer who played for Victoria in the Victorian football league (VFL). 5: John Ryder, also known as the king of Collingwood (8 August 1889 -4 April 1977) was an Australian cricketer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>E</head><label></label><figDesc>x?p(x) [D KL (q(z|x) p(x))] =E q(z|x)p(x) [log q(z|x) ? log p(z)] = ? H(z|x) ? E q(z) log p(z) = ? H(z|x) + H(z) + D KL (q(z) p(z)) =I(z, x) + D KL (q(z) p(z))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>THE PRESERVING-TEMPLATE LOSS WHEN POSTERIOR COLLAPSE HAPPENS When posterior collapse happens, D KL (q(z|y)||p(z)) ? 0,L pt (Y,? ) =E? ?p(?),y?p(y) E z?q(z|y) log p ? (?|z) =E? ?p(?) E z?p(z) log p ? (?|z) log p ? (?|z)dz d? =E z E?[log p ? (y)|z] = E? log p ? (y)During the back-propagation, || z L pt (Y,? )|| = 0 thus, ? z is not updated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table: name[nameVariable], eatType[pub], food[Japanese], priceRange[average], customerRating[low], area[riverside] Template1: [name] is a [food] restaurant, it is a [eatType] and it has an [priceRange] cost and [customerRating] rating. it is in [area]. Sentence1: nameVariable is a Japanese restaurant, it is a pub and it has an average cost and low rating. it is in riverside. Template2: [name] has an [priceRange] price range with a [customerRating] rating, and [name] is an [food] [eat-Type] in [area]. Sentence2: nameVariable has an average price range with a low rating, and nameVariable is an Japanse pub in riverside. Template3: [name] is a [eatType] with a [customerRating] rating and [priceRange] cost, it is a [food] restaurant and [name] is in [area]. Sentence3:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>An example: generating sentences based on different templates.Second, we can hardly obtain promising sentences by sampling in the template space, if the template space is less informative. Namely, either encoder-decoder models or VAE-based models requires</figDesc><table /><note>abundant parallel table-text pairs during the training. In such case, constructing high-quality parallel dataset is often labor-intensive. With limited table-sentence pairs, a VAE model cannot construct an informative template space. How to fully utilize raw sentences (without aligned table) to enrich the latent template space is under study.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table-text pair data Dp = {(x, y)i} N i=1 ; raw text data Dr = {yj} M j=1 ; M N Procedure TRAIN(Dp, Dr): 1: Update ?z, ?c, ?, ? by gradient descent on LELBO p + LMI + Lpt + Lpc 2: Update ?z, ?c, ? by gradient descent on LELBO r + LMI 3: Update ?z, ?c, ?, ? by gradient descent on Ltot</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>TrainValid Test Dataset #table-text pair #raw text #table-text pair #raw text #table-text pair SPNLG</figDesc><table><row><cell></cell><cell>14, 906</cell><cell>149, 058</cell><cell>20, 495</cell><cell>/</cell><cell>20, 496</cell></row><row><cell>WIKI</cell><cell>84, 150</cell><cell>841, 507</cell><cell>72, 831</cell><cell>42, 874</cell><cell>72, 831</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics in our experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Result for SPNLG data set. Under the 0.05 significance level, VTM gets significantly higher results in all the fluency metrics than all the baselines except Table2seq-beam.</figDesc><table><row><cell>0.42</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BLEU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Table2seq</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>VTM</cell><cell></cell></row><row><cell>0.32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Self-BLEU</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Human evaluation results on different models. The bold numbers are significantly higher then others under 0.01 significance level.</figDesc><table><row><cell>Methods</cell><cell cols="4">BLEU NIST ROUGE Self-BLEU</cell></row><row><cell>Table2seq-beam</cell><cell>26.74</cell><cell>5.97</cell><cell>48.20</cell><cell>92.00</cell></row><row><cell>Table2seq-sample</cell><cell>21.75</cell><cell>5.32</cell><cell>42.09</cell><cell>36.07</cell></row><row><cell>Table2seq-pretrain</cell><cell>25.43</cell><cell>5.44</cell><cell>45.86</cell><cell>99.88</cell></row><row><cell>Temp-KN</cell><cell>11.68</cell><cell>2.04</cell><cell>40.54</cell><cell>73.14</cell></row><row><cell>VTM</cell><cell>25.22</cell><cell>5.96</cell><cell>45.36</cell><cell>74.86</cell></row><row><cell>-Lpc</cell><cell>22.16</cell><cell>4.28</cell><cell>40.91</cell><cell>80.39</cell></row><row><cell>VTM-noraw</cell><cell>21.59</cell><cell>5.02</cell><cell>39.07</cell><cell>78.19</cell></row><row><cell>-LMI</cell><cell>21.30</cell><cell>4.73</cell><cell>40.99</cell><cell>79.45</cell></row><row><cell>-LMI-Lpt</cell><cell>16.20</cell><cell>3.81</cell><cell>38.04</cell><cell>84.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Results for WIKI dataset. All the metrics are significant under 0.05 significance level.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc>(in Appendix E), despite template-like structures vary much in a forward sampling model, the information in sentences may be wrong. For example, Sentence 3 says that the restaurant is a Japanese place. Notably, VTM produces correct texts with more diversity of templates. VTM is able to generate different number of sentences and conjunctions.</figDesc><table><row><cell>For example,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Computational cost for each model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table name [</head><label>name</label><figDesc>Jack Ryder], country[Australia], fullname[John Ryder], nickname[the king of Collingwood],</figDesc><table><row><cell></cell><cell>birth date[8 August 1889], birth place[Collingwood, Victoria, Australia], death date[4 April 1977],</cell></row><row><cell></cell><cell>death place[Fitzroy, Victoria, Australia], club[Victoria], testdebutyear[1920 england], aritcle title[Jack</cell></row><row><cell></cell><cell>Ryder (cricketer)]</cell></row><row><cell>Reference</cell><cell>John "Jack" Ryder, mbe (8 August 1889 -3 April 1977) was a cricketer who played for Victoria and Australia.</cell></row><row><cell></cell><cell>1: john Ryder ( 8 August 1889 -3 April 1977) was an Australian cricketer .</cell></row><row><cell></cell><cell>2: john Ryder Ryder ( 8 August 1889 -3 April 1977) was an Australian cricketer .</cell></row><row><cell></cell><cell>3: john Ryder Ryder ( 8 August 1889 -3 April 1977) was an Australian cricketer who played for glouces-</cell></row><row><cell>Table2seq-sample</cell><cell>tershire cricket club in 1912 .</cell></row><row><cell></cell><cell>4: john Ryder ( 8 August 1889 -3 April 1977) was an Australian cricketer .</cell></row><row><cell></cell><cell>5: john Ryder oliveira ( 8 August 1889 -3 April 1977) was an Australian test cricketer who played against</cell></row><row><cell></cell><cell>great Britain with international cricket club .</cell></row><row><cell></cell><cell>1: jack Ryder ( born August 8, 1889) is a former professional cricketer) .</cell></row><row><cell></cell><cell>2: "jack" Ryder ( born August 8, 1889) is a former professional cricketer) who played in the national</cell></row><row><cell></cell><cell>football league.</cell></row><row><cell>Temp-KN</cell><cell>3: jack Ryder ( born 8 August 1889 in Collingwood, Victoria,) is a former professional cricketer) .</cell></row><row><cell></cell><cell>4: Jack Ryder ( born August 8, 1889, in Collingwood, Victoria, Australia) is a former professional football</cell></row><row><cell></cell><cell>player who is currently a member of the united states .</cell></row><row><cell></cell><cell>5: jack Ryder ( born August 8, 1889) is a former professional cricketer) .</cell></row><row><cell></cell><cell>1: John Ryder (8 August 1889 -4 April 1977) was an Australian cricketer.</cell></row><row><cell>VTM-noraw</cell><cell></cell></row></table><note>2: Jack Ryder (born August 21, 1951 in Melbourne, Victoria) was an Australian cricketer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>An example of the generated text by our model and the baselines on WIKI dataset. which implies that VTM can generate more diverse (lower Self-BLEU) under the commensurate BLEU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Table 7shows an example of sentences generated by different models. Although forward sampling enables the Table2seq model to generate diversely, it is more likely to generate incorrect and irrelevant content. For example, it generates the wrong club name in Sentence 3. By sampling from template space, VTM-noraw can generate texts with multiple templates, like different expressions for birth date and death date, while preserving readability. Furthermore, with extra raw data, VTM is able to generate more diverse expressions, which other models cannot pro-</figDesc><table /><note>duce, such as "[fullname], also known as [nickname] ([birth date] -[daeth date]) was a [country] [article name 4]." (Sentence 5). It implies that raw sentences not in the pairwise dataset could ad- ditionally enrich the information in template space.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>table. Neural language models have made distinguished progress by generating sentences from the table in an end-to-end style. Jain et al. (2018) proposed a mixed hierarchical attention model to generate weather report from the standard table. Gong et al. (2019) proposed a hierarchical table-encoder and a decoder with dual attention. Although encoder-decoder models can generate fluent sentences, they are criticized for deficiency in sentence diversity. Other works focused on controllable and interpretable generation by introducing templates as latent variables. Wiseman et al. (2018) designed a Semi-HMM decoder to learn discrete templates representation, and Dou et al. (2018) created a platform, Data2TextStudio, equipped with a Semi-HMMs model, to extract template and generate from table input in an interactive way.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table name [</head><label>name</label><figDesc>nameVariable], eatType[pub], food[French], priceRange[20-25], area[riverside]</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Proof can be found in Appendix C 3 https://nlds.soe.ucsc.edu/sentence-planning-NLG 4 http://www.macs.hw.ac.uk/InteractionLab/E2E/ 5 https://eaglew.github.io/patents/ 6 https://github.com/tuetschek/e2e-metrics</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for their insightful comments. Hao Zhou and Zhongyu Wei are the corresponding authors of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple domain-independent probabilistic approach to generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tableto-text: Describing table region with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating sentences from disentangled syntactic and semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the Association for Computational Linguistics</title>
		<meeting>the Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computational Natural Language Learning</title>
		<meeting>the Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using monolingual data in neural machine translation: a systematic study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Burlot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Machine Translation: Research Papers</title>
		<meeting>the Conference on Machine Translation: Research Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to generate one-sentence biographies from wikidata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data2text studio: Automated text generation from structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longxu</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Controlling linguistic style aspects in neural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Ficler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Stylistic Variation</title>
		<meeting>the Workshop on Stylistic Variation</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Table-to-text generation with effective hierarchical encoder on three dimensions (row, column and time)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Heng Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A mixed hierarchical attention based encoder-decoder approach for standard table summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parag</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Laha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preksha</forename><surname>Nema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shetty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Disentangled representation learning for non-parallel text style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hareesh</forename><surname>Bahuleyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the Association for Computational Linguistics</title>
		<meeting>the Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural text generation from structured data with application to the biography domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Table-to-text generation by structure-aware seq2seq learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Key fact as pivot: A two-stage model for low resource table-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Can neural generators for dialogue learn sentence planning and discourse structuring?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shereen</forename><surname>Oraby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Natural Language Generation</title>
		<meeting>the International Conference on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Style transfer from non-parallel text by cross-alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Describing a knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Natural Language Generation</title>
		<meeting>the International Conference on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Describing a knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Natural Language Generation</title>
		<meeting>the International Conference on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning neural templates for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A generalized mean field algorithm for variational inference in exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02262</idno>
		<title level="m">Infovae: Information maximizing variational autoencoders</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised discrete sentence representation learning for interpretable neural dialog generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyusong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-space variational encoder-decoders for semi-supervised labeled sequence transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reference nameVariable is a French place with a price range of 20-25</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<meeting>the International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Texygen: A benchmarking platform for text generation models. It is in riverside. It is a pub</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">It is a French restaurant in riverside. 2: nameVariable is a French restaurant in riverside with a price range of 20-25. nameVariable is a pub. 3: nameVariable is a pub with a price range of 20-25 and nameVariable is a French restaurant in riverside. 4: nameVariable is a pub with a price range of 20-25, also it is in riverside</title>
		<idno>Table2seq-sample 1</idno>
		<imprint>
			<biblScope unit="page" from="20" to="25" />
		</imprint>
	</monogr>
	<note>it is a Japanese place. 5: nameVariable is a pub with a average rating and it is a French place in riverside</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">1: nameVariable is in riverside, also it is in riverside. 2: nameVariable is a French restaurant. 3: nameVariable is the best restaurant. 4: nameVariable is in riverside</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Temp-Kn</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and nameVariable is in [location</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Its a French restaurant and it is in [location] with food and, even if nameVariable is</title>
		<imprint/>
	</monogr>
	<note>food qual], it is the best place</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">It is in riverside. 3: nameVariable is a French place in riverside with a price range of 20-25. It is a pub. 4: nameVariable is a French place in riverside with a price range of 20-25. It is a pub. 5: nameVariable is a French place in riverside with a price range of 20-25. It is a pub. VTM 1: nameVariable is a French place in riverside with a price range of 20-25</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vtm-Noraw</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="20" to="25" />
		</imprint>
	</monogr>
	<note>It is a French place in riverside. It is a pub. 2: nameVariable is a pub with a price range of 20-25. It is in riverside. It is a French place. 3: nameVariable is a French pub in riverside with a price range of 20-25, and it is a pub. 4: nameVariable is a French restaurant in riverside and it is a pub. 5: nameVariable is a French place in riverside with a price range of 20-25. It is a pub</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Table 8: An example of the generated text by our model and the baselines on SPNLG dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
