<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pose-Invariant Face Alignment with a Single CNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Jourabloo</surname></persName>
							<email>jourablo@msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
							<email>mao.ye2@us.bosch.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Visualization Group</orgName>
								<orgName type="institution">Bosch Research and Technology</orgName>
								<address>
									<addrLine>Center North America 1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Ren</surname></persName>
							<email>liu.ren@us.bosch.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Visualization Group</orgName>
								<orgName type="institution">Bosch Research and Technology</orgName>
								<address>
									<addrLine>Center North America 1</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pose-Invariant Face Alignment with a Single CNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face alignment has witnessed substantial progress in the last decade. One of the recent focuses has been aligning a dense 3D face shape to face images with large head poses. The dominant technology used is based on the cascade of regressors, e.g., CNN, which has shown promising results. Nonetheless, the cascade of CNNs suffers from several drawbacks, e.g., lack of end-to-end training, handcrafted features and slow training speed. To address these issues, we propose a new layer, named visualization layer, that can be integrated into the CNN architecture and enables joint optimization with different loss functions. Extensive evaluation of the proposed method on multiple datasets demonstrates state-of-the-art accuracy, while reducing the training time by more than half compared to the typical cascade of CNNs. In addition, we compare multiple CNN architectures with the visualization layer to further demonstrate the advantage of its utilization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face alignment, also known as face landmark detection, is an essential process for many facial analysis tasks, such as face recognition <ref type="bibr" target="#b35">[36]</ref>, expression estimation <ref type="bibr" target="#b0">[1]</ref> and 3D face reconstruction <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref>. During the last decade, face alignment technologies have been substantially improved <ref type="bibr">[8-10, 32, 41]</ref>. One recent advancement in this area is to tackle challenging cases with large face poses, e.g., frontal to profile views with ?90 ? yaw angles <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>The dominant technology for large-pose face alignment (LPFA) utilizes a cascade of regressors which combines different types of regression designs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref> with feature extraction methods <ref type="bibr" target="#b17">[18]</ref>. At each stage of this procedure, the target parameters, e.g., 2D landmarks or the head pose and 3D face shape, are refined by regressing an update of these parameters. Due to the proven power of Convolutional Neural Network (CNN) in vision tasks, it is also adopted as the regressor in this framework and has achieved the state- of-the-art performance on face alignment <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b49">50</ref>]. Despite the recent success, the cascade of CNNs, when applied to LPFA, suffers from the following drawbacks.</p><p>Lack of end-to-end training: It is a consensus that endto-end training is desired for CNN <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref>. However, one CNN regressor is trained independently at each cascade stage. Sometimes even multiple CNNs are applied independently at each stage. E.g., locations of different landmark sets are estimated by various CNNs and combined by a separate fusing module <ref type="bibr" target="#b32">[33]</ref>. Therefore, these CNNs can not be optimized jointly and might lead to a sub-optimal solution.</p><p>Hand-crafted feature extraction: Since the CNNs are trained independently, feature extraction is required to utilize the result of previous CNN and provide input to the current CNN. Simple feature extraction methods are used, e.g., extracting patches <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b44">45]</ref> based on 2D or 3D face shapes without considering other factors including pose and expression. Normally, the cascade of CNNs is a collection of shallow CNNs where each one has less than five layers. Hence, this framework can not extract deep features by building upon the extracted features of early-stage CNNs.</p><p>Slow training speed: Training a cascade of CNNs is usually time-consuming for two reasons. Firstly, the CNNs are trained sequentially, one after another. Secondly, feature extraction is required between two consecutive CNNs.</p><p>To address these issues, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we introduce a novel layer, named the visualization layer, into a CNN architecture, for the LPFA problem. Our CNN architecture consists of several blocks, which are called visualization blocks. This architecture can be considered as a cascade of shallow CNNs. The new layer visualizes the alignment result of the previous visualization block and utilizes it in the current block. It is designed based on several guidelines. Firstly, it is derived from the surface normals of the underlying 3D face model and encodes the relative pose between the face and camera. The use of surface normals is partially inspired by the success of adopting surface normals for 3D face recognition <ref type="bibr" target="#b24">[25]</ref>. Secondly, the visualization layer is differentiable, which allows the gradient to be computed analytically, enabling end-to-end training. Lastly, a mask is utilized to differentiate between pixels in the middle and contour parts of a face, and to also make the pixel values of the visualized images similar across various poses.</p><p>Benefiting from the design of the visualization layer, our method has the following advantages and contributions:</p><p>The proposed method allows a block in the CNN to utilize the extracted features from previous blocks and extract deeper features. Therefore, extraction of hand-crafted features is no longer necessary.</p><p>The visualization layer is differentiable, allowing for backpropagation of an error from a later block to an earlier one. To the best of our knowledge, this is the first method for large-pose face alignment, that utilizes only one single CNN and allows end-to-end training.</p><p>The proposed method converges faster during the training phase compared to the cascade of CNNs. Therefore, the training time is dramatically reduced.</p><p>The source code of the proposed method with the trained model are released at here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior Work</head><p>This section reviews the relevant prior work in three topics: cascade of regressors for face alignment, convolutional recurrent neural network and visualization in deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cascade of Regressors for Face Alignment Cascade of</head><p>Regressors is a classic approach in not only conventional face alignment <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b50">51]</ref>, but also the large-pose face alignment <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b48">49]</ref>. To handle large poses, many approaches go beyond 2D landmarks and also estimate 3D landmarks and 3D face shapes <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b49">50]</ref>. Zhu et al. <ref type="bibr" target="#b48">[49]</ref> use a set of local regressors to estimate the 2D shape update, and fuse their results with another regressor. The occlusioninvariant approach of RCPR <ref type="bibr" target="#b4">[5]</ref> is applicable to large poses since self-occlusion is one type of occlusions. An iterative probabilistic method is utilized in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref> for registering 3D shape to the pre-computed 2D landmarks. Tulyakov et al. <ref type="bibr" target="#b34">[35]</ref> also use a cascade of regressors to estimate 3D landmark updates directly from a single image. Some even use two regressors at each cascade stage. Wu et al. <ref type="bibr" target="#b38">[39]</ref> use one regressor to estimate the 2D shape update and the other to estimate the visibility of each landmark. Similarly, Liu et al. <ref type="bibr" target="#b22">[23]</ref> employ one regressor for 2D shape update and the other uses the 2D shape to estimate the 3D face shape.</p><p>Among methods with cascade of regressors, CNN is a popular choice of regressors due to its strong learning ability. These methods typically extract hand-crafted features between consecutive regressors. TCDCN <ref type="bibr" target="#b45">[46]</ref> use one CNN to estimate five landmarks, with yaw angles within ?60 ? . A cascade of stacked autoencoder (SAE) progressively estimates 2D landmark updates from extracted patches <ref type="bibr" target="#b44">[45]</ref>. Similarly, cascades of CNNs with global or local patches are combined at each stage, and their results are fused via averaging <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b47">48]</ref>. The methods in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b49">50]</ref> combine cascade of CNNs with 3D feature extraction to estimate the dense 3D face shape. All aforementioned methods lack the ability to end-to-end train the network, which is our novel contribution to large-pose face alignment. Convolutional Recurrent Neural Network (CRNN) The face alignment methods based on the CRNNs <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref> are the first attempts to combine cascade of regressors with joint optimization, for aligning mostly frontal faces. Their convolutional part extracts features from the whole image <ref type="bibr" target="#b39">[40]</ref> or from the patches at the landmark locations <ref type="bibr" target="#b33">[34]</ref>. The recurrent part facilitates the joint optimization by sharing information among all regressors. The main differences between the proposed method and CRNNs are: 1) existing CRNN methods are designed for near-frontal face alignment, while ours is for LPFA; 2) the CRNN methods share the same CNN at all stages, while our CNN of each block is different which might be more suitable for estimating the coarse-to-fine mappings during the course of alignment; 3) due to our new differentiable visualization layer, our method has one additional flow of the gradient backpropagation (note the two blue arrows between consectutive blocks in <ref type="figure">Fig. 2</ref>). Visualization in Deep Learning Visualization techniques have been used in deep learning to assist in making a relative comparison among the input data and focusing on the region of interest. These methods can be categorized in two groups. The first exploits the deconvolutional and upsampling layers to either expand response maps <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> or represent estimated parameters <ref type="bibr" target="#b42">[43]</ref>. Alternatively, various types of feature maps, e.g., heatmaps and Z-Buffering, can represent the current estimation of landmarks and parameters. In <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>, 2D landmark heatmaps represent the landmarks' locations. <ref type="bibr" target="#b3">[4]</ref> proposes a two step large pose alignment based on heatmaps to make more precise estimations. The heatmaps suffer from three drawbacks: 1) lack of the capability to represent objects in details; 2) requirement of one heatmap per landmark due to its weak representation power. 3) they cannot estimate visibility of landmarks.  <ref type="figure">Figure 2</ref>. The proposed CNN architecture. We use green, orange, and purple to represent the visualization layer, convolutional layer, and fully connected layer, respectively. Please refer to <ref type="figure" target="#fig_3">Fig. 3</ref> for the details of the visualization block.</p><p>The Z-Buffer rendered using the estimated 3D face is fed to the CNNs <ref type="bibr" target="#b49">[50]</ref> to convey the results of a previous CNN to the next one. However, the Z-Buffer representation is not differentiable, and hence does not allow end-to-end training. In contrast, our visualization layer is differentiable and encodes the face geometry details via surface normals. It guides the CNN to focus on the face area that incorporates both the pose and expression information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Given a single face image with an arbitrary pose, our goal is to estimate the 2D landmarks with their visibility labels by fitting a 3D face model. Towards this end, we propose a CNN architecture with end-to-end training for model fitting, as shown in <ref type="figure">Fig. 2</ref>. In this section, we will first describe the underlying 3D face model used in this work, followed by our CNN architecture and the visualization layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D and 2D Face Shapes</head><p>We use the 3D Morphable Model (3DMM) for representing the 3D shape of a face. 3DMM represents a 3D face S p as a linear combination of mean shape S 0 , identity bases S I and expression bases S E as follows:</p><formula xml:id="formula_0">S p = S 0 + N I k p I k S I k + N E k p E k S E k .<label>(1)</label></formula><p>We use vector p = [p I , p E ] to indicate the 3D shape parameters, where p I = [p I 0 , ? ? ? , p I N I ] are the identity parameters and p E = [p E 0 , ? ? ? , p E N E ] are the expression parameters. We use the Basel 3D face model <ref type="bibr" target="#b26">[27]</ref>, which has 199 bases, as our identity bases and the face wearhouse model <ref type="bibr" target="#b6">[7]</ref> with 29 bases as our expression bases. Each 3D face shape consists of a set of Q 3D vertexes:</p><formula xml:id="formula_1">S p = ? ? x p 1 x p 2 . . . x p Q y p 1 y p 2 . . . y p Q z p 1 z p 2 . . . z p Q ? ? .<label>(2)</label></formula><p>The 2D face shapes are the projection of 3D shapes. In this work, we use the weak perspective projection model with 6 degrees of freedoms, i.e., one for scale, three for rotation angles and two for translations, which projects the 3D face shape S p onto 2D images to obtain the 2D shape U:</p><formula xml:id="formula_2">U = f (P) = M S p (:, b) 1 ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">M = m 1 m 2 m 3 m 4 m 5 m 6 m 7 m 8 ,<label>(4)</label></formula><p>and</p><formula xml:id="formula_4">U = x t 1 x t 2 . . . x t N y t 1 y t 2 . . . y t N .<label>(5)</label></formula><p>Here U collects a set of N 2D landmarks, M is the camera projection matrix, with misuse of notation P = {M, p}, and the N -dim vector b includes 3D vertex indexes which are semantically corresponding to 2D landmarks. We denote</p><formula xml:id="formula_5">m 1 = [m 1 m 2 m 3 ] and m 2 = [m 5 m 6 m 7 ]</formula><p>as the first two rows of the scaled rotation component, while m 4 and m 8 are the translations. Eqn. 3 establishs the relationship, or equivalency, between 2D landmarks U and P, i.e., 3D shape parameters p and the camera projection matrix M. Given that almost all the training images for face alignment have only 2D labels, i.e., U, we preform a data augmentation step similar to <ref type="bibr" target="#b17">[18]</ref> to compute their corresponding P. Given an input image, our goal is to estimate the parameter P, based on which the 2D landmarks and their visibilities can be naturally derived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed CNN Architecture</head><p>Our CNN architecture resembles the cascade of CNNs, while each "shallow CNN" is defined as a visualization block. Inside each block, a visualization layer based on the latest parameter estimation serves as a bridge between consecutive blocks. This design enables us to address the drawbacks of typical cascade of regressors in Sec. 1. We now describe the visualization block and CNN architecture, and dive into the details of the visualization layer in Sec. 3.3.</p><p>Visualization Block <ref type="figure" target="#fig_3">Fig. 3</ref> shows the structure of our visualization block. The visualization layer generates a feature map based on the current estimated, or input, parameter P, and will be described in Sect. 3.3. Each convolutional layer is followed by a batch normalization (BN) layer and a ReLU layer, it extracts deeper features based on the input features provided by the previous visualization block and visualization layer output. Between the two fully connected layers, the first one is followed by a ReLU layer and a dropout layer, while the second one simultaneously estimates the update of M and p, ?P. The outputs of the visualization block are deeper features and the new estimation  of the parameters, when adding ?P to the input P. As in <ref type="figure" target="#fig_3">Fig. 3</ref>, basically the top part of the visualization block focuses on learning deeper features, while the bottom part utilizes such features to estimate the parameters in a ResNetlike structure <ref type="bibr" target="#b11">[12]</ref>. During the backward pass of the training phase, the visualization block backpropagates the loss through both of its inputs to adjust the convolutional and fully connected layers in the previous blocks. This allows the block to extract better features that are suitable for the next block and improve the overall parameter estimation. CNN Architecture The proposed CNN architecture consists of several connected visualization blocks as shown in <ref type="figure">Fig. 2</ref>. The inputs include the image and an initial estimation of the parameter P 0 ; and the output is the final estimation of the parameters. Compared to the typical cascade of CNNs, due to the joint optimization of all visualization blocks with backpropagation of the loss functions, the proposed architecture is able to converge in substantially fewer epochs during training.</p><p>Loss Functions Two types of loss functions are employed in our CNN architecture. The first one is an Euclidean loss between the estimation and the target of the parameter update, with each parameter weighted separately:</p><formula xml:id="formula_6">E i P = (?P i ? ?P i ) T W(?P i ? ?P i ),<label>(6)</label></formula><p>where E i P is the loss, ?P i is the estimation and ?P i is the target (or ground truth) at the i-th visualization block. The diagonal matrix W contains the weights. For each element of the shape parameter p, its weight is the inverse of the standard deviation that was obtained from the data used in 3DMM training. To compensate the relative scale among the parameters of M, we compute the ratio r between the average of scaled rotation parameters and average of translation parameters in the training data. We set the weights of the scaled rotation parameters of M to 1 r and the weights of the translation of M to 1. The second type of loss function is the Euclidean loss on the resultant 2D landmarks:</p><formula xml:id="formula_7">E i S = f (P i + ?P i ) ?? 2 ,<label>(7)</label></formula><p>where? is the ground truth 2D landmarks, and P i is the input parameter to the i-th block, i.e., the output of the i?1-th block. f (? ) computes 2D landmark locations using the currently updated parameters via Eqn. 3. For backpropagation of this loss function to the parameter ?P, we use the chain rule to compute the gradient (see supplemental material for the detailed derivation).</p><formula xml:id="formula_8">?E i S ??P i = ?E i S ?f ?f ?P i .</formula><p>For the first three visualization blocks, the Euclidean loss on the parameter updates (Eqn. 6) is used, while the Euclidean loss on 2D landmarks (Eqn. 7) is applied to the last three blocks. The first three blocks estimate parameters to align 3D shape to the face image roughly and the last three blocks leverage the good initialization to estimate the parameters and the 2D landmark locations more precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Visualization Layer</head><p>Several visualization techniques have been explored for facial analysis. In particular, Z-Buffering, which is widely used in prior works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, is a simple and fast 2D representation for the 3D shape. However, this representation is not differentiable. In contrast, our visualization is based on surface normals of the 3D face, which describes surface's orientation in a local neighbourhoods. It has been successfully utilized for different facial analysis tasks, e.g., 3D face reconstruction <ref type="bibr" target="#b29">[30]</ref> and 3D face recognition <ref type="bibr" target="#b24">[25]</ref>.</p><p>In this work, we use the z coordinate of surface normals of each vertex, transformed with the pose. It is an indicator of "frontability" of a vertex, i.e., the amount that the surface normal is pointing towards the camera. This quantity is used to assign an intensity value at its projected 2D location to construct the visualization image. The frontability measure g, a Q-dim vector, can be computated as,</p><formula xml:id="formula_9">g = max 0, (m 1 ? m 2 ) m 1 m 2 N 0 ,<label>(8)</label></formula><p>where ? is the cross product, and . denotes the L 2 norm. The 3 ? Q matrix N 0 is the surface normal vectors of a 3D face shape. To avoid the high computational cost of computing the surface normals after each shape update, we approximate N 0 as the surface normals of the mean 3D face. Note that both the face shape and pose are still continuously updated across various visualization blocks, and are used to determine the projected 2D location. Hence, this approximation would only slightly affect the intensity value. To transform the surface normal based on the pose, we apply the estimation of the scaled rotation matrix (m 1 and m 2 ) to the surface normals computed from the mean face. The value is then truncated with the lower bound of 0 (Eqn. 8).</p><p>The pixel intensity of a visualized image V(u, v) is computed as the weighted average of the frontability measures within a local neighbourhood:</p><formula xml:id="formula_10">V(u, v) = q?D(u,v) g(q)a(q)w(u, v, x t q , y t q ) q?D(u,v) w(u, v, x t q , y t q ) ,<label>(9)</label></formula><p>where D(u, v) is the set of indexes of vertexes whose 2D projected locations are within the local neighborhood of the pixel (u, v). (x t q , y t q ) is the 2D projected location of q-th 3D vertex. The weight w is the distance metric between the pixel (u, v) and the projected location (x t q , y t q ),</p><formula xml:id="formula_11">w(u, v, x t q , y t q ) = exp ? (u ? x t q ) 2 + (v ? y t q ) 2 2? 2 .</formula><p>(10) a is a Q-dim mask vector with positive values for vertexes in the middle area of the face and negative values for vertexes around the contour area of the face:</p><formula xml:id="formula_12">a(q) = exp ? (x n ? x p q ) 2 + (y n ? y p q ) 2 + (z n ? z p q ) 2 2? 2 n ,<label>(11)</label></formula><p>where (x n , y n , z n ) is the vertex coordinate of the nose tip. a is pre-computed and normalized for zero-mean and unit standard deviation. The mask is utilized to discriminate between the central and boundary areas of the face, as well as to increase similarity across visualization of different faces. A visualization of the mask is provided in <ref type="figure" target="#fig_4">Fig. 4</ref>.</p><p>Since the human face is a 3D object, visualizing it at an arbitrary view angle requires the estimation of the visibility of each 3D vertex. To avoid the computationally expensive visibility test via rendering, we adopt two strategies for approximation. Firstly, we prune the vertexes whose frontability measures g equal 0, i.e., the vertexes pointing against the camera. Secondly, if multiple vertexes projects to a same image pixel, we keep only the one with the smallest depth values. An example is illustrated in <ref type="figure">Fig. 5</ref>.</p><p>Backpropagation To allow backpropagation of the loss functions through the visualization layer, we compute the derivative of V with respect to the elements of the parameters M and p. Firstly, we compute the partial derivatives,</p><formula xml:id="formula_13">?g ?m k , ?w(u,v,x t i ,y t i ) ?m k and ?w(u,v,x t i ,y t i ) ?pj</formula><p>, then the derivatives of ?V ?m k and ?V ?pj can be computed based on Eqn. 9 (the details are provided in the supplemental material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We evaluate our proposed method on two challenging LPFA datasets, namely AFLW and AFW, both qualitatively and quantitatively, as well as the near-frontal face dataset of 300W. Further, we conduct experiments on different CNN architectures to validate our visualization layer design. Implementation details Our implementation is built upon the Caffe toolbox <ref type="bibr" target="#b15">[16]</ref>. In all of the experiments, we use Normal vectors with negative z</p><p>Normal vector with positive z and smaller depth <ref type="figure">Figure 5</ref>. The projections of four vertexes fall in the same image pixel. The surface normal vectors (red arrows) of two vertexes have positive z coordinates and the other two have negative z. Between the two vertexes with positive z, the one with the smaller depth (closer to the image plane) is used to fill the pixel. six visualization blocks (N v ) with two convolutional layers (N c ) and fully connected layers in each block <ref type="figure" target="#fig_3">(Fig. 3)</ref>. Details of the network structure are provided in Tab. 1. Instead of using the sequentially pretrain strategy <ref type="bibr" target="#b41">[42]</ref>, we perform the joint end-to-end training from scratch. To better estimate the parameter update in each block and to increase the effectiveness of using visualization block, we set the weight of the loss function in the first visualization block to 1, and linearly increase the weights by one for each block, i.e., the loss weight of the last block is 6. This strategy helps the CNN to pay more attention to the landmark loss used in later blocks. On the one hand, backpropagation of loss functions in the last blocks has more impact in the first block, and on the other hand the last block can adopt itself more quickly to the changes in the first block.</p><p>In the training phase, we set the weight decay to 0.005, the momentum to 0.99, the initial learning rate to 1e?6. Besides, we decrease the learning rate to 5e?6 and 1e?7 after 20 and 29 epochs. In total, the training phase is continued for 33 epochs for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Evaluations on AFLW and AFW</head><p>The AFLW dataset <ref type="bibr" target="#b20">[21]</ref> is a very challenging dataset with large-pose face images (?90 ? yaw). We use the subset of this dataset released by <ref type="bibr" target="#b17">[18]</ref>, which includes 3, 901 training images and 1, 299 testing images. All face images in this subset are labeled with 34 landmarks and a bounding box. The AFW dataset <ref type="bibr" target="#b50">[51]</ref> contains 205 images with 468 faces. Each face image is labeled with at most 6 landmarks with visibility labels, as well as a bounding box. AFW is used only for testing in our experiments. The bounding boxes in both datasets are used as initilization for our algorithm, as well as the baselines. We crop the face image inside the bounding box and normalize it to 114 ? 114. Due  to the memory constraint of GPUs, we have a pooling layer in the first visualization block after the first convolutional layer to decrease the size of feature maps to half, and the input to the subsequent visualization blocks is of 57 ? 57.</p><p>To augment the training data, we generate 20 different variations for each training image by adding noise to the location, width and height of the provided bounding boxes.</p><p>For quantitative evaluations, we use two conventional metrics. The first one is Mean Average Pixel Error (MAPE) <ref type="bibr" target="#b43">[44]</ref>, which is the average of the pixel errors for the visible landmarks. The other one is Normalized Mean Error (NME), i.e., the average of the normalized estimation error of visible landmarks. The normalization factor is the square root of the face bounding box size <ref type="bibr" target="#b16">[17]</ref>, instead of the eye-to-eye distance in the frontal-view face alignment.</p><p>We compare our method with several state-of-the-art methods in LPFA. For AFLW, we compare with LPFA <ref type="bibr" target="#b17">[18]</ref>, PIFA <ref type="bibr" target="#b16">[17]</ref> and RCPR <ref type="bibr" target="#b4">[5]</ref> with the NME metric. Tab. 2 shows that the proposed method achieved a higher accuracy than the baseline methods. Also, CALE <ref type="bibr" target="#b3">[4]</ref>, a heatmapbased 2D face alignment method, reports NME of 2.96% on the AFLW. We discuss about the advantage of the proposed method over heatmap-based methods in section 2. To demonstrate the capabilities of each visualization block, the NME computed using the estimated P after each block is shown in Tab. 3. If a higher alignment speed is desirable, it is possible to skip the last two visualization blocks with a reasonable NME.</p><p>On the AFW dataset, the comparisons are conducted with LPFA <ref type="bibr" target="#b17">[18]</ref>, PIFA <ref type="bibr" target="#b16">[17]</ref>, CDM <ref type="bibr" target="#b43">[44]</ref> and TSPM <ref type="bibr" target="#b50">[51]</ref> with the MAPE metric. The evaluations are provided in Tab. 4, which also shows the superiority of the proposed method.</p><p>Some examples of alignment results of the proposed method on AFLW and AFW datasets are shown in <ref type="figure">Fig. 9</ref>. Three examples of visualization layer output at each visualization block are shown in <ref type="figure" target="#fig_0">Fig. 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on 300W dataset</head><p>While our main goal is LPFA, we further evaluate on the most widely used near frontal 300W dataset <ref type="bibr" target="#b30">[31]</ref>. 300W containes 3, 148 training and 689 testing images, which are divide into common and challenging sets with 554 and 135 images, respectively. Tab. 5 shows the NME (normalized by the interocular distance) of the proposed and state-of-the-art methods. The most related method to ours is 3DDFA <ref type="bibr" target="#b49">[50]</ref>, which also estimates M and p. Our method outperforms it on both common and challenging sets. Other near frontal alignment methods do not employ shape constraints e.g., 3DMM which is an advantage for them. Because the span of the 3D shape bases cannot cover all possible locations of landmarks. To comapre with the MDM <ref type="bibr" target="#b33">[34]</ref>, we compute the failure rate with threshold of 0.08. The failure rates of our method are 16.83% (6.80% for MDM) and 8.99% (4.20% for MDM) with 68 and 51 landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis of the Visualization Layer</head><p>We perform four sets of experiments to study the properties of the visualization layer and network architectures.</p><p>Influence of visualization layers To analyze the influence of the visualization layer in the testing phase, we add 5% noise to the fully connected layer parameters of each visualization block, and compute the alignment error on the AFLW test set. The NMEs are <ref type="bibr">[4.46, 4.53, 4.60, 4.66, 4.80, 5.16</ref>] when each block is modified seperately. This analysis shows that visualized image has more influence on the later blocks, since imprecise parameters of early blocks could be compensated by later blocks. To evaluate the influence of the visualization layer in the training phase, we train the network without any visualization layer. The final NME on AFLW is 7.18% which shows the importance of visualization layers for guiding the network training. Advantage of deeper features We train three CNN architectures ( <ref type="figure" target="#fig_7">Fig. 6</ref>) on AFLW. The inputs of the visualization block in the first architecture are the input images I, feature maps F and the visualization image V. The inputs of the second and the third architectures are {F, V} and {I, V}, respectively. The NME of each architecture is shown in Tab. 6. While the first one performs the best, the substantial lower performance of the third one demonstrates the importance of deeper features learned across blocks.</p><p>At the first convolutional layer of each visualization block, we compute the average of the filter weights, across both the kernel size and number of maps. The averages for three types of input features are shown in <ref type="figure" target="#fig_8">Fig. 7</ref>. As can be observed, from the first to the sixth block, the weights continue to decrease, making a more precise estimation of small-scale parameter updates. Considering the number of      filters in Tab. 1, the total impact of feature maps are higher than the other two inputs in all blocks. This again shows the importance of deeper features in guiding the network to estimate parameters. Furthermore, the average of the visualization filter is higher than that of the input image filter, which validates the stronger influence of the proposed visualization during training. Advantage of using masks To show the advantage of using the mask in the visualization layer, we conduct an experiment with different masks. Specifically, we define another mask for comparison, which is shown in <ref type="figure" target="#fig_9">Fig. 8</ref>. It has five positive areas, i.e., the eyes, nose tip and two lip corners. The values are normalized to zero-mean and unit standard deviation. Compared to the original mask in <ref type="figure" target="#fig_4">Fig. 4</ref>, this mask is more complicated and conveys more information about the informative facial areas to the network. Moreover, to show the necessity of using the mask, we also test using visualization layers without any mask. The NMEs of the trained networks with different masks are shown in Tab. 7.</p><p>Comparing the first and third columns shows the advantage of using the mask in the network. The mask makes the pixel value of visualized images to be similar for faces with different poses and discriminate between the middle-area and contour-area of the face. By comparing the first and second columns, we can see that utilizing more complicated mask does not further improve the result, meaning the original mask provides sufficient information for its purpose. Different numbers of blocks and layers Given the total  number of 12 convolutional layers in our network, we can partition them to visualization blocks in various sizes. To compare their performance, we train two additional CNNs, one with 4 visualization blocks and each with 3 convolutional layers; and the other with 3 block and 4 convolutional layers per block, where all three architectures have 12 total convolutional layers. The NME of these architectures are shown in Tab. 8. It shows the same conclusion as in <ref type="bibr" target="#b4">[5]</ref> that the number of regressors is important for face alignment and we can potentially achieve a higher accuracy by increasing the number of visualization blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Time complexity</head><p>Compared to the cascade of CNNs, one of the main advantages of end-to-end training a single CNN is the reduced training time. The training of the proposed method needs 33 epochs and takes around 2.5 days. The state of the art <ref type="bibr" target="#b17">[18]</ref>, that uses the same train and test sets as ours, trains six CNNs and each needs 70 epochs. The total time of <ref type="bibr" target="#b17">[18]</ref> is around 7 days. Similarly, the method in <ref type="bibr" target="#b49">[50]</ref> needs around 12 days to train three CNNs each one with 20 epochs, despite using different training data. Compared to <ref type="bibr" target="#b17">[18]</ref>, the proposed method reduces the training time by more than half. The testing speed of proposed method is 4.3 FPS on a Titan X GPU. It is much faster than the 0.6 FPS speed of <ref type="bibr" target="#b17">[18]</ref> and is simalar to 4 FPS speed of <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We propose a large-pose face alignment method with end-to-end training in a single CNN. We present a differentiable visualization layer, which is integrated to the network and enables joint optimization by backpropagating the error from a later visualization blocks to early ones. It allows the visualization block to utilize the extracted features from previous blocks and extract deeper features, without extracting hand-crafted features. Also, the proposed method converges faster during the training phase compare to the cascade of CNNs. Finally, we demonstrate the superior results of the proposed method over the state-of-the-art methods. <ref type="figure">Figure 9</ref>. Results of alignment on AFLW and AFW datasets, green landmarks show the estimated locations of visible landmarks and red landmarks show estimated locations of invisible landmarks. First row: provided bounding box by AFLW with initial locations of landmarks, Second: estimated 3D dense shapes, Third: estimated landmarks, Fourth to sixth: estimated landmarks for AFLW, Seventh: estimated landmarks for AFW. <ref type="figure" target="#fig_0">Figure 10</ref>. Three examples of outputs of visualization layer at each visualization block. The first row shows that the proposed method recovers the expression of the face gracefully, the third row shows the visualizations of a face with a more challenging pose.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>For the purpose of learning an end-to-end face alignment model, our novel visualization layer reconstructs the 3D face shape (a) from the estimated parameters inside the CNN and synthesizes a 2D image (b) via the surface normal vectors of visible vertexes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>A visualization block consists of a visualization layer, two convolutional layers and two fully connected layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The frontal and side views of the mask a that has positive values in the middle and negative values in the contour area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>NME</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Architectures of three CNNs with different inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>The average of filter weights for input image, visualization and feature maps in three architectures ofFig. 6. The y-axis and x-axis shows the average and the block index, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Mask 2, a different designed mask with five positive areas on the eyes, top of the nose and sides of the lip.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Number and size of convolutional filters in each visualization block. For all blocks, the two fully connected layers have the same length of 800 and 236.</figDesc><table><row><cell>Block #</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5, 6</cell></row><row><cell>Conv.</cell><cell cols="5">12 (5?5) 20 (3?3) 28 (3?3) 36 (3?3) 40 (3?3)</cell></row><row><cell>layers</cell><cell cols="5">16 (5?5) 24 (3?3) 32 (3?3) 40 (3?3) 40 (3?3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>NME (%) of four methods on AFLW dataset.</figDesc><table><row><cell cols="4">Proposed method LPFA [18] PIFA RCPR</cell></row><row><cell>4.45</cell><cell>4.72</cell><cell>8.04</cell><cell>6.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>NME (%) of the proposed method at each visualization block on AFLW dataset. The initial NME is 25.8%.</figDesc><table><row><cell>Block #</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell>NME</cell><cell cols="6">9.26 6.77 5.51 4.98 4.60 4.45</cell></row><row><cell cols="7">Table 4. MAPE of five methods on AFW dataset.</cell></row><row><cell cols="7">Proposed method LPFA [18] PIFA CDM TSPM</cell></row><row><cell>6.27</cell><cell></cell><cell>7.43</cell><cell></cell><cell>8.61</cell><cell>9.13</cell><cell>11.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>The NME of different methods on 300W dataset.</figDesc><table><row><cell>Method</cell><cell cols="3">Common Challenging Full</cell></row><row><cell>ESR [8]</cell><cell>5.28</cell><cell>17.00</cell><cell>7.58</cell></row><row><cell>RCPR [5]</cell><cell>6.18</cell><cell>17.26</cell><cell>8.35</cell></row><row><cell>SDM [41]</cell><cell>5.57</cell><cell>15.40</cell><cell>7.50</cell></row><row><cell>LBF [29]</cell><cell>4.95</cell><cell>11.98</cell><cell>6.32</cell></row><row><cell>CFSS [50]</cell><cell>4.73</cell><cell>9.98</cell><cell>5.76</cell></row><row><cell>RCFA [37]</cell><cell>4.03</cell><cell>9.85</cell><cell>5.32</cell></row><row><cell>RAR [40]</cell><cell>4.12</cell><cell>8.35</cell><cell>4.94</cell></row><row><cell>3DDFA [50]</cell><cell>6.15</cell><cell>10.59</cell><cell>7.01</cell></row><row><cell>3DDFA+SDM</cell><cell>5.53</cell><cell>9.56</cell><cell>6.31</cell></row><row><cell>Proposed method</cell><cell>5.43</cell><cell>9.88</cell><cell>6.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell cols="3">The NME (%) of three architectures with different inputs</cell></row><row><cell cols="3">(I: Input image, V: Visualization, F: Feature maps).</cell></row><row><cell cols="3">Architecture a Architecture b Architecture c</cell></row><row><cell>(I, F, V)</cell><cell>(F, V)</cell><cell>(I, V)</cell></row><row><cell>4.45</cell><cell>4.48</cell><cell>5.06</cell></row><row><cell>Input image filters</cell><cell>Visualization filters</cell><cell>Feature maps filters</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>NME (%) of utilizing different masks.</figDesc><table><row><cell cols="3">Mask 1 Mask 2 No Mask</cell></row><row><cell>4.45</cell><cell>4.49</cell><cell>5.31</cell></row><row><cell cols="3">Table 8. NME (%) of utilizing different numbers of visualization</cell></row><row><cell cols="3">blocks (Nv) and convolutional layers (Nc).</cell></row><row><cell cols="3">Nv = 6 , Nc = 2 Nv = 4 , Nc = 3 Nv = 3 , Nc = 4</cell></row><row><cell>4.45</cell><cell>4.61</cell><cell>4.83</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Face expression recognition and analysis: the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bettadapura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1203.6722</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional aggregation of local evidence for large pose face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Region-based semantic segmentation with end-to-end training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="381" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3D facial expression database for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boosted regression active shape models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D alignment of face in a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1305" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Regressive tree structured model for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3855" to="3861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dense 3D face alignment from 2D videos in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pose-invariant 3D face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-pose face alignment via cnnbased dense 3D model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pose-invariant face alignment via cnn-based dense 3D model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Face reconstruction in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1746" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, realworld database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?stinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Face detection with end-to-end integration of a convnet and a 3D model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint face alignment and 3D face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="545" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment with a deformable hough transform model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="569" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Iterative closest normal point for 3D face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mohammadzade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hatzinakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A 3D face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A recurrent encoder-decoder network for sequential face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="38" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive 3D face reconstruction from unconstrained photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Face alignment through subspace constrained mean-shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1034" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Regressing a 3D face shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3748" to="3755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Toward a practical face recognition system: Robust alignment and illumination by sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="372" to="386" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recurrent convolutional face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Single image 3D interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection under significant head poses and occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3658" to="3666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via recurrent attentiverefinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hd-cnn: hierarchical deep convolutional neural networks for large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2740" to="2748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Weaklysupervised disentangling with recurrent transformations for 3D view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1099" to="1107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Posefree facial landmark fitting via optimized part mixtures and cascaded deformable shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Coarse-to-Fine Auto-encoder Networks (cfan) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast and precise face alignment and 3D shape reconstruction from a single 2D image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="590" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Extensive facial landmark localization with coarse-to-fine convolutional network cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="386" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unconstrained face alignment via cascaded compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3D solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
