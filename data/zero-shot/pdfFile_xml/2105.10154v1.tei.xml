<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ViPNAS: Efficient Video Pose Estimation via Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lumin</forename><surname>Xu</surname></persName>
							<email>luminxu@link.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingda</forename><surname>Guan</surname></persName>
							<email>guanyingda@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
							<email>jinsheng@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
							<email>liuwentao@sensetime.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">SenseTime Research and Tetras.AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
							<email>qianchen@sensetime.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">SenseTime Research and Tetras.AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@cs.hku.hkwanli.ouyang@sydney.edu.auxgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ViPNAS: Efficient Video Pose Estimation via Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human pose estimation has achieved significant progress in recent years. However, most of the recent methods focus on improving accuracy using complicated models and ignoring real-time efficiency. To achieve a better trade-off between accuracy and efficiency, we propose a novel neural architecture search (NAS) method, termed ViP-NAS, to search networks in both spatial and temporal levels for fast online video pose estimation. In the spatial level, we carefully design the search space with five different dimensions including network depth, width, kernel size, group number, and attentions. In the temporal level, we search from a series of temporal feature fusions to optimize the total accuracy and speed across multiple video frames. To the best of our knowledge, we are the first to search for the temporal feature fusion and automatic computation allocation in videos. Extensive experiments demonstrate the effectiveness of our approach on the challenging COCO2017 and PoseTrack2018 datasets. Our discovered model family, S-ViPNAS and T-ViPNAS, achieve significantly higher inference speed (CPU real-time) without sacrificing the accuracy compared to the previous state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation has made impressive progress in recent years with the development of stronger neural networks. Most state-of-the-art models <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b51">49,</ref><ref type="bibr" target="#b58">56]</ref> only focus on improving the accuracy, but ignore the computational complexity and real-time performance. However, both accuracy and efficiency are critical for real-world applications of video pose estimation. In this paper, we aim to build a lightweight pose estimator that achieves state-of-the-art performance with significant model complexity reduction.</p><p>For video pose estimation, there is commonly consider- <ref type="figure">Figure 1</ref>. Speed-accuracy trade-off on PoseTrack2018 <ref type="bibr" target="#b0">[1]</ref> validation set. Methods involve SBL <ref type="bibr" target="#b58">[56]</ref>, LightTrack <ref type="bibr" target="#b39">[38]</ref> and our ViPNAS with various backbones. With accuracy comparable to state-of-the-art networks, ViPNAS achieves CPU real-time with significantly lower computation.</p><p>able temporal redundancy that leads to superfluous computation, i.e. adjacent frames in a video share similar global context information. The temporal contextual information can be used for improving pose estimation. Therefore, it is critical to fuse features from adjacent frames to the current frame in order to effectively utilize the temporal contextual information for balancing accuracy and efficiency. However, there are still several open questions: 1. Low-level local features are important for accurate localization, while higher-level global features are robust to occlusion and large pose variations. Which stage of features should be fused?</p><p>2. For temporal feature fusion, various fusion operations (e.g. addition, multiplication, or concatenation) are chosen by trial-and-error. How to choose the optimal operation? 3. The goal is to optimize the total accuracy subject to the total computation complexity (Flops) constraints over the whole video. Previous works generally explicitly enforce different frames to apply the same model, which will result in sub-optimal performance. How to efficiently allocate computation across different video frames?</p><p>Manually exploring the design choices regarding the above questions via trial-and-error can be tedious. We instead apply neural architecture search (NAS) to give a unified solution to them. We propose a novel spatial-temporal NAS framework for efficient video pose estimation, termed ViPNAS. For spatial-level search, we optimize the neural architecture by a wide spectrum of five dimensions (depth, width, kernel size, group number, and attention). For temporal-level search, we jointly search three aspects of designs: 1) the stage of features to be fused, 2) the feature fusion operation, and 3) the allocation of computation across video frames. The spatial-level and temporal-level search are jointly optimized through a single framework. Given the total Flops over multiple frames as constraints, we can efficiently allocate computation across different video frames for optimizing performance. Experiments show that ViP-NAS significantly improves over the state-of-the-art methods, such as SBL <ref type="bibr" target="#b58">[56]</ref> and LightTrack <ref type="bibr" target="#b39">[38]</ref>, with various well-known backbones (ResNet <ref type="bibr" target="#b12">[13]</ref>, CPN <ref type="bibr" target="#b5">[6]</ref>, Mo-bileNets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>, ShuffleNet <ref type="bibr" target="#b36">[35]</ref> and EfficientNet <ref type="bibr" target="#b53">[51]</ref>).</p><p>Our main contributions can be summarized as follows:</p><p>? We propose the novel spatial-temporal neural architecture search (NAS) framework for efficient video pose estimation, termed ViPNAS.</p><p>? ViPNAS learns to allocate computational resources (e.g. Flops) for different frames under the total computation complexity constraints across frames.</p><p>? ViPNAS automatically searches temporal connections, i.e. the fusion module and positions. In the task of video pose estimation, we achieve the state-of-the-art accuracy with CPU real-time performance (&gt; 25 FPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Human Pose Estimation</head><p>Recent works in human pose estimation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b51">49,</ref><ref type="bibr" target="#b56">54,</ref><ref type="bibr" target="#b58">56]</ref> focus on designing stronger neural network architectures with higher model capacity to improve accuracy. To better capture the context information, the attention mechanism has been successfully applied in human pose estimation. For example, Chu et al. <ref type="bibr" target="#b7">[8]</ref> proposes multi-context attention to improve model robustness and accuracy. Su et al. <ref type="bibr" target="#b50">[48]</ref> proposes SCARB module to enhance pyramid features via spatial and channel-wise context. Other popular attention modules have also been widely explored. For example, Squeeze-and-Excitation (SE) block <ref type="bibr" target="#b15">[16]</ref> models channel-wise relationship and Global Context (GC) block <ref type="bibr">[4]</ref> models the global context via addition fusion as NLNet <ref type="bibr" target="#b55">[53]</ref>. Different from manually design in these works, we propose to apply NAS to automatically search for optimal architectures.</p><p>For online video pose estimation, some works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b51">49,</ref><ref type="bibr" target="#b58">56,</ref><ref type="bibr" target="#b59">57,</ref><ref type="bibr" target="#b60">58]</ref> directly apply the image-based pose models on each video frame. However, such approaches do not capture the temporal consistency, suffering from motion blur or occlusion. Other works utilize temporal cues in order to keep geometric consistency across frames. Such approaches include directly processing concatenated consecutive frames along the channel-axis <ref type="bibr" target="#b44">[42]</ref>, applying 3D temporal convolution <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b54">52]</ref>, using dense optical flow to produce smooth movement <ref type="bibr" target="#b42">[41,</ref><ref type="bibr" target="#b49">47]</ref>. These models are typically computationally expensive, making them not applicable in real-time applications. Recently, some works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b38">37]</ref> follow the pose propagation paradigm, that transfer features from previous frames to the current frame in an online fashion. However, how to choose the temporal feature fusion sites and the fusion operations are still open questions. We aim to answer this by applying the ViPNAS framework to explore the most effective combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Neural Architecture Search</head><p>NAS for image-level tasks. Neural architecture search (NAS) focuses on automating the neural network architecture design. Early NAS approaches <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b52">50,</ref><ref type="bibr" target="#b66">64,</ref><ref type="bibr" target="#b67">65]</ref> sample a large number of architectures and trained them from scratch, which are very time consuming. Recent NAS approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b57">55,</ref><ref type="bibr" target="#b63">61,</ref><ref type="bibr" target="#b65">63]</ref> adopt a weight sharing strategy and train the super-network. Our method also follows this paradigm that trains the supernetwork only once, and evaluates various sub-networks.</p><p>NAS for video-level tasks. NAS has been applied in video-level tasks, such as video recognition <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b45">43,</ref><ref type="bibr" target="#b46">44,</ref><ref type="bibr" target="#b48">46]</ref>. EVANet <ref type="bibr" target="#b46">[44]</ref> searches for sequential or parallel model configurations via evolutionary algorithm. AssembleNet <ref type="bibr" target="#b48">[46]</ref> searches for multi-stream (RGB and optical flow) network connectivity. TinyVideoNet <ref type="bibr" target="#b45">[43]</ref> searches for computationally efficient classification model for video recognition.</p><p>NAS for single-image pose estimation. PoseNFS <ref type="bibr" target="#b61">[59]</ref> introduces the prior structure of the human body and searches for multiple personalized modules for part-based representations. AutoPose <ref type="bibr" target="#b11">[12]</ref> proposes a bi-level optimization method that combines reinforcement learning and gradient-based method.</p><p>Our work is different from existing works on NAS in three aspects. First, we are the first to apply NAS for a challenging task of video pose estimation. Second, existing works for image-level and video-level tasks do not search for different architectures at once, but our work search frame-specialized models for further leveraging the ability of NAS in video pose estimation. Third, we propose the novel spatial and temporal search space for the task.</p><p>To fully exploit the temporal information, we search for the optimal combination when fusing features from previous frames to the current frame, which was not explored in these works. Our method also inherits the merit of oncefor-all <ref type="bibr" target="#b1">[2]</ref>, i.e. training only once and obtaining many subnetworks, which effectively reduces the searching cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Video pose estimation aims to localize the human body parts (also referred to as keypoints or joints) of a person instance in each frame.</p><p>In this paper, based on the online pose propagation paradigm, we propose a novel NAS framework for efficient video pose estimation (ViPNAS). The pipeline of ViPNAS is shown in <ref type="figure">Figure 2</ref>. The first frame is selected as the key frame for every T + 1 frames in the video. For a key frame, a high-accuracy spatial video pose estimation network (S-ViPNet) is applied to localize human poses. We follow the common settings to use heatmaps to encode the joint locations as Gaussian peaks. For a non-key frame, a lightweight temporal video pose estimation network (T-ViPNet) is used for pose propagation. In T-ViPNet, some CNN layers are used for extracting the features of the current frame, then a temporal feature fusion module fuses the features of the current frame and the heatmaps of the last frame. The fused features are then processed by the remaining CNN layers of the T-ViPNet to obtain the heatmaps. The predicted heatmaps encode the per-pixel likelihood of each joint, which are informative cues to guide the keypoint localization in the subsequent frames. The propagation continues until the next key frame.</p><p>ViPNAS contains two levels of search space, i.e. the spatial-level and the temporal-level. The architecture of the key frame (S-ViPNet) is searched in the spatial-level search space. The architectures of the non-key frames (T-ViPNets), including the temporal feature fusion module and CNN layers, are searched in both spatial-level and temporal-level search space. Different non-key frames have different architectures in both the feature fusion module (fusion operation and feature fusion stage) and CNN layers, as shown by the example for frames t + 1 and t + 2 in <ref type="figure">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial-level Search Space</head><p>Motivated by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b63">61]</ref>, we design the weight shared supernetwork for model architecture search and search for the block number and block structure. Our architecture search spaces extend <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b63">61]</ref> to include group and attention for a wider spectrum of five dimensions (depth, width, kernel size, group, and attention). We find out the best configuration of these settings. Our super-network is divided into several stages in series and each stage consists of several blocks having the same spatial resolution of output features. We search on five dimensions as follows:</p><p>Elastic Depth: The number of blocks for each stage. We activate the first D blocks of a stage when the depth D is selected for this stage. Elastic Width: The number of output channels in each block. We keep the first W filters when the width W is selected. Elastic Kernel Size: The kernel size of convolutional layers in each block. We reserve the centering K ? K convolutional kernel when the kernel size K is selected. The possible choices of kernel size K are {3, 5, 7} for normal convolutional layers and {2, 4} for deconvolutional layers. Elastic Group Number: The group number of convolutional layers <ref type="bibr" target="#b22">[22]</ref> in each block. It ranges from 1 (standard convolution) to N (depth-wise convolution) for N input channels. Elastic Attention Module: Using the attention module or not at the end of each block. Since attention modules are shown to be effective for pose estimation in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b50">48]</ref>, we include attention modules in our search space. We investigate whether to use the attention module (e.g. GC block <ref type="bibr">[4]</ref> or SE Block <ref type="bibr" target="#b15">[16]</ref>) at the end of each block. If the attention module is not selected, we skip the attention module and identity mapping is applied. Please refer to Sec. A.1 for more details about the spatiallevel search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal-level Search Space</head><p>Lightweight pose models alone have difficulty in capturing the global information and distinguishing the joints with similar appearance. However, considering that poses in adjacent video frames are temporally correlated, lightweight models can estimate the joint locations with the local appearance and the guidance from previous frames.</p><p>Temporal feature fusion is critical to the task of video pose estimation, which has also been explored in literature <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b38">37]</ref>. Previous works on temporal fusion mainly differ in two main aspects, i.e. the fusion operations and the feature fusion stages. Popular fusion operations may include addition (Add), multiplication (Mul), and concatenation (Cat), etc. As different pose networks prefer different fusion operations, the choice of the fusion operation is carefully hand-crafted. Besides, different stages of the input features are fused in different approaches. Generally, low-level features may contain more detailed localization information, while higher-level features may contain more global information. In previous works, the levels of features used are mainly chosen by trial-and-error. In ViPNAS, we instead allow the networks to automatically search for the optimal fusion operation and the best stage of features to fuse in a single run of the search.</p><p>As shown in <ref type="figure">Figure 2</ref>, our designed temporal feature fusion module includes two inputs, i.e. heatmaps of the previous frame and features of the current frame t + 1. The temporal feature fusion module first selects the location of . The heatmaps and features are then fused by the selected fusion operator (Cat), which are then processed by one 1 ? 1 convolution, making the fused features (F t+1 2 ) have the same shape as the input features. Our temporal-level search space for a non-key frame includes N O choices for the feature fusion operations, e.g. addition (Add), multiplication (Mul), and concatenation (Cat); and N S choices for the input feature stages, e.g.</p><formula xml:id="formula_0">F 1 , F 2 , F 3 , and F 4 . The size of temporal search space is (N O ? N S ) T ,</formula><p>which is impossible to optimize by trial-and-error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Train and Search for ViPNAS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Train and Search for S-ViPNet</head><p>Based on the spatial-level search space defined in Section 3.2, we use the approach in <ref type="bibr" target="#b63">[61]</ref> to train the supernetwork. Sandwich rule <ref type="bibr" target="#b62">[60,</ref><ref type="bibr" target="#b63">61]</ref> and in-place distillation <ref type="bibr" target="#b62">[60,</ref><ref type="bibr" target="#b63">61]</ref> are applied. Then we sample the sub-networks under the given constraint and evaluate each of them on the validation set to search the architecture of S-ViPNet, which is the network for the key frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Training for T-ViPNet</head><p>In this section, we introduce the multi-frame propagation training scheme of our ViPNAS. The goal is to optimize the overall model accuracy at spatial and temporal levels simultaneously in the process of poses propagation across multiple video frames. The overall objective function can be formulated as follows:</p><formula xml:id="formula_1">min ? T T t=1 arch t L(T (I t , H t?1 ; {? T , arch t })),<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">H t = T (I t , H t?1 ; {? T , arch t }), t ? 1, S(I t ; {? S }), t = 0.<label>(2)</label></formula><p>S is the key frame model S-ViPNet, whose weights are denoted by ? S . It is pre-trained and fixed when training and searching for T-ViPNets. T is the super-network of T-ViPNets, which is parameterized by ? T . During training, we sample sub-network consisting of architecture arch t from T and the weights of this architecture copied from the super-network weights ? T . For each frame t, I t is the input image, and H t is the predicted heatmaps. We use MSE loss function L to measure the difference between the target heatmaps and the predicted ones of each non-key frame.</p><p>The multi-frame pose propagation training of T-ViPNet is shown in <ref type="figure" target="#fig_2">Figure 3</ref>, where the heatmaps of the key frame are propagated to T (T ? 2) non-key frames iteratively. We apply a single super-network T for all the non-key frames and all T-ViPNets share the weights, which saves memory during training. Moreover, with one-time training of the super-network, we can search for multiple sets of T-ViPNets with various numbers of propagation frames, see <ref type="table">Table.</ref> 4. We make the super-network T to share the same CNN architecture as the discovered S-ViPNet. First, since the tasks of image-based and video-based pose estimation are highly correlated, the good-performing image-based pose estimator can serve as a good candidate architecture for video pose estimation. Second, the pre-trained weights of S-ViPNet can be reloaded for the initialization of the super-network. Third, by sharing similar architectures, the features for the key frame and non-key frames are better aligned.</p><p>We jointly train the temporal models (T-ViPNets) in the spatial-level and temporal-level search space for the global optimum. We apply the Sandwich rule <ref type="bibr" target="#b62">[60,</ref><ref type="bibr" target="#b63">61]</ref> to sample the smallest sub-network, the biggest sub-network and N randomly sampled sub-networks (N = 2 in our experiments) for each mini-batch. We train and search for the CNN architectures and temporal fusion module (including fusion operations and fusion stages) simultaneously. For the biggest (or smallest) sub-network, T-ViPNets of all the frames use the biggest (or smallest) CNN architectures, while the temporal-level search spaces are randomly sampled. For N randomly sampled sub-networks, each T-ViPNet samples unique architecture at both spatial and temporal search spaces. Inplace knowledge distillation <ref type="bibr" target="#b62">[60,</ref><ref type="bibr" target="#b63">61]</ref> takes the prediction of biggest sub-network as the soft labels to enhance supervision for other sub-networks. The biggest sub-network is supervised by ground truth heatmaps with MSE loss, while others are supervised by both the soft labels and the ground truth heatmaps with equal loss weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Automatic Computation Allocation</head><p>As stated above, the sub-networks (T-ViPNets) of different frames do not necessarily share the same architecture. In ViPNAS, different model complexities are assigned to different frames automatically.</p><p>Formally, we aim to search for a group of sub-network architectures ({arch t } t=1:T ) that optimize the overall Average Precision (AP) under the overall computation complexity (Flops) constraints C:</p><formula xml:id="formula_3">max arch 1:T T t=1 AP(T (I t , H t?1 ; {? T , arch t })) s.t. T t=1 Flops(arch t ) ? C<label>(3)</label></formula><p>In the search process, we simply follow <ref type="bibr" target="#b63">[61]</ref> to randomly sample sub-networks that fulfill the given constraints and evaluate the accuracy on the validation set. The sampled sub-networks with the highest AP on the validation set under the Flops constraint are used as the T-ViPNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>COCO2017 Dataset <ref type="bibr" target="#b28">[28]</ref> is a standard benchmark for human pose estimation. It contains over 200,000 images and 250,000 person instances. We train the models on the COCO train2017 dataset (57K images), and evaluate them on the val2017 set (5K images) and test-dev2017 set (20K images) using the official evaluation metric 1 : Average Precision (AP) and Average Recall (AR), which are based on the standard object keypoints similarity (OKS).</p><formula xml:id="formula_4">OKS = i exp(?d 2 i /2s 2 k 2 i )?(vi&gt;0) i ?(vi&gt;0)</formula><p>, where d i is the Euclidean distance between each ground-truth and the detected keypoint, v i is the visibility flag, s is the scale of person, and k i is a constant to control falloff.</p><p>PoseTrack2018 Dataset <ref type="bibr" target="#b0">[1]</ref> is a large-scale dataset for human pose estimation in videos. It contains various videos of human activities with 6 person instances per frame on average. We use PoseTrack2018 V0.25 annotation, which includes 593 training videos, 74 validation videos and 375 testing videos. We follow the common settings <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b51">49,</ref><ref type="bibr" target="#b58">56]</ref> to pre-train models on COCO train2017 dataset and finetune them on PoseTrack2018 training set. The evaluation follows <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b38">37]</ref> for video pose estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b64">62]</ref> that estimates human poses given ground-truth bounding boxes. Pose estimation accuracy is evaluated using the standard AP metric 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We train and search our single-frame pose estimator, termed S-ViPNAS, on COCO dataset. For training, we resize the cropped person image to 256 ? 192, and apply random rotation ([?40 ? , 40 ? ]) and random flip as data augmentation. We train the super-network with inplace knowledge distillation for 250 epochs. Weights are initialized from zero-mean Gaussian distribution with ? = 0.001. The basic learning rate is 1e-3, and is reduced by a factor of 10 at the 200th and 230th epoch. We sample 500 models under the Flops constraints and search for S-ViPNAS with the highest AP on the validation set.</p><p>We directly transfer the discovered architecture (S-ViPNAS) on the COCO dataset to the PoseTrack dataset. We fine-tune S-ViPNAS on PoseTrack dataset for 20 epochs. The basic learning rate is 1e-4, and drops to 1e-5 at 10 epochs then 1e-6 at 15 epochs. We use S-ViPNAS as the key frame pose estimator and the super-network for temporal propagation models (T-ViPNAS). During multi-frame super-network training, the same augmentation methods are applied across T +1 frames (T = 3 by default). We train the super-network using the sandwich rule for 60 epochs with initial learning rate 1e-3 and cosine learning rate schedule. The search cost is 16 GPU days for training and 2GPU days for search on V100 GPUs. <ref type="table">Table 1</ref> compares our proposed ViPNAS with the stateof-the-art methods on PoseTrack2018 <ref type="bibr" target="#b0">[1]</ref> validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">ViPNAS for efficient video pose estimation</head><p>SBL <ref type="bibr" target="#b58">[56]</ref> proposes to add deconvolutional layers to the backbone network, which has been proved effective. We extend <ref type="bibr" target="#b58">[56]</ref> to include more well-known efficient backbones for comparisons, such as EfficientNet <ref type="bibr" target="#b53">[51]</ref>, ShuffleNet <ref type="bibr" target="#b36">[35]</ref>, and MobileNet <ref type="bibr" target="#b13">[14]</ref>. These models are pre-trained on COCO dataset and fine-tuned on PoseTrack dataset with the same experimental configurations as <ref type="bibr" target="#b58">[56]</ref>. LightTrack <ref type="bibr" target="#b39">[38]</ref> is a recently proposed light-weight framework for video pose estimation. The results are obtained using the official codes <ref type="bibr" target="#b2">3</ref> with the released pre-trained models.</p><p>We evaluate our methods on two well-known backbones, i.e. ResNet-50 <ref type="bibr" target="#b12">[13]</ref> and MobileNet-V3 <ref type="bibr" target="#b13">[14]</ref>. For both backbone models, we build the super-network based on the spatial-level search space (Sec. 3.2) and temporal-level search space (Sec. 3.3). Please refer to Sec. A.2 for more details. SBL, LightTrack, and S-ViPNAS directly apply the image-based pose models on each video frame, while T-ViPNAS searches for temporal feature fusion for more efficient pose estimation. #Param and Flops are calculated by averaging over the whole video frames including both key frames and non-key frames. From <ref type="table">Table 1</ref>, we see that ViPNAS achieves the state-of-the-art accuracy with signif-  <ref type="figure" target="#fig_3">Figure 4</ref> compares SBL <ref type="bibr" target="#b58">[56]</ref>, S-ViPNAS and T-ViPNAS with ResNet-50 backbone on PoseTrack2018 validation set. We report mAP, GFlops, and speed (FPS). Speed is evaluated on a single core of an Intel i7-8700 CPU (3.2GHz). We show that T-ViPNAS is significantly faster (41FPS on CPU) than the baseline, with comparable accuracy, making it practical for real-world applications. <ref type="table">Table 2</ref> demonstrates the performance of the discovered S-ViPNAS models on COCO2017 dataset, compared with other state-of-the-art hand-crafted methods and concurrent NAS based pose estimators. We report our discovered results based on multiple backbones (i.e. HRNet-W32 <ref type="bibr" target="#b51">[49]</ref>, ResNet-50 <ref type="bibr" target="#b12">[13]</ref> and MobileNetV3 <ref type="bibr" target="#b13">[14]</ref>). For fair comparisons, we retrain S-ViPNAS models using the same training recipe and use the same Faster-RCNN human detection bounding boxes as SBL <ref type="bibr" target="#b58">[56]</ref> and HRNet <ref type="bibr" target="#b51">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">ViPNAS for image-based pose estimation</head><p>We see that our discovered S-ViPNAS-HRNetW32 significantly outperforms the popular hand-crafted models and the NAS based models. Compared with the current state-ofthe-art HRNet <ref type="bibr" target="#b51">[49]</ref>, we achieve higher accuracy and lower complexity (5.64 vs 7.10 GFlops). Compared with other NAS pose models PoseNFS <ref type="bibr" target="#b61">[59]</ref> and AutoPose <ref type="bibr" target="#b11">[12]</ref>, ViP-NAS also shows superiority in terms of both accuracy and computation complexity. Note that AutoPose <ref type="bibr" target="#b11">[12]</ref> uses a stronger human detector <ref type="bibr" target="#b4">[5]</ref> on COCO val2017 set.</p><p>We further search for lightweight pose estimators to boost the model efficiency. Based on ResNet-50 <ref type="bibr" target="#b58">[56]</ref>, we obtain a 6x smaller (1.44 vs 8.90 GFlops) model (S-ViPNAS-Res50) without sacrificing the accuracy. For MobileNet-V3 <ref type="bibr" target="#b13">[14]</ref>, our method finds a 5.8x smaller (0.69 vs 4.06 GFlops) model (S-ViPNAS-MobileNet) with 3.1mAP gain (67.8 vs 64.7) on COCO val2017 set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>Effect of temporal-level search. To validate the effect of temporal-level search, we search S-ViPNAS under the constraints of the same model complexity as T-ViPNAS. We apply the image-based S-ViPNAS models independently for each frame. As shown in <ref type="table">Table 3</ref>, we see that given the same Flops constraints, T-ViPNAS discovers better model architectures with higher accuracy (81.6 vs 80.3 mAP for ResNet-50 based models (-a) and 78.2 vs 77.2 mAP for MobileNet-V3 based models (-b)).</p><p>Effect of temporal feature fusion. As shown in <ref type="figure" target="#fig_5">Figure 6(a)</ref>, we explore the effect of temporal feature fusion on the PoseTrack2018 validation set. We search for four groups of T-ViPNAS models with ResNet-50 backbone in a range of average computation complexity levels (from 0.8 to 1.2 GFLOPs) for comparisons. The number of propagation frames is set as T = 3, so for each group, we have 4 different models (i.e. 1 S-ViPNet and 3 T-ViPNets) in total.</p><p>To validate the effectiveness of temporal feature fusion, we remove the temporal feature fusion from T-ViPNAS (red) in each group, keep the model architecture the same, and re-train them based on single images (blue). We see that our T-ViPNAS consistently improves over the baselines for various Flops requirements. Our experiments show that temporal fusion captures the consistency among adjacent frames and propagates poses efficiently using extremely lightweight models.</p><p>Effect of automatic computation allocation. As shown in <ref type="figure" target="#fig_5">Figure 6</ref>(b), we further explore the effect of automatic computation allocation on the PoseTrack2018 validation set. For comparisons, we search for the temporal models sharing both the spatial and temporal architectures (blue) under the same Flops constraints as our T-ViPNAS (red).  Comparing T-ViPNAS with (a) baselines without temporal feature fusion modules (b) baselines with the same architectures for different frames. We see that our proposed T-ViPNAS consistently improves over the baseline architectures for a range of complexity levels (from 0.8 to 1.2 GFlops). We visualize the architecture of one example T-ViPNAS (red star) in <ref type="figure" target="#fig_4">Figure 5</ref>.</p><p>We find that our T-ViPNAS consistently improves over the baseline architectures by at least 0.5% mAP, demonstrating the effectiveness of automatic computation allocation that searches for frame-specialized models. Example architectures of our discovered models are visualized in <ref type="figure" target="#fig_4">Figure 5</ref>.</p><p>Effect of the number of propagation frames. We evaluate the transferability of our proposed ViPNAS training scheme to various propagation frames T . During training of T-ViPNAS (Sec. 3.4.2) with ResNet-50 backbone, we set the number of non-key frames as T = 3, but search on different propagation lengths without re-training the supernetwork. As shown in <ref type="table" target="#tab_1">Table 4</ref>, we set the constraints of the average model computation complexity to be 1.0 GFlops, and search for different propagation frame numbers, i.e. T = 2, T = 3 or T = 4 frames. We see that our ViPNAS is relatively robust to the number of propagation frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose ViPNAS for online video pose estimation, trading-off between accuracy and the computation cost. ViPNAS automatically allocates computation resources (i.e. Flops) for different frames to achieve the overall optimum. By designing the novel spatial-temporal We give implementation details of the spatial search space of ViPNAS in this section. Elastic Depth. Elastic depth allows dynamic numbers of blocks in each stage. For example, the maximum number of the blacks in stage S is 4 as shown in <ref type="figure">Figure A1</ref>. When the depth D (D ? 4) is selected, the first D blocks are activated and the rest (4 ? D) blocks are skipped. Note that the minimum depth of any stage should be no less than 1 (D ? 1), as the first block may change the spatial resolution of the feature maps.</p><p>Elastic Width. Elastic width allows dynamic numbers of output channels in each block. For a convolutional layer, the shape of the filter is O?I ?K ?K given the input channels I, output channels O, and kernel size K ?K. When the output channel W (W ? O) is selected, the filter is tailored to the shape of W ? I ? K ? K as shown in <ref type="figure">Figure A2</ref>. We keep the first W out of O in the dimension of output channels.</p><p>Elastic Kernel Size. Elastic kernel size allows dynamic kernel sizes of convolutional layers in each block. The weights of the kernels are shared. As shown in <ref type="figure" target="#fig_3">Figure A4</ref>, we directly extract a K ? K kernel filter from the centering of the super-network kernel filter, when the kernel size K is selected. This enables the weight sharing for kernels of different sub-networks, which has been shown simple but effective in our experiments. To avoid imbalance and biases of kernel extraction, we set the stride of the kernel size choice as 2, keeping all the selected kernels center-aligned.</p><p>Elastic Group Number. Elastic group number allows dynamic group numbers of convolutional layers in each block. A convolutional layer has a filter with the shape O ? I ? K ? K given the input channels I, output channels O and kernel size K ? K. For example, when the group number is 2 (as shown in <ref type="figure" target="#fig_2">Figure A3</ref>), two filters with shape O 2 ? I 2 ?K ?K are applied. In the figure, we concatenate the two groups of filters in the dimension of output channels for better illustration. We tailor the original filter to the shape of O ? I 2 ? K ? K and keep the first half in the dimension of input channels.</p><p>Elastic Attention Module. Elastic attention module allows the network to choose whether or not to use the attention module in each block. As shown in <ref type="figure" target="#fig_4">Figure A5</ref>, the attention module is used if attention module is selected. We skip the attention module and identity mapping is applied if attention module is not selected. The attention module will keep both the spatial resolution and the feature channels the same before and after. <ref type="figure">Figure A1</ref>. Elastic Depth. The first D blocks are activated if the depth D is selected in stage S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Super-Network Design</head><p>In this section, we introduce the structure of our supernetwork as well as the concrete search space designs for each super-network. As the search space increases with the exponential explosion, directly searching for block-level network architecture is hard. In our experiments, we explicitly enforce the same width, kernel size, group number and attention module for all the blocks in the same stage and search for stage-wise optimum.</p><p>MobileNet-V3 <ref type="bibr" target="#b13">[14]</ref>. Our MobileNet-V3 based supernetwork consists of one convolutional layer, six stages, and three deconvolutional layers (followed by one 1 ? 1 convolutional layer for output). Each stage contains a stack with mobile blocks <ref type="bibr" target="#b13">[14]</ref>, which consists of one 1 ? 1 expansion convolution, a middle convolution and one 1 ? 1 projection convolution. We search for the kernel size and the group number of the middle convolution in mobile blocks. The expansion convolution expands the input features to a higherdimensional feature space. We search the expansion ratio, which is similar to the elastic width. The detailed search space is summarized in <ref type="table">Table A1</ref>.</p><p>ResNet-50 <ref type="bibr" target="#b12">[13]</ref>. Following SBL <ref type="bibr" target="#b58">[56]</ref>, our ResNet-50 based super-network consists of one convolutional layer, four stages and three deconvolutional layers. Each block in stages is Bottleneck <ref type="bibr" target="#b12">[13]</ref>, which contains one 1 ? 1 convolution followed by a middle convolution and another 1?1 convolution. Similar to our MobileNet-V3 based supernetwork design, we search for the kernel size and the group number of the middle convolution in the Bottleneck. We also search for whether to use a GC attention module <ref type="bibr">[4]</ref> in each block. <ref type="table">Table A2</ref> specifies the search space of our ResNet-50 based super-network.</p><p>HRNet-W32 <ref type="bibr" target="#b51">[49]</ref>. We conduct experiments based on HRNet-W32 to further demonstrate the effectiveness of our proposed ViPNAS. Our HRNet-W32 based super-network consists of two convolutional layers followed by several Bottleneck blocks, three multi-resolution stages, and one <ref type="figure">Figure A2</ref>. Elastic Width. Given the input channels I and kernel size K ? K, the first W output channels out of O is kept if the width W is selected. The filter is tailored from the shape O ? I ? K ? K to W ? I ? K ? K. <ref type="figure" target="#fig_2">Figure A3</ref>. Elastic Group Number. An example of Group=2 is illustrated in the figure. Given the input channels I, output channels O, and kernel size K ? K, the filter is tailored from the shape O ? I ? K ? K to O ? I 2 ? K ? K. Two groups of filters with shape O 2 ? I 2 ? K ? K are applied and are concatenated in the dimension of output channels. <ref type="figure" target="#fig_3">Figure A4</ref>. Elastic Kernel Size. The centering K ? K kernel is reserved if the kernel size K is selected. <ref type="figure" target="#fig_4">Figure A5</ref>. Elastic Attention Module. The attention module is applied if attention is selected, and is skipped if not.</p><p>1 ? 1 convolutional head for output. Each multi-resolution stage contains parallel branches with different spatial reso-lution, and each branch includes several BasicBlock <ref type="bibr" target="#b12">[13]</ref>. Both the convolutions in BasicBlock apply the same width, kernel size, and group number. We search the configurations of each stage and each branch for the best performance. <ref type="table">Table A3</ref> displays the detailed search space of our HRNet-W32 based super-network. Our search space is discrete. Take ResNet-50 backbone as an example, we set the search step to be 1 for depth, 16 for width, 2 for kernel size and 16 for group. <ref type="figure" target="#fig_5">Figure A6</ref> shows the qualitative results of our T-ViPNAS-Res50 on four adjacent frames. S-ViPNet localizes human poses on the first frame (key frame), and three different T-ViPNets propagate poses on the following frames (non-key frame). Our lightweight models keep the temporal consistency and are robust to occlusion, motion blur and unusual illumination. ViPNAS achieves state-ofthe-art accuracy with CPU real-time performance. <ref type="figure" target="#fig_5">Figure A6</ref>. Qualitative results of T-ViPNAS-Res50 on four adjacent frames. S-ViPNet localizes human poses on the first frame, and three different T-ViPNets propagate poses on the following frames. Our proposed ViPNAS is robust to occlusion, motion blur and unusual illumination, and achieves state-of-art accuracy with CPU real-time performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 . 2 .</head><label>22</label><figDesc>ViPNAS consists of one image-based key frame pose model S-ViPNet, and T video-based pose model T-ViPNets containing temporal feature module and various CNN architectures. Videos are processed frame-by-frame in an online mode. S-ViPNet first predicts the pose heatmaps H t of the key frame t, and propagates them to the next frame t + 1. T-ViPNet selects the CNN architecture, as well as the input features (e.g. F1 to F4) and fusion operation (e.g. Add, Cat and Mul) of fusion module. The fusion module combines the selected feature F t+1 2 with the propagated heatmaps H t , and generates the fused featuresF t+1 2 for predicting the heatmaps H t+1 . the input features F t+1 The pose heatmaps from the adjacent frame are processed by one 1?1 convolution, followed by one bi-linear interpolation layer to adjust the channels and the resolution (width &amp; height) to match those of the selected features F t+1 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Multi-frame pose propagation training of ViPNAS. The key frame S-ViPNet is first pre-trained and fixed. T (T ? 2) various non-keyframe sub-networks are sampled from a single supernetwork with sharing weights, and jointly supervised by MSE loss for each frame. The solid lines indicate the forward process and the dotted lines indicate the back propagation process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Comparisons among SBL<ref type="bibr" target="#b58">[56]</ref>, S-ViPNAS and T-ViPNAS with ResNet-50 backbone. ViPNAS discovers models with much less computational complexity and significantly higher speed (single core of a 3.2GHz Intel i7-8700 CPU).icantly lower model complexity. T-ViPNAS significantly boosts the model efficiency and reduces the computation without sacrificing the overall accuracy. For example, T-ViPNAS-MobileNetV3 achieves 10x Flops reduction (0.37 vs 4.1) without accuracy drop (78.2 vs 78.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Example of T-ViPNAS with ResNet-50 backbone. {Depth, Width, Kernel Size, Group} are listed in the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Comparing T-ViPNAS with (a) baselines without temporal feature fusion modules (b) baselines with the same architectures for different frames. We see that our proposed T-ViPNAS consistently improves over the baseline architectures for a range of complexity levels (from 0.8 to 1.2 GFlops). We visualize the architecture of one example T-ViPNAS (red star) in Figure 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparisons with other video pose estimation approaches on PoseTrack2018 validation set. Our ViPNAS achieves the state-ofthe-art performance with significantly lower computation complexity. Backbone Image Size #Params GFLOPs Head Sho. Elb. Wri. Hip Knee Ank. Comparisons on COCO2017 dataset. Our approach significantly outperforms other hand-crafted and NAS models in terms of both speed and accuracy on COCO val2017 set and test-dev2017 set. ? means using a stronger person bounding box detector (HTC<ref type="bibr" target="#b4">[5]</ref>).MethodImage Size #Params GFLOPs AP AP 50 AP 75 AP M AP</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Effect of the number of propagation frames. We experiment with training the super-network for T = 3 frames and searching for T = {2, 3, 4} frames with 1.0 GFlops complexity constraint on average.</figDesc><table><row><cell cols="3">#Propagation Frames (T ) GFLOPs mAP</cell></row><row><cell>2</cell><cell>1.0</cell><cell>81.7</cell></row><row><cell>3</cell><cell>1.0</cell><cell>81.7</cell></row><row><cell>4</cell><cell>1.0</cell><cell>81.4</cell></row><row><cell cols="3">search space, we can simultaneously search for CNN archi-</cell></row><row><cell cols="3">tectures and temporal connections, i.e. the fusion operations</cell></row><row><cell cols="3">and the feature fusion sites. Empirical experiments demon-</cell></row><row><cell cols="3">strate that our proposed ViPNAS successfully discovers the</cell></row><row><cell cols="3">architecture that achieves the state-of-the-art accuracy with</cell></row><row><cell>CPU real-time performance.</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://cocodataset.org/#keypoints-eval 2 https://posetrack.net/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/Guanghan/lighttrack</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis. Worksh</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Trb: a novel triplet representation for understanding 2d human body</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haodong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detect-and-track: Efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Autopose: Searching multi-scale branch aggregation for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07018</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-person articulated tracking with spatial and temporal embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Differentiable hierarchical graph grouping for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Towards multiperson pose tracking: Bottom-up and top-down methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xujie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Whole-body human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lumin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal feature enhancing network for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haihan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingmin</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal feature correlation for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving one-shot nas by suppressing the posterior fading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Computation reallocation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<title level="m">Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DARTS: differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Inception convolution with efficient dilation search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13587</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Block proposal neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunfeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A cascaded inception of inception network with attention modulated feature fusion for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lstm pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic kernel distillation for efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lighttrack: A generic framework for online top-down human pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lighttrack: A generic framework for online top-down human pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Video action recognition via neural architecture searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Int</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<title level="m">Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for efficient pose estimation in gesture videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06961</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Tiny video networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Evolving space-time neural architectures for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Assemblenet: Searching for multi-stream neural connectivity in video architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Thin-slicing network: A deep structured model for pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with enhanced channelwise and spatial information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Combining detection and tracking for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Modolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Pose flow: Efficient online pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<editor>Brit. Mach. Vis. Conf.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hieve acm mm grand challenge 2020: Pose tracking in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lumin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07068</idno>
		<title level="m">Pose neural fabrics search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Universally slimmable networks and improved training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Bignas: Scaling up neural architecture search with big single-stage models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Econas: Finding proxies for economical neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongzhan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Expansion ratio indicates the feature channel expansion rate in the middle of mobile blocks, and resolution indicates the ratio between the shapes of current features and those of input images. Kernel size and group number of the middle convolution in mobile blocks are searched. Stage Operator Depth Width Kernel Size Group Attention (SE [16]) Expansion Ratio Resolution Conv</title>
		<idno>Table A1. MobileNet-V3 [14] based search space. [</idno>
		<imprint/>
	</monogr>
	<note>min, max] indicates the range of each search space. 16, 16] [3, 3] [1, 1</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<idno>1/4</idno>
		<imprint/>
	</monogr>
	<note>256, 256. 4, 4] [32, 256</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">min, max] indicates the range of each search space, and expansion ratio indicates the feature channel expansion rate in the middle of Bottleneck. The first convolution and max pooling with stride 2 down-sample the spatial resolution to 1/4 of the input image. Kernel size and group number of the middle convolution in Bottleneck are searched. Stage Operator Depth Width Kernel Size Group Attention (GC [4]) Expansion Ratio Resolution Conv+Pool</title>
		<idno>A2. ResNet-50 [13] based search space. [</idno>
		<imprint/>
	</monogr>
	<note>32, 64. 7, 7] [1, 1] --1/4</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<idno>1/4</idno>
		<imprint/>
	</monogr>
	<note>64, 256. 4, 4] [16, 64</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">We search depth of each stage, and search width and attention of each branch. Kernel size and group number of the middle convolution in Bottleneck and both the convolutions in BasicBlock are searched. Stage Depth Branch Operator Width Kernel Size Group Attention (SE [16]) Resolution -Conv</title>
		<idno>A3. HRNet-W32 [49] based search space</idno>
		<imprint/>
	</monogr>
	<note>HRNet includes parallel branches with different resolution in stages, which indicates the ratio between the spatial shape of current features and input images. 16, 64. 3, 3] [1, 1</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basicblock</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>32, 128. 3, 3] [1, 128] [0, 1] 1/16</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basicblock</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>64, 256. 3, 3] [1, 256] [0, 1] 1/32</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
