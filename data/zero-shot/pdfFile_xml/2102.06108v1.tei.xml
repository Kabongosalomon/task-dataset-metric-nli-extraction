<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SWAGAN: A Style-based WAvelet-driven Generative Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rinon</forename><surname>Gal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Cohen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bermano</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SWAGAN: A Style-based WAvelet-driven Generative Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, considerable progress has been made in the visual quality of Generative Adversarial Networks (GANs). Even so, these networks still suffer from degradation in quality for high-frequency content, stemming from a spectrally biased architecture, and similarly unfavorable loss functions. To address this issue, we present a novel general-purpose Style and WAvelet based GAN (SWA-GAN) that implements progressive generation in the frequency domain. SWAGAN incorporates wavelets throughout its generator and discriminator architectures, enforcing a frequency-aware latent representation at every step of the way. This approach yields enhancements in the visual quality of the generated images, and considerably increases computational performance. We demonstrate the advantage of our method by integrating it into the SyleGAN2 framework, and verifying that content generation in the wavelet domain leads to higher quality images with more realistic high-frequency content. Furthermore, we verify that our model's latent space retains the qualities that allow Style-GAN to serve as a basis for a multitude of editing tasks, and show that our frequency-aware approach also induces improved downstream visual quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image synthesis is a cornerstone of modern deep learning research, owing to the applicability of deep generative networks to a multitude of image related tasks. Such tasks range from content generation to novel view synthesis, rendering, image in-painting, super resolution or modifications of specific image properties <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b30">31]</ref>. The performance of such downstream tasks has seen rapid improvements in recent years, due in part to the significant improvement in the visual quality of generative models, and in particular Generative Adversarial Networks (GANs).</p><p>However, both generative models and their associated downstream tasks consistently suffer from a common flawtheir performance degrades rapidly when handling high frequency content <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7]</ref>. This demeaned performance often manifests as blurriness, or the lack of sharp edges around</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>StyleGAN2 SWAGAN <ref type="figure">Figure 1</ref>. By working directly in the wavelet-domain, rather than image or feature space, we can alleviate the spectral bias of neural networks and successfully generate high frequency data where other models fail. Our model can produce patterns that elude state-of-the-art models such as StyleGAN2 even in an over-fitting setup, where the training set includes only a single image. We show (from left to right): the original image and the outputs of StyleGAN2 and SWAGAN (our model), respectively, after being trained for 24 hours on each image. Zooming in is encouraged.</p><p>finer image content. Prior works have attributed these shortcomings to the nature of the network architecture and common cost functions. The oft-used mean squared reconstruc-tion error, for example, is much less sensitive to small perturbations, thus leading networks that utilize this loss to favor the accuracy of low frequency and large-scale content over finer features <ref type="bibr" target="#b8">[10]</ref>. In addition, recent works have pointed to an inherent spectral bias in neural networks that manifests as a frequency-dependent learning speed <ref type="bibr" target="#b26">[27]</ref>.</p><p>A straightforward approach to combat this shortcoming is to push resolutions to new heights. By doing so, previously minute details become more dominant and thus are better captured by the traditional losses. This approach, however, does little to overcome the frequency-dependent learning rate.</p><p>The current state-of-the-art results for high resolution generation are achieved by style-based models <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b15">17]</ref>. These models employ a new approach to content generation -rather than progressively growing a set of features and converting them to an image representation at the very end, they first generate a low resolution image and then grow it in size in a hierarchical manner <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15]</ref>. This approach affords additional regularization and stability to the trained model and has been shown to achieve improved visual results. While these models lead to remarkable strides in visual quality, they rely on networks which are larger and notoriously expensive to train, with the development of recent state-of-the-art networks requiring upwards of 50 years of computation time using a top of the line GPU <ref type="bibr" target="#b15">[17]</ref>.</p><p>An alternative approach to addressing the highfrequency gap is to directly employ frequency-aware training objectives, for example by empowering a discriminator to consider not only an image, but also a spectral decomposition of it <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7]</ref>. Others opt to generate high-frequency content directly by predicting a full set of wavelet coefficients. Such an approach has been used successfully on a range of tasks, from image super resolution <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b35">36]</ref> to compressed video enhancement <ref type="bibr" target="#b31">[32]</ref>. However, to the best of our knowledge, these prior works focus on training a constrained generator for a specific task (e.g., face super resolution), and none of them are adapted to the modern approach of hierarchical image growth, rather than purely feature-based growth.</p><p>We argue that frequency-based representations hold another advantage that is often overlooked or squandered due to misuse. By using an appropriate representation, the network can be biased to affect high-frequency modifications in some target domain through low frequency modifications of the representation. Such bias can help bypass the need to learn any high-frequency functions and thus avoid the spectral bias of neural networks.</p><p>In our work, we propose to marry the core ideas from these different approaches. Our goal is to train a general purpose image generator. However, rather than creating an image by predicting deep features or the color content of each pixel in a progressively growing series of reso- <ref type="figure">Figure 2</ref>. Our style-based generator architecture predicts waveletcoefficients at increasing resolution scales. lutions, we instead propose to work directly in frequency space by predicting coefficients of basis functions of increasing frequency. Similarly, we propose to make use of a discriminator that compares not only the pixel content of generated images and real samples, but also their frequency space decomposition. These frequency domain considerations are incorporated not only as a training goal, but are baked directly into the architecture of our model at each resolution step.</p><p>We demonstrate the benefits of our approach using the popular StyleGAN2 framework <ref type="bibr" target="#b15">[17]</ref> (see <ref type="figure" target="#fig_0">Figure 3</ref>). We show that by adapting its architecture to work directly in a wavelet-based space, we achieve improved visual quality and more realistic content in the high-frequency range. By working directly in the wavelet-domain, rather than image or feature space, we alleviate the spectral bias of neural networks, converge with fewer epochs, and successfully generate high frequency data where other models fail (see <ref type="figure">Figure 1</ref>)</p><p>Another surprising benefit of our approach is a significant reduction in training iteration and inference times. Our experiments show that the wavelet-based model not only converges with fewer epochs, but also that the tradeoff between spatial dimensions and channel depth inherent to wavelet decomposition, results in a significant speedup of network computations. This behavior can be dependent on the specific details of the convolution operations implemented by each framework, but popular implementations such as GEMM or Winograd variants show only a mild increase in computation time when increasing the number of filters <ref type="bibr" target="#b10">[12]</ref>.</p><p>Finally, we show that our model's latent space can support the editing operations that are commonplace in StyleGAN-based works. We show that our high-frequency performance can be successfully transmitted downstream, and that by using our model one can avoid many of the blurry artifacts present in prior works. Our experiments indicate that shifting content generation to the wavelet domain offers a significant set of advantages, with no apparent drawbacks.</p><p>In summary, our key contributions are:</p><p>? A hierarchical, wavelet-based approach to image generation that achieves improved visual fidelity and more realistic spectra.</p><p>? An approach to network design that reduces the natural spectral-bias inclination of networks, resulting in a significant reduction of the computational budget required to train high resolution generative models.</p><p>Our code and trained models will be made public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and related work 2.1. Generative Adversarial Networks</head><p>Generative Adversarial Networks (GANs), first proposed by Goodfellow et al. <ref type="bibr" target="#b7">[9]</ref>, have been successfully used in a wide range of image synthesis tasks, ranging from general content creation to more specific tasks such as image super resolution and in-painting. Due to their ubiquity, considerable effort has gone into improving the quality of images generated by such models. Earlier models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b24">26]</ref> performed content generation in feature space, growing a feature representation in a hierarchical manner before converting the final feature set into an image.</p><p>More recently, progressive <ref type="bibr" target="#b13">[15]</ref> and style-based <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b15">17]</ref> generative networks have appeared. These models achieved unprecedented resolutions and visual fidelity and solidified themselves as leading paradigms in the content generation field. One of the core differences between these models and previous generative works is that they generate hierarchical content directly in image space, without resorting to a conversion from learned features at the very end. This is achieved through direct supervision in the progressive case, or merely by applying naive bilinear-upsampling between different scales and teaching the network to augment them with high-resolution detail as in Karras et al. <ref type="bibr" target="#b15">[17]</ref>. These additional constraints serve to regularize the network, preventing any learned features from deviating too widely and eventually improving the quality of results.</p><p>Our work aims to improve upon this idea by progressively generating image decompositions in a frequency domain. Rather than training the different layers to implicitly add higher-frequency data to an image by modifying its color content, we train them to do so explicitly by modifying the coefficients of a wavelet band decomposition. Similarly, our discriminator extracts features not only from the image space representations but also from wavelet domain sub-bands. The network is thus empowered to better identify content that lacks high-frequency data, and can more directly restore it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Frequency-based Approaches</head><p>The use of frequency based information, and in particular wavelet transforms, has been well studied in the deep learning literature. Convolutional neural networks have been augmented with wavelet-based features, inputs or pooling operations <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b32">33]</ref> in order to improve performance on style transfer <ref type="bibr" target="#b33">[34]</ref>, denoising <ref type="bibr" target="#b17">[19]</ref> and even medical analysis tasks <ref type="bibr" target="#b11">[13]</ref>. Others <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b29">30]</ref> have proposed a decomposition of implicit representation input coordinates into a set of Fourier-based features. They showed, both empirically and theoretically, that explicitly providing the network with high frequency inputs allows it to overcome spectral bias and better reconstruct high-frequency outputs. In the generative domain, several works <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref> have proposed empowering networks to create not only images, but also full wavelet based decompositions. Others <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b31">32]</ref> have augmented discriminators by providing them with not only an image, but also frequency sub-bands derived from a wavelet packet transform.</p><p>None of these works, however, are general purpose generative models. Instead, they utilize adversarial losses in order to improve performance on specific tasks such as image super resolution or denoising. As such, none of them can generate a novel image from a latent code but rather only add or improve upon existing images.</p><p>Our work, meanwhile, seeks to train a fully generative model which can synthesize novel images from noise samples. Such a generator can later be adapted to a multitude of tasks by projecting images into its richer latent space, either through direct optimization <ref type="bibr" target="#b0">[1]</ref> or through the use of a trained encoder <ref type="bibr" target="#b27">[28]</ref>. This methodology has shown considerable success since the inception of powerful generative models such as Karras et al. <ref type="bibr" target="#b14">[16]</ref>.</p><p>More recently, a significant number of studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">6]</ref> have highlighted the failures of general-purpose generative networks in the high-frequency domain. All of them, however, address the problem by introducing frequency related terms to the loss function. In contrast, we modify not only the loss term, but also introduce a frequency-based inductive bias into the generator itself. This bias is shown to further bolster the visual quality of synthesized images, while also leading to significant reductions in training and inference times.</p><p>Lastly, our work is further distinguished from prior wavelet-based approaches in that we adopt the modern viewpoint of progressive generation in the frequency domain. Whereas previous works opted to utilize a progressive feature representation and generate wavelet content only at the final layer, or alternatively provide the discriminator with wavelets at the onset but merely encode them into features -our model generates content directly in wavelet space throughout the entire process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Wavelet Transform</head><p>At the core of our method are wavelet transforms, which decompose an image into a sequence of channels, each of which represents a different range of frequency content. We employ Haar Wavelets as the basis function for our transformations, owing to their simple nature coupled with their well documented ability to represent multi-frequency information <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">5]</ref>. Our model works with the first-level wavelet decomposition, where each image is broken into four subbands: LL, LH, HL and HH. These bands are attained by passing the image through a series of low-pass and highpass wavelet filters and serve as the wavelet coefficients. The first sub-band, LL, corresponds to low frequency information, and indeed it is visually similar to a blurred version of the input image. The remaining sub-bands, LH, HL and HH, correspond to high frequency content in the horizontal, vertical and diagonal directions, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>Our wavelet-aware architecture is based on the original implementation of StyleGAN2 <ref type="bibr" target="#b15">[17]</ref>. While the original implementation generates content directly in image-space, our proposed architecture operates in the frequency domain. Similarly, our discriminator is designed to consider not only the RGB space of images, but their entire wavelet decomposition.</p><p>By allowing the generator to generate content directly in the wavelet domain, we gain on two fronts: First, as shown by <ref type="bibr" target="#b26">[27]</ref>, neural networks prioritize learning in the low frequency domain. By converting our representation to a frequency based one, we empower the network to affect high-frequency changes in the image domain by learning low-frequency modifications of the representation. This differs from a simple loss-based modification, as the later can provide an incentive for the network to learn highfrequencies, but it does not make the learning task easier. Second, wavelet decompositions are more spatially compact. In a first-level wavelet decomposition, each 2N ? 2N image is fully represented by four channels of N ? N coefficients each. This allows us to employ convolutions on lower resolution representations throughout the entire generative process, at the cost of requiring additional filters. However, this tradeoff can be advantageous when utilizing the popular deep learning frameworks <ref type="bibr" target="#b10">[12]</ref>.</p><p>Similarly, by providing frequency information to the discriminator, the network is able to better identify the high frequency content that is often missing in generated images. As a result, the generator is better motivated to re-create plausible high-frequency data.</p><p>In the proposed generator, each resolution block receives a full wavelet decomposition as an input. In a similar fash-ion to StyleGAN2, the wavelet coefficients are refined using a set of high dimensional features mapped back to the frequency domain via skip connections. In StyleGAN2, the image is resized between blocks using simple bilinear up-sampling. Performing this up-sampling in the frequency domain would not carry the same meaning. Instead, we choose the natural alternative and perform up-sampling by converting the wavelet representation back to the image domain by applying the inverse wavelet transform (IWT), resizing the image as usual, and then predicting the next set of wavelet coefficients from the higher-resolution image. The network's output is formed by applying an IWT to the wavelet decomposition provided by the output of the last layer. The generator's architecture is illustrated in <ref type="figure" target="#fig_0">Figure 3</ref> (left).</p><p>In the discriminator, we similarly provide the wavelet coefficients of the corresponding resolution to each block. At each resolution step, a skip-connection based network is used to extract features from the wavelet decomposition and merge them into the feature representations derived from higher resolution blocks. In order to downscale the image between blocks, the wavelet coefficients are combined back into a full image via IWT, the image is bilinearly downsampled, and the lower-resolution image is decomposed back into the wavelet coefficients, which are fed into the next block. The input to the first block of the discriminator is simply the DWT of any image (real or fake). The discriminator's architecture is illustrated in <ref type="figure" target="#fig_0">Figure 3 (right)</ref>.</p><p>In addition to the architecture described above, we explored a series of variations, including different upsampling steps and mixing image and wavelet-domain generation steps in the same network. We describe and analyze these variations in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Downstream Tasks</head><p>Any adaptation of the style-based network would be incomplete if it could not be used as a backbone for the downstream tasks built around the original. We demonstrate that our wavelet-based network can still support the same applications, and in some cases even achieve better results. We analyze our model using the optimization-based approach to inversion, where we use gradient descent to find the latent space representation for which the generator outputs the best approximation of a given source image. We use the latent space projector of Karras et al. <ref type="bibr" target="#b15">[17]</ref>, which finds a latent representation in the so-called W space of a style-based network, using a target of minimizing a perceptual based loss (LPIPS). The later is crucial for our needs, as any L2 based reconstruction target will inevitably discard high-frequency information in the optimization process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation and Training Details</head><p>We build upon the official TensorFlow implementation of StyleGAN2 <ref type="bibr" target="#b15">[17]</ref>. Specifically, we modify the architecture of the skip-based generator and discriminators, but otherwise inherit all of the corresponding training details and parameters from the configuration F setup. Our models were trained on FFHQ at resolutions of 256 ? 256 and 1024 ? 1024 and on LSUN Churches at a resolution of 256 ? 256. For FFHQ at 1024 ? 1024, we utilized a batch size of 8 for the wavelet-based models and 2 for Style-GAN2 comparisons. For FFHQ at 256 ? 256 resolution, we used batch sizes of 16 and 8, respectively. All models were trained on a single NVIDIA GeForce RTX 2080 GPU, i.e., using commercially available, non-corporate grade computational power. For quantitative evaluations of downstream tasks, we utilized the publicly available CelebA dataset <ref type="bibr" target="#b20">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We have conducted a series of experiments in order to evaluate the increased computational and visual performance of our generator, and to evaluate the importance of high frequency data to downstream tasks. We additionally conduct an ablation study to highlight the importance of generation in the wavelet-domain, the importance of a wavelet-based discriminator and to investigate natural alternatives to some of our ad-hoc architectural choices. In all of the experiments below, we dubbed the bilinear upsampling variation of our generator (presented in Section 3) as SWAGAN-Bi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Computational Performance and Visual Quality</head><p>We first demonstrate our ability to capture highfrequency data on a set of toy problems. We generated three datasets, each containing only a single image of a pattern with high-frequency detail. In addition, we created a fourth dataset composed of a single real face with considerable high-frequency data. We trained our model and the original StyleGAN2 model on these single image datasets for a period of 24 hours. <ref type="figure">Figure 1</ref> shows the outputs of the trained models. When dealing with repeated patterns, our model can cleanly capture and recreate them with high accuracy. The original StyleGAN2 model, meanwhile, fails to recreate the data in a meaningful manner, showing that purely high-frequency content is beyond its scope. The face image is well reconstructed by both models. However, our model is better able to capture details such as the patterns on the shirt or fine wrinkles.</p><p>We compared the time required to train our model and the visual quality of generated results throughout the process to those achieved by StyleGAN2. <ref type="figure" target="#fig_1">Figure 4</ref> shows an uncurated sample of faces drawn from SWAGAN-Bi model trained on FFHQ at 1024x1024. <ref type="figure" target="#fig_2">Figure 5</ref> shows uncurated samples drawn from a SWAGAN-Bi model trained on LSUN Church at a resolution of 256x256. As can be seen, our model can produce high quality results with significant detail around regions of high frequency-content, such as wrinkles or hair.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we report the time required for the model to process 1000 real images on a single GeForce RTX 2080. This metric is equivalent to time per epoch (up to  a constant, multiplicative factor), and is the standard unit of work used in <ref type="bibr" target="#b14">[16]</ref> and its derivatives. The wavelet-based variants require a significantly reduced training time across both resolutions. <ref type="table">Table 2</ref> reports the final Fr?chet Inception Distance (FID) of our models at various resolutions and on multiple datasets. As the computational overhead for training a model for the number of iterations reported by StyleGAN2 remains high, we elected to reduce the resolution and the number of training steps used for training the models. In all such cases, we provide results on a StyleGAN2 model trained for the same number of iterations and resolution, utilizing the official implementation of Karras et al. <ref type="bibr" target="#b15">[17]</ref>. Note that this comparison favors the StyleGAN2 models by allotting them roughly double the computational budget, due to increased training times and higher memory requirements.</p><p>Even under this biased comparison, the wavelet-based approach can capture more realistic details and achieve better visual quality. <ref type="figure" target="#fig_3">Figure 6</ref> shows a comparison of FID metrics over the duration of training between a wavelet based model (SWAGAN-Bi) and the original StyleGAN2 model, trained on FFHQ at 1024x1024 resolution. We show the progress, both as a function of the number of images viewed by the discriminator and as a function of wall-time. Our model, not only trains faster, but also converges with fewer epochs on the same data set. This hints that the inductive bias built into our model is indeed capable of alleviating the spectral bias of neural network learning. In <ref type="figure">Figure 7</ref>, we plot the distance between the power spectrum of real images and those generated by a subset of our trained models, averaged over 4050 images per model and normalized by the spectra of real images (dubbed "spectrum gap"). For convenience, we additionally plot the spectrum gap for real images blurred via Gaussian kernels with filter sizes of 3 ? 3 and 5 ? 5. Our frequency-domain model consistently generates more realistic spectra, particularly in the   <ref type="table">Table 2</ref>. The best FID achieved by all tested setups until the discriminator observed the indicated number of images. Our SWAGAN-Bi model achieves the best results for different resolutions and datasets. medium and high frequency range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>In order to measure the importance of each aspect of our suggested solution, we asked ourselves the following questions, and conducted a series of experiments to answer each in turn. The quantitative results of these experiments are provided in tables 1 and 2 and in <ref type="figure">Figure 7</ref>.</p><p>Is it important to progressively generate content in the frequency domain? We investigate this question with two experiments. In the first, we observe the behaviour of the network when all layers, but the last utilize the original tRGB mapping. In such a setup, content is progressively generated in the RGB domain and any wavelet information is only added at the final layer. The model which only predicts wavelet information in the final layer has shown inferior performance, both in time-based metrics and in vi- <ref type="figure">Figure 7</ref>. The power spectrum distance between real images and those produced by the listed models, normalized by the spectra of real images. To assist comparisons, we also provide the spectrum gap observed in real images blurred using Gaussian kernels of size 3x3 and 5x5. Our full model (orange) achieves more realistic results in the middle and high frequency range. Note that a model without a wavelet-based discriminator, or one where the generative process is not entirely wavelet, based both fail to capture the same degree of high frequency data. sual quality, highlighting the importance of wavelet-domain generation throughout the entire process. Note that this model performs worse than even the original StyleGAN2. This may be due to the network's need to conserve both frequency and image based information in the hidden feature representation. Next, we investigated a variation on the model's upscaling layer. In this setup, rather than resizing the image through bilinear up-sampling, we utilize a network to predict an initial set of wavelet coefficients for the next layer, effectively generating content at the next resolution step. This upscaling layer is visualized in <ref type="figure" target="#fig_4">Figure 8</ref>. We dubbed this neural upscaling variation as SWAGAN-NU.</p><p>A reasonable expectation would be for the network to perform better in this scenario, as it has sufficient degrees of freedom to learn a better upsampling step. However, although it sounds promising, such a change also breaks the inductive bias built into the network. In the bilinear up-sampling approach, the network must truly generate wavelet based content at each scale because it has no means through which to convert the hidden representation back to the wavelet domain before the output layer. The added neural mapping of the SWAGAN-NU architecture, however, could also be used to learn a mapping from any feature representation back to wavelet space, thus allowing the network to ignore our intent and merely work in feature space throughout. The loss of this bias adversely affects the visual quality of generated results. We further examined this hypothesis by investigating the images observed in the intermediate scales of the network. A comparison of such images is provided in <ref type="figure">Figure 9</ref>, where we plot the four wavelet maps predicted at increasing resolution scales, resized to 256 ? 256. As can be seen, intermediate representations in the SWAGAN-Bi generator portray a gradual growth in wavelet space, while those of the SWAGAN-NU setup do not.</p><p>As our ablation experiments show, progressive generation in the frequency domain is key to improving results.</p><p>Does the network benefit from modifying both the generator and the discriminator? To answer this question, we trained a model utilizing the SWAGAN-Bi gen-SWAGAN-Bi SWAGAN-NU <ref type="figure">Figure 9</ref>. Intermediate wavelet representations in the generative path of our SWAGAN-Bi and SWAGAN-NU models. At each row, we plot the four wavelet coefficient maps obtained at a given resolution step, prior to the IWT operation. All outputs were resized to 256 ? 256 using bicubic interpolation. The SWAGAN-Bi outputs portray an increasing level of detail in the wavelet space, while the SWAGAN-NU outputs portray a deep feature representation until the final resolution. Zooming in is encouraged erator, along with the original residual based discriminator of StyleGAN2 (dubbed as NWD for non-waveletdiscriminator). The trained model offers significantly degraded performance when compared to the full SWAGAN-Bi model. This demeaned performance can be observed in both the FID metrics of <ref type="table">Table 2</ref> and in the power spectrum comparisons of <ref type="figure">Fig 7,</ref> where the average spectra of images generated by the full model more closely resemble that of real images. Indeed, the results of such a model are worse than those of the original StyleGAN2 across the board. In addition to the degraded results, we also observe a significant increase in training instability when using a wavelet-based generator without a wavelet-based discriminator. When the loss term is insensitive to high-frequencies, the corresponding high-frequency bands in the wavelet decomposition receive diminished gradients, and the network fails to learn how to modify them in a meaningful manner. These results indicate that a wavelet-based generator should be accompanied by an appropriate frequency aware loss term (and the original StyleGAN2 discriminator is insufficient in that regard, as shown in Chen et al. <ref type="bibr" target="#b2">[3]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">GAN Inversion and Applications</head><p>One of the key benefits of the style-based generative framework is its well behaved latent space, a property which can be exploited for editing tasks or for interpolating images in the generator's latent space. As we deem this to be a crucial property, we conducted a series of experiments to ensure that our latent space still supports these operations.</p><p>In <ref type="figure">Figure 10</ref>, we encode pairs of images into the latent spaces of a SWAGAN-Bi generator and StyleGAN2's original generator using the projector provided in their original implementation. We perform linear interpolation between each pair of images and compare the results of both models. As can be seen, both latent spaces seem to behave comparably well, although our model generates sharp images throughout the interpolation process, without any of the blurring often observed during the transition (see for example the hair in the the central images of the figure).</p><p>In <ref type="figure">Figure 11</ref>, we show the results of semantic editing in our generator's latent space. As in the case of linear interpolation, our results do not exhibit blur around high-frequency areas such as the hair. We further analyzed the spectrum gap of images generated in all our downstream experiments. For the reconstruction experiment, we compare the spectra of 500 reconstructed images to those of their corresponding projection targets. For the editing and interpolation experiments, we sampled random images and compared their spectra to 5000 real images from the FFHQ dataset. The results are shown in <ref type="figure">Figure 12</ref>. In all scenarios, our model is able to generate more realistic spectra, indicating that our improvements can be transmitted to downstream tasks.</p><p>Our experiments show that our frequency based model preserves the versatility of StyleGAN2's latent space, allowing us to support similar operations while generating sharper results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have outlined an alternative approach to the stylebased generative framework, operating on the frequency domain rather than directly in RGB-space. This approach was shown to lead to more realistic visual results, particularly in the middle-to-high frequency range. By operating directly in frequency space, the model can affect high frequency changes in the output through low frequency changes in the representation, i.e. the wavelet coefficients. This directly tackles the spectral bias of neural networks and prompts a significant increase in rate of convergence. As it turns out, adopting the wavelet-based generative approach allows for training a style-based generative model with similar performance at roughly a fourth of the computational budget. We have further shown that our quality benefits can be extended to downstream tasks, allowing for more realistic image reconstructions or editing operations.</p><p>The landscape of frequency-domain generation leaves many venues for future research. Our exploration focused on a single wavelet function (the Haar Wavelet) which may not necessarily be optimal. Indeed, it may be possible to learn a better wavelet, or to utilize a set of different wavelets, while allowing the network to mix and match between them using appropriate weights. Another interesting direction for further research may be an adaptation of frequency-domain generation to the realm of videos, for example by building upon existing spatio-temporal wavelet schemes.</p><p>Where downstream tasks are concerned, we have limited ourselves to those that are tackled through direct optimization methods. Recently, however, encoder-based methods <ref type="bibr" target="#b27">[28]</ref> have been gaining popularity, offering a faster alternative to optimization, with increasing quality for various image-to-image translation tasks. These encoders suffer from acute high-frequency shortcomings, in part due to their use of L2 based losses. It may therefore be beneficial to explore a similar frequency-based treatment for such encoders. Additionally, some disentanglement tasks probably lend themselves better to a spectral representation, especially those that are frequency related. This avenue is also one that should probably be explored.</p><p>In summary, we hope our work inspires others to consider alternative representation for content generation, since these can be, as demonstrated here, a more natural fit for the network to learn, yielding broad advantages in quality, runtime, and training convergence. <ref type="figure">Figure 10</ref>. Latent space interpolation. In each cluster of images, the left-most and right-most images are the ground truth, taken from the FFHQ dataset. We project them into the generator's latent W-space using the StyleGAN projector, and interpolate between both ends. The top row of each cluster contains images obtained using a StyleGAN2 configuration F model, while the bottom row contains images obtained using the SWAGAN-Bi model. Interpolated StyleGAN2 images show considerable blur around regions of high frequency such as the hair, while the SWAGAN images do not. <ref type="figure">Figure 11</ref>. Semantic face editing using the latent projection method of <ref type="bibr" target="#b28">[29]</ref>. We modify (from top to bottom): smile, age and hair length. <ref type="figure">Figure 12</ref>. The power spectrum gap between real images and those generated by our downstream experiments. Though performance varies greatly between tasks, our high-frequency advantage is maintained for all of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StyleGAN2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StyleGAN2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SWAGAN-Bi</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SWAGAN-Bi</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Our SWAGAN generator (left) and discriminator (right) architectures. Each ConvBlock is equivalent to a feature-resolution increasing block of the StyleGAN2 architecture, which is itself composed of two style blocks. tWavelets and fWavelets correspond to the tRGB and fRGB layers of StyleGAN2 and their purpose is to learn a mapping between wavelet decompositions and high dimensional features. Inverse wavelet transforms are denoted by IWT, while Up and Down are non-learning layers responsible for converting an image to an initial wavelet-decomposition of a higher or lower resolution, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Uncurated face samples generated by our SWAGAN-Bi model, using truncation with ? = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Uncurated samples generated by our SWAGAN-Bi model trained on the LSUN Church dataset at a resolution of 256x256, using truncation with ? = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Model FID as a function of: (a) wall-clock time, (b) number of images viewed by the discriminator. Our model outperforms StyleGAN2 in both scenarios and displays increased stability during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>The upscaling blocks for SWAGAN-NU (top) and SWAGAN-Bi (bottom) architectures. SWAGAN-NU architecture includes 3 ? 3 convolutions in order to predict the new wavelet channels; The SWAGAN-Bi architecture is comprised of bilinear up sampling, followed by discrete wavelet transform (DWT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Average time (in seconds) for the network to process 1000 real images during training. Wavelet-based generators consistently outperform the alternatives. The reported times are based on the calculation provided in StyleGAN2's original implementation and do not include any of the (significant) time spent on benchmark calculations or sample generation.</figDesc><table><row><cell>Model</cell><cell cols="2">Resolution Seconds / 1k images ?</cell></row><row><cell>StyleGAN2</cell><cell>1024</cell><cell>184.97</cell></row><row><cell>SWAGAN-Bi</cell><cell>1024</cell><cell>95.06</cell></row><row><cell>StyleGAN2</cell><cell>256</cell><cell>143.04</cell></row><row><cell>SWAGAN-Bi</cell><cell>256</cell><cell>80.07</cell></row><row><cell>SWAGAN-Bi w/ NWD</cell><cell>256</cell><cell>129.65</cell></row><row><cell>SWAGAN-NU</cell><cell>256</cell><cell>70.06</cell></row><row><cell>Wavelet at final layer</cell><cell>256</cell><cell>116.06</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Im-age2stylegan: How to embed images into the stylegan latent space?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4432" to="4441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Ssd-gan: Measuring the realness in the spatial and spectral domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cece</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05535</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The wavelet transform, time-frequency localization and signal analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingrid</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on information theory</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="961" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Watch your up-convolution: Cnn based generative deep neural networks are failing to reproduce spectral distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricard</forename><surname>Durall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janis</forename><surname>Keuper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarik</forename><surname>Dzanic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freddie</forename><surname>Witherden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06465</idno>
		<title level="m">Fourier spectrum discrepancies in deep network generated images</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A hybrid wavelet convolution network with sparse-coding for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1439" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wavelet-srnet: A wavelet-based cnn for multi-scale face super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1689" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Wavelet domain generative adversarial network for multiscale face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="763" to="784" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Performance evaluation of cudnn convolution algorithms on nvidia volta gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Jord?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Valero-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">J</forename><surname>Pe?a</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A deep convolutional neural network using directional wavelets for lowdose x-ray ct reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunhee</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong Chul</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="360" to="375" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Msg-gan: multiscale gradient gan for stable image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Karnewar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06048</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Wavelet-based dual-branch network for image demoir?ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07173</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-level wavelet convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengju</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="74973" to="74985" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attribute-aware face aging with wavelet-based generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11877" to="11886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Large-scale celebfaces attributes (celeba) dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nerf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08934</idno>
		<title level="m">Representing scenes as neural radiance fields for view synthesis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hologan: Unsupervised learning of 3d representations from natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thu</forename><surname>Nguyen-Phuoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Liang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7588" to="7597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Curated face samples generated by our SWAGAN-Bi model without truncation</title>
		<imprint/>
	</monogr>
	<note>Figure 13</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the spectral bias of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Nasim Rahaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Draxler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Nitzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stav</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00951</idno>
		<title level="m">Encoding in style: a stylegan encoder for image-to-image translation</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interpreting the latent space of gans for semantic face editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithin</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10739</idno>
		<title level="m">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deferred neural rendering: Image synthesis using neural textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-level wavelet-based generative adversarial network for perceptual quality enhancement of compressed video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00499</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wavelet pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Photorealistic style transfer via wavelet transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongkyu</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9036" to="9045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Superresolution reconstruction algorithms based on fusion of deep learning mechanism and wavelet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huafeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sichen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenle</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Artificial Intelligence and Pattern Recognition</title>
		<meeting>the 2nd International Conference on Artificial Intelligence and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="102" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Image superresolution using a wavelet-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huafeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sichen</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10213</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
