<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video-based Person Re-identification with Spatial and Temporal Memory Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Eom</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geon</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyup</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Video-based Person Re-identification with Spatial and Temporal Memory Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video-based person re-identification (reID) aims to retrieve person videos with the same identity as a query person across multiple cameras. Spatial and temporal distractors in person videos, such as background clutter and partial occlusions over frames, respectively, make this task much more challenging than image-based person reID. We observe that spatial distractors appear consistently in a particular location, and temporal distractors show several patterns, e.g., partial occlusions occur in the first few frames, where such patterns provide informative cues for predicting which frames to focus on (i.e., temporal attentions). Based on this, we introduce a novel Spatial and Temporal Memory Networks (STMN). The spatial memory stores features for spatial distractors that frequently emerge across video frames, while the temporal memory saves attentions which are optimized for typical temporal patterns in person videos. We leverage the spatial and temporal memories to refine frame-level person representations and to aggregate the refined frame-level features into a sequence-level person representation, respectively, effectively handling spatial and temporal distractors in person videos. We also introduce a memory spread loss preventing our model from addressing particular items only in the memories. Experimental results on standard benchmarks, including MARS, DukeMTMC-VideoReID, and LS-VID, demonstrate the effectiveness of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-Identification (reID) aims at retrieving a person of interest from a set of pedestrian images/videos taken from non-overlapping cameras. Convolutional neural networks (CNNs) have made remarkable advances in imagebased person reID <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b45">46]</ref> over the last decade. Video-based person reID has recently attracted increasing attention in accordance with the prevalence of video capturing systems. Video frames provide rich information to specify a particular person, but they often con-* Corresponding author. <ref type="figure">Figure 1</ref>. Examples of (a) spatial distractors that appear frequently in surveillance videos and (b) prototypes of temporal patterns that provide important clues to predict temporal attentions. tain spatial distractors, e.g., trees, bicycles, and concrete pavers. In particular, person videos, typically cropped by off-the-shelf object detectors from a whole sequence, also have temporal distractors, e.g., misaligned persons across video frames or partial occlusions within a sequence.</p><p>Recent video reID methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5]</ref> attempt to tackle these issues by exploiting spatial and temporal attention modules, which are useful for extracting person representations robust to noisy regions (e.g., background clutter) and temporal variations (e.g., partial occlusions). They, however, do not consider a global view in a sequence <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b10">11]</ref>, suggesting that these approaches may focus on less discriminative parts or video frames. Several works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> instead propose to use non-local <ref type="bibr" target="#b35">[36]</ref> or graph convolutional networks <ref type="bibr" target="#b12">[13]</ref> to capture co-attention over frames. They focus on the shared information across multiple frames to obtain a person representation from a video, taking into account the temporal context. The coattention, however, may concentrate on distracting scene details or partial occlusions, which are often shared in successive video frames, producing an incorrect video representation.</p><p>We present in this paper Spatial and Temporal Memory Networks (STMN) to extract person representations robust to spatial and temporal distractors for video-based person reID. The main idea is based on the following observations: 1) Since video sequences are captured by a stationary camera, it is likely that they constantly contain background clutter such as a playfield, a street light, or concrete pavers in a particular location ( <ref type="figure">Fig. 1(a)</ref>); 2) Temporal patterns, e.g., a person of interest disappears at the end of the sequence ( <ref type="figure">Fig. 1(b)</ref> center) or partial occlusions occur in the first few frames ( <ref type="figure">Fig. 1(b)</ref> right), provide crucial clues to determine which frames we have to focus on (i.e., temporal attentions).</p><p>Based on the observations, we propose to exploit two external memories called spatial and temporal memories. The spatial memory is trained to store spatial distractors that frequently appear across video frames, while the temporal memory is trained to memorize attentions which are optimized for typical temporal patterns in person videos. At test time, we leverage the memories as look-up tables, and ease the difficulty of handling the spatial and temporal distractors from videos of unseen identities. Specifically, we exploit the spatial memory to suppress features for distracting scene details from each frame-level person representation, and the temporal one to aggregate the framelevel person representations focusing more on discriminative frames. We also propose a memory spread loss that encourages our model to access all items in the memories during training. We demonstrate the effectiveness of our method on the MARS <ref type="bibr" target="#b44">[45]</ref>, DukeMTMC-VideoReID <ref type="bibr" target="#b37">[38]</ref>, and LS-VID <ref type="bibr" target="#b15">[16]</ref> datasets. To our best knowledge, this is an early effort that jointly leverages multiple types of memories. The main contributions of our work can be summarized as follows:</p><p>? We introduce a simple yet effective method for videobased person reID, dubbed STMN, which extracts a robust video representation to spatial and temporal distractors using spatial and temporal memories.</p><p>? We propose a memory spread loss that prevents our model from accessing few items repeatedly, encouraging all items in the memories to be used.</p><p>? We achieve the state of the art on standard video reID benchmarks. Ablation studies further validate the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Here, we briefly introduce representative works closely related to ours, and clarify their differences from ours. Video-based person reID. The key for video-based reID is to extract person representations robust to spatial and temporal distractors. Many methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5]</ref> propose to use attention modules for video-based person reID. QAN <ref type="bibr" target="#b21">[22]</ref> uses a temporal attention to aggregate frame-level features, focusing on discriminative frames. DRSA <ref type="bibr" target="#b17">[18]</ref> and STA <ref type="bibr" target="#b4">[5]</ref> additionally use a spatial attention to suppress features for spatial distractors. They, however, assign attentions to each frame without considering whole frames in a sequence, indicating that they may aggregate less discriminative parts or frames in the sequence <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b10">11]</ref>. Recent methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref> propose to use co-attention modules between frames by adopting non-local <ref type="bibr" target="#b35">[36]</ref> or graph convolutional networks <ref type="bibr" target="#b12">[13]</ref>. Specifically, GLTR <ref type="bibr" target="#b15">[16]</ref> adds a co-attention module at the end of backbone CNNs, while M3D <ref type="bibr" target="#b16">[17]</ref>, STE-NVAN <ref type="bibr" target="#b19">[20]</ref> and COSAM <ref type="bibr" target="#b29">[30]</ref> insert multiple co-attention modules into different levels of backbone CNNs, to refine frame-level person representations, considering contextual temporal relations between frames. The work of <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> introduces a hierarchical co-attention module, dividing frames into multiple granularities, to capture discriminative spatial and temporal features from different semantic levels. These approaches highlight shared information between frames, suppressing features from distracting scene details and occlusion, which is useful only when such distractors appear in a few frames. When similar backgrounds and/or occlusion are shared across frames, the features from these distractors are propagated, which rather interferes with retrieving persons. The works of <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b48">49]</ref> propose to use recurrent neural networks (RNNs) for aggregating frame-level person representations robust to temporal distractors. The hidden states of RNNs store the temporal context in previous frames, and allow to aggregate the person representations selectively, based on the context. We also exploit RNNs in STMN, but we do not use them directly to aggregate frame-level representations, which may be suboptimal, since RNNs do not consider a temporal context in whole frames (except at the last time step). We instead leverage RNNs to encode a temporal pattern of a sequence for accessing a temporal memory.</p><p>Previous works overlook the fact that several scene details and temporal patterns repeatably appear in surveillance videos, which may provide important cues to handle spatial and temporal distractors. STMN stores the scene details and attentions for the temporal patterns in the spatial and temporal memories, respectively, providing person representations robust against the spatial and temporal distractors. Memory network. The work of <ref type="bibr" target="#b36">[37]</ref> first introduces memory networks to handle long-term dependencies for question and answering. They, however, require extra supervisory signals to access the memory, and are not able to be trained end-to-end. The soft addressing technique <ref type="bibr" target="#b31">[32]</ref> addresses these problems by using attention maps to access the memory. Key-value memory networks <ref type="bibr" target="#b24">[25]</ref> propose to adopt different encodings for accessing and reading operations, where they address relevant memory items by keys, and their corresponding values are subsequently returned. Recently, many computer vision methods exploit memory networks for, e.g., one-shot learning <ref type="bibr" target="#b0">[1]</ref>, video object segmentation <ref type="bibr" target="#b25">[26]</ref>, domain adaptation <ref type="bibr" target="#b47">[48]</ref>, image colorization <ref type="bibr" target="#b41">[42]</ref>, and anomaly detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>. Our work also leverages memory networks but for recording features for distracting scene details and temporal attentions. By using the memory networks, we are able to extract person representations robust against spatial and temporal distractors. In addition, we propose a memory spread loss to penalize our model when it keeps accessing particular items only, while other items remain unused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we provide a brief overview of our approach to exploiting spatial and temporal memories for video-based reID (Sec. 3.1). We then present a detailed description for a network architecture (Sec. 3.2) and training losses (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>STMN mainly consists of three components: an encoder, a spatial memory <ref type="figure" target="#fig_0">(Fig. 2)</ref>, and a temporal memory ( <ref type="figure">Fig. 3</ref>). For each frame, the encoder extracts a person representation and two query maps, where each query is used to access either spatial or temporal memories. The spatial memory stores features for scene details, frequently appearing across video frames, such as street lights, trees, and concrete pavers. We extract such features from the spatial memory using the corresponding query map, and use them to refine the person representation, removing information that interferes with identifying persons. The temporal memory saves attentions optimized for typical temporal patterns that repeatably occur in person videos. We access the temporal memory with the corresponding query map, and use the output to aggregate the refined frame-level features into a sequence-level person representation. We train our model end-to-end using memory spread, triplet, and cross-entropy terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network architecture</head><p>Encoder. The encoder takes a video sequence F i | L i=1 as an input, where F i is the i-th frame of the sequence, and L is the total number of frames. We exploit ResNet <ref type="bibr" target="#b6">[7]</ref> cropped at conv4 layer as our backbone network, where the network parameters are pre-trained for ImageNet classification <ref type="bibr" target="#b13">[14]</ref>. We add three heads on top of the backbone network to extract feature maps for each frame: a frame- level person representation f o i , and query maps, q s i and q t i , for accessing the spatial and temporal memories, respectively. Each feature map has a size of D?H ?W , where D, H, and W are the number of channels, height, and width, respectively. We denote by f o i,k , q s i,k , and q t i,k individual features of size D at position k, where k ? {1, 2, ..., K} and K = H ? W . Spatial memory. Frame-level person representations extracted by the encoder may contain features for distracting scene details (e.g., trees, concrete paver, bicycles, or cars), which may prevent distinguishing different pedestrians in similar scenes. To handle this problem, we refine the framelevel person representations using a spatial memory <ref type="figure" target="#fig_0">(Fig. 2)</ref>.</p><p>The spatial memory has a key-value structure, and contains M items. The values v s ? R D?M encode distracting scene details over the video sequence, while the keys k s ? R D?M are used to access corresponding values. We denote by k s n ? R D and v s n ? R D each key and value in the memory, respectively, where n ? {1, 2, ..., M }. The spatial memory takes a person representation f o i ? R D?K and a query map q s i ? R D?K of the frame F i as inputs. Since different parts of the input frame may contain distinct scene details, we access the memory with individual components of the input query map, q s i,k ? R D . Specifically, we compute cosine similarities between the query q s i,k and all keys k s in the memory, resulting in a correlation map of size 1 ? M . We then normalize it as follows:</p><formula xml:id="formula_0">a s i,k,n = exp((q s i,k ) T k s n ) M n =1 exp((q s i,k ) T k s n )</formula><p>.</p><p>(</p><p>The matching probability a s i,k,n represents a likelihood that the scene detail recorded in the n-th memory item exists in the k-th position of the i-th frame. The memory outputs a weighted average of values v s n using the corresponding probabilities a s i,k,n as follows: <ref type="figure">Figure 3</ref>. The temporal memory takes a sequence of query maps q t i | L i=1 and the person representations f s i | L i=1 that are refined by the spatial memory as inputs. We aggregate the query maps by using global average pooling and LSTM modules, and use the output to address the memory. The memory outputs temporal attentions o t , and the attentions are used to aggregate the frame-level representations into a sequence-level one. (Best viewed in color.)</p><formula xml:id="formula_2">o s i,k = M n=1 a s i,k,n v s n ,<label>(2)</label></formula><p>where the output of the spatial memory, o s i,k , contains uninformative features, that interfere with identifying persons, for the k-th position of the i-th frame. We use the output of the spatial memory to refine the person representation as follows:</p><formula xml:id="formula_3">f s i,k = f o i,k ? BN(o s i,k ).<label>(3)</label></formula><p>Motivated by <ref type="bibr" target="#b35">[36]</ref>, we use a batch normalization (BN) layer to adjust the distribution gap between outputs from the encoder and the spatial memory. Note that our spatial memory is similar to non-local networks <ref type="bibr" target="#b35">[36]</ref> in that they both refine input features in a residual manner. However, ours is clearly different from the non-local networks. Keys and values in our method are external parameters stored in the memory, and they are updated by backpropagation during training in order to memorize the scene details. On the contrary, keys, queries, and values in the non-local networks are computed from input features, similar to a self-attention method <ref type="bibr" target="#b32">[33]</ref>.</p><p>Temporal memory. The refinement process using the spatial memory operates on each frame independently, which is not capable of capturing temporal contexts in video sequences. This may lead our framework susceptible to occlusion or misalignment between frames. To address this problem, we propose to use an additional temporal memory network <ref type="figure">(Fig. 3)</ref>. The temporal memory also has a key-value structure, and contains N items, where the keys k t ? R D?N encode prototypes of temporal patterns that repeatably appear in person videos, and the values v t ? R L?N memorize temporal attentions which are optimized for the corresponding temporal patterns. We denote by k t n ? R D and v t n ? R L each key and value in the memory, respectively, where n ? {1, 2, ..., N }. The temporal memory takes a sequence of query maps q t i | L i=1 and the person representations refined by the spatial memory f s i | L i=1 as inputs. We first encode a temporal context of a given sequence, e.g., the occlusion arises in the middle frame, using the query maps. Concretely, we spatially aggregate the input query maps by a global average pooling (GAP), and feed them into a long short-term memory (LSTM) <ref type="bibr" target="#b9">[10]</ref> as follows:</p><formula xml:id="formula_4">q t = LSTM([GAP(q t 1 ), GAP(q t 2 ), ..., GAP(q t L )]),<label>(4)</label></formula><p>where q t ? R D is an output of the last time step, representing the temporal context of the sequence. We then use the temporal context q t to access the temporal memory in a similar way to the spatial one as follows:</p><formula xml:id="formula_5">a t n = exp((q t ) T k t n ) N n =1 exp((q t ) T k t n ) ,<label>(5)</label></formula><p>where a t n represents a probability that the encoded temporal context q t belongs to the temporal pattern stored in the nth memory item k t n . We synthesize a temporal attention specific for the given sequence by taking weighted average over the values with the corresponding probability a t n as follows:</p><formula xml:id="formula_6">o t = N n=1 a t n v t n ,<label>(6)</label></formula><p>where the memory output o t ? R L represents the temporal attention, and o t i , the i-th element of the output, indicates the relative importance of the i-th frame in the sequence. We then apply a softmax function on the temporal attention o t , and use it to aggregate the refined frame-level features f s i as follows:</p><formula xml:id="formula_7">f t = L i=1? t i GAP(f s i ),<label>(7)</label></formula><formula xml:id="formula_8">where? t i = exp(o t i )/ L i =1 exp(o t i )</formula><p>, and f t is our final person representation for the input video sequence F i | L i=1 . Note that previous methods, e.g., <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref>, decide which frames to focus on during temporal fusion based on person representations. This may enforce the representations to encode temporal contexts as well as identity-related cues, preventing the representations from being discriminative, particularly when video sequences of different identities contain similar temporal contexts. In our framework, on the contrary, person representations are decoupled from encoding temporal contexts, where query maps q t i and keys in the temporal memory, k t , encode such contexts. This encourages our model to extract person representations focusing on information that is useful for discriminating different identities, leading to performance gains on the reID task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training loss</head><p>We use two terms to train our model end-to-end as follows:</p><formula xml:id="formula_9">L total = L S + L ID ,<label>(8)</label></formula><p>where we denote by L S and L ID memory spread and identification losses, respectively. The memory spread term penalizes our model when it accesses a particular memory item only, while the identification term allows to extract discriminative person representations from video sequences. The detailed descriptions of each loss are presented in the following. Memory spread term. We denote by A s ? R LKB?M and A t ? R B?N matching probability maps for the spatial and temporal memories, respectively, in a mini-batch, where B is the number of sequences in the mini-batch. Note that we address the spatial and temporal memories LKB and B times for each mini-batch, respectively. Since we do not have extra supervisory signals except identification labels, we do not know which key should be matched to the input query. In this context, our model may address particular keys continually, while others are left unused <ref type="figure" target="#fig_1">(Fig. 4  left)</ref>. This causes memories to produce similar outputs regardless of input frames or sequences. To address this problem, we propose a memory spread loss as follows:</p><formula xml:id="formula_10">L S = M n=1 [ min(a s n ) ? max(a s n ) + ? ] + +[ min(a t n ) ? max(a t n ) + ? ] + ,<label>(9)</label></formula><p>where a s n ? R LKB and a t n ? R B are the n-th column vector of A s and A t , respectively, representing matching probabilities of the n-th key in each memory w.r.t all queries in a mini-batch. min(?) and max(?) return the minimum and maximum values of an input vector. The memory spread loss enforces the minimum and maximum values of a s n and a t n to differ by at least a pre-defined margin ?. This prevents the case when our model keeps addressing a particular memory item ( <ref type="figure" target="#fig_1">Fig. 4 left)</ref>, while encouraging it to access all memory items during training <ref type="figure" target="#fig_1">(Fig. 4 right)</ref>.</p><p>Identification term. Following other person reID methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b2">3]</ref>, we exploit a combination of crossentropy and batch-hard triplet <ref type="bibr" target="#b7">[8]</ref> terms, with identification labels as a supervisory signal. The former encourages our model to learn a person representation f t by focusing on identity-related cues, while the latter enforces the representations of the same identity to be closer to each other than those of different identities in the embedding space. Motivated by a deep supervision technique <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34]</ref>, we also use the frame-level representations f s i | L i=1 to compute the cross-entropy and triplet losses, where global and temporal average pooling are used to aggregate frame-level representations into a sequence-level one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we provide implementation details of STMN (Sec. 4.1), and show ablation studies and visual analysis on spatial and temporal memories to validate the effectiveness of STMN (Sec. 4.2). Lastly, we compare our method with the state of the art (Sec. 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>Dataset and evaluation metric. We evaluate our model on MARS <ref type="bibr" target="#b44">[45]</ref>, DukeMTMC-VideoReID <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b37">38]</ref> (abbreviated as "DukeV"), and LS-VID <ref type="bibr" target="#b15">[16]</ref>, following the standard protocol of each dataset. Note that we do not use PRID <ref type="bibr" target="#b8">[9]</ref> and iLIDS-VID <ref type="bibr" target="#b34">[35]</ref> for evaluation, since they contain few sequences captured with two cameras only. We report cumulative matching characteristics at rank-1 and mean average precision (mAP) for quantitative comparisons.</p><p>Training. We train our model end-to-end for 200 epochs using the Adam <ref type="bibr" target="#b11">[12]</ref> optimizer, where ? 1 and ? 2 are set to 0.9 and 0.999, respectively. The learning rate, initially set to 1e-4, is reduced by a factor of 10 for every 50 epochs. To train our model, we randomly choose 8 identities, and sample 4 sequences for each identity. Following the restricted random sampling (RRS) strategy <ref type="bibr" target="#b17">[18]</ref>, we then divide each sequence into L chunks, and randomly choose one frame from each chunk. We resize input frames into the size of 256 ? 128, and augment them with horizontal flipping and random erasing <ref type="bibr" target="#b46">[47]</ref>.</p><p>Hyperparameter. To set the sizes of spatial and temporal memories, M and N , the pre-defined margin ? in the memory spread loss, and the length of an input sequence L, we divide the training set of MARS <ref type="bibr" target="#b44">[45]</ref> into two subsets. Specifically, we randomly divide identities in the training  <ref type="table">Table 1</ref>. Quantitative comparison for variants of our model on MARS <ref type="bibr" target="#b44">[45]</ref>, DukeV <ref type="bibr" target="#b37">[38]</ref> and LS-VID <ref type="bibr" target="#b15">[16]</ref>. Numbers in bold indicate the best performance and underscored ones are the second best. SM: spatial memory; TM: temporal memory. <ref type="figure">Figure 5</ref>. Matching probability maps of spatial and temporal memories, when they are trained (a) without and (b) with the memory spread loss. We randomly select 10 query features from a gallery set of MARS <ref type="bibr" target="#b44">[45]</ref>. We can see that the memory spread loss encourages our model to access all items in the memories. We fix all hyperparameters, and train our model on the training splits of MARS <ref type="bibr" target="#b44">[45]</ref>, DukeV <ref type="bibr" target="#b37">[38]</ref> and LS-VID <ref type="bibr" target="#b15">[16]</ref>. Please refer to the supplementary materials for the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Discussion</head><p>Ablation study. We show in <ref type="table">Table 1</ref> an ablation study of our model on MARS <ref type="bibr" target="#b44">[45]</ref>, DukeV <ref type="bibr" target="#b37">[38]</ref>, and LS-VID <ref type="bibr" target="#b15">[16]</ref> in terms of rank-1 accuracy(%) and mAP(%). For the baseline, we use the same network architecture as the encoder, while removing two heads for query maps, and exploit global and temporal average pooling to aggregate person representations. From 1 and 3 , we can clearly see that the feature refinement process using a spatial memory boosts the reID performance, while 1 and 5 demonstrate that using a temporal memory for aggregating frame-level representations gives better results. 3 , 5 , and 7 further show <ref type="figure">Figure 6</ref>. Top-5 retrieved frames from a gallery set of MARS <ref type="bibr" target="#b44">[45]</ref>, whose query features have high matching probabilities with a key of the spatial memory. <ref type="figure">Figure 7</ref>. The magnitude difference of person representations before and after the refinement using the spatial memory.</p><p>the spatial and temporal memories are complementary to each other. Note that LS-VID provides person videos with more diverse spatial and temporal distractors than the other datasets. It contains videos of three times larger number of identities than MARS and DukeV, which are captured under two times larger number of cameras. Our memories help the baseline model to handle such distractors, giving the significant performance gains on LS-VID. The performance gains by the memories are relatively small on DukeV, since it contains person videos that are manually annotated by humans, i.e., with less distractors, where the simple baseline already gives 95% rank-1 accuracy. By comparing 2 to 3 , 4 to 5 , and 6 to 7 , we can see that enforcing our model to address all memory items during training by a memory spread loss consistently enhances the performance.</p><p>To further verify the effectiveness of the memory spread loss, we visualize matching probability maps of spatial and temporal memories on MARS, when the memories are trained without ( <ref type="figure">Fig. 5(a)</ref>) and with ( <ref type="figure">Fig. 5(b)</ref>) the loss. We randomly choose frames or sequences from a gallery set of MARS, and extract query features, q s i,k and q t , from them. We then compute matching probabilities with keys of the spatial and temporal memories using Eq. (1) and Eq. (5), respectively. We can see that the memory spread loss encourages our model to leverage all items in the memories, while preventing it from accessing particular items only. This enables our spatial and temporal memories to produce diverse outputs depending on both frame-level scene details and sequence-level temporal contexts. Spatial memory. In <ref type="figure">Fig. 6</ref>, we visualize video frames whose query features q s i,k have high matching probabilities with randomly chosen keys from the spatial memory (see Eq. (1)). We can observe that each key retrieves the video frames that share similar scene details such as a play field (1st row), a street light (2nd row), or concrete pavers (3rd row). This verifies that our model accesses the spatial memory depending on the scene details for each video frame. The spatial memory aggregates the features Results with green boxes have the same identity as the query, while those with red boxes do not. We show the first frame of sequences for the purpose of visualization. (Best viewed in color.) <ref type="figure">Figure 9</ref>. Examples of sequences from a gallery set of MARS <ref type="bibr" target="#b44">[45]</ref>, whose query features show high matching probabilities with a particular key in the temporal memory. We also visualize temporal attentions stored in corresponding values of the memory. (Best viewed in color.) for scene details, and we use them to refine frame-level person representations (see Eq. <ref type="formula" target="#formula_3">(3)</ref>). To see the effect of the refinement, we visualize in <ref type="figure">Fig. 7</ref> the magnitude difference of person representations, overlaid on input images, using bilinear interpolation, before and after the refinement, i.e.,</p><formula xml:id="formula_11">f s i,k 2 ? f o i,k 2 k ? H ? W .</formula><p>We can observe that the differences mainly occur from distracting scene details, e.g., concrete pavers, playfield, or street lights, implying that the memory suppresses features from them. Note that the video frames in the 1st row of <ref type="figure">Fig. 7</ref> share the same background while pedestrians appear in different positions. However, regardless of the person's position, the memory removes features from background clutter. <ref type="figure" target="#fig_3">Figure 8</ref> compares retrieval results when we use initial person representations f o i (top) and refined ones f s i (bottom). Note that we use global and temporal average pooling to obtain the person representations, instead of exploiting the temporal memory, to see the effect of the refinement by the spatial memory. We can see that the initial representations retrieve person sequences of different identities from the query but with similar scene details (e.g., a play field). On the other hand, the refined ones retrieve person sequences with the same identity as the query correctly, regardless of background clutter in each frame. This also suggests that the refinement process using the spatial memory suppresses information of the scene details in person representations. Temporal memory. We visualize in <ref type="figure">Fig. 9</ref> person sequences whose query features q t show high matching prob- <ref type="figure">Figure 10</ref>. Examples of temporal attentions generated by the temporal memory on the test split of MARS <ref type="bibr" target="#b44">[45]</ref>. Note that the sequence on the right side of the 3rd row is made by reordering the sequence on the left side. (Best viewed in color.) abilities (see Eq. (5)) with randomly chosen keys from the temporal memory. We also visualize the corresponding values of the memory in the below. We can observe that each key retrieves the sequences with similar temporal patterns, e.g., persons disappear at the end of the sequence (left) or appear in all frames with similar appearances (right), and the values highlight discriminative frames in each sequence. This verifies that the keys encode prototypes of temporal patterns in person videos, and the values of the memory store temporal attentions which are optimized for the corresponding temporal patterns. Note that we aggregate individual values of the memory with matching probabilities between keys and input query features to synthesize temporal attentions specific for input person sequences (see Eq. <ref type="formula" target="#formula_6">(6)</ref>). <ref type="figure">Figure 10</ref> shows examples of the aggregated temporal attentions. When the temporal memory takes a sequence with less temporal distractors as an input, the memory generates similar attentions for all frames (1st row). Namely, the memory works similarly to the temporal average pooling which fuses video frames with equal probabilities. On the other hand, in case of a sequence with severe temporal distractors, e.g., misalignments between frames (2nd row) or occlusions (3rd row), the memory lowers attentions for the frames where such variations occur, suggesting that the temporal memory allows our model to extract person representations robust to the temporal variations. Note that we can replace the temporal memory with multilayer perceptrons (MLPs) by directly regressing attentions from the encoded context q t <ref type="bibr" target="#b17">[18]</ref>. To compare this approach with ours, we use two-layer perceptrons whose sizes are 2048 ? N and N ? L, respectively, which makes the number of parameters the same as ours. We found that MLP often produce attentions that focus more on few certain frames, ignoring features from the other frames (see the last row of <ref type="figure">Fig. 10</ref>), and this leads to large performance drops, 1.3/1.5 (R-1/mAP) on MARS. The result are similar, even the size of MLPs increase (e.g., 2048x512 and 512x6). These show the effectiveness of our approach that predicts attentions by discovering repetitive temporal patterns in a dataset and searching the most relevant patterns to the context of an input video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the state of the art</head><p>We compare in <ref type="table" target="#tab_1">Table 2</ref> STMN with the state of the art in terms of rank-1 accuracy and mAP on MARS <ref type="bibr" target="#b44">[45]</ref>, DukeV <ref type="bibr" target="#b37">[38]</ref> and LS-VID <ref type="bibr" target="#b15">[16]</ref>. We found that previous methods compare their performance using different test strategies. For fair comparisons, we classify them into two groups, depending on whether they follow RRS or allframes strategies for evaluation. The methods, e.g., <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, which follow the RRS strategy <ref type="bibr" target="#b17">[18]</ref>, divide an input video into L chunks of equal length. They then sample the first frame of each chunk to obtain a sequence of L frames, regardless of the total number of frames. On the other hand, several works use all frames in an input video by grouping them into multiple sequences of length L. They extract a person representation from each sequence independently, and average all the representations to represent the input video. Note that we reproduce TCLNet <ref type="bibr" target="#b10">[11]</ref> and MGH <ref type="bibr" target="#b39">[40]</ref> to evaluate them on the both strategies. Using all frames in given videos to extract person representations does give performance gains for TCLNet, MGH, and STMN. This, however, is far from practical usages in that it runs, e.g., 35 times slower than the RRS strategy on LS-VID, requiring more than three hours for evaluation using a Titan RTX 2080Ti GPU. Furthermore, the time for searching persons increases linearly as the number of video frames increases.</p><p>From <ref type="table" target="#tab_1">Table 2</ref>, we have following observations: 1) On the RRS setting, STMN sets a new state of the art on the three benchmarks. The results of STMN using the RRS even surpass those of previous methods, e.g., COSAM <ref type="bibr" target="#b29">[30]</ref>, M3D <ref type="bibr" target="#b16">[17]</ref>, and GLTR <ref type="bibr" target="#b15">[16]</ref> on the all-frames setting. This suggests that STMN already extracts essential information for identifying a person with sampled frames only, showing its efficiency over the previous methods. This characteristic is crucial for massive surveillance systems which need to search for a person of interest from lots of videos in a very short time; 2) DRSA <ref type="bibr" target="#b17">[18]</ref> leverages attention modules for handling spatial and temporal distractors in videos, while STMN exploits spatial and temporal memories instead. The performance gap between these two methods demonstrates the superiority of our framework over the attentionbased method; 3) Co-attention-based methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40]</ref> may propagate non-discriminative features across frames when multiple frames share common background clutter or occlusion. As a result, there are large performance gaps between these methods and STMN on LS-VID, the most challenging dataset, which contains sequences captured under various conditions (e.g., lighting/background changes, indoor/outdoor changes) with frequent occlusions; 4) TCLNet <ref type="bibr" target="#b10">[11]</ref> and MGH <ref type="bibr" target="#b39">[40]</ref> are the most recently in-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>MARS DukeV LS-VID rank-1 mAP rank-1 mAP rank-1 mAP RRS EUG <ref type="bibr" target="#b37">[38]</ref> 62.7 42.5 72.8 63.2 --SeeForest <ref type="bibr" target="#b48">[49]</ref> 70.6 50.7 ----QAN <ref type="bibr" target="#b21">[22]</ref> 73.7 51.7 ----DRSA <ref type="bibr" target="#b17">[18]</ref> 82.3 65.8 ----CSA <ref type="bibr" target="#b1">[2]</ref> 86.3 76.1 ----STE-NVAN <ref type="bibr" target="#b19">[20]</ref> 88.9 81.2 95.2 93.5 (72.1) (56.6) TCLNet <ref type="bibr" target="#b10">[11]</ref> (88.5) (80.9) (95.0) (92.8) (75.0) (60.2) MGH <ref type="bibr" target="#b39">[40]</ref> (89.  <ref type="bibr" target="#b37">[38]</ref>, and LS-VID <ref type="bibr" target="#b15">[16]</ref> in terms of rank-1 accuracy(%) and mAP(%). Numbers in bold indicate the best performance and underscored ones are the second best. Results in brackets are obtained with the source codes provided by the authors.</p><p>troduced video reID methods. They boost the reID performance using a temporal saliency erasing module and a multi-granular hypergraph, respectively. They, however, give results worse than STMN on the RRS setting. By using all frames, they may show comparable results to STMN, however note that the size of a person representation is much larger than that of STMN (TCLNet:4, 096, MGH:5, 120 vs. STMN:2, 048).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a novel video-based person reID method, dubbed STMN, that extracts robust person representations against spatial and temporal distractors in videos. To this end, we have proposed to exploit two external memory networks, spatial and temporal memories, to refine frame-level representations and to aggregate them into a sequence one, focusing on discriminative frames. We have also proposed a memory spread loss that prevents certain memory items from remaining redundant. We have shown that STMN achieves state-of-the-art performance on standard video-based reID benchmarks, and demonstrated the effectiveness of each component of our method with an extensive ablation study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The spatial memory takes a person representation f o i ? R D?K and a query map q s i ? R D?K of the i-th frame as inputs. We access the memory based on the matching probability between the query feature q s i,k ? R D and keys k s , and use the output to refine the input representation f o i,k ? R D . (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Example of matching probability maps for the case when our model addresses a particular memory item only (left) and the case when it uses all items in the memory (right). (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(Best viewed in color.) set into two subsets of sizes 500/125, and use corresponding 7075/1223 sequences as training/validation splits. For query sequences, we randomly select 200 sequences from the validation split. For the sizes of memories, we perform a grid search over (M, N ) pairs, where M, N ? {5, 10, 20}. We choose a pair of M = 10 and N = 5 for our final model, which shows the best result in terms of the mean and standard deviation of rank-1 accuracy and mAP for five trials. For the margin ? and the sequence length L, we also use a grid search over ? ? {0.1, 0.3, 0.5, 0.7, 1.0} and L ? {4, 6, 8, 10}, respectively, setting ? = 0.3 and L = 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Comparison of top-10 retrieval results on the test split of MARS [45] using the original frame-level features f o i (top) and refined ones f s i (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with the state of the art on MARS [45], DukeV</figDesc><table><row><cell></cell><cell></cell><cell cols="4">2) (83.4) (95.3) (93.4) (75.3) (58.9)</cell></row><row><cell></cell><cell>STMN</cell><cell cols="4">89.9 83.7 96.7 94.6 80.6 66.6</cell></row><row><cell></cell><cell>COSAM [30]</cell><cell cols="3">83.7 77.2 94.4 94.0</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>STMP [23]</cell><cell>84.4 72.7</cell><cell>-</cell><cell>-</cell><cell>56.8 39.1</cell></row><row><cell>All frames</cell><cell cols="5">M3D [17] Part-Aligned [31] 84.7 75.9 84.4 74.1 STA [5] 86.3 80.8 96.0 95.0 ----GLTR [16] 87.0 78.5 96.3 93.7 63.1 44.3 57.7 40.1 ----</cell></row><row><cell></cell><cell>TCLNet [11]</cell><cell cols="4">(89.1) (83.4) (96.7) (95.6) (81.0) (67.2)</cell></row><row><cell></cell><cell>MGH [40]</cell><cell cols="4">(89.4) (85.3) (95.0) (94.6) (79.6) (61.8)</cell></row><row><cell></cell><cell>STMN</cell><cell cols="4">90.5 84.5 97.0 95.9 82.1 69.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Memory matching networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Temporal coherence or temporal motion: Which is more critical for video-based person re-identification? In ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning disentangled representation for robust person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Eom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">STA: Spatial-temporal attention for large-scale video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Person re-identification by descriptive and discriminative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCIA</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>computation. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal complementary learning for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Global-local temporal representations for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiscale 3D convolution network for video based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diversity regularized spatiotemporal attention for videobased person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatially and temporally efficient non-local attention network for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Yi</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hydraplus-net: Attentive deep features for pedestrian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quality aware network for set to set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial and temporal mutual promotion for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent convolutional network for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niall</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><surname>Martinez Del Rincon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning memory-guided normality for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoun</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Athira Nambiar, and Anurag Mittal. Co-segmentation inspired attention networks for videobased person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arulkumar</forename><surname>Subramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Endto-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Training deeper convolutional networks with deep supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02496</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Person re-identification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploit the unknown gradually: One-shot video-based person re-identification by stepwise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Person re-identification via recurrent feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning multi-granular hypergraphs for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spatial-temporal graph convolutional network for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinrui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qize</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Cong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Coloring with limited data: Few-shot colorization via memory augmented networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyojin</forename><surname>Bahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyo</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyuk</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-granularity reference-aided attentive feature aggregation for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SpindleNet: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">MARS: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>In ECCV. 2, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">See the forest for the trees: Joint spatial and temporal recurrent neural networks for video-based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
