<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic2Graph: Graph-based Multi-modal Feature for Action Segmentation in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Cheng Kung University</orgName>
								<address>
									<postCode>701</postCode>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hsuan</forename><surname>Tsai</surname></persName>
							<email>tsaimh@csie.ncku.edu.tw.</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Manufacturing Information and Systems</orgName>
								<orgName type="institution">National Cheng Kung University</orgName>
								<address>
									<postCode>701</postCode>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Meng-Hsun</forename><surname>Tsai</surname></persName>
							<email>phtsai@mail.ncku.edu.tw.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Cheng Kung University</orgName>
								<address>
									<postCode>701</postCode>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic2Graph: Graph-based Multi-modal Feature for Action Segmentation in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video action segmentation</term>
					<term>graph neural networks</term>
					<term>computer vision</term>
					<term>semantic features</term>
					<term>multi-modal fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video action segmentation and recognition tasks have been widely applied in many fields. Most previous studies employ large-scale, high computational visual models to understand videos comprehensively. However, few studies directly employ the graph model to reason about the video. The graph model provides the benefits of fewer parameters, low computational cost, a large receptive field, and flexible neighborhood message aggregation. In this paper, we present a graph-based method named Semantic2Graph, to turn the video action segmentation and recognition problem into node classification of graphs. To preserve fine-grained relations in videos, we construct the graph structure of videos at the framelevel and design three types of edges: temporal, semantic, and self-loop. We combine visual, structural, and semantic features as node attributes. Semantic edges are used to model long-term spatio-temporal relations, while the semantic features are the embedding of the label-text based on the textual prompt. A Graph Neural Networks (GNNs) model is used to learn multimodal feature fusion. Experimental results show that Semantic2Graph achieves improvement on GTEA and 50Salads, compared to the state-of-the-art results. Multiple ablation experiments further confirm the effectiveness of semantic features in improving model performance, and semantic edges enable Semantic2Graph to capture long-term dependencies at a low cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>HE goal of the video action segmentation and recognition tasks <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> is to categorize the action labels of each video frame based on clip-level [2]- <ref type="bibr" target="#b5">[6]</ref> or frame-level <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>. It is widely applied in the fields of human behavior analysis <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, video surveillance <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, video comprehension <ref type="bibr" target="#b8">[9]</ref>, human-computer interaction <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, autonomous driving <ref type="bibr" target="#b5">[6]</ref>, among others, and has attracted a great deal of study interest in recent years. Owing to the fruitful progress of deep learning in computer vision tasks, early methods utilize video-based vision models for action segmentation and recognition <ref type="bibr" target="#b4">[5]</ref>. In video-based vision models, video is viewed as a sequence of RGB frames. Common methods of action classification include expanding sliding windows (receptive fields) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, increasing network dept <ref type="bibr" target="#b5">[6]</ref>, introducing attention mechanisms, and stacking modules <ref type="bibr" target="#b5">[6]</ref>, which extract spatio-temporal varying local and global features from videos to represent complex relations in videos <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>. However, their common shortcomings are large-scale, inflexibility, information forgetting, multiple streams, and costly computing <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>With the advances in Graph Neural Networks (GNNs), numerous video-based graph models have been implemented for video action segmentation and recognition over the last few years <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Most video-based graph model methods are skeleton-based, which extract skeleton graphs from video frames relying on human pose information instead of RGB pixel data <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>. A video is represented as a sequence of skeleton graphs. This method, however, degrades visual features and is only appropriate for specific scenes. Consequently, several studies <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> transform a video (or video clip) into a graph in which each node represents a video frame (or video clip). These methods are graph-based in this paper. The graph-based method has a larger receptive field and lower cost to capture global features <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Furthermore, the graph-based method enables also capture non-sequential temporal relations.</p><p>The feature representation of videos is crucial for graphbased methods to comprehensively reason the given video <ref type="bibr" target="#b3">[4]</ref>. According to feature classification of computer vision, there are low-level, middle-level, and high-level features <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Low-level features are RGB features of an image <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>. It has the advantage of containing precise target locations, but the feature information is relatively discrete, such as color, edge, outline, texture, and shape. Middle-level features are spatio-temporal properties of objects in an image, such as the state or position changes of an object in time or space <ref type="bibr" target="#b7">[8]</ref>. High-level features are image expressed semantics that facilitates human comprehension <ref type="bibr" target="#b7">[8]</ref>. Its richness in features, although it comprises coarse target locations. For instance, symbols, audio, text, object, image language descriptions, video captions, etc. In recent years, many works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> have demonstrated that multi-modal features that combine low-level, middle-level, and high-level features substantially improve the accuracy of results <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Therefore, multi-modal features are adopted to facilitate video reasoning, especially for natural language supervised information <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> in this paper.</p><p>In this paper, we employ a graph-based method named Semantic2Graph, to turn the video action segmentation and recognition task into node classification. In particular, the graph representation of the video is constructed at the framelevel. To preserve the intricate relations in the video frame sequences, this graph has three types of directed edges, including temporal edges, semantic edges, and self-loop edges. Moreover, node attributes are multi-modal, combining visual, structural, and semantic features. Node neighborhood information is reflected through structural features. Notably, semantic features are the text embedding of sentences encoded by a visual language pre-trained model <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, and the sentence is label-text extended using prompt-based method <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. A GNNs model is employed for multi-modal feature fusion to predict node action labels. Our contributions are summarized as follows:</p><p>? We present a small-scale, low-cost, and flexible graph model for video action segmentation and recognition tasks. ? We describe a method for constructing the graph structure of videos and design three kinds of edges to model fine-grained relations in videos. Especially, the semantic edges reduce the cost of capturing long-term temporal relations. ? We introduce multi-modal features combining visual, structural, and semantic features as node attributes. In particular, the semantic features of textual promptbased are more effective than only label words. ? We use a GNNs model to fuse visual, structural, and semantic features, and demonstrate that middle-level and high-level features are the key to further model performance improvement. ? Experimental results demonstrate the potential and effectiveness of our method, which achieves state-ofthe-art results on the GTEA and 50Salads datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph Representation of Video</head><p>Most of the prior work <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b28">[29]</ref> utilizes visual models (such as CNN, 2D-CNNs, 3D-CNNs, VIT, etc.) to perform comprehensive video action segmentation and recognition. Vision models treat video as a sequence of RGB frames. They model complex and meaningful relations in videos through spatio-temporal feature extraction, attention mechanism, module stacking, and network depth <ref type="bibr" target="#b5">[6]</ref>. To obtain global or long-term relation, however, costly, and computationally intensive models are required.</p><p>Several studies suggest that transforming video into a graph-structure makes visual perception more flexible and efficient <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b29">[30]</ref>. A well-defined graph representation is critical for model performance <ref type="bibr" target="#b15">[16]</ref>. Nodes, edges, and attributes are the fundamental elements of a graph. Common methods for obtaining nodes from a video include clip-level (snippet-level) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> and frame-level <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref> methods, among others <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Zeng et al. <ref type="bibr" target="#b4">[5]</ref> introduced a Graph Convolution Module (GCM) that constructs a graph at the snippet-level for the temporal action localization in videos. GCM extracts action units of interest from a video and represents each action unit as a node. Temporal Contrastive Graph Learning (TCGL) was proposed by Liu et al. <ref type="bibr" target="#b6">[7]</ref> handle video action recognition and retrieval issues. To construct the graph at the frame-level, the video is clipped into snippets consisting of consecutive frames, and each snippet is split into frame sets of equal length. A node in the graph represents a frame in the frame set. Zhang et al. <ref type="bibr" target="#b3">[4]</ref> developed a Multimodal Interaction Graph Convolutional Network (MIGCN), which constructed a graph containing clip nodes from videos and word nodes from sentences. Regardless, clip-level methods sacrifice the video's fine-grained features in comparison to frame-level methods. In addition, other methods are only appropriate for specific task scenarios, such as MIGCN, which demands sentences to obtain word nodes.</p><p>For defining edges, the temporal relation of the video is usually used as the baseline edge [4]- <ref type="bibr" target="#b6">[7]</ref>. In TCGL, edges are temporal prior relation (that is, the correct sequence of frames in a frame-set) <ref type="bibr" target="#b6">[7]</ref>. Since the graph with only temporal edges loses the semantic relation implicit in videos. Therefore, some studies also introduce semantic edges <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> or other edges <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. GCM added three types of edges to the graph, including contextual edges, surrounding edges, and semantic edges, to obtain contextual information, neighborhood information, and action similarity information from videos, respectively. To preserve the complex relations of video in the graph, MIGCN designs three types of edges. Specifically, the Clip-clip edge reflects the temporal adjacency relation of videos; the Word-word edge reflects the syntactic dependency between words; the Clip-word edge enhances the transmission of information between various modalities <ref type="bibr" target="#b3">[4]</ref>.</p><p>For attributes, the RGB feature is an essential basic attribute <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. In GCM, the attributes of the nodes are the fused embedding of image features from all frames in an action unit. For TCGL, node attributes are spatial-temporal features extracted from snippets via C3D, R3D, or R(2+1)D models. Furthermore, they also obtain graph augmentation from different views by masking node features and removing edges. For MIGCN, the initialization of clip node attributes is the visual feature of the clip. Then, use BiGRU (bi-directional GRU network) to encode the semantic information of all clips in the whole video to update clip node attributes <ref type="bibr" target="#b3">[4]</ref>. The initialization of word node attributes is the embedding of Glove, which is then encoded by BiGRU. Some studies also present multi-modal features <ref type="bibr" target="#b3">[4]</ref>, which are detailed upon in Section II. C.</p><p>In this paper, our method adopts the frame-level to construct a graph representation of the video. In this graph, three types of edges, such as temporal edges, semantic edges, and self-loop edges, are added to preserve the complex relations in the video. Moreover, node attributes consist of multi-modal features, which include visual, structural, and semantic features. Semantic features are encoded using a visual-language pre-trained model to encode label-text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-modal Fusion</head><p>In recent years, multi-modal learning has received significant attention as multi-modal features remarkable improve model performance in various tasks <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Audio <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b30">[31]</ref>, appearance <ref type="bibr" target="#b10">[11]</ref> or depth <ref type="bibr" target="#b5">[6]</ref> modality information is introduced in visual tasks. Some research works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref> have shown that incorporating natural language supervision in vision tasks enhance representational power and substantially improve model performance. Similarly, Multi-modal features are also introduced into graph learning tasks <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>For example, Chen et al. <ref type="bibr" target="#b3">[4]</ref> exploited the inter-modal interaction of objects and text to enhance model inference. Bajaj et al. <ref type="bibr" target="#b3">[4]</ref> fuse phrases and visuals to augment intramodal representations to address language grounding. In the temporal language localization problem in video, Zhang et al. <ref type="bibr" target="#b3">[4]</ref> argue that in addition to sequence dependencies, the semantic similarity between video clips and syntactic dependencies between sentence words also contribute to reason video and sentence. They used graph neural networks to explicitly model edges in the graph to learn these intramodal relations. Lin et al. <ref type="bibr" target="#b14">[15]</ref> proposed a multi-modal approach called REMAP, which extracts structured disease relations and text information. from multi-modal datasets to construct a complete disease knowledge graph.</p><p>In this paper, we focus on facilitating node classification learning of video transfer graph representations to integrate the advantages of multi-modal features, especially label-text semantic features, to achieve efficient downstream video action segmentation and recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR APPROACH</head><p>This work aims to address the challenges of video action segmentation and recognition utilizing graph models. To do this, we present a graph-based method named Semantic2Graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notation</head><p>Let V = {v1, ..., vN} denote the set of untrimmed videos. The i-th video vi = {f t ? ? H?W?C } t=1 ?V contains Ti frames, where ft denote the t-th frame with height H, width W and channel C of a video <ref type="bibr" target="#b4">[5]</ref>. Let a directed graph ( , ?) with T nodes denote a graph-structured of video. In ( , ?) , ? ? 1?T is a set of nodes and a node t ? . ? is a set of edges and an</p><formula xml:id="formula_0">edge ? t = ( t , t ' ) ? ? , where ( t , t ' ) means the node t goes into t ' . is weight of an edge, ? {-1, 0, 1}. The node features of the graph are X = {X1, ..., XT} ? ? T? , here D is the dimension of the node feature. = {y t } t=1</formula><p>T is a set of node labels. G = { 1 , . . . , N} denote the set of graphs. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the Semantic2Graph pipeline, which is divided into the following (a) to (e) steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. General Scheme of Our Approach</head><p>Step (a) is visual feature extraction, as shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. The video is split into frames. Semantic2Graph utilizes a 3D convolutional network to extract visual features from video frames for each frame.</p><p>Step (b) is graph construction, as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. Its inputs are frame-set and visual features from step (a) and the detailed video labels. Each frame represents a node. The attribute and label of a node are the visual feature and the action label of the frame, respectively. Nodes with different labels are distinguished by their color. There are three types of edges to maximize the preservation of relations in video frames. The detailed edge construction is described in Section ?. C. The output is a directed graph.</p><p>Step (c) is structure embedding which is to encode the neighborhood information of nodes as structural features, as shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>. The neighborhood information is provided by the directed graph from step (b).</p><p>Step (d) is semantic embedding, as shown in <ref type="figure" target="#fig_0">Fig. 1(d)</ref>. The label text of video frames is expanded into sentences by prompt-based CLIP. Sentences are encoded by a text encoder to obtain semantic features.</p><p>Step (e) is multi-modal feature fusion, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>(e). Its inputs include visual features from step (a), structural features from step (c), semantic features from step (d), and a directed graph from step (b). A weighted adjacency matrix is generated from the directed graph. The multi-modal feature matrix is obtained by concatenating visual features, structural features, and semantic features. A GNNs is selected as the backbone model to learn multi-modal feature fusion. The output is the node's predicted action label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Graph construction of video</head><p>This section is to describe how a video is transformed into our defined graph. The defined graph is a directed graph ( , ?) composed of a set of nodes and a set of edges ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Node</head><p>The input is a set of untrimmed videos. A frame ft is the frame at time slot t in the video. Video v is a set of ft, where the set contains T frames. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref> </p><formula xml:id="formula_1">(b) that video v is represented by a directed graph ( , ?)</formula><p>. A frame f in a video is represented by a node in the directed graph. In other words, a directed graph with T nodes represents a video with T frames.</p><p>For each node, there is a label y converted from the label of the represented frame. Without loss of generality, we assume that the video has pre-obtained frame-level action annotations by some methods, such as frame-based methods <ref type="bibr" target="#b32">[33]</ref>, segment-based methods <ref type="bibr" target="#b4">[5]</ref>, or proposal-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b30">[31]</ref>. In addition, each node has three attributes that are multi-modal features extracted from frames. The details of extracting frame features are subsequently specified in Section ?. D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Edge</head><p>Videos contain not only rich sequential dependencies between frames, but also potential semantic relations, such as captions, context, action interactions, object categories, and more. These dependencies are crucial for a comprehensive understanding of video <ref type="bibr" target="#b3">[4]</ref>.</p><p>To enhance the graph representation learning of videos, there are two types of edges in the directed graph, temporal edges, and semantic edges, as shown in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>. In addition, to enhance the complexity of a graph and preserve the features of the node itself, another type of edge self-loop is added to the directed graph <ref type="bibr" target="#b3">[4]</ref>. Each edge has a weight value determined by the type of edge.</p><p>Temporal edge. It is the baseline edge of the directed graph to represent the sequential order of frames. Intuitively, it reflects the temporal adjacency relation between two neighbor nodes <ref type="bibr" target="#b3">[4]</ref>. In general, the weight of the temporal edge is set to 1. The temporal edge is formulated as</p><formula xml:id="formula_2">= ( , +1 ), ? [1, ? 1] (1) l = 1 (2) Semantic edge.</formula><p>It is to model the potential semantic relations of videos <ref type="bibr" target="#b4">[5]</ref>. Semantic edges are categorized into two types, namely, positive semantic edge and negative semantic edge. Positive semantic edge is to group nodes with identical labels while negative semantic edge is to distinguish nodes with different labels. A simple rule to construct semantic edges is to add either positive or negative edges between any two nodes based on their labels. However, the size of edges is O( 2 ) resulting in the consumption of memory space and computing time. We here propose a semantic edge construction method to optimize the directed graph with minimum edges and sufficient semantic relations of video.</p><p>Each node has a sequence identifier which is the same as the ID of the representative frame. The smaller the ID number, the earlier the frame in a video. For each node, adds a positive edge to the following nodes whose j &gt; i until the label of is different from the label of and add a negative edge to . Positive semantic edge and negative semantic edge are defined as</p><formula xml:id="formula_3">_ = ( , ), _ = 1, , ? [1, ] &gt; = (3) _ = ( , ), _ = ? 1, , ? [1, ] &gt; ? (4) Self-loop edge.</formula><p>It is referring to the settings of the MIGCN model that each node adds a self-loop edge with a weight of 1. During message aggregation, the self-loop edge maintains the node's information <ref type="bibr" target="#b3">[4]</ref>. The self-loop edge is formulated as</p><formula xml:id="formula_4">? = ( , ), ? [1, ]<label>(5)</label></formula><formula xml:id="formula_5">? = 1 (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multi-modal Features</head><p>Video contents are extracted as features and added to nodes in the graph. They are visual, structure, and semantic features, belonging to low-level, middle-level, and high-level features <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, respectively, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Visual feature. It is an image embedding containing RGB and optical-flow information of each frame in a video, as shown in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>. Many visual feature extractors have been developed, such as I3D, C3D, ViT, etc <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b20">[21]</ref>. In this study, we use I3D to extract visual features. The I3D feature extractor takes n consecutive frame inputs of RGB and optical-flow and outputs two tensors with 1024-dimensional features: for RGB and optical-flow streams. Visual features concatenate RGB and optical-flow feature tensors. Visual feature is represented by X ? ? T?D , Dvisual = 2048. X = I3D( t?n , t ) (7) where usually n=64.</p><p>Structure Feature. It is the node embedding transferring features into a low-dimensional space with minimum loss of information of the neighborhood nodes. General structure features are the structure of a graph. In this paper, structure features reflect the structural properties of a graph.</p><p>The structural properties are relational properties of a video, including intrinsic sequence-structure properties and semantic properties. They are preserved by our designed temporal edge and semantic edge. See Section 3.3 for details.</p><p>In computer vision, most of the existing methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref> use Recurrent Neural Networks (RNN) sequence models such as GRU, BiGRU <ref type="bibr" target="#b3">[4]</ref>, LSTM or Transformer <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref> to capture the intrinsic sequence-structure properties of videos, and then to reason the mapping relation between video and action. However, the methods mentioned above are not suitable for graphs. The reason is that they are not good at dealing with non-Euclidean forms of graphs.</p><p>For a graph, the structure features are obtained by using a node embedding algorithm. There are many node embedding algorithms, such as DeepWalk <ref type="bibr" target="#b33">[34]</ref> for learning the similarity of neighbors, LINE <ref type="bibr" target="#b34">[35]</ref> for learning the similarity of firstorder and second-order neighbors, and node2vec <ref type="bibr" target="#b35">[36]</ref> for learning the similarity of neighbors and structural similarity.</p><p>In this paper, node2vec is used to get the structural features, as shown in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>. The structure feature is represented by Xstructure ? ? T?D , where dimension Dstructure = 128. Xstructure = node2vec( ( , ?)) ( 8 ) where the node attributes of the graph ( , ?) are only visual features.</p><p>Semantic feature. It is the language embedding of each frame of a video, such as textual prompt <ref type="bibr" target="#b17">[18]</ref> or the semantic information of label-text <ref type="bibr" target="#b8">[9]</ref>, as shown in <ref type="figure" target="#fig_0">Fig. 1 (d)</ref>. CLIP <ref type="bibr" target="#b17">[18]</ref> and ActionCLIP <ref type="bibr" target="#b8">[9]</ref> are common approaches to getting semantic features. In this study, we use ActionCLIP.</p><p>Following the ActionCLIP model proposed by Wang et al. <ref type="bibr" target="#b8">[9]</ref>, based on the filling locations, the filling function T has the following three varieties:</p><p>? Prefix prompt <ref type="bibr" target="#b8">[9]</ref>: label, a video of action; ? Cloze prompt <ref type="bibr" target="#b8">[9]</ref>: this is label, a video of action;</p><p>? Suffix prompt <ref type="bibr" target="#b8">[9]</ref>: human action of label. ActionCLIP uses the label-text to fill the sentence template Z = {z1, ..., zk} with the filling function to obtain the prompted textual <ref type="bibr" target="#b8">[9]</ref>. ' = ( , Z) (9) Compared to only label words, the textual prompt extends the label-text. A text encoder encodes is to obtain semantic features, which are used as language supervision information to improve the performance of vision tasks. The semantic feature is represented by X ? ? T?D , where dimension Dsemantic = 512. X = Text_Encoder( ') (10) As shown in <ref type="figure" target="#fig_0">Fig. 1 (e)</ref> that the multi-modal feature of the node is X = Xvisual || Xstructure || Xsemantic (11) where || represents a concatenation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Graph Construction Algorithms</head><p>Here is the algorithm pseudo code to describe how to create a directed graph from a video. In Algorithm 1, the input is a video v and its set of action labels , and the output is a directed graph. It is divided into 7 steps as follows:</p><p>Step 1. Initialization (see lines 1-2). Create an empty directed graph ( , ?) consisting of node set and edge set ?. Split the video v into a set of frames {f1, ..., fT}.</p><p>Step 2. Create nodes (see lines <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Creates a node i based on frame-level. Node i represents video frame fi, so the label of node i is the label yi of video frame fi. Then add node i to a node-set .</p><p>Step 3. Create temporal edges (see lines <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Traverse the node set { 2 , ..., T }. If the label yi of the current node i is the same as the label yi-1 of the previous node i?1 , then a temporal edge ( i?1 , i ) with weight 1 from node i?1 to node i is added to the edge set ?. Step 4. Create positive semantic edges (see lines <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>.</p><p>First, create a node group node_group whose initial value is node 1 . Then, traverse the node set { 2 , ..., T }. If the label yi of the current node i is the same as the label yi-1 of the previous node i?1 , then further traverse the node group. If node j in node_group is not a neighbor of node i (that is, j ? i-1), then edge set ? is added with a positive semantic edge ( j , i ) with weight 1 from node j to node i . Otherwise, node_group is emptied. Finally, node i is added to node_group.</p><p>Step 5. Create negative semantic edges (see lines <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>. Negative semantic edges and positive semantic edges have similar execution procedures. The difference is as follows: In line 26, only when the label yi of the current node i is different from the label yi-1 of the previous node i?1 , the node group is further traversed. All nodes in node_group construct a negative semantic edge with node i respectively. See line 28, edge set ? is added with a negative semantic edge ( j , i ) with weight -1 from node j to node i . Then node_group is emptied.</p><p>Step 6. Create self-loop edges (see lines <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>. Traverse the node set { 1 , ..., T }, and add a self-loop edge ( i , i ) with weight 1 to the node set ? for each node i .</p><p>Step 7. Get node attributes (see lines 37-41). First, extract visual features Xvisual (RGB and optical flow) from a set of frames {f1, ..., fT} using a visual feature extractor. Then, the visual features Xvisual are used as node attributes of the directed graph ( , ?). On ( , ?), a node embedding model is used to encode the structural features Xstructure of node neighborhoods. Semantic features Xsemantic are obtained by using a textual prompt model to encode the label text of nodes. Concatenating Xvisual, Xstructure and Xsemantic compositional multi-modal features X as new attributes of nodes.</p><p>Finally, save the directed graph ( , ?) with nodes, node attributes, node labels, edges, and edge weights transferred from video v.</p><p>An instance of the design of edges according to Algorithm 1 as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. It is worth noting that for adjacent nodes, according to the definitions of temporal and semantic edges in Section ?. C. If their labels are the same, then a temporal edge and a positive semantic edge should be added between them. But in Algorithm 1 (see lines 7 and 8), only the temporal edges that are used to save the time series information of the video are added. See line 15, the purpose of setting the condition j &lt; i-1 is to avoid adding positive semantic edges ( i?1 , i ). As shown in <ref type="figure" target="#fig_0">Fig. 2, nodes 1 and 2</ref>, nodes 2 and 3, nodes 3 and 4, and nodes 5 and 6 have only temporal edges.</p><p>And if their labels are not the same, then a temporal edge and a negative semantic edge should be added between them. But in Algorithm 1 (see lines 26 to 28), only negative semantic edges are added. As shown in <ref type="figure" target="#fig_1">Fig. 2, nodes 4</ref> and 5 and nodes 6 and 7 have only negative semantic edges.</p><p>For human-annotated videos, two consecutive frames may have different labels despite their similar visual features. To enhance the boundaries of semantic labels in the graph, the weight of positive semantic edges is set to 1 and the weight of negative semantic edges is set to -1. In <ref type="figure" target="#fig_1">Fig. 2</ref>, positive semantic edges enhance the semantic relation that node 4 belongs to the same label as nodes 1, 2, and 3. Negative semantic edges enhance the semantic relation between node 5 and nodes 1, 2, and 3 belonging to different labels. In summary, semantic edges also help in label class prediction for two adjacent nodes whose true labels are not identical.</p><p>In a graph with only temporal edges, a node has only two neighbors, so the message aggregation within 1 hop is limited. Moreover, if the nodes want to aggregate messages from more nodes or more distant nodes, the model needs to go through multi-hop and expensive computation. Conversely, semantic edges allow the model to be implemented within 1 hop. Hence, semantic edges significantly reduce the computational cost of message aggregation in GNNs <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Graph-based Fusion Model</head><p>In this paper, we treat video action segmentation and recognition as a node classification problem on graphs. In a sense, our objective is to learn the following mapping function ? <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b31">[32]</ref>: ?: ( , ?) ? = {y } t=1 T (12) To this end, we use a Graph Neural Network (GNN) model ? = (X, ) as the backbone model to fuse the multi-modal features of nodes. Specifically, it is a two-layer graph convolutional neural network (GCN). The forward model has the following form: X = ? 1 (norms( XW (0) )) (13) X = dropout(X) (14) X = ? 2 (MLP( XW (1) )) (15) here, X is the multi-modal feature matrix of node attributes. is a weighted symmetric adjacency matrix. = ? 1 2 1 2 is the normalized adjacency matrix and is the degree matrix of the node <ref type="bibr" target="#b31">[32]</ref>. W (0) and W <ref type="bibr" target="#b0">(1)</ref> are the weight matrices of the network layers. ?1 is a Leaky ReLU activation function. ?2 is a logsoftmax activation function. In the first layer network, a 1D batch normalization function and dropout function are used. In the second layer network, an MLP function composed of linear units, dropout, and linear units is used.</p><p>We need to define that relies on the three types of edges and their weights proposed in Section ?. C. Calculated as follows </p><formula xml:id="formula_6">i, j = 1 if ( i , j ) ? 1 if ( i , j ) ? _ ?1 if ( i , j ) ? _ 1 if ( i , j ) ? ? 0 if( i , j ) ? ?</formula><p>(16) Our model is optimized by node classification loss Lcls and edge alignment loss Ledge. Lcls is a cross-entropy loss <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, which computes the discrepancy between the predicted node action y i and the ground-truth yi. The formula is as</p><formula xml:id="formula_7">=? 1 ? _ =1 ? _ log( ) 1 ? _ =1 ? _ log( ) (17)</formula><p>where Batch_T is the number of nodes in each batch of graphs. Ledge is the Kullback-Leibler (KL) <ref type="bibr" target="#b8">[9]</ref> divergence, which calculates the divergence of the predicted adjacency matrix ' and the target . It is formulated as follows =</p><formula xml:id="formula_8">1 2 ( , ' )~( , ' ) 1 2 ( , ' )~( , ' )<label>(18)</label></formula><p>where ( , ' )~is meant to be an adjacency matrix of a graph. The total loss function is as follows = + (19) where ? is the balance factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>To evaluate our method, we perform experiments on two action datasets of food preparation. <ref type="table" target="#tab_1">Table ?</ref> displays the basic information for the two benchmark datasets.</p><p>GTEA. Georgia Tech Egocentric Activity (GTEA) datasets consist of a first-person instructional video of food preparation in a kitchen environment. It has 28 videos with an average length of 1 minute and 10 seconds <ref type="bibr" target="#b16">[17]</ref>. Each video is split into frame sets at 15 fps <ref type="bibr" target="#b16">[17]</ref>. Each video has an average of 20 action instances, and each frame is annotated with 11 action categories (including background) <ref type="bibr" target="#b16">[17]</ref>. Our method is evaluated using a 5-fold cross-validation average for this dataset <ref type="bibr" target="#b16">[17]</ref>.</p><p>50Salads. This dataset includes instructional videos of 25 testers preparing two mixed salads in a kitchen environment <ref type="bibr" target="#b12">[13]</ref>. It contains 50 videos with an average length of 6 minutes and 24 seconds <ref type="bibr" target="#b12">[13]</ref>. Each video is also split into frame sets at 15 fps. Each frame is labeled with 19 action categories (including background), and each video has 20 action instances on average <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>Following previous works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b21">[22]</ref>, we used the following metrics for evaluation: node-wise accuracy (Acc.), segmental edit score (Edit), and segmental overlap F1 score. Node-wise accuracy corresponds to the frame-wise accuracy of the video, which is the most used metric in action segmentation. Segmental edit score is used to compensate for the lack of node-wise accuracy for over-segmentation. Segmental overlap F1 score is used to evaluate the quality of the prediction, with the thresholds of 0.1, 0.25, and 0.5 (F1@10, F1@25, F1@50) respectively. Additionally, we also use Top-1 and Top-5 to evaluate our method for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>For all datasets, each video is divided into a frameset at 15fps. We use I3D to extract RGB features and optical-flow of each frame as visual features. Due to computer resource constraints, a frame set constructs a graph every 500 frames sequentially sampled. The remaining less than 500 frames have also constructed a graph. Node2vec <ref type="bibr" target="#b35">[36]</ref> is used in the directed graph to encode the neighborhood information of each node as structural features. For training data, we follow ActionCLIP <ref type="bibr" target="#b8">[9]</ref> to encode the label text as semantic features. For unlabeled test data, first, use ActionCLIP to predict actions as initial pseudo-labels. Then, the graph and semantic features of the test data be obtained according to the initial pseudo-labels.  In the training phase, a 2-layer GCN is used as the backbone model, where the hidden layer dimension is 512 and the optimizer is Adam. The dimension of the node attribute matrix is 2816, of which the dimension of visual features, structural features, and semantic features are 2048, 128, and 512, respectively. The batch size is set to 8, which means that each batch has 8 graphs. Furthermore, the learning rate is 0.004, the weight delay is 5e-4, and the dropout probability is 0.5. The balance factor ? is 0.1. The model has trained 30 epochs. All experiments are performed on a computer with 1 NVIDIA GeForce RTX 3090 GPU, 128G memory, Ubuntu 20.04 system, and PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison with State-of-the-art Methods</head><p>On the GTEA and 50Salads datasets, we evaluate the performance of our method versus other models. As seen by <ref type="table" target="#tab_2">Table ?</ref>, the previous methods are all video-based visual models, and only our method belongs to video-based graph models. Our approach leverages graph edge design and neighborhood message aggregation to capture long-term and short-term temporal relationships in videos, rather than transformer (such as Bridge-Prompt, CETNet, ASFormer), attention mechanism (such as SSTDA, DA), or other methods (such as DPRN, ASRF, ETSN, C2F-TCN) in vision models. The results show that our method achieves the performance of SOTA, which demonstrates that it is feasible to use graph models to learn and reason about video relations. Furthermore, although both Bridge-Prompt and our approach use promptbased methods, we also consider structural features that reflect neighborhood information. This is the reason our method outperforms Bridge-Prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Efficiency Analysis</head><p>In this paper, we generate initial pseudo-labels for test data using ActionCLIP. On the test data, Semantic2Graph constructs graphs and obtains semantic features based on initial pseudo-labels. To evaluate the effectiveness of our method, we train and test ActionCLIP and Semantic2Graph on GTEA and 50Salads, respectively. Compared to ActionCLIP, our method improves the Top-1 score for action recognition by approximately 10%, as shown in <ref type="table" target="#tab_3">Table ?</ref>. Semantic2Graph is very effective for the correction of the initial pseudo-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization Analysis</head><p>We visualize the action segmentation results of Semantic2Graph on GTEA to quantify the impact of semantic edges on the elimination of over-segmentation errors. It is observed from the color bar results in <ref type="figure" target="#fig_2">Fig. 3</ref> that the segmentation results of Semantic2Graph on short-duration actions have very high Intersection over Union (IoU) result with the ground truth. Results confirm the role of semantic edges in boundary adjustment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>In this section, we conduct ablation studies on the GTEA dataset to determine crucial parameters and evaluate the effectiveness of components in Semantic2Graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) The Number of Hop</head><p>To capture long-term relations of videos, vision models need to consider long sequences of video frames and employ LSTM, attention mechanism, or transformer <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>. These methods inevitably increase the computational cost. In contrast, Semantic2Graph utilizes node2vec to encode node neighborhood information as structural features, which contains both short-term and long-term relations in the video. The lower the number of hops, the lower the cost for Semantic2Graph to capture the long-term relations of video. We conduct experiments with different hops for Node2vec.</p><p>From <ref type="table" target="#tab_4">Table ?</ref>, we observe that the performance of Semantic2Graph is comparable to Bridge-Prompt <ref type="bibr" target="#b16">[17]</ref> (which is SOTA) when node2vec selects 3 hops to capture structural features. And when selecting 4 hops, Semantic2Graph    <ref type="bibr">(16, 32, or 64</ref>). Semantic2Graph has an obvious cost advantage over visual models. The primary reason is that semantic edges establish direct links between nodes with long spans. Therefore, we consider 4 hops for our subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Importance of Semantic in Test Data</head><p>As we all know, the test data is unlabeled; consequently, our method does not apply semantic edges or semantic features to the test data. To evaluate the importance of semantic edges and semantic features in test data for our method, we conduct two experiments. One of the experiments uses the initial pseudo-labels predicted by ActionCLIP to obtain semantic edges and semantic features. From <ref type="table" target="#tab_5">Table ?</ref>, the test data without semantic edges and semantic features leads to severe over-segmentation errors in the model. This just suggests the importance of semantic edges and semantic features, and that the initial pseudo-labels are reliable for our method to process test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) The Necessity of Edges</head><p>Semantic2Graph constructs the graph with three types of edges to preserve meaningful relations in video sequences. To evaluate their necessity, we performed experiments. In these experiments, the temporal edge was used as a baseline edge and was combined with the other two edges.</p><p>As seen in <ref type="table" target="#tab_6">Table ?</ref>, when self-loop edges are added to the baseline edge, the results rise slightly. Even though baseline and self-looping edges were highly accurate, other metrics, especially Edit, had lower scores. Obviously, they are undersegmentation errors <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The reason is that the structure of the graph and the neighbors of the nodes are too monotonous, resulting in the structural features captured by node2vec being mainly short-term relations. As shown in <ref type="table" target="#tab_6">Table ?</ref>, the model performance improves significantly when semantic edges are added to the baseline edge. It is not difficult to find that semantic edges improve undersegmentation. Unsurprisingly, Semantic2Graph achieves the best performance over SOTA on graphs with all edges. The above results fully demonstrate the necessity of adding semantic edges and self-loop edges to capture both long-term and short-term relations cost-effectively.</p><p>To evaluate the robustness of Semantic2Graph for three edges, we also utilized a 50% random probability to add semantic edges and all edges (excluding the baseline edge). <ref type="table" target="#tab_6">Table ?</ref> shows that when semantic edges are added with a 50% random probability, the results drop dramatically. It is difficult for nodes lacking semantic edges to capture sufficient long-term relations when the number of hops is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) The Contribution of Semantic Features</head><p>To analyze the contribution of semantic features, we conduct some experiments in which visual and structural features serve as baseline node attributes. <ref type="table" target="#tab_7">Table ?</ref> illustrates the results of semantic features. For instance, using only label words and CLIP of textual prompt improves F1 scores, Edit, and Acc. by about 30% and 40%, respectively. In addition, the results indicate that CLIP is more effective than only label words. As a result of the fact that CLIP expands the label text into full sentences to acquire more robust semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) The Efficiency of Different Modalities</head><p>To evaluate the effectiveness of multi-modal features, we conduct a series of experiments combining various combinations of visual, structural, and semantic modalities. The experimental results are shown in <ref type="table">Table ?</ref>. For unimodal, semantics features with textual prompt achieve better results than visual or structural features, which indicates that high-level features are one of the keys to further improving the performance of models. Compared to unimodal, the performance of bimodal models is significantly enhanced, particularly the combination of semantic features. For multimodal, it contains low-level, middle-level, and high-level features, achieving SOTA results <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we convert the challenge of video action   "vis" means visual, "str" means structure, "sem" means semantic. segmentation and recognition into graph node classification utilizing a graph-based method. To preserve the fine-grained relations in the video, we construct the directed graph of the video at the frame-level and add three types of edges, including temporal, semantic, and self-loop edges. In addition to visual features, node attributes also include structural and semantic features. More critically, semantic features are the embedding of label texts based on the textual prompt. To learn multi-modal feature fusion, a GNNs model is utilized. The results of our experiments suggest that our method outperforms SOTA. Ablation experiments also confirm the effectiveness of semantic features in enhancing model performance, and semantic edges facilitate our method to capture long-term relations at a low cost. Future work will involve applying our method to more downstream tasks such as action verification, subgraph extraction, and graph matching.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of the pipeline for Semantic2Graph. (a) Extracting visual features from video frames. (b) An instance of a video-based graph. (c) Encoding node neighborhood information as structural features. (d) Encoding the label text to obtain semantic features. (e) A GNNs is trained to fusion multi-modal features to predict node labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>An instance of an edge in a directed graph. The color of the node represents different labels. Positive semantic edges (pink dashed lines) and negative semantic edges (blue dashed lines) are examples of semantic edges. The edge's value</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Visualization results of action segmentation on GTEA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1</head><label>1</label><figDesc>Video to a Directed Graph Input: Load a video v ? V and a ground_truth_labels = {y1, ...,</figDesc><table><row><cell>yT}</cell><cell></cell></row><row><cell cols="2">Output:</cell><cell>( , ?)</cell></row><row><cell cols="3">// Step 1: Initialization 1: Create an empty directed graph</cell><cell>( , ?)</cell></row><row><cell cols="3">2: v split set of frames {f1, ..., fT}</cell></row><row><cell></cell><cell cols="2">// Step 2: Create nodes</cell></row><row><cell cols="3">3: for i ? [1, T] do 4: Create note i and ? add node i 5: end for</cell></row><row><cell></cell><cell cols="2">// Step 3: Create temporal edges</cell></row><row><cell cols="3">6: for i ? [2, T] do</cell></row><row><cell>7:</cell><cell cols="2">if yi == yi-1 then</cell></row><row><cell>8:</cell><cell cols="2">? ? add temporal edge etemporal = ( i?1 , i ) and wtemporal = 1</cell></row><row><cell>9:</cell><cell cols="2">end if</cell></row><row><cell cols="3">10: end for</cell></row><row><cell></cell><cell cols="2">// Step 4: Create positive semantic edges</cell></row><row><cell cols="3">11: node_group = { 1 }</cell></row><row><cell cols="3">12: for i ? [2, T] do</cell></row><row><cell>13:</cell><cell cols="2">if yi == yi-1 then</cell></row><row><cell>14:</cell><cell cols="2">for j in node_group do</cell></row><row><cell>15:</cell><cell></cell><cell>if j &lt; i-1 then</cell></row><row><cell>16:</cell><cell cols="2">? ? add positive semantic edge epositive_semantic = ( j , i ) and wpositive_semantic = 1</cell></row><row><cell>17:</cell><cell></cell><cell>end if</cell></row><row><cell>18:</cell><cell cols="2">end for</cell></row><row><cell>19:</cell><cell>else</cell></row><row><cell>20:</cell><cell cols="2">node_group = {}</cell></row><row><cell>21:</cell><cell cols="2">end if</cell></row><row><cell>22:</cell><cell cols="2">node_group ? add i</cell></row><row><cell cols="3">23: end for</cell></row><row><cell></cell><cell cols="2">// Step 5: Create negative semantic edges</cell></row><row><cell cols="3">24: node_group = { 1 } 25: for i ? [2, T] do</cell></row><row><cell>26:</cell><cell cols="2">if yi ? yi-1 then</cell></row><row><cell>27: 28:</cell><cell cols="2">for j in node_group do ? ? add negative semantic edge enegative_semantic = ( j , i ) and wnegative_semantic = -1</cell></row><row><cell>29:</cell><cell cols="2">end for</cell></row><row><cell>30:</cell><cell cols="2">node_group = {}</cell></row><row><cell>31:</cell><cell cols="2">end if</cell></row><row><cell cols="3">32: 33: end for node_group ? add i</cell></row><row><cell></cell><cell cols="2">// Step 6: Create self-loop edges</cell></row><row><cell cols="3">34: for i ? [1, T] do 35: ? ? add self-loop edge eself-loop = ( i , i ) and wself-loop = 1 36: end for</cell></row><row><cell></cell><cell cols="2">// Step 7: Get node attributes</cell></row><row><cell cols="3">37: Xvisual = {x1, ..., xT} ? GetVisualFeatures({f1, ..., fT})</cell></row><row><cell cols="3">38: 39: Xstructure ? GetStructureFeatures( .attribute ? Xvisual</cell><cell>( , ?))</cell></row><row><cell cols="3">40: Xsemantic ? GetSemanticFeatures( )</cell></row><row><cell cols="3">41: 42: save .attribute ? X = Xvisual || Xstructure || Xsemantic ( , ?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE ?</head><label>?</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">BENCHMARK DATASETS</cell><cell></cell></row><row><cell>Dataset</cell><cell>Video</cell><cell>Max</cell><cell>Frame Min</cell><cell>Mean</cell><cell>Action</cell></row><row><cell>GTEA</cell><cell>28</cell><cell>2009</cell><cell>634</cell><cell>1321</cell><cell>11</cell></row><row><cell>50Salads</cell><cell>50</cell><cell cols="3">18143 7804 12973</cell><cell>19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE ?</head><label>?</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="5">COMPARISONS WITH OTHER MODELS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">GTEA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50Salads</cell><cell></cell><cell></cell></row><row><cell>Type</cell><cell>Model</cell><cell></cell><cell>F1@{10,25,50}</cell><cell></cell><cell>Edit</cell><cell>Acc.</cell><cell></cell><cell>F1@{10,25,50}</cell><cell></cell><cell>Edit</cell><cell>Acc.</cell></row><row><cell></cell><cell>TDRN [28]</cell><cell>79.2</cell><cell>74.4</cell><cell>62.7</cell><cell>74.1</cell><cell>70.1</cell><cell>72.9</cell><cell>68.5</cell><cell>57.2</cell><cell>66.0</cell><cell>68.1</cell></row><row><cell></cell><cell>DA [27]</cell><cell>90.5</cell><cell>88.4</cell><cell>76.2</cell><cell>85.8</cell><cell>80.0</cell><cell>82.0</cell><cell>80.1</cell><cell>72.5</cell><cell>75.2</cell><cell>83.2</cell></row><row><cell></cell><cell>BCN [26]</cell><cell>88.5</cell><cell>87.1</cell><cell>77.3</cell><cell>84.4</cell><cell>79.8</cell><cell>82.3</cell><cell>81.3</cell><cell>74.0</cell><cell>74.3</cell><cell>84.4</cell></row><row><cell></cell><cell>C2F-TCN [25]</cell><cell>90.3</cell><cell>88.8</cell><cell>77.7</cell><cell>86.4</cell><cell>80.8</cell><cell>84.3</cell><cell>81.8</cell><cell>72.6</cell><cell>76.4</cell><cell>84.9</cell></row><row><cell></cell><cell>ETSN [24]</cell><cell>91.1</cell><cell>90.0</cell><cell>77.9</cell><cell>86.2</cell><cell>78.2</cell><cell>85.2</cell><cell>83.9</cell><cell>75.4</cell><cell>78.8</cell><cell>82.0</cell></row><row><cell>visual</cell><cell>SSTDA [23] ASFormer [19]</cell><cell>90.0 90.1</cell><cell>89.1 88.8</cell><cell>78.0 79.2</cell><cell>86.2 84.6</cell><cell>79.8 79.7</cell><cell>83.0 85.1</cell><cell>81.5 83.4</cell><cell>73.8 76.0</cell><cell>75.8 79.6</cell><cell>83.2 85.6</cell></row><row><cell></cell><cell>ASRF [22]</cell><cell>89.4</cell><cell>87.8</cell><cell>79.8</cell><cell>83.7</cell><cell>77.3</cell><cell>84.9</cell><cell>83.5</cell><cell>77.3</cell><cell>79.3</cell><cell>84.5</cell></row><row><cell></cell><cell>ASRF+HASR [29]</cell><cell>90.9</cell><cell>88.6</cell><cell>76.4</cell><cell>87.5</cell><cell>78.7</cell><cell>86.6</cell><cell>85.7</cell><cell>78.5</cell><cell>83.9</cell><cell>81.0</cell></row><row><cell></cell><cell>CETNet [21]</cell><cell>91.8</cell><cell>91.2</cell><cell>81.3</cell><cell>87.9</cell><cell>80.3</cell><cell>87.6</cell><cell>86.5</cell><cell>80.1</cell><cell>81.7</cell><cell>86.9</cell></row><row><cell></cell><cell>DPRN [20]</cell><cell>92.9</cell><cell>92.0</cell><cell>82.9</cell><cell>90.9</cell><cell>82.0</cell><cell>87.8</cell><cell>86.3</cell><cell>79.4</cell><cell>82.0</cell><cell>87.2</cell></row><row><cell></cell><cell>Bridge-Prompt [17]</cell><cell>94.1</cell><cell>92.0</cell><cell>83.0</cell><cell>91.6</cell><cell>81.2</cell><cell>89.2</cell><cell>87.8</cell><cell>81.3</cell><cell>83.8</cell><cell>88.1</cell></row><row><cell>graph</cell><cell>Semantic2Graph (Ours)</cell><cell>95.7</cell><cell>94.2</cell><cell>91.3</cell><cell>92.0</cell><cell>89.8</cell><cell>91.5</cell><cell>90.2</cell><cell>87.3</cell><cell>89.1</cell><cell>88.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE ? MODEL</head><label>?</label><figDesc>PERFORMANCE IN ACTION RECOGNITION</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell>GTEA</cell><cell>ActionCLIP Ours</cell><cell>69.49 89.84</cell><cell>98.98 99.92</cell></row><row><cell>50Salads</cell><cell>ActionCLIP Ours</cell><cell>80.82 88.61</cell><cell>98.44 99.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE ?</head><label>?</label><figDesc></figDesc><table><row><cell>outperforms SOTA. For visual models, however, 4 hops may</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>only capture short-term relations. TCGL [7] selects 4 frames</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>for short-term temporal modeling. To capture long-term</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>relations, Bridge-Prompt [17], ActionCLIP [9], and ASFormer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[19] et al. utilize longer frame sequences</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">MODEL PERFORMANCE WITH DIFFERENT HOP NUMBER MODELING VIDEO DEPENDENCIES</cell></row><row><cell>Method</cell><cell>Hop</cell><cell cols="2">F1@{10,25,50}</cell><cell>Edit</cell><cell>Acc.</cell></row><row><cell>Bridge-Prompt [17]</cell><cell>16</cell><cell>94.10 92.00</cell><cell>83.00</cell><cell>91.60</cell><cell>81.20</cell></row><row><cell>Semantic2Graph +Node2vec</cell><cell>2</cell><cell>92.96 91.55</cell><cell>88.73</cell><cell>86.95</cell><cell>88.41</cell></row><row><cell>Semantic2Graph +Node2vec</cell><cell>3</cell><cell>95.65 92.75</cell><cell>91.30</cell><cell>91.88</cell><cell>88.04</cell></row><row><cell>Semantic2Graph +Node2vec</cell><cell>4</cell><cell>95.65 94.20</cell><cell>91.30</cell><cell>91.98</cell><cell>89.84</cell></row><row><cell>Semantic2Graph +Node2vec</cell><cell>5</cell><cell>92.86 90.00</cell><cell>87.14</cell><cell>87.84</cell><cell>87.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE ? COMPARING</head><label>?</label><figDesc>MODEL PERFORMANCE ON TEST DATA WITH AND WITHOUT SEMANTIC</figDesc><table><row><cell>Test data</cell><cell cols="2">F1@{10,25,50}</cell><cell>Edit</cell><cell>Acc.</cell></row><row><cell>w/ all edges</cell><cell>95.65 94.20</cell><cell cols="3">91.30 91.98 89.84</cell></row><row><cell>w/o semantic edges and features</cell><cell>53.72 52.07</cell><cell cols="3">47.93 38.21 80.35</cell></row><row><cell cols="2">"w/" is with and "w/o" is without.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE ? COMPARING</head><label>?</label><figDesc>MODEL PERFORMANCE FOR THREE TYPES OF EDGES</figDesc><table><row><cell>Edge type</cell><cell cols="2">F1@{10,25,50}</cell><cell>Edit</cell><cell>Acc.</cell></row><row><cell>temporal edges (baseline)</cell><cell>80.98 79.75</cell><cell cols="3">77.30 74.54 97.15</cell></row><row><cell>baseline+self-loop edges</cell><cell>83.54 82.28</cell><cell cols="3">79.75 77.09 97.19</cell></row><row><cell>baseline+semantic edges</cell><cell>89.21 87.77</cell><cell cols="3">80.58 84.13 73.53</cell></row><row><cell>all edges</cell><cell>95.65 94.20</cell><cell cols="3">91.30 91.98 89.84</cell></row><row><cell>baseline+semantic edges (random=50%)</cell><cell>81.48 80.25</cell><cell cols="3">77.78 73.78 88.18</cell></row><row><cell>all edges (random=50%)</cell><cell>72.53 72.53</cell><cell cols="3">71.43 69.96 90.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE ? MODEL</head><label>?</label><figDesc>PERFORMANCE WITH DIFFERENT SEMANTIC FEATURES</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">F1@{10,25,50}</cell><cell>Edit</cell><cell>Acc.</cell></row><row><cell cols="2">w/o semantic</cell><cell cols="2">55.78</cell><cell>51.70</cell><cell>46.26</cell><cell>47.38</cell><cell>50.30</cell></row><row><cell>only label</cell><cell></cell><cell cols="2">88.89</cell><cell>87.50</cell><cell>81.94</cell><cell>82.59</cell><cell>78.45</cell></row><row><cell>CLIP</cell><cell></cell><cell cols="2">95.65</cell><cell>94.20</cell><cell>91.30</cell><cell>91.98</cell><cell>89.84</cell></row><row><cell cols="8">TABLE ? COMPARING MODEL PERFORMANCE FOR UNIMODAL, BIMODAL AND MULTIMODAL FEATURES.</cell></row><row><cell></cell><cell></cell><cell>Unimodal</cell><cell></cell><cell></cell><cell>Bimodal</cell><cell></cell><cell>Multi-modal</cell></row><row><cell>Metric</cell><cell>vis</cell><cell>str</cell><cell>sem</cell><cell>vis +str</cell><cell>vis +sem</cell><cell>str +sem</cell><cell>vis+str+sem</cell></row><row><cell>Top-1</cell><cell>38.6</cell><cell>13.3</cell><cell>87.3</cell><cell>50.3</cell><cell>88.8</cell><cell>85.8</cell><cell>89.8</cell></row><row><cell>Top-5</cell><cell>90.4</cell><cell>52.3</cell><cell>89.4</cell><cell>94.8</cell><cell>99.8</cell><cell>99.7</cell><cell>99.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Global2local: Efficient structure search for video action segmenta-tion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16805" to="16814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving action segmentation via graph-based temporal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14024" to="14034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Timestamp-Supervised Action Segmentation with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Z</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.15031</idno>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-modal interac-tion graph convolutional network for temporal language localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="8265" to="8277" />
			<date type="published" when="2021-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph convolutional module for temporal action localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3090167</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Video Is Graph: Structured Graph Module for Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05904</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">TCGL: Temporal Contrastive Graph for Self-Supervised Video Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1978" to="1993" />
			<date type="published" when="2022-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A continuous learning framework for activity recognition using deep hybrid feature models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1909" to="1922" />
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Actionclip: A new paradigm for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08472</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-world graph convolution networks (RW-GCNS) for action recognition in smart video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM Symposium Edge Comput. (SEC)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="121" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph interaction net-works for relation transfer in human activity videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cir-cuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2872" to="2886" />
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-localized sensitive auto-encoderattention-lstm for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedi</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1678" to="1690" />
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bridging Video-Text Retrieval With Multiple Choice Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="16167" to="16176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Hierarchical Feature-Aware Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05751</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multimodal Learning on Graphs for Disease Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.08893</idno>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="443" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="19880" to="19889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">ASFormer: Transformer for Action Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08568</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Maximization and restoration: Action segmentation through dilation passing and temporal recon-struction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page">108764</biblScope>
			<date type="published" when="2022-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cross-Enhancement Transformer for Action Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.09445</idno>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Alleviating oversegmentation errors by detecting action boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2322" to="2331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Action segmentation with joint self-supervised temporal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9454" to="9463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient two-step networks for temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">454</biblScope>
			<biblScope unit="page" from="373" to="381" />
			<date type="published" when="2021-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Coarse to fine multi-resolution temporal convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10859</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Boundary-aware cascade networks for temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Proc. Eur. Conf. Comput. Vis</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="34" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Action segmentation with mixed temporal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="605" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6742" to="6751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Refining action segmentation with hierarchical video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16302" to="16310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Vision GNN: An Image is Worth Graph of Nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00272</idno>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-Channel Attentive Graph Convolutional Network with Sentiment Fusion for Multimod-al Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="4578" to="4582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cola: Weaklysupervised temporal action localization with snippet contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16010" to="16019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th ACM SIGKDD</title>
		<meeting>20th ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Line: Largescale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Int. Conf. WWW, 2015</title>
		<meeting>24th Int. Conf. WWW, 2015</meeting>
		<imprint>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM SIGKDD</title>
		<meeting>22nd ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
