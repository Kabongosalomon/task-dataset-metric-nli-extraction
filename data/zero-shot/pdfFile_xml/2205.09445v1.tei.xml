<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Enhancement Transformer for Action Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shijiazhuang Tiedao University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shijiazhuang Tiedao University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hebei Key Laboratory for Electromagnetic Environmental Effects and Information Processing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanna</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shijiazhuang Tiedao University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hebei Key Laboratory for Electromagnetic Environmental Effects and Information Processing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Hebei Key Laboratory for Electromagnetic Environmental Effects and Information Processing</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Enhancement Transformer for Action Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Action segmentation Self-attention mechanism Temporal struc- ture Transformer</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal convolutions have been the paradigm of choice in action segmentation, which enhances long-term receptive fields by increasing convolution layers. However, high layers cause the loss of local information necessary for frame recognition. To solve the above problem, a novel encoder-decoder structure is proposed in this paper, called Cross-Enhancement Transformer. Our approach can be effective learning of temporal structure representation with interactive self-attention mechanism. Concatenated each layer convolutional feature maps in encoder with a set of features in decoder produced via self-attention. Therefore, local and global information are used in a series of frame actions simultaneously. In addition, a new loss function is proposed to enhance the training process that penalizes over-segmentation errors. Experiments show that our framework performs state-of-the-art on three challenging datasets: 50Salads, Georgia Tech Egocentric Activities and the Breakfast dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video action segmentation and classification for untrimmed videos of complex activities which requires to label each frame in a long video by an action class. It has been a hot topic in human action analysis, which is widely used in video surveillance <ref type="bibr" target="#b5">[6]</ref>, action teaching, and robotics <ref type="bibr" target="#b33">[34]</ref>. Recently, some works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21]</ref> have studied the long range dependencies between correlated actions in action segmentation using temporal convolution networks (TCNs) for models. The TCNs enhance long-term receptive fields by increasing convolution layers. However, as the depth of the convolutional layers increases, the finegrained information required for frame recognition will be missing.</p><p>The novel transformer architecture <ref type="bibr" target="#b32">[33]</ref> has led to a big leap forward in capabilities for sequence-to-sequence modeling in NLP tasks. Transformer is famous for using self-attention to extract long-term dependencies in data features. The great transformation of Transformer in NLP has attracted special attention of arXiv:2205.09445v1 [cs.CV] <ref type="bibr" target="#b18">19</ref> May 2022 computer vision. The great transformation of Transformer in NLP has attracted special attention in computer vision, hoping to use Transformer to optimize convolutional neural network-based architectures (CNN) in computer vision tasks. Over the past year, Transformers have enjoyed tremendous success in many computer vision applications, especially in image classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22]</ref>, video recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref>, video recognition <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b40">41]</ref>, semantic segmentation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b40">41]</ref>, object detection <ref type="bibr" target="#b38">[39]</ref>. The Transformer for Action Segmentation (ASFormer) is the first to adopt the transformer architecture in the action segmentation task. ASFormer the explicitly introduced local connectivity inductive and pre-defined hierarchical representation pattern. However, The ASFomer mainly focus on improving hierarchical receptive fields for modeling long-term dependency which is hard-to-excavate the contextual relations between adjacent actions.</p><p>The main problem of this work is how to adaptively learn representations from input features to effectively capture global dependencies and the contextual information of adjacent frames. In this paper, we consider using self-attention to enhance the ability of convolution to extract features. Concatenated each layer convolutional feature maps in encoder with a set of features in decoder produced via self-attention, local fine-grained and global information are used in a series of frame actions simultaneously.</p><p>The loss for action segmentation are all trained with frame-level losses, however, these do not adequately penalize sequence-level missclassification. At present, the circle loss provides a paired similarity optimization view of deep feature learning, aiming at maximizing the similarity within the class and minimizing the similarity between classes. We propose to address this over-segmentation by reshaping the Circle Loss <ref type="bibr" target="#b28">[29]</ref> such that it down-weights the loss assigned to well-classified examples. Our ensemble loss is not only more accurate, but also has a smoothing effect and yields more accurately calibrated sequences. In conclusion, the main contributions of this work are as follows:</p><p>1. We propose a novel encoder-decoder structure for action segmentation, called Cross-Enhancement Transformer (CETNet) . Our approach can be effective learning of temporal structure representation with interactive self-attention mechanism. Concatenated each layer convolutional feature maps in encoder with a set of features in decoder produced via self-attention, so that it simultaneously exploits both local and global information from a series of frame actions.</p><p>2. We propose a loss function to enhance the training process and punish over-segmentation. Learning deep features by weighting each similarity score, the loss function has flexible optimization and explicit convergence. Such a loss is highly advantageous in mitigating the effects of over-segmentation and preventing fragmented sequence segmentation. Combining the loss function with a class weighted classification loss function, F1 score can be increased by 5.1% and segmental edit distance can be increased by 2.3%.</p><p>3. Our approach performs state-of-the-art on three challenging datasets: 50Salads <ref type="bibr" target="#b26">[27]</ref>, GTEA <ref type="bibr" target="#b8">[9]</ref>, and Breakfast <ref type="bibr" target="#b14">[15]</ref>. Up to 7.8% segment F1 score improvement, 3.7% segment editing distance improvement and 1.9% accuracy improvement.  The 3D CNN-based framework has spatiotemporal modeling capabilities and improves the performance of video action recognition models. 3D ConvNets <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24]</ref> extended 2D image models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36]</ref> to the spatial-temporal domain, treating spatial and temporal dimensions in the same way. C3D <ref type="bibr" target="#b30">[31]</ref> stacked spatiotemporal convolution kernels to efficiently represent video dense structure. I3D <ref type="bibr" target="#b2">[3]</ref> extended the convolution and pooling kernels in a very deep image classification network from 2D to 3D to seamlessly learn spatiotemporal features. Our work focuses on frame-level action classification, and video feature extraction is beyond the scope of our work. Following <ref type="bibr" target="#b7">[8]</ref>, we use I3D <ref type="bibr" target="#b2">[3]</ref> for feature extraction as the input to the network, since the videos used for action segmentation are generally long videos that are hard to conduct direct analysis based on raw data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Action segmentation</head><p>The traditional sliding-window paradigm <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>, which is applied to length and context information, has a long and rich history, and other methods use Markov <ref type="bibr" target="#b15">[16]</ref> models or RNN <ref type="bibr" target="#b39">[40]</ref> models to apply rough time modeling. Recently, inspired by the success of temporal convolution in speech synthesis <ref type="bibr" target="#b31">[32]</ref> , temporal convolutional networks (TCNs) transformed a commonly used architecture for temporal video segmentation. Some TCNS works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b7">8]</ref> mainly focus on improving receptive fields that model long-term dependencies with encoder structures, dilated convolutions, or deformable convolutions. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref> build architecture on the two-branch approach : One branch exploits wide long-term time receptive fields based on TCNs. The second exploits frame-boundary based on action boundary regression. <ref type="bibr" target="#b38">[39]</ref> explore the Transformer on action segmentation task, which introduced inductive local connectivity and a preset hierarchical representation model. The above methods capture long-term dependencies by increasing the depth layers of temporal convolution, which lead to fine-grained novel loss between adjacent frames. An innovative compared to previous methods, our approach uses self-attention mechanism to augment convolutional operators by concatenating the maps of the convolution features of each layer in the encoder with a set of features in the self-attention decoder. And it solves the problem of fine-grained loss in adjacent frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In the section, we present our CETNet structure for action segmentation. Our CETNet uses an encoder-decoder architecture with self-attention, which simultaneously exploits both local and global information from a series of frame actions, as shown <ref type="figure" target="#fig_0">Fig. 1</ref>. The encoder will first capture global temporal information by expanding the layers of the self-attention blocks which use a deep series of dilated convolutions. Then the decoders will use the initialized predictions and hierarchical features, obtained from the encoder, to perform incremental refinement. Finally, the result will be passed to the combined loss function to optimize the frame-level classification. Section 3.1 illustrates the details of self-Attention block with expanded dilated convolutions. Section 3.2 shows how to utilize the encoder to capture hierarchical features and a long-term feature extractor. Section 3.3 introduces our refinement scheme in decoder.Section 3.4 introduces the combined loss function and training details of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Self-Attention block</head><p>As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, Given the input features X l ? R T ?D which is extracted from input videos or previous l th layer, where D is the dimension and T is the video length. The first of the self-attention Block is a feed-forward layer which consists of 1D dilated temporal convolution and RELU activation. We increase the dilation rates with kernel size 3 for conducting different temporal receptive fields:</p><formula xml:id="formula_0">S = {2 i , i = 0, 1, 2....}<label>(1)</label></formula><p>where i is index of Self-Attention block. The receptive field grows exponentially with the number of layers, which helps prevent the model from overfitting the training data <ref type="bibr" target="#b16">[17]</ref>. We use instance normalization after feed-forward to improve performance:</p><formula xml:id="formula_1">X l = In(F F N (X l ))<label>(2)</label></formula><p>where In is instance normalzation, F F N is the feed-forward layer, X is input features and X is the output after instance normalization.</p><formula xml:id="formula_2">Q = X l W q , K = X l W k , V = X l W v (3) Att = Attention(Q, K, V ) = sof t max(QK T / d k ) V (4) where W q , W k , W v ? R T ?(C/r)</formula><p>are the query, key and value matrices of learnable parameters, ? d k is the scaling factor, C and r are the dimension and hyperparameter. Note that the input of V is different between encoder and decoder. In the encoder(described in Section 3.2) , the input of V is the same as Q. In the decoders(described in Section 3.3), in order to alleviate the information leakage by temporal correlation, we propose to use each encoder hierarchy feature as the V value to perform incremental optimization. We also include a 1 ? 1 conv and residual connection after the self-attention operation, as this helps to adjust the dimension for subsequent operations:</p><formula xml:id="formula_3">F = LN (? (Att)) + X l<label>(5)</label></formula><p>where X l is the input of encoder or decoder , LN is Layer normalzationand ? is 1 ? 1 convolution operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder</head><p>The encoder consists of N sequential Self-Attention Block layers, We set N=10 in our paper(ablation experiment show in section 4.5). Before the first layer of encoder, we use a FC layer to reduce the input feature dimension from D to D .</p><formula xml:id="formula_4">X l+1 = ?(X l )<label>(6)</label></formula><p>where ? is a Self-Attention Block discussed in Section 3.1. X l+1 is the next layer of encoder. The last layer of the encoder uses the softmax output as the initial embedding for each frame prediction, which contains an abstract representation of the global features.</p><formula xml:id="formula_5">Y c = sof t max(W X l + b)<label>(7)</label></formula><p>where Y c ? R c , c is classify frame-level action classes, W and b are the weights and bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoders</head><p>Decoder consists of N sequential decoder block that is similar to encoder structure. A little different is that the query Q and key K are obtained by concatenating the encoder output and the preceding layer, while the value V is only obtained from the self-attention of the corresponding layer in the encoder. Inspired by <ref type="bibr" target="#b38">[39]</ref>, The self-attention mechanism utilizes training to focus attention weights at each location and continuously refines all locations. Furthermore, since the output of each decoder is an initial prediction with different hierarchy of temporal relationships, the decoder is aligned with the encoder's self-attention layer to continuously optimize the global and local information, reduce fine-grained information loss to prevent over-segmentation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function and Implementation Details</head><p>Loss Function Circle loss <ref type="bibr" target="#b28">[29]</ref> is a loss function that learns deep features by weighting each similarity score, and has flexible optimization and explicit convergence:</p><formula xml:id="formula_6">L circle = log[1 + K i=1 L j=1 exp(?(? i n s j n ? ? i p s j p ))] = log[1 + L j=1 exp(?? j n s j n ) K i=1 exp(??? i p s i p )]<label>(8)</label></formula><p>in which ? i n and ? i p are weighting factors, and x is a scale factor. There are similarity scores as {s i p } (i = 1, 2, ? ? ?, K) and {s j n } (i = 1, 2, ? ? ?, L), respectively. K is the similarity scores in the class and L is the similarity score between classes.</p><p>When we regard the softmax value in a classification loss function as the probability that the sample belongs to a certain class, constant weight scaling is a common operation. Circle loss has an independent weighting factor, which is multiplied by each similarity score before rescaling. Therefore, optimization <ref type="figure">Fig. 3</ref>. Qualitative result from the GTEA dataset for comparing different method of action segmentation. Only part of the whole video is shown for clarity. We can see that our CETNet method is most closed to groundtruth.</p><p>is more flexible without the constraint of constant weight scaling. So we use a simple set prediction loss L loss to use the given set of frame actions.</p><p>L loss = L cls + ?L mse + ?L circle <ref type="bibr" target="#b8">(9)</ref> where L cls is a cross-entropy loss, L mse is the smooth loss in <ref type="bibr" target="#b7">[8]</ref>. ? and ? are balance weight. All losses of the encoder and decoder are accumulated and trained to search for the minimum optimal value. Implementation Details The final CETNET structure consists of encoderdecoder. The encoder consists of 10 self-attention layers, and the number of decoders corresponds to the number of self-attention layers of the encoder, in other words, the decoder contains 10 decoders, each containing 10 self-attention layers. In all experiments, our deep learning model framework is based on the pytorch framework, and the physical hardware uses two NVIDIA RTX 2080ti GPUs and ubuntu with cuda10.1.</p><p>In order to prove the effectiveness of our model, we adopt the same preprocessing and super parameters in ASFormer <ref type="bibr" target="#b38">[39]</ref>. Keeping the fps of the 50Salads dataset the same as the other datasets, we take a frame step of 2 in 50Salads , and a frame step of 1 in GTEA and Breakfast datasets. We train the model for 120 epochs,batch size is 1 and the kernel size in all layers is 3. For loss hyperparameter setting, we set ? = 0.15, ? = 0.001.  <ref type="bibr" target="#b26">[27]</ref> dataset consists of 50 videos belonging to 17 action classes. The average length of each video is 6 minutes and contains 20 actions. GTEA <ref type="bibr" target="#b8">[9]</ref> dataset consists of 28 videos belonging to 11 action classes.We use four different training-test splitting strategies to guarantee the validity of the experiment. Breakfast <ref type="bibr" target="#b14">[15]</ref> dataset consists of 1712 videos belonging to 48 action classes with 18 different kitchens. It is the largest and most challenging dataset in action segmentation. In addition to using 5-fold cross-validation on the 50salads dataset, we use 4-fold cross-validation for evaluation on the other two datasets and report the average results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation metrics</head><p>Accuracy (Acc), edit distance (Edit) and F1 scores (F1@10,25,50) are three evaluation metrics commonly used in action segmentation. The frame-wise accuracy is the accuracy of the action prediction per frame in a video. However, it is unable to penalize the over-segmentation errors. Edit and F1 scores are the action segmentation metrics used to evaluate whether it is over-segmented. Edit is a measure representing the similarity between predicted and groudtruth. F1 scores represent the scores at different overlap thresholds, which score at 10%, 25%, 50%, denote by F1@10,25,50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact of multi-head self-attention</head><p>In the transformer <ref type="bibr" target="#b32">[33]</ref>, Multi-Head self-attention is used to divide the model into multiple heads to form multiple subspaces, so that the model can focus on different aspects of information and splice the results of multiple projections. The final result is then obtained by linear transformation that enhance the feature transformation. Here, we explore the effect of different self-attention heads on GTEA. Except the last one (the divide heads in 2-head set 64 dimension), all other setting is the same as the single-head attention. According to Tab. 1, we can find that multi-heads are insensitive to our method. This could be an overfitting problem due to the increased number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of the decoders hierarchical refinement</head><p>To demonstrate that our decoder exploits multiple levels of temporal relationships for refinement, we perform ablation studies that stack different numbers of decoders. We evaluate the importance of global cross level self-attention by changing the number of decoder layers. We compare all-layer cross model compared to defect-layer cross models with different number of decoders on GTEA and 50Salads datasets. As shown in the <ref type="table">Table.</ref> 2, we explore the cross-attention effect of the first five layers (ahead-cross) and the last five layers (behind-cross), other layers are replaced by the output of the encoder. We also conduct experiments using only the cross-attention of the front and rear five layers. The all-layer cross model utilizes all temporal-level information for refinement and perform the best. By comparing the segment edit distances and F1 scores of these models, we can see that only the previous hierarchy layers in the encoder produce a lot of over-segmentation errors. Deep layers contain more abstract temporal information and the cross-attention of the back layers is better than the previous layers. On the other hand, the cross-enhancement architecture can reduce oversegmentation errors and improve F1 scores. This improvement is clearly visible when all hierarchies are used, greatly improving the segmentation metrics. In the experiment, our cross self-attention structure is better than the commonly used attention mechanism, which does not consider the relationship between the previous encoder layers and decoder outputs predicted by each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation study on hyper-parameters</head><p>Ablations of the number of blocks Stacking more self-attention blocks can get a larger receptive field (as introduced in Sec. 3.1) , but it will cost more computation and memory. We conduct ablation studies on different numbers of self-attention blocks in the encoder-decoder of GTEA and 50salads datasets, as shown in table. 4. Although the two F1 scores in the GTEA dataset achieve the best performance when the self-attention layer is set to 7, it does not perform well in the 50salads dataset. So we set the self-attention block to 10 by default.</p><p>Comparing loss functions for the CETNet As shown in <ref type="table">Table.</ref> 6, compares different hyper-parameters combination of loss functions. Our proposed loss function improves frame-level accuracy, F1 score and edit distance, and achieves the best performance when hyperparameters ? = 0.15, ? = 0.001. Learning deep features by weighting each similarity score, the loss function has flexible optimization and explicit convergence. Such a loss is highly advantageous in mitigating the effects of over-segmentation and preventing fragmented sequence segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Comparison with the state of the art</head><p>In the experiment, we show that our framework performs state-of-the-art on three challenging datasets: 50Salads, GTEA,and Breakfast datasets. As shown in <ref type="table" target="#tab_3">Table. 5 and Table.</ref> 6. Our model achieves the state-of-the-art methods on the 50Salads and Breakfast datasets compared to previous work. Our CETNet is having up to 5.4% and 6.8% improvement for the segmental F1 score on 50Salads and Breakfast respective. Although the accuracy of the C2F-TCN Method is higher than CETNet on GTEA, the F1 score perform a large margin up to 4.63% for the F1 score. We visualized the prediction of labels as shown in <ref type="figure">Fig.  3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present CETNet, a novel encoder-decoder interactive selfattention mechanism for learning global features, to improve the classification accuracy for action segmentation. To address the effects of over-segmentation and prevent fragmented sequence segmentation, we further develop a loss function to re-weighting each similarity score under supervision. With extensive experiments, we demonstrate that our proposed CETNet outperforms the state-ofthe-art models by a large margin on 50Salads, GTEA and Breakfast. While our current work has only scratched the surface of cross-ehancement transformers for action segmentation, we anticipate that more work will be done in the future to develop effective cross-ehancement transformers for other action applications, including action recognition, action assessment, and action correction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of the Cross-Enhancement Transformer Network (CETNet) comprised of an encoder-decoder architecture with self-attention, which simultaneously exploits both local and global information from a series of frame actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Illustrates the details of self-Attention block with expanded dilated convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Impact of Mulit-head and dimension on the GTEA dataset. Impact of the decoders hierarchical refinement on GTEA and 50salads dataset</figDesc><table><row><cell cols="4">Multi-head dim F1@{10,25,50} Edit Acc</cell></row><row><cell></cell><cell>1</cell><cell cols="2">64 91.8 91.2 81.3 87.9 80.3</cell></row><row><cell></cell><cell>2</cell><cell cols="2">64 90.8 89.3 80.9 87.6 79.7</cell></row><row><cell></cell><cell>3</cell><cell cols="2">64 90.6 88.9 79 87.1 78.8</cell></row><row><cell></cell><cell>4</cell><cell cols="2">64 90.4 88.9 79.7 86.3 78.6</cell></row><row><cell></cell><cell>2</cell><cell cols="2">128 90.7 89.4 80.1 87.2 79.4</cell></row><row><cell>Dataset</cell><cell></cell><cell>GTEA</cell><cell>50salads</cell></row><row><cell>Cross-decoder</cell><cell cols="3">F1@{10,25,50} Edit Acc F1@{10,25,50} Edit Acc</cell></row><row><cell>no-cross</cell><cell cols="3">90.4 88.3 79.5 86 78.4 81 79.3 71.5 73.8 83.6</cell></row><row><cell>ahead-cross</cell><cell cols="3">77.9 74.5 67.5 69.3 79.1 54.2 53 47 43.9 84.3</cell></row><row><cell cols="4">ahead-cross (only) 80.2 79 69.3 73.3 78.7 54.3 52.7 48.7 43.1 85.9</cell></row><row><cell>behind-cross</cell><cell cols="3">90.9 89.8 80.7 86.8 79.3 86.8 85.1 79 80.9 86.3</cell></row><row><cell cols="4">behind-cross (only) 91.2 90.4 81 86.8 78.8 86.1 85 77.6 80.7 85.7</cell></row><row><cell>all-cross</cell><cell cols="3">91.8 91.2 81.3 87.9 80.3 87.6 86.5 80.1 81.7 86.9</cell></row><row><cell>4 Experiments</cell><cell></cell><cell></cell></row><row><cell>4.1 Dataset</cell><cell></cell><cell></cell></row><row><cell>50Salads</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of loss function on the GTEA and 50salads dataset.</figDesc><table><row><cell>Dataset</cell><cell>GTEA</cell><cell>50salads</cell></row><row><cell>Loss</cell><cell cols="2">F1@{10,25,50} Edit Acc F1@{10,25,50} Edit Acc</cell></row><row><cell cols="3">L cls + ?Lmse(? = 0.15) 90.3 89.4 80.5 86.2 79.6 86.3 85 77.7 79.9 86</cell></row><row><cell cols="3">L cls + ?Lmse(? = 0.75) 90.9 89.7 79.7 86.8 79 85.5 83.8 76.6 79 85</cell></row><row><cell cols="3">L loss (? = 0.75, ? = 0.001) 91.3 90 80.5 87.7 79.7 85.1 84.3 77.8 78.9 85.5</cell></row><row><cell>L</cell><cell></cell><cell></cell></row></table><note>loss (? = 0.15, ? = 0.001) 91.8 91.2 81.3 87.9 80.3 87.6 86.5 80.1 81.7 86.9</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Impact of layers on the GTEA and 50salads dataset.</figDesc><table><row><cell>Dataset</cell><cell>GTEA</cell><cell>50salads</cell></row><row><cell cols="3">Layer (N) F1@{10,25,50} Edit Acc F1@{10,25,50} Edit Acc</cell></row><row><cell>5</cell><cell cols="2">88.4 87.2 76 83.5 76.7 57.6 54 43,7 47.4 75.4</cell></row><row><cell>6</cell><cell cols="2">91.2 90.1 79.1 87.4 78.3 73.1 70.3 61.2 63.8 79.7</cell></row><row><cell>7</cell><cell cols="2">92.1 90.9 81.9 87.6 79.4 79.6 77.5 69.7 72.2 82.1</cell></row><row><cell>8</cell><cell cols="2">90.3 88.8 77.4 86.4 77.6 83.4 82 74.4 76.6 84</cell></row><row><cell>9</cell><cell cols="2">90 89.1 77.8 85.5 79 86.1 84.3 78.3 79.8 85.7</cell></row><row><cell>10</cell><cell cols="2">91.8 91.2 81.3 87.9 80.3 87.6 86.5 80.1 81.7 86.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Comparig our proposed method with existing methods on GTEA and 50salads dataset</figDesc><table><row><cell>Dataset</cell><cell>GTEA</cell><cell>50salads</cell></row><row><cell>Method</cell><cell cols="2">F1@{10,25,50} Edit Acc F1@{10,25,50} Edit Acc</cell></row><row><cell>MS-TCN[8]</cell><cell cols="2">87.5 85.4 74.6 81.4 79.2 76.3 74 64.5 67.9 80.7</cell></row><row><cell cols="3">MS-TCN++[19] 88.8 85.7 76 83.5 80.1 80.7 78.5 70.1 74.3 83.7</cell></row><row><cell>SSTDA[5]</cell><cell cols="2">90 89.1 78 86.2 79.8 83 81.5 73.8 75.8 83.2</cell></row><row><cell cols="3">SSTDA+HASR[1] 90.9 88.6 76.4 87.5 78.7 83 81.5 73.8 75.8 83.2</cell></row><row><cell>BCN[38]</cell><cell cols="2">88.5 87.1 77.3 84.4 79.8 82.3 81.3 74 74.3 84.4</cell></row><row><cell>C2F-TCN[26]</cell><cell cols="2">90.3 88.8 77.7 86.4 80.8 84.3 81.8 72.6 76.4 84.9</cell></row><row><cell>ETSN[21]</cell><cell cols="2">91.1 90 77.9 86.2 78.2 85.2 83.9 75.4 78.8 82</cell></row><row><cell>ASRF[12]</cell><cell cols="2">89.4 87.8 79.8 83.7 77.3 84.9 83.5 77.3 79.3 84.5</cell></row><row><cell>ASFormer[39]</cell><cell cols="2">90.1 88.8 79.2 84.6 79.7 85.1 83.4 76 79.6 85.6</cell></row><row><cell cols="3">CETNet(ours) 91.8 91.2 81.3 87.9 80.3 87.6 86.5 80.1 81.7 86.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Comparing our proposed method with existing methods on Breakfast dataset</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell>Breakfast</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">F1{10,25,50}</cell><cell>Edit</cell><cell>Acc</cell></row><row><cell>MS-TCN[8]</cell><cell>52.6</cell><cell>48.1</cell><cell>37.9</cell><cell>61.7</cell><cell>66.3</cell></row><row><cell>MS-TCN++[19]</cell><cell>64.1</cell><cell>58.6</cell><cell>45.9</cell><cell>65.6</cell><cell>67.6</cell></row><row><cell>BCN[38]</cell><cell>68.7</cell><cell>65.5</cell><cell>55</cell><cell>66.2</cell><cell>70.4</cell></row><row><cell>ETSN[21]</cell><cell>74</cell><cell>69</cell><cell>56.2</cell><cell>70.3</cell><cell>67.8</cell></row><row><cell>ASRF[12]</cell><cell>74.3</cell><cell>68.9</cell><cell>56.1</cell><cell>72.4</cell><cell>67.6</cell></row><row><cell>SSTDA[5]</cell><cell>75</cell><cell>69.1</cell><cell>55.2</cell><cell>73.7</cell><cell>70.2</cell></row><row><cell>C2F-TCN[26]</cell><cell>76.3</cell><cell>69.9</cell><cell>54.6</cell><cell>74.5</cell><cell>70.8</cell></row><row><cell>ASFormer[39]</cell><cell>76</cell><cell>70.6</cell><cell>57.4</cell><cell>75</cell><cell>73.5</cell></row><row><cell>CETNet(ours)</cell><cell>79.3</cell><cell>74.3</cell><cell>61.9</cell><cell>77.8</cell><cell>74.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Refining action segmentation with hierarchical video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action segmentation with joint self-supervised temporal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Introduction to the special section on video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3281" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sct: Set constrained temporal transformer for set supervised action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Alleviating over-segmentation errors by detecting action boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast saliency based pooling of fisher encoded dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV THUMOS Workshop</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An end-to-end generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ms-tcn++: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Abufarha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Trear: Transformer-based rgb-d egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient two-step networks for temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">454</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transformer with peak suppression and knowledge guidance for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">492</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="149" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3d-tdc: A 3d temporal dilation convolution framework for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">450</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10859</idno>
		<title level="m">Coarse to fine multi-resolution temporal convolutional network</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing</title>
		<meeting>the 2013 ACM international joint conference on Pervasive and ubiquitous computing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7262" to="7272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR (2021) 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SSW</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">From stochastic grammar to bayes network: Probabilistic parsing of complex activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gated forward refinement network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">407</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Boundary-aware cascade networks for temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Asformer: Transformer for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08568</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
