<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Both Style and Fog Matter: Cumulative Domain Adaptation for Semantic Foggy Scene Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzheng</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<address>
									<addrLine>3 MPI for Informatics 4 NTHU</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacheng</forename><surname>Zhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<address>
									<addrLine>3 MPI for Informatics 4 NTHU</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
						</author>
						<title level="a" type="main">Both Style and Fog Matter: Cumulative Domain Adaptation for Semantic Foggy Scene Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although considerable progress has been made in semantic scene understanding under clear weather, it is still a tough problem under adverse weather conditions, such as dense fog, due to the uncertainty caused by imperfect observations. Besides, difficulties in collecting and labeling foggy images hinder the progress of this field. Considering the success in semantic scene understanding under clear weather, we think it is reasonable to transfer knowledge learned from clear images to the foggy domain. As such, the problem becomes to bridge the domain gap between clear images and foggy images. Unlike previous methods that mainly focus on closing the domain gap caused by fog -defogging the foggy images or fogging the clear images, we propose to alleviate the domain gap by considering fog influence and style variation simultaneously. The motivation is based on our finding that the style-related gap and the fog-related gap can be divided and closed respectively, by adding an intermediate domain. Thus, we propose a new pipeline to cumulatively adapt style, fog and the dual-factor (style and fog). Specifically, we devise a unified framework to disentangle the style factor and the fog factor separately, and then the dual-factor from images in different domains. Furthermore, we collaborate the disentanglement of three factors with a novel cumulative loss to thoroughly disentangle these three factors. Our method achieves the state-ofthe-art performance on three benchmarks and shows generalization ability in rainy and snowy scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic foggy scene understanding (SFSU) is important for autonomous driving <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Although great progress has been made on semantic understanding of clear scenes, SFSU tends to have unsatisfactory performance due to visibility degradation caused by fog <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30]</ref>. Besides, unlike the abundant data and annotation under clear scenes, the lack of data and annotation under dense fog weather Our goal is to transfer the knowledge from a labeled domain s to an unlabeled domain t. However, direct knowledge transfer is challenging due to the mixed dual-factor gap (orange arrow). By adding an intermediate domain m as a bridge, we can decompose the mixed dualfactor gap into two single-factor gaps: the style gap and the fog gap. Since images in both domains s and m are captured in clear scenes, we assume there is only the style gap between domains s and m (blue arrow). Likewise, images in both domains m and t are collected in the same city (Zurich), we assume there exists only the fog gap between them (green arrow).</p><p>further complicates this problem. Therefore, handling the challenging SFSU problem often requires transferring the segmentation knowledge learned from labeled clear images to the unlabeled foggy images. Intuitively, we may tackle this problem by closing the domain gap between clear images and foggy images with state-of-the-art domain adaptation methods. However, these methods mainly align domains in an adversarial <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b38">39]</ref> or a self-training <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref> manner, regardless of how the domain gap is caused. Besides, as has been validated by <ref type="bibr" target="#b25">[26]</ref>, they fail to address the SFSU problem well due to the large domain gap.</p><p>Consequently, the attention of SFSU has been focused on the fog factor, which is regarded as the dominating cause of the domain gap in the SFSU problem. One solution is to close this gap by defogging real foggy images, using an existing defogging method <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b36">37]</ref>. Whereas, the defogging method will also introduce artifacts. They act as noise to hinder the domain adaptation to some extent <ref type="bibr" target="#b21">[22]</ref>. Another solution is to add synthetic fog to clear images and learn with these synthetic foggy images and annotations of clear images in a supervised manner <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Nevertheless, these rendered synthetic foggy images, not as real as real foggy ones, could also widen the domain gap between clear and foggy images and yield unsatisfactory performance. Moreover, we argue that these methods overconcerned the fog factor while ignoring other factors, which may affect the domain gap in the SFSU problem.</p><p>Thinking out of the box, we propose to explicitly investigate the domain gap in SFSU 1) to avoid directly treating the total domain gap; 2) without using synthetic foggy data or knowledge of defogging. We assume that the domain gap is caused by the mixed fog influence and style variation, and both of them are important to SFSU. That is, we assume there exist the style-related gap and the fog-related gap in the domain gap of SFSU, and we can decompose the mixed dual-factor gap into these two single-factor gaps by adding an intermediate domain. Next, we elaborate on why we can disentangle the style-related gap and what relationship lies between the style-related gap and the fog-related gap in the SFSU problem, using the following empirical finding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Motivation</head><p>We first investigate the influence of the style and fog factors across different domains, i.e., we want to know how the style and fog factors affect the performance of a segmentation model. To this end, as shown in <ref type="figure">Figure 2</ref>, we utilize Mean Variance Value (MVV) 1 to represent how a segmentation model functions in each domain and how the gap between two different domains is closed.</p><p>Specifically, in <ref type="figure">Figure 2</ref>, we trained a segmentation model Model (s) with s-domain data and calculate MVV in domains s, m and t, yielding V s s , V m s and V t s , respectively. We use the length of the bar to indicate the performance in each domain. Ideally, since we only have the model learned from domain s, its performance should be good when segmenting images in domain s (i.e., MVV should be low), but tends to degrade when segmenting images in domain m and t (i.e., MVV should be relatively high). Our experiments results are as expected and the yellow bar becomes cumulatively longer when dealing with the images in domain s, m, and t. Besides, we can use the difference of two MVVs as the performance gap between two domains (i.e., domain gap). For example, V m s ? V s s can represent the gap between domain s and m, which we assume as "style gap". Likewise, we obtain the "dual gap" and "fog gap".</p><p>Then, we adapt Model (s) with m-domain data to obtain Model (s + m) and calculate the MVV in three domains. Compared with Model (s), Model (s + m) can learn domain <ref type="bibr" target="#b0">1</ref> As has been validated by <ref type="bibr" target="#b39">[40]</ref>, the variance, which is calculated from different-level features in a segmentation model, has a strong ability in measuring the uncertainty of a segmentation model when predicting pixel labels. We obtain one Variance Value to represent the uncertainty when the model segments one image. Thus, we calculate the MVV of all images in a specific domain dataset to show the overall performance in this domain. knowledge (related to the style factor) between domains s and m and thus close the style gap (from 0.089 to 0.067). However, the fog gap (0.037) remains large and approximately equals to the fog gap (0.033) before adapting Model (s) with m-domain data. That is, after closing the style gap, the fog gap still remains unchanged, which means the two gaps can be divided and closed respectively. Meanwhile, the dual gap is always an accumulation of the style gap and the fog gap before and after this adaptation.</p><p>The main contributions are summarized as follows. (1) To the best of our knowledge, we are the first to indicate the dual (style and fog) domain gap in SFSU, and also the first to investigate how to close the dual domain gap. Specifically, we propose a novel Cumulative Domain Adaptation (CuDA-Net) method, first disentangling the style factor and fog factor separately, and then the dual-factor jointly. <ref type="bibr" target="#b1">(2)</ref> We find the cumulative relationship of style, fog, and dual factors and thus propose a novel cumulative loss to further disentangle the three factors in a cyclical manner. (3) Our method outperforms state-of-the-arts on three widely used datasets in SFSU and shows generalization ability on other adverse scenes, such as rainy and snowy scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><formula xml:id="formula_0">Suppose that we have N s labeled images {(x i s , y i s )} Ns i=1</formula><p>from the source domain s, where y i s is the label, and N t unlabeled images {x i t } Nt i=1 from the target domain t. Our goal is to transfer the segmentation knowledge from the source domain s to the target domain t by our proposed CuDA-Net. Motivated by the success of <ref type="bibr" target="#b3">[4]</ref>, we use a similar framework as our basic unit to disentangle domaininvariant features from the domain-specific counterparts. However, since images in domain s and t are taken under different cities and weathers, they encounter large domain gaps caused by mixed style and fog factors, which challenges this method. Therefore, we propose to decompose the mixed factors into separate ones by introducing an intermediate domain m with N m unlabeled images {x i m } Nm i=1 , which share similar fog influence (no fog) with the source domain and similar style variation with the target domain (same city). <ref type="figure">Figure 3</ref> depicts the framework of our proposed method. It includes three sub-networks: F s?m , F m?t and F s?t , which share the same prototypes to disentangle the domain-invariant features from the domain-specific counterparts ( <ref type="figure">Figure 3a</ref>). They are fed with different input pairs (x s , x m ), (x m , x t ) and (x s , x t ) to close the style gap, the fog gap and the dual gap respectively. We train them one by one ( <ref type="figure">Figure 3b</ref>) and share the domain-invariant knowledge forward. After training these three sub-networks (initialization), we conduct a cyclical training ( <ref type="figure">Figure 3d</ref>) using the cumulative relation ( <ref type="figure">Figure 3c</ref>) as auxiliary loss, to help better disentangle the domain-invariant (content) features, which are used to produce segmentation heatmaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature Disentanglement Networks</head><p>Feature Disentanglement Networks (FDN) is the basic unit of our method, as shown in <ref type="figure">Figure 3a</ref>. Given images x 1 and x 2 from two different domains, with the "shared content space" assumption <ref type="bibr" target="#b15">[16]</ref>, it can disentangle domaininvariant content features c 1 and c 2 of these images from the domain-specific counterparts z 1 and z 2 . As has been validated by <ref type="bibr" target="#b3">[4]</ref>, the content features contributes most to the semantic segmentation task. Therefore, through feature disentanglement, we can transfer the segmentation knowledge from x 1 domain to x 2 domain.</p><p>Specifically, we first use a shared content encoder E c (black line) to extract c 1 and c 2 and two private encoders to extract domain-specific feature z 1 and z 2 respectively (red and blue line). Then, we use an shared image decoder D to decode an image using the content features c 1 , c 2 , and domain-specific feature z 1 , z 2 . Depending on which c and z we use, we can perform within-domain reconstruction and cross-domain translation to supervise the disentanglement learning. Besides, we use a segmentation head S to produce segmentation heatmaps h from the content feature c, where label y 1 is used as the supervision signal.</p><p>We build our FDN with a similar framework as DISE <ref type="bibr" target="#b3">[4]</ref> because both of us adopt the "shared content space" assumption <ref type="bibr" target="#b15">[16]</ref>. However, we only design four necessary losses to train our FDN, aiming to enable the FDN to close three different gaps (style gap, fog gap and dual gap). While, DISE <ref type="bibr" target="#b3">[4]</ref> utilizes seven losses to close one gap between synthetic clear data and real clear data, which is timeconsuming to train and hard to converge. Within-domain reconstruction. We expect images decoded using content feature c and private feature z extracted from the same image can perfectly reconstruct the original ones. Thus, we define the reconstruction loss as:</p><formula xml:id="formula_1">L rec = L pixel (x 1 ,x 1 ) + L pixel (x 2 ,x 2 ) ,<label>(1)</label></formula><p>where the pixel-wise loss L pixel (?, ?) is implemented by the perceptual loss <ref type="bibr" target="#b28">[29]</ref> with shadow layer features highlighted. Cross-domain translation. We recombine the content feature c from one domain image and private feature z from another domain image to generate the translated image. For example, in our sub-network F m?t in <ref type="figure">Figure 3b</ref>, by recombining the content feature c 2 t from x t and the private feature z 2 m from x m , we can generate an image, which can be regarded as the defogged version of x t . For the translated images x 1?2 and x 2?1 whose private features have been changed, we impose content consistency losses L con , which is implemented by the perceptual loss <ref type="bibr" target="#b28">[29]</ref> with the deep layer features highlighted, to constrain the content aspect of translated images and original images:</p><formula xml:id="formula_2">L trans = L con (x 1 , x 1?2 ) + L con (x 2 , x 2?1 ) . (2)</formula><p>Dense pixel prediction. Thanks to the domain invariance discovered by disentanglement learning, we can transfer the semantic knowledge across domains. We apply the segmentation head S on c 1 and c 2 to obtain the probability outputs of each pixel h 1 , h 2 ? R H?W ?C , where H, W, C represents the height, the width and the number of class categories, respectively. To supervise the training of the shared content encoder E c and the segmentation head S, we use the crossentropy to calculate segmentation loss L 1 seg between h 1 and its corresponding label y 1 . Besides, since the 1-domain-like image x 1?2 share the same content as x 1 , the labels y 1 can be the pseudo labels for x 1?2 . Hence, we calculate L 1?2 seg between h 1?2 and label y 1 , also using the cross-entropy. Aside from supervised losses, an adversarial loss L segadv at the output of the segmentation head S is introduced, in the hopes of making the content encoder E c and the S generalize well on the domain 2. To this end, we introduce the domain discriminator Dis and fool Dis by maximizing the probability of target domain prediction h 2 being considered as the source domain prediction:</p><formula xml:id="formula_3">? ? ? ? ? ? ? ? ? ? ? ?</formula><formula xml:id="formula_4">L segadv = ? h,w log Dis (h 2 ) (h,w,1) ,<label>(3)</label></formula><p>where 1 means the discriminator Dis perceive h 2 as the source domain prediction. Feature disentanglement loss. The disentanglement loss function in FDN is a weighted combination of each loss:</p><formula xml:id="formula_5">L 1?2 =? rec L rec + ? trans L trans +? seg (L 1 seg + L 1?2 seg ) + ? segadv L segadv ,<label>(4)</label></formula><p>where L 1?2 can be L s?m , L m?t or L s?t for the following disentanglement and the weights ? rec , ? trans , ? seg and ? segadv are empirically set as 0.5, 0.1, 1 and 1 to control the relative importance of reconstruction/translation quality, the prediction accuracy and domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Style and Fog Decomposition</head><p>The aforementioned FDN is designed to transfer the segmentation knowledge by disentangling the domaininvariant features and domain-specific features. However, directly applying FDN to domain s and domain t cannot achieve ideal performance. The reason we suppose is that the mixed dual-factor gap between domain s and domain t is too large to close, which is also the weakness of other domain adaptation methods. Thus, we introduce the intermediate domain m, decomposing the dual-factor gap into two single-factor gaps: style and fog. Since images in both domain s and m are captured in clear scenes, we assume there is only the style gap between them. Similarly, as images in both domain m and t are collected in the same city (Zurich), we assume there exists only the fog gap between them. Therefore, we use three sub-network F s?m , F m?t and F s?t to disentangle the style factor, fog factor and dualfactor one by one and gradually transfer the segmentation knowledge from domain s to t <ref type="figure">(Figure 3b</ref>).</p><p>Concretely, F s?m first utilizes two specific private style encoders E s sty and E m sty to extract the latent style features z 1 s and z 1 m , respectively. The labels {y i s } Ns i=1 supervise the training process. After then, except for the two private style encoders, the remaining part of this trained F s?m , which we perceive as domain shared part and represents segmentation knowledge, will be passed to the next sub-network F m?t . In other words, an content encoder E c , segmentation head S, image decoder D and domain discriminator Dis are used as initialization of sub-network F m?t . Note that the domain m has no labels, we used the trained F s?m to generate pseudo labels for training the F m?t . After training F m?t , except for the two fog encoders E m fog and E t fog , the domain shared part of F m?t are used as the initialization of sub-network F s?t . Likewise, F s?t uses two dualfactor (style and fog) encoders E s dual and E t dual to extract the latent dual-factor features z 3 s and z 3 t , respectively. To put it simply, through the training of F s?m , F m?t and F s?t , we pass down the segmentation knowledge from domain s to domain t in a more effective way and obtain three pairs of domain-specific feature encoders for further feature disentanglement in the cumulative domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Cumulative Domain Adaptation</head><p>Cumulative loss. As verified in our motivation, there exists a cumulative relationship among three kinds of domain factors (private features). As shown in <ref type="figure">Figure 3c</ref>, if we take ?(z m , z s ) as the style discrepancy between domain m and s, take ?(z t , z m ) as the fog discrepancy between domain t and m, and take ?(z t , z s ) as the dual discrepancy between domain t and s, it is reasonable to assume that the dual discrepancy is a cumulation of the style and fog discrepancies, namely, ?(z m , z s ) + ?(z t , z m ) = ?(z t , z s ). Thus, we design the cumulative relationship loss function as:</p><formula xml:id="formula_6">L cum = ?(z m , z s ) + ?(z t , z m ) ? ?(z t , z s ) 2 . (5)</formula><p>Then, we take a step further and use this cumulative loss L cum as an additional loss to conduct our proposed cumulative domain adaptation, by utilizing private encoders trained in the first three steps before. Training pipeline. <ref type="figure">Figure 3d</ref> depicts the whole training process. The three trained sub-network F s?m , F m?t and F s?t are used as the initialization of the cumulative domain adaptation. Specifically, we use all modules in F s?t (an content encoder, two dual-factor encoders, an image decoder and an segmentation head), two style encoders in F s?m and two fog encoders in F m?t to build up the whole network. Next, as shown in <ref type="figure">Figure 3b</ref>, we input (x s , x m , x t ) tuples for extracting style, fog and dual private features. Then, we freeze the two fog encoders and two dual-factor encoders and train other modules (especially two style encoders and content encoders) in the whole network, using the final loss below:</p><formula xml:id="formula_7">L f inal = L s?m + ? cum L cum .<label>(6)</label></formula><p>After this training, we assume the style encoders can capture domain-specific style features better due to combining feature disentanglement loss L s?m with the cumulative loss L cum and the content encoders can better extract shared content features, which is used to produce the segmentation heatmap. The following two steps in <ref type="figure">Figure 3d</ref> have the same function and the only difference is which private encoders we train and which encoders we freeze. Note that the shared content encoder is always trainable in the three steps, and we use the content encoder to update pseudo labels for training fog encoders. Besides, we train the whole network in a cyclical manner, hoping to improve the disentangling ability of three pairs of private encoders alternatively and continuously enhance the shared content encoder. Empirically, we set T as 3, which means we conduct the cyclical cumulative training three times. Finally, we use the trained content encoder and segmentation head S in F s?t to produce the segmentation heatmaps for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>Cityscapes <ref type="bibr" target="#b7">[8]</ref> is a real-world dataset composed of street view images captured in 50 different cities. Its data split includes 2975 training images and 500 validation images. Foggy Cityscapes DBF <ref type="bibr" target="#b26">[27]</ref> has 550 synthetic foggy images in total, including 498 training images and 52 testing images. These images are selected from Cityscapes and synthesized with fog using depth information. We use 498 clear images from Cityscapes as the source domain dataset, named as Clear Cityscapes. Note that images in Clear Cityscapes are not captured in Zurich city. Foggy Zurich* <ref type="bibr" target="#b25">[26]</ref> contains 3808 real-world foggy road scenes in the city of Zurich and its suburbs. According to fog density, it is split into two categories -light and medium, consisting of 1552 images and 1498 images, respectively. We use the medium category as the target domain dataset, named as Foggy Zurich. Besides, it has a test set-Foggy Zurich-test including 40 images with labels that are compatible with Cityscapes <ref type="bibr" target="#b7">[8]</ref>.</p><p>Foggy Driving <ref type="bibr" target="#b25">[26]</ref>. It is a collection of 101 real-world foggy road-scenes images, in which 33 images are finely annotated and the rest 68 images are coarsely annotated. They are purely used for testing. Clear Zurich. We manually select 248 images from the light category of Foggy Zurich* <ref type="bibr" target="#b25">[26]</ref> and term this dataset as Ours CuDA-Net+ DeepLab-v2 49.1 53.5 ? Since the synthesis-based methods use additional synthetic data, for fair comparison, we also add these data to train our sub-network Fm?t before cumulative domain adaptation, named CuDA-Net+. ACDC <ref type="bibr" target="#b27">[28]</ref>. It contains four adverse-condition categories (fog, rain, snow and nighttime) with pixel-level annotations. Each of them contains 1000 images and is split into train set, validation set and test set for roughly 4:1:5 proportion. The test set is withheld for testing online. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Performance Comparison</head><p>We compare our method against several kinds of methods, including 1) backbones: RefineNet <ref type="bibr" target="#b18">[19]</ref> and DeepLab-v2 <ref type="bibr" target="#b4">[5]</ref>; 2) defogging-based: MSCNN <ref type="bibr" target="#b22">[23]</ref>, DCP <ref type="bibr" target="#b14">[15]</ref>, Nonlocal <ref type="bibr" target="#b2">[3]</ref>, DCPDN <ref type="bibr" target="#b36">[37]</ref> and GFN <ref type="bibr" target="#b23">[24]</ref>; 3) DA-based: Multitask <ref type="bibr" target="#b0">[1]</ref>, AdSegNet <ref type="bibr" target="#b32">[33]</ref>, ADVENT <ref type="bibr" target="#b33">[34]</ref>, CCM <ref type="bibr" target="#b17">[18]</ref>, SAC <ref type="bibr" target="#b1">[2]</ref>, ProDA <ref type="bibr" target="#b37">[38]</ref>, DMLC <ref type="bibr" target="#b12">[13]</ref>, DACS <ref type="bibr" target="#b30">[31]</ref> and our baseline DISE <ref type="bibr" target="#b3">[4]</ref>; 4) synthesis-based: SFSU <ref type="bibr" target="#b26">[27]</ref>, CMAda2 <ref type="bibr" target="#b25">[26]</ref>, CMAda3+ <ref type="bibr" target="#b8">[9]</ref>, CycleGAN <ref type="bibr" target="#b40">[41]</ref>, MUNIT <ref type="bibr" target="#b15">[16]</ref>, Ana-logicalGAN <ref type="bibr" target="#b11">[12]</ref>. 5) Defogging/Synthesis + DA-based: MSCNN+DISE, SFSU+DISE. The mean Intersection-Over-Union (mIoU) results on Foggy Zurich and Foggy Driving are reported in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>For Defogging-based methods, we first use these methods to defog the real foggy test images and then use the backbone segmentation model to produce the predictions. For Domain Adaptation based methods, we set the source domain data of as clear cityscapes, s domain of our method. As for the target domain data, we combine the Clear Zurich and Foggy Zurich, which are used as the m domain and t domain in our method. By using the same amount of training data, we ensure a fair comparison with DA-based methods. For Defogging+Domain Adaptation methods, we first use the MSCNN <ref type="bibr" target="#b22">[23]</ref> to defog the target domain data (including training data and test data) and then use DISE <ref type="bibr" target="#b3">[4]</ref> to bridge the domain gap.</p><p>For Synthesis-based methods, the paradigm is to finetune the segmentation model pretrained on the real clear weather images (Cityscapes) with synthetic foggy images, e.g., Foggy Cityscapes DBF, and labels corresponding to its clear weather images. The difference in these Synthesis-based methods is that they use different methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41]</ref> to generate the synthetic foggy images. Finally, the finetuned model is tested on real foggy images. For a fair comparison with CMAda3+ <ref type="bibr" target="#b8">[9]</ref>, we also add Foggy Cityscapes</p><formula xml:id="formula_8">Input MSCNN DISE CMAda3+</formula><p>CuDA-Net (ours) Ground Truth DBF as extra data to train sub-network F m?t before cumulative training, which we name as CuDA-Net+. The results in <ref type="table" target="#tab_2">Table 1</ref> show that although the backbone model DeepLab-v2 performs not well as RefineNet, our proposed method CuDA-Net (using DeepLab-v2 as the backbone) achieves a top performance, outperforming all state-of-the-art methods. We also achieve SOTA on ACDC <ref type="bibr" target="#b27">[28]</ref> dataset (see ACDC-fog benchmark website). Besides, we can see that the DA-based methods, which directly adapt the segmentation model from domain s to domain t, can not significantly improve the performance compared to our method. This is consistent with our assumption that general domain adaptation methods can not perform well when the domain gap is too large and affected by different factors (style and fog), also proving the necessity of investigating both style and fog factor in this setting. The results also show that the defogging-based methods cannot always obtain good performances. It is because defoggingbased methods require pair-wise training data to remove the fog, but we can not obtain this kind of data in SFSU.</p><p>In <ref type="table" target="#tab_2">Table 1</ref>, when we introduce the synthetic foggy scene datasets -Foggy Cityscapes DBF simulated in CMAda3+ to our method, our CuDA-Net+ further improve the performance, outperforming CMAda3+ by 2.3% on FZ (3.7% on FD). Note that, our CuDA-Net+ improves by 0.9% from CuDA-Net on FZ when we only introduce 498 dense synthetic foggy images, indicating synthesizing foggy images and our CuDA-Net can complement each other very well. However, combining DISE <ref type="bibr" target="#b3">[4]</ref> with the defogging method MSCNN <ref type="bibr" target="#b22">[23]</ref> or the fog synthesis method SFSU <ref type="bibr" target="#b26">[27]</ref> can- not yield better performance than only using DISE <ref type="bibr" target="#b3">[4]</ref>. The qualitative comparison is shown in <ref type="figure">Figure 4</ref>. The red boxes clearly show that our method CuDA-Net can better deal with the details than CMAda3+, especially for the classes in the boundary of sky and other objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussion</head><p>In this section, we conduct a series of ablation studies to validate the contributions of individual components to the final foggy scene understanding. Effectiveness of style and fog decomposition. In <ref type="table" target="#tab_4">Table 3</ref>, the non-adapted model Deeplabv2, which is also the backbone of our CuDA-Net, only gives 25.89 mIoU on FZ. When using 'F s?m ', the performance increases to 39.16, revealing that the style adaptation matters. When using 'F s?m +F m?t ', i.e. first conducting style adaptation and then fog adaptation, we bring +3.33 mIoU gain, additionally indicating that the fog adaptation matters. Note that using 'F s?m +F m?t ' is 2.28 higher than only using 'F s?t ', which demonstrates that directly transferring with style and fog is not as good as two step adaptation. Other than that, using 'F s?m +F m?t +F s?t ' boosts the performance to 43.06, showing the necessity of dual-factor adaptation. Effectiveness of cyclical training. We investigate the importance of cyclical training without L cum , i.e.using Equation (4). As shown in <ref type="table" target="#tab_4">Table 3</ref>, when we set T as 2, cyclical training improves the performance by 2.72. When we set T as 1 or 3, both results are close to 45.78, which shows the performance is not sensitive to the selection of T . Effectiveness of cumulative loss. We also investigate the effects of cumulative loss L cum in <ref type="table" target="#tab_4">Table 3</ref>. We fix the T as 2 and use different distance metrics to calculate the domain discrepancy between two domains in the cumulative training. We find the L2 distance attains top performance. Input GFN Ours (F m?t ) <ref type="figure">Figure 7</ref>. The ability of defogging. We compare our defogged images generated by the Fm?t in CuDA-Net with those from the conventional defogging method GFN <ref type="bibr" target="#b23">[24]</ref>. The input images are randomly selected from Foggy Zurich.</p><p>We also show some subjective segmentation results in <ref type="figure" target="#fig_2">Figure 6</ref>. They clearly indicate that segmentation results go better as more components are used in CuDA-Net. Effects of the hyper-parameter ? cum in Equation <ref type="bibr" target="#b5">(6)</ref>. We conduct ablation study on ? cum on Foggy Zurich-test dataset in <ref type="figure">Figure 5</ref>. The results show our model is not sensitive to ? cum and we set ? cum as 0.25 because its top performance. We also conduct an loss ablation study on Equation (4), results are shown in the supplementary material. Manual selection v.s. CNN-based selection for constructing m domain dataset. For constructing Clear Zurich, we manually select 248 images from the light category of Foggy Zurich* <ref type="bibr" target="#b25">[26]</ref> based on the human vision, to see whether they are clear or not. To prove its effectiveness, we also train a CNN to discriminate the clearness of the images in the light category of Foggy Zurich* <ref type="bibr" target="#b25">[26]</ref> and select the top 248 images to construct the m domain. We set random seeds as 10, 20 and 50 to train the whole CuDA-Net framework. As shown in <ref type="table" target="#tab_5">Table 4</ref>, we find the manual selection functions better than CNN-based selection, which shows the necessity of our manual selection scheme. Visualization of Defogging. Although our CuDA-Net aims to transfer style and fog for foggy scene understanding, it is also capable of defogging foggy images during disentanglement learning, as mentioned in the cross-domain translation part of Section 2.1. In <ref type="figure">Figure 7</ref>, we visualize the results <ref type="table">Table 5</ref>. Generalization to rainy and snowy scenes. We train our baseline on the ACDC rainy and snowy subsets and test it on the corresponding validation set, where Fs?m+t means we combine the m domain and t domain data as the whole target domain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting</head><p>F s?m+t F s?m F s?m +F m?t ACDC (rain) 46.2 43.9 48.5 ACDC (snow) 44.8 42.6 47.2 of defogging and compare our method with the defogging method GFN <ref type="bibr" target="#b23">[24]</ref>. The results clearly show that our method can remove the fog well and does not destroy the content of the images, while GFN <ref type="bibr" target="#b23">[24]</ref> brings in the color distortion. Generalization to rainy and snowy scenes. Thanks to the ACDC <ref type="bibr" target="#b27">[28]</ref> datasets, we can test our method on rainy and snowy scenes in <ref type="table">Table 5</ref>. The results show that our proposed two-steps adaptation is better than directly adapting from the source domain to the target domain in other adverse scenes, indicating the potential of our method to address the understanding of different adverse scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we propose the Cumulative style-fog-dual disentanglement Domain Adaptation method (CuDA-Net) for the SFSU task. We assume that the dual (style and fog) domain gap exists in SFSU, and that style, fog and dual factors have a cumulative relationship. Our method outperforms state-of-the-art methods on three widely-used datasets in SFSU and shows generalization ability to other adverse scenes, such as rainy and snowy scenes. We will make the code publicly available. Limitation. (1) Our method chose DISE <ref type="bibr" target="#b3">[4]</ref> as our baseline, which can be replaced with other new stronger disentanglement-based domain adaptation methods. By doing so, we believe our CuDA-Net can achieve better performance. <ref type="bibr" target="#b1">(2)</ref> We conduct primary experiments to showcase certain generalization ability to rainy and snowy scenes, and more detailed analysis can be done to verify whether the cumulative relationship exists in other adverse settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The problem and our main idea.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>The qualitative comparison with the SOTA methods. The input images are randomly selected from Foggy Zurich-test. The red boxes clearly show that our method can better deal with the details than the SOTA methods. Ablation study on ?cum in Eq. (6) on Foggy Zurichtest dataset. The results show our model is not sensitive to ?cum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results of ablation study. These experiments are conducted on the Foggy Zurich-test dataset. Each column shows the results of the proposed method with different components. The results show more clear spatial structure as more components are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The proposed method. a, The feature disentanglement network (FDN) disentangles domain-invariant content features from the domain-specific counterparts for images from two different domains. b, By introducing the intermediate domain m, we can obtain three different input domain combinations, (xs, xm), (xm, xt) and (xs, xt), for three FDNs, Fs?m, Fm?t and Fs?t, to tackle the style gap, the fog gap and the dual gap respectively. Three FDNs are trained one by one, where the domain-invariant knowledge is shared. As there are no labels for both domain m and t, we use Fs?m to tag domain m for training Fm?t. d, The whole pipeline. We first initialize three FDNs by training each of them once, as in b. Then, we conduct cyclical training, using the cumulative relation (c) as an auxiliary loss, for better disentangling the domain-invariant (content) features, which are used to produce segmentation heatmaps.</figDesc><table><row><cell>source domain</cell><cell>target domain</cell><cell>domain-specific</cell><cell>domain-shared</cell><cell>reconstruction loss</cell><cell>translation loss</cell><cell>CE loss</cell><cell>label</cell></row><row><cell>encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>decoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0/1</cell><cell></cell><cell></cell></row><row><cell cols="2">within-domain reconstruction</cell><cell cols="2">cross-domain translation</cell><cell cols="5">dense pixel predictio?1</cell></row><row><cell></cell><cell></cell><cell cols="3">a. feature disentanglement network (FDN)</cell><cell></cell><cell></cell><cell></cell><cell>c. cumulative relation</cell></row><row><cell cols="2">style gap</cell><cell></cell><cell>2 fog gap</cell><cell></cell><cell>3 dual gap</cell><cell></cell><cell></cell><cell>Initialization</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 2 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>cumulative loss Eq. (5)</cell><cell>1 2 3 1 2 3</cell></row><row><cell></cell><cell>FDN</cell><cell></cell><cell></cell><cell>FDN</cell><cell></cell><cell></cell><cell>FDN</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>frozen</cell></row><row><cell>real label</cell><cell></cell><cell>tagging</cell><cell>pseudo label</cell><cell></cell><cell>real label</cell><cell></cell><cell></cell><cell>? trainabl? End</cell></row><row><cell></cell><cell></cell><cell cols="3">b. three steps of the pipeline</cell><cell></cell><cell></cell><cell></cell><cell>d. whole pipeline</cell></row><row><cell>Figure 3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison. Experiments are conducted on Foggy Zurich (FZ) and Foggy Driving (FD), measured with mean IoU (mIoU %) over all classes. For results on ACDC, please refer to the ACDC-fog benchmark website.</figDesc><table><row><cell>Experiment</cell><cell>Method</cell><cell>Backbone</cell><cell>FZ</cell><cell>FD</cell></row><row><cell>Backbone</cell><cell>--</cell><cell cols="3">DeepLab-v2 [19] 25.9 35.7 RefineNet [5] 34.6 35.8</cell></row><row><cell></cell><cell>MSCNN [23]</cell><cell>RefineNet</cell><cell cols="2">34.4 38.3</cell></row><row><cell></cell><cell>DCP [15]</cell><cell>RefineNet</cell><cell cols="2">31.2 33.2</cell></row><row><cell>Defogging</cell><cell>Non-local [3]</cell><cell>RefineNet</cell><cell cols="2">27.6 32.8</cell></row><row><cell></cell><cell>GFN [24]</cell><cell>DeepLab-v2</cell><cell cols="2">27.5 37.2</cell></row><row><cell></cell><cell>DCPDN [37]</cell><cell>DeepLab-v2</cell><cell cols="2">28.7 37.9</cell></row><row><cell></cell><cell>Multi-task [1]</cell><cell>-</cell><cell cols="2">26.1 31.6</cell></row><row><cell></cell><cell>AdSegNet [33]</cell><cell>DeepLab-v2</cell><cell cols="2">26.1 37.6</cell></row><row><cell></cell><cell>ADVENT [34]</cell><cell>DeepLab-v2</cell><cell cols="2">24.5 36.1</cell></row><row><cell>Domain Adaptation</cell><cell>DISE [4] CCM [18] SAC [2]</cell><cell>DeepLab-v2 DeepLab-v2 DeepLab-v2</cell><cell cols="2">40.7 45.2 35.8 42.6 37.0 43.4</cell></row><row><cell></cell><cell>ProDA [38]</cell><cell>DeepLab-v2</cell><cell cols="2">37.8 41.2</cell></row><row><cell></cell><cell>DMLC [13]</cell><cell>DeepLab-v2</cell><cell cols="2">33.5 32.6</cell></row><row><cell></cell><cell>DACS [31]</cell><cell>DeepLab-v2</cell><cell cols="2">28.7 35.0</cell></row><row><cell cols="3">Defogging+DA MSCNN [23]+DISE [4] DeepLab-v2</cell><cell cols="2">38.6 37.1</cell></row><row><cell>Ours</cell><cell>CuDA-Net</cell><cell>DeepLab-v2</cell><cell cols="2">48.2 52.7</cell></row><row><cell></cell><cell>SFSU [27]</cell><cell>RefineNet</cell><cell cols="2">35.7 35.9</cell></row><row><cell></cell><cell>CMAda2 [26]</cell><cell>RefineNet</cell><cell cols="2">42.9 37.3</cell></row><row><cell>Synthesis  ?</cell><cell>CycleGAN [41] MUNIT [16]</cell><cell>RefineNet RefineNet</cell><cell cols="2">40.5 47.7 39.1 47.8</cell></row><row><cell></cell><cell>AnalogicalGAN [12]</cell><cell>RefineNet</cell><cell cols="2">42.3 47.5</cell></row><row><cell></cell><cell>CMAda3+ [9]</cell><cell>RefineNet</cell><cell cols="2">46.8 49.8</cell></row><row><cell>Synthesis+DA</cell><cell>SFSU [27]+DISE [4]</cell><cell>DeepLab-v2</cell><cell cols="2">39.3 39.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Training data comparison with CMAda3+. Both our CuDA-Net and CuDA-Net+ outperform CMAda3+, using less synthetic foggy data and less real foggy data. 'light', 'medium' and 'dense' in the table indicates the different fog density. Clear Zurich. We use this Clear Zurich as an intermediate domain dataset because we perceive these images visually as clear scene images.</figDesc><table><row><cell>Training data used</cell><cell>CMAda3+</cell><cell>CuDA-Net</cell><cell>CuDA-Net+</cell></row><row><cell>Clear Cityscapes</cell><cell>498</cell><cell>498</cell><cell>498</cell></row><row><cell>Foggy Cityscapes DBF (synthetic fog)</cell><cell>498 (light) 498 (medium) 498 (dense)</cell><cell>---</cell><cell>--498 (dense)</cell></row><row><cell>Foggy Zurich*</cell><cell>1552 (light)</cell><cell>248 (light)</cell><cell>248 (light)</cell></row><row><cell>(real fog)</cell><cell cols="3">1498 (medium) 1498 (medium) 1498 (medium)</cell></row><row><cell>Total Number</cell><cell>5042</cell><cell>2244</cell><cell>2742</cell></row><row><cell>mIoU (on FZ)</cell><cell>46.8</cell><cell>48.2</cell><cell>49.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation study. These experiments are performed on Foggy Zurich-test dataset.</figDesc><table><row><cell></cell><cell>Components</cell><cell></cell><cell>mIoU</cell><cell>gain</cell></row><row><cell>Initialization</cell><cell>Deeplabv2</cell><cell></cell><cell cols="2">25.89 +0.00</cell></row><row><cell>F s?m</cell><cell>F m?t</cell><cell cols="2">F s?t mIoU</cell><cell>gain</cell></row><row><cell>Style and Fog Decomposition</cell><cell></cell><cell></cell><cell cols="2">39.16 +13.27 42.49 +16.60 40.21 +14.32</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">43.06 +17.17</cell></row><row><cell>T = 1</cell><cell cols="3">T = 2 T = 3 mIoU</cell><cell>gain</cell></row><row><cell>Cyclical Training</cell><cell></cell><cell></cell><cell cols="2">45.32 +19.43 45.78 +19.89</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">45.45 +19.56</cell></row><row><cell>L1</cell><cell>Cosine</cell><cell>L2</cell><cell>mIoU</cell><cell>gain</cell></row><row><cell>Cumulative Loss</cell><cell></cell><cell></cell><cell cols="2">47.64 +21.75 47.23 +21.34</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">48.21 +22.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Different selection schemes for constructing m domain. We set three random seeds to ensure fair comparison and test the trained model on Foggy Zurich-test dataset.</figDesc><table><row><cell>Random seed</cell><cell>10</cell><cell>20</cell><cell>50</cell></row><row><cell cols="2">CNN-based selection 47.5</cell><cell>47.5</cell><cell>47.2</cell></row><row><cell>Manual selection</cell><cell>48.0</cell><cell>48.2</cell><cell>48.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Based on this finding, we propose a cumulative domain adaptation framework to address semantic foggy scene understanding, considering both style factor and fog factor in this task. As shown inFigure 1, by adding an intermediate domain m as a bridge, we can decompose the mixed dualfactor gap into two single-factor gaps: the style gap and the fog gap. Specifically, we disentangle the style and fog factor separately, and then the dual-factor (style and fog) jointly, which ensures an effective segmentation knowledge transfer from the source domain to the target domain. Besides, we assume that the dual-factor gap is an accumulation of the style gap and the fog gap. Thus, we further propose a novel cumulative loss to represent this relationship and collaborate the disentanglement of three factors with the cumulative loss in a cyclical manner, enabling our network to transfer segmentation knowledge continuously and further improving the performance.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Competitive simplicity for multi-task learning for real-time foggy scene understanding via domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naif</forename><surname>Alshammari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<idno>abs/2012.05304</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-supervised augmentation consistency for adapting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">All about structure: Adapting structural information across domains for boosting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui-Po</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsiao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selfensembling with gan-based data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taekyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6829" to="6839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Curriculum model adaptation with synthetic and real data for semantic foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1182" to="1204" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning of image dehazing models for segmentation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ihsen</forename><surname>S?bastien De Blois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hedhli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gagn?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUSIPCO</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dsp: Dual soft-paste for unsupervised domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2825" to="2833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analogical image translation for fog generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Metacorrection: Domain-aware meta loss correction for unsupervised domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic understanding of foggy scenes with purely synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hahner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Nico</forename><surname>Zaech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3675" to="3681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning texture invariant representation for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeongjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Content-consistent matching for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Instance adaptive self-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="415" to="430" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXVI 16</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrast restoration of weather degraded images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shree</forename><forename type="middle">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="713" to="724" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Does haze removal help cnn-based image classification? ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanting</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single image dehazing via multiscale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wenqi Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gated fusion network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3253" to="3261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep video dehazing with semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingang</forename><surname>Wenqi Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1895" to="1908" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="687" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="973" to="992" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ACDC: The adverse conditions dataset with correspondences for semantic driving scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dacs: Domain adaptation via crossdomain mixed sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bimal: Bijective maximum likelihood approach to domain adaptation in semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Dat</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Nhan</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><forename type="middle">Lam</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chase</forename><surname>Rainwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8548" to="8557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="642" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Densely connected pyramid dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6810" to="6818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
