<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Semi-Supervised Learning with Graph Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
							<email>zhiliny@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>wcohen@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Semi-Supervised Learning with Graph Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.</p><p>Recently developed unsupervised representation learning</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semi-supervised learning aims to leverage unlabeled data to improve performance. A large number of semisupervised learning algorithms jointly optimize two training objective functions: the supervised loss over labeled data and the unsupervised loss over both labeled and unlabeled data. Graph-based semi-supervised learning defines the loss function as a weighted sum of the supervised loss over labeled instances and a graph Laplacian regularization term <ref type="bibr" target="#b24">(Zhu et al., 2003;</ref><ref type="bibr" target="#b23">Zhou et al., 2004;</ref><ref type="bibr" target="#b0">Belkin et al., 2006;</ref><ref type="bibr" target="#b20">Weston et al., 2012)</ref>. The graph Laplacian regularization is based on the assumption that nearby nodes in a graph are likely to have the same labels. Graph Laplacian regularization is effective because it constrains the labels to be consistent with the graph structure. methods learn embeddings that predict a distributional context, e.g. a word embedding might predict nearby context words <ref type="bibr" target="#b11">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b12">Pennington et al., 2014)</ref>, or a node embedding might predict nearby nodes in a graph <ref type="bibr" target="#b13">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b17">Tang et al., 2015)</ref>. Embeddings trained with distributional context can be used to boost the performance of related tasks. For example, word embeddings trained from a language model can be applied to partof-speech tagging, chunking and named entity recognition <ref type="bibr" target="#b6">(Collobert et al., 2011;</ref><ref type="bibr" target="#b22">Yang et al., 2016)</ref>.</p><p>In this paper we consider not word embeddings but graph embeddings. Existing results show that graph embeddings are effective at classifying the nodes in a graph, such as user behavior prediction in a social network <ref type="bibr" target="#b13">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b17">Tang et al., 2015)</ref>. However, the graph embeddings are usually learned separately from the supervised task, and hence do not leverage the label information in a specific task. Hence graph embeddings are in some sense complementary to graph Laplacian regularization that does not produce useful features itself and might not be able to fully leverage the distributional information encoded in the graph structure.</p><p>The main highlight of our work is to incorporate embedding techniques into the graph-based semi-supervised learning setting. We propose a novel graph-based semisupervised learning framework, Planetoid (Predicting Labels And Neighbors with Embeddings Transductively Or Inductively from Data). The embedding of an instance is jointly trained to predict the class label of the instance and the context in the graph. We then concatenate the embeddings and the hidden layers of the original classifier and feed them to a softmax layer when making the prediction.</p><p>Since the embeddings are learned based on the graph structure, the above method is transductive, which means we can only predict instances that are already observed in the graph at training time. In many cases, however, it may be desirable to have an inductive approach, where predictions can be made on instances unobserved in the graph seen at training time. To address this issue, we further develop an inductive variant of our framework, where we define the arXiv:1603.08861v2 <ref type="bibr">[cs.</ref>LG] 26 May 2016 embeddings as a parameterized function of input feature vectors; i.e., the embeddings can be viewed as hidden layers of a neural network.</p><p>To demonstrate the effectiveness of our proposed approach, we conducted experiments on five datasets for three tasks, including text classification, distantly supervised entity extraction, and entity classification. Our inductive method outperforms the second best inductive method by up to 18.7% 1 points and on average 7.8% points in terms of accuracy. The best of our inductive and transductive methods outperforms the best of all the other compared methods by up to 8.5% and on average 4.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semi-Supervised Learning</head><p>Let L and U be the number of labeled and unlabeled instances. Let x 1:L and x L+1:L+U denote the feature vectors of labeled and unlabeled instances respectively. The labels y 1:L are also given. Based on both labeled and unlabeled instances, the problem of semi-supervised learning is defined as learning a classifier f : x ? y. There are two learning paradigms, transductive learning and inductive learning. Transductive learning <ref type="bibr" target="#b24">(Zhu et al., 2003;</ref><ref type="bibr" target="#b23">Zhou et al., 2004)</ref> only aims to apply the classifier f on the unlabeled instances observed at training time, and the classifier does not generalize to unobserved instances. For instance, transductive support vector machine (TSVM) <ref type="bibr" target="#b10">(Joachims, 1999)</ref> maximizes the "unlabeled data margin" based on the low-density separation assumption that a good decision hyperplane lies on a sparse area of the feature space. Inductive learning <ref type="bibr" target="#b0">(Belkin et al., 2006;</ref><ref type="bibr" target="#b20">Weston et al., 2012)</ref>, on the other hand, aims to learn a parameterized classifier f that is generalizable to unobserved instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph-Based Semi-Supervised Learning</head><p>In addition to labeled and unlabeled instances, a graph, denoted as a (L + U ) ? (L + U ) matrix A, is also given to graph-based semi-supervised learning methods. Each entry a ij indicates the similarity between instance i and j, which can be either labeled or unlabeled. The graph A can either be derived from distances between instances <ref type="bibr" target="#b24">(Zhu et al., 2003)</ref>, or be explicitly derived from external data, such as a knowledge graph <ref type="bibr" target="#b21">(Wijaya et al., 2013)</ref> or a citation network between documents <ref type="bibr" target="#b9">(Ji et al., 2010)</ref>. In this paper, we mainly focus on the setting that a graph is explicitly given and represents additional information not present in the feature vectors (e.g., the graph edges correspond to hyperlinks between documents, rather than distances between the bag-of-words representation of a document). 1 % refers to absolute percentage points thoughout the paper.</p><p>Graph-based semi-supervised learning is based on the assumption that nearby nodes tend to have the same labels. Generally, the loss function of graph-based semisupervised learning in the binary case can be written as</p><formula xml:id="formula_0">L i=1 l(y i , f (x i )) + ? i,j a ij f (x i ) ? f (x j ) 2 = L i=1 l(y i , f (x i )) + ?f T ?f<label>(1)</label></formula><p>In Eq. (1), the first term is the standard supervised loss function, where l(?, ?) can be log loss, squared loss or hinge loss. The second term is the graph Laplacian regularization, which incurs a large penalty when similar nodes with a large w ij are predicted to have different labels</p><formula xml:id="formula_1">f (x i ) = f (x j ). The graph Laplacian matrix ? is defined as ? = A ? D,</formula><p>where D is a diagonal matrix with each entry defined as d ii = j a ij . ? is a constant weighting factor. (Note that we omit the parameter regularization terms for simplicity.) Various graph-based semi-supervised learning algorithms define the loss functions as variants of Eq. (1). Label propagation <ref type="bibr" target="#b24">(Zhu et al., 2003)</ref> forces f to agree with labeled instances y 1:L ; f is a label lookup table for unlabeled instances in the graph, and can be obtained with a closed-form solution. Learning with local and global consistency <ref type="bibr" target="#b23">(Zhou et al., 2004)</ref> defines l as squared loss and f as a label lookup table; it does not force f to agree with labeled instances. Modified Adsorption (MAD) <ref type="bibr" target="#b16">(Talukdar &amp; Crammer, 2009</ref>) is a variant of label propagation that allows prediction on labeled instances to vary and incorporates node uncertainty. Manifold regularization <ref type="bibr" target="#b0">(Belkin et al., 2006)</ref> parameterizes f in the Reproducing Kernel Hilbert Space (RKHS) with l being squared loss or hinge loss. Since f is a parameterized classifier, manifold regularization is inductive and can naturally handle unobserved instances.</p><p>Semi-supervised embedding <ref type="bibr" target="#b20">(Weston et al., 2012)</ref> extends the regularization term in Eq. (1) to be i,j a ij g(x i ) ? g(x j ) 2 , where g represents embeddings of instances, which can be the output labels, hidden layers or auxiliary embeddings in a neural network. By extending the regularization from f to g, this method imposes stronger constraints on a neural network. Iterative classification algorithm (ICA) <ref type="bibr" target="#b14">(Sen et al., 2008)</ref> uses a local classifier that takes the labels of neighbor nodes as input, and employs an iterative process between estimating the local classifier and assigning new labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Learning Embeddings</head><p>Extensive research was done on learning graph embeddings. A probabilistic generative model was proposed to learn node embeddings that generate the edges in a graph <ref type="table">Table 1</ref>. Comparison of various semi-supervised learning algorithms and graph embedding algorithms.</p><p>? means using the given formulation or information; ?means not available or not using the information. In the column graph, regularization means imposing regularization with the graph structure; features means using graph structure as features; context means predicting the graph context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Features Labels Paradigm Embeddings Graph TSVM <ref type="bibr" target="#b10">(Joachims, 1999</ref>) <ref type="bibr" target="#b24">(Zhu et al., 2003)</ref> ? ? Transductive ? Regularization Manifold Reg <ref type="bibr" target="#b0">(Belkin et al., 2006)</ref> ? ? Inductive ? Regularization ICA <ref type="bibr" target="#b14">(Sen et al., 2008)</ref> ? <ref type="bibr" target="#b15">(Snijders &amp; Nowicki, 1997)</ref>. A clustering method <ref type="bibr" target="#b8">(Handcock et al., 2007)</ref> was proposed to learn latent social states in a social network to predict social ties.</p><formula xml:id="formula_2">? ? Transductive ? ? Label propagation</formula><formula xml:id="formula_3">? Transductive ? Features MAD (Talukdar &amp; Crammer, 2009) ? ? Transductive ? Regularization Semi Emb (Weston et al., 2012) ? ? Inductive ? Regularization Graph Emb (Perozzi et al., 2014) ? ? Transductive ? Context Planetoid (this paper) ? ? Both ? Context</formula><p>More recently, a number of embedding learning methods are based on the Skipgram model, which is a variant of the softmax model. Given an instance and its context, the objective of Skipgram is usually formulated as minimizing the log loss of predicting the context using the embedding of an instance as input features. Formally, let {(i, c)} be a set of pairs of instance i and context c, the loss function can be written as</p><formula xml:id="formula_4">? (i,c) log p(c|i) = ? (i,c) w T c e i ? log c ?C exp(w T c e i )</formula><p>(2) where C is the set of all possible context, w's are parameters of the Skipgram model, and e i is the embedding of instance i. Skipgram was first introduced to learn representations of words, known as word2vec <ref type="bibr" target="#b11">(Mikolov et al., 2013)</ref>. In word2vec, for each training pair (i, c), the instance i is the current word whose embedding is under estimation; the context c is each of the surrounding words of i within a fixed window size in a sentence; the context space C is the vocabulary of the corpus. Skipgram was later extended to learn graph embeddings. Deepwalk <ref type="bibr" target="#b13">(Perozzi et al., 2014)</ref> uses the embedding of a node to predict the context in the graph, where the context is generated by random walk. More specifically, for each training pair (i, c), the instance i is the current node whose embedding is under estimation; the context c is each of the neighbor nodes within a fixed window size in a generated random walk sequence; the context space C is all the nodes in the graph. LINE <ref type="bibr" target="#b17">(Tang et al., 2015)</ref> extends the model to have multiple context spaces C for modeling both first and second order proximity.</p><p>Although Skipgram-like models for graphs have received much recent attention, many other models exist. TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> learns the embeddings of entities in a knowledge graph jointly with their relations. Autoen-coders were used to learn graph embeddings for clustering on graphs <ref type="bibr" target="#b18">(Tian et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Comparison</head><p>We compare our approach in this paper with other methods in semi-supervised learning and embedding learning in Table 1. Unlike our approach, conventional graph Laplacian based methods <ref type="bibr" target="#b24">(Zhu et al., 2003;</ref><ref type="bibr" target="#b0">Belkin et al., 2006;</ref><ref type="bibr" target="#b16">Talukdar &amp; Crammer, 2009</ref>) impose regularization on the labels but do not learn embeddings. Semi-supervised embedding method <ref type="bibr" target="#b20">(Weston et al., 2012)</ref> learns embeddings in a neural network, but our approach is different from this method in that instead of imposing regularization, we use the embeddings to predict the context in the graph. Graph embedding methods <ref type="bibr" target="#b13">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b18">Tian et al., 2014)</ref> encode the graph structure into embeddings; however, different from our approach, these methods are purely unsupervised and do not leverage label information for a specific task. Moreover, these methods are transductive and cannot be directly generalized to instances unseen at training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Semi-Supervised Learning with Graph Embeddings</head><p>Following the notations in the previous section, the input to our method includes labeled instances x 1:L , y 1:L , unlabeled instances x L+1:L+U and a graph denoted as a matrix A. Each instance i has an embedding denoted as e i .</p><p>We formulate our framework based on feed-forward neural networks. Given the input feature vector x, the k-th hidden layer of the network is denoted as h k , which is a nonlinear function of the previous hidden layer h k?1 defined as:</p><formula xml:id="formula_5">h k (x) = ReLU(W k h k?1 (x) + b k ),</formula><p>where W k and b k are parameters of the k-th layer, and h 0 (x) = x. We adopt rectified linear unit ReLU(x) = max(0, x) as the nonlinear function in this work.</p><p>The loss function of our framework can be expressed as L s + ?L u , <ref type="figure">Figure 1</ref>. An example of sampling from context distribution p(i, c, ?) when ? = 1 and d = 2. In circles, +1 denotes positive instances, ?1 denotes negative instances, and ? denotes unlabeled instances. If random &lt; r2, we first sample a random walk 2 ? 1 ? 4 ? 6, and then sample two nodes in the random walk within distance d. If random ? r2, we sample two instances with the same labels.</p><p>where L s is a supervised loss of predicting the labels, and L u is an unsupervised loss of predicting the graph context. In the following sections, we first formulate L u by introducing how to sample context from the graph, and then formulate L s to form our semi-supervised learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sampling Context</head><p>We formulate the unsupervised loss L u as a variant of Eq.</p><p>(2). Given a graph A, the basic idea of our approach is to sample pairs of instance i and context c, and then formulate the loss L u using the log loss ? log p(c|i) as in Eq.</p><p>(2). We first present the formulation of L u by introducing negative sampling, and then discuss how to sample pairs of instance and context.</p><p>It is usually intractable to directly optimize Eq.</p><p>(2) due to normalization over the whole context space C. Negative sampling was introduced to address this issue <ref type="bibr" target="#b11">(Mikolov et al., 2013)</ref>, which samples negative examples to approximate the normalization term. In our case, we are sampling (i, c, ?) from a distribution, where i and c denote instance and context respectively, ? = +1 means (i, c) is a positive pair and ? = ?1 means negative. Given (i, c, ?), we minimize the cross entropy loss of classifying the pair (i, c) to a binary label ?:</p><formula xml:id="formula_6">?I(? = 1) log ?(w T c e i ) ? I(? = ?1) log ?(?w T c e i ),</formula><p>where ? is the sigmoid function defined as ?(x) = 1/(1 + e ?x ), and I(?) is an indicator function that outputs 1 when the argument is true, otherwise 0. Therefore, the unsuper-vised loss with negative sampling can be written as</p><formula xml:id="formula_7">L u = ?E (i,c,?) log ?(?w T c e i )<label>(3)</label></formula><p>The distribution p(i, c, ?) is conditioned on labels y 1:L and the graph A. However, since they are the input to our algorithm and kept fixed, we drop the conditioning in our notation.</p><p>We now define the distribution p(i, c, ?) directly using a sampling process, which is illustrated in Algorithm 1.</p><p>There are two types of context that are sampled in this algorithm. The first type of context is based on the graph A, which encodes the structure (distributional) information, and the second type of context is based on the labels, which we use to inject label information into the embeddings. We use a parameter r 1 ? (0, 1) to control the ratio of positive and negative samples, and use r 2 ? (0, 1) to control the ratio of two types of context.</p><p>With probability r 2 , we sample the context based on the graph A. We first uniformly sample a random walk sequence S. More specifically, we uniformly sample the first instance S 1 from the set 1 : L + U . Given the previous instance S k?1 = i, the next instance S k = j is sampled with probability a ij / L+U j =1 a ij . With probability r 1 , we sample a positive pair (i, c) from the set {(S j , S k ) : |j ? k| &lt; d}, where d is another parameter determining the window size. With probability (1 ? r 1 ), we uniformly corrupt the context c to sample a negative pair.</p><p>With probability (1 ? r 2 ), we sample the context based on the class labels. Positive pairs have the same labels and negative pairs have different labels. Only labeled instances 1 : L are sampled.</p><p>Our random walk based sampling method is built upon Deepwalk <ref type="bibr" target="#b13">(Perozzi et al., 2014)</ref>. In contrast to their method, our method handles real-valued A, incorporates negative sampling, and explicitly samples from labels with probability (1 ? r 2 ) to inject supervised information.</p><p>An example of sampling when ? = 1 is shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transductive Formulation</head><p>In this section, we present a method that infers the labels of unlabeled instances y L+1:L+U without generalizing to unobserved instances. Transductive learning usually performs better than inductive learning because transductive learning can leverage the unlabeled test data when training the model <ref type="bibr" target="#b10">(Joachims, 1999)</ref>.</p><p>We apply k layers on the input feature vector x to obtain h k (x), and l layers on the embedding e to obtain h l (e), as illustrated in <ref type="figure" target="#fig_0">Figure 2(a)</ref>. The two hidden layers are concatenated, and fed to a softmax layer to predict the class Algorithm 1 Sampling Context Distribution p(i, c, ?)</p><p>Input: graph A, labels y 1:L , parameters r 1 , r 2 , q, d Initialize triplet (i, c, ?) if random &lt; r 1 then ? ? +1 else ? ? ?1 if random &lt; r 2 then Uniformly sample a random walk S of length q</p><formula xml:id="formula_8">Uniformly sample (S j , S k ) with |j ? k| &lt; d i ? S j , c ? S k if ? = ?1 then uniformly sample c from 1 : L + U else if ? = +1 then Uniformly sample (i, c) with y i = y c else Uniformly sample (i, c) with y i = y c end if end if return (i, c, ?)</formula><p>label of the instance. More specifically, the probability of predicting the label y is written as:</p><formula xml:id="formula_9">p(y|x, e) = exp[h k (x) T , h l (e) T ]w y y exp[h k (x) T , h l (e) T ]w y ,<label>(4)</label></formula><p>where [?, ?] denotes concatenation of two row vectors, the super script h T denotes the transpose of vector h, and w represents the model parameter.</p><p>Combined with Eq. (3), the loss function of transductive learning is defined as:</p><formula xml:id="formula_10">? 1 L L i=1 log p(y i |x i , e i ) ? ?E (i,c,?) log ?(?w T c e i ),</formula><p>where the first term is defined by Eq. (4), and ? is a constant weighting factor. The first term is the loss function of class label prediction and the second term is the loss function of context prediction. This formulation is transductive because the prediction of label y depends on the embedding e, which can only be learned for instances observed in the graph A during training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inductive Formulation</head><p>While we consider transductive learning in the above formulation, in many cases, it is desirable to learn a classifier that can generalize to unobserved instances, especially for large-scale tasks. For example, machine reading systems <ref type="bibr" target="#b5">(Carlson et al., 2010)</ref> very frequently encounter novel entities on the Web and it is not practical to train a semisupervised learning system on the entire Web. However, since learning graph embeddings is transductive in nature, it is not straightforward to do it in an inductive setting. Perozzi et al. <ref type="formula" target="#formula_0">(2014)</ref>  beddings incrementally, which is time consuming and does not scale (and not inductive essentially).</p><p>To make the method inductive, the prediction of label y should only depend on the input feature vector x. Therefore, we define the embedding e as a parameterized function of feature x, as shown in <ref type="figure" target="#fig_0">Figure 2(b)</ref>. Similar to the transductive formulation, we apply k layers on the input feature vector x to obtain h k (x). However, rather than using a "free" embedding, we apply l 1 layers on the input feature vector x and define it as the embedding e = h l1 (x). Then another l 2 layers are applied on the embedding h l2 (e) = h l2 (h l1 (x)), denoted as h l (x) where l = l 1 + l 2 . The embedding e in this formulation can be viewed as a hidden layer that is a parameterized function of the feature x.</p><p>With the above formulation, the label y only depends on the feature x. More specifically,</p><formula xml:id="formula_11">p(y|x) = exp[h k (x) T , h l (x) T ]w y y exp[h k (x) T , h l (x) T ]w y<label>(5)</label></formula><p>Replacing e i in Eq.</p><p>(3) with h l1 (x i ), the loss function of inductive learning is</p><formula xml:id="formula_12">? 1 L L i=1 log p(y i |x i ) ? ?E (i,c,?) log ?(?w T c h l1 (x i ))</formula><p>where the first term is defined by Eq. (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Model Training (Transductive)</head><p>Input: A, x 1:L+U , y 1:L , ?, batch iterations T 1 , T 2 and sizes N 1 , N 2 repeat for t ? 1 to T 1 do Sample a batch of labeled instances i of size N 1 L s = ? 1 N1 i p(y i |x i , e i ) Take a gradient step for L s end for for t ? 1 to T 2 do Sample a batch of context from p(i, c, ?) of size N 2 L u = ? 1 N2 (i,c,?) log ?(?w T c e i ) Take a gradient step for L u end for until stopping </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>We adopt stochastic gradient descent (SGD) <ref type="bibr">(Bottou, 2010)</ref> to train our model in the mini-batch mode. We first sample a batch of labeled instances and take a gradient step to optimize the loss function of class label prediction. We then sample a batch of context (i, c, ?) and take another gradient step to optimize the loss function of context prediction. We repeat the above procedures for T 1 and T 2 iterations respectively to approximate the weighting factor ?. Algorithm 2 illustrates the SGD-based training algorithm for the transductive formulation. Similarly, we can replace p(y i |x i , e i ) with p(y i |x i ) in L s to obtain the training algorithm for the inductive formulation. Let ? denote all model parameters. We update both embeddings e and parameters ? in transductive learning, and update only parameters ? in inductive learning. Before the joint training procedure, we apply a number of training iterations that optimize the unsupervised loss L u alone and use the learned embeddings e as initialization for joint training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In our experiments, Planetoid-T and Planetoid-I denote the transductive and inductive formulation of our approach. We compare our approach with label propagation (LP) <ref type="bibr" target="#b24">(Zhu et al., 2003)</ref>, semi-supervised embedding (SemiEmb) <ref type="bibr" target="#b20">(Weston et al., 2012)</ref>, manifold regularization (ManiReg)  <ref type="bibr" target="#b0">(Belkin et al., 2006)</ref>, TSVM <ref type="bibr" target="#b10">(Joachims, 1999)</ref>, and graph embeddings (GraphEmb) <ref type="bibr" target="#b13">(Perozzi et al., 2014)</ref>. Another baseline method, denoted as Feat, is a linear softmax model that takes only the feature vectors x as input. We also derive a variant Planetoid-G that learns embeddings to jointly predict class labels and graph context without use of feature vectors. The architecture of Planetoid-G is similar to <ref type="figure" target="#fig_0">Figure 2(a)</ref> except that the input feature and the corresponding hidden layers are removed. Among the above methods, LP, GraphEmb and Planetoid-G do not use the features x, while TSVM and Feat do not use the graph A. We include these methods into our experimental settings to better evaluate our approach. Our preliminary experiments on the text classification datasets show that the performance of our model is not very sensitive to specific choices of the network architecture 2 . We adapt the implementation of GraphEmb 3 to our Skipgram implementation. We use the Junto library <ref type="bibr" target="#b16">(Talukdar &amp; Crammer, 2009</ref>) for label   propagation, and SVMLight 4 for TSVM. We also use our own implementation of ManiReg and SemiEmb by modifying the symbolic objective function in Planetoid. In all of our experiments, we set the model hyper-parameters to r 1 = 5/6, q = 10, d = 3, N 1 = 200 and N 2 = 200 for Planetoid. We use the same r 1 , q and d for GraphEmb, and the same N 1 and N 2 for ManiReg and SemiEmb. We tune r 2 , T 1 , T 2 , the learning rate and hyper-parameters in other models based on an additional data split with a different random seed.</p><p>The statistics for five of our benchmark datasets are reported in <ref type="table" target="#tab_0">Table 2</ref>. For each dataset, we split all instances into three parts, labeled data, unlabeled data, and test data. Inductive methods are trained on the labeled and unlabeled data, and tested on the test data. Transductive methods, on the other hand, are trained on the labeled, unlabeled data, and test data without labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Text Classification</head><p>We first considered three text classification datasets 5 , Citeseer, Cora and Pubmed <ref type="bibr" target="#b14">(Sen et al., 2008)</ref>. Each dataset contains bag-of-words representation of documents and citation links between the documents. We treat the bag-ofwords as feature vectors x. We construct the graph A based on the citation links; if document i cites j, then we set a ij = a ji = 1. The goal is to classify each document into one class. We randomly sample 20 instances for each class as labeled data, 1, 000 instances as test data, and the rest are used as unlabeled data. The same data splits are used for different methods, and we compute the average accuracy for comparison.</p><p>The experimental results are reported in <ref type="table" target="#tab_1">Table 3</ref>. Among the inductive methods, Planetoid-I achieves the best performance on all the three datasets with the improvement of up to 6.1% on Pubmed, which indicates that our embedding techniques are more effective than graph Laplacian regularization. Among the transductive methods, Planetoid-T achieves the best performance on Cora and Pubmed, while TSVM performs the best on Citeseer. However, TSVM does not perform well on Cora and Pubmed. Planetoid-I slightly outperforms Planetoid-T on Citeseer and Pubmed, while Planetoid-T gets up to 14.5% improvement over Planetoid-I on Cora. We conjecture that in Planetoid-I, the feature vectors impose constraints on the learned embeddings, since they are represented by a parameterized function of the input feature vectors. If such constraints are appropriate, as is the case on Citeseer and Pubmed, it improves the non-convex optimization of embedding learning and leads to better performance. However, if such constraints rule out the optimal embeddings, the inductive model will suffer.</p><p>Planetoid-G consistently outperforms GraphEmb on all three datasets, which indicates that joint training with label information can improve the performance over training the supervised and unsupervised objectives separately. <ref type="figure" target="#fig_2">Figure 3</ref> displays the 2-D embedding spaces on the Cora dataset using t-SNE ( <ref type="bibr" target="#b19">Van der Maaten &amp; Hinton, 2008)</ref>. Note that different classes are better separated in the embedding space of Planetoid-T than that of GraphEmb and SemiEmb, which is consistent with our empirical findings. We also observe similar results for the other two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Distantly-Supervised Entity Extraction</head><p>We next considered the DIEL (Distant Information Extraction using coordinate-term Lists) dataset <ref type="bibr" target="#b1">(Bing et al., 2015)</ref>. The DIEL dataset contains pre-extracted features for each entity mention in text, and a graph that connects entity mentions to coordinate lists. The goal is to extract medical entities from text given feature vectors and the graph.</p><p>We follow the exact experimental setup as in the original DIEL paper <ref type="bibr" target="#b1">(Bing et al., 2015)</ref>, including data splits of different runs, preprocessing of entity mentions and coordinate lists, and evaluation. We treat the top-k entities given by a model as positive instances, and compute recall@k for evaluation (k is set to 240, 000 following the DIEL paper). We report the average result of 10 runs in <ref type="table">Table 4</ref>, where Feat refers to a result obtained by SVM (referred to as DS-Baseline in the DIEL paper). The result of LP was also taken from <ref type="bibr" target="#b1">(Bing et al., 2015)</ref>. DIEL in <ref type="table">Table 4</ref> refers to the method proposed by the original paper, which is an improved version of label propagation that trains classifiers on feature vectors based on the output of label propagation. We did not include TSVM into the comparison since it does not scale. Since we use Freebase as ground truth and some entities are not present in text, the upper bound of recall as shown in <ref type="table">Table 4</ref> is 0.617.</p><p>Both Planetoid-I and Planetoid-T significantly outperform all other methods. Each of Planetoid-I and Planetoid-T achieves the best performance in 5 out of 10 runs, and they give a similar recall on average, which indicates that there is no significant difference between these two methods on this dataset. Planetoid-G clearly outperforms GraphEmb, which again shows the benefit of joint training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Entity Classification</head><p>We sorted out an entity classification dataset from the knowledge base of Never Ending Language Learning (NELL) <ref type="bibr" target="#b5">(Carlson et al., 2010)</ref> and a hierarchical entity classification dataset <ref type="bibr" target="#b7">(Dalvi &amp; Cohen, 2016</ref>) that links NELL entities to text in ClueWeb09. We extracted the entities and the relations between entities from the NELL knowledge base, and then obtained text description by linking the entities to ClueWeb09. We use text bag-of-words representation as feature vectors of the entities.</p><p>We next describe how to construct the graph based on the knowledge base. We first remove relations that are not populated in NELL, including "generalizations", "haswikipediaurl", and "atdate". In the knowledge base, each relation is denoted as a triplet (e 1 , r, e 2 ), where e 1 , r, e 2 denote head entity, relation, and tail entity respectively. We treat each entity e as a node in the graph, and each relation r is split as two nodes r 1 and r 2 in the graph. For each (e 1 , r, e 2 ), we add two edges in the graph, (e 1 , r 1 ) and (e 2 , r 2 ).</p><p>We removed all classes with less than 10 entities. The goal is to classify the entities in the knowledge base into one of the 210 classes given the feature vectors and the graph. Let ? be the labeling rate. We set ? to 0.1, 0.01, and 0.001. max(?N, 1) instances are labeled for a class with N entities, so each class has at least one entity in the labeled data.</p><p>We report the results in <ref type="table" target="#tab_2">Table 5</ref>. We did not include TSVM since it does not scale to such a large number of classes with the one-vs-rest scheme. Adding feature vectors does not improve the performance of Planetoid-T, so we set the feature vectors for Planetoid-T to be all empty, and therefore Planetoid-T is equivalent to Planetoid-G in this case.</p><p>Planetoid-I significantly outperforms the best of the other compared inductive methods-i.e., SemiEmb-by 4.8%, 16.0%, and 18.7% respectively with three labeling rates. As the labeling rate decreases, the improvement of Planetoid-I over SemiEmb becomes more significant.</p><p>Graph structure is more informative than features in this dataset, so inductive methods perform worse than transductive methods. Planetoid-G outperforms GraphEmb by 5.0%, 3.2% and 3.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Our contribution is three-fold: a) incontrast to previous semi-supervised learning approaches that largely depend on graph Laplacian regularization, we propose a novel approach by joint training of classification and graph context prediction; b) since it is difficult to generalize graph embeddings to novel instances, we design a novel inductive approach that conditions embeddings on input features; c) we empirically show substantial improvement over existing methods (up to 8.5% and on average 4.1%), and even more significant improvement in the inductive setting (up to 18.7% and on average 7.8%).</p><p>Our experimental results on five benchmark datasets also show that a) joint training gives improvement over unsupervised learning; b) predicting graph context is more effective than graph Laplacian regularization; c) the performance of the inductive variant depends on the informativeness of feature vectors.</p><p>One direction of future work would be to apply our framework to more complex networks, including recurrent networks. It would also be interesting to experiment with datasets where a graph is computed based on distances between feature vectors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>addressed this issue by retraining the em-Network architecture: transductive v.s. inductive. Each dotted arrow represents a feed-forward network with an arbitrary number of layers (we use only one layer in our experiments). Solid arrows denote direct connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>t-SNE Visualization of embedding spaces on the Cora dataset. Each color denotes a class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Dataset statistics.</figDesc><table><row><cell>DATASET</cell><cell>#CLASSES</cell><cell>#NODES</cell><cell>#EDGES</cell></row><row><cell>CITESEER</cell><cell>6</cell><cell>3,327</cell><cell>4,732</cell></row><row><cell>CORA</cell><cell>7</cell><cell>2,708</cell><cell>5,429</cell></row><row><cell>PUBMED</cell><cell>3</cell><cell>19,717</cell><cell>44,338</cell></row><row><cell>DIEL</cell><cell cols="3">4 4,373,008 4,464,261</cell></row><row><cell>NELL</cell><cell>210</cell><cell>65,755</cell><cell>266,144</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Accuracy on text classification. Upper rows are inductive methods and lower rows are transductive methods.</figDesc><table><row><cell>METHOD</cell><cell cols="2">CITESEER CORA PUBMED</cell></row><row><cell>FEAT</cell><cell>0.572</cell><cell>0.574 0.698</cell></row><row><cell>MANIREG</cell><cell>0.601</cell><cell>0.595 0.707</cell></row><row><cell>SEMIEMB</cell><cell>0.596</cell><cell>0.590 0.711</cell></row><row><cell>PLANETOID-I</cell><cell>0.647</cell><cell>0.612 0.772</cell></row><row><cell>TSVM</cell><cell>0.640</cell><cell>0.575 0.622</cell></row><row><cell>LP</cell><cell>0.453</cell><cell>0.680 0.630</cell></row><row><cell>GRAPHEMB</cell><cell>0.432</cell><cell>0.672 0.653</cell></row><row><cell cols="2">PLANETOID-G 0.493</cell><cell>0.691 0.664</cell></row><row><cell cols="2">PLANETOID-T 0.629</cell><cell>0.757 0.757</cell></row><row><cell cols="3">Table 4. Recall@k on DIEL distantly-supervised entity extrac-</cell></row><row><cell cols="3">tion. Upper rows are inductive methods and lower rows are trans-</cell></row><row><cell cols="3">ductive methods. Results marked with  *  are taken from the origi-</cell></row><row><cell cols="3">nal DIEL paper (Bing et al., 2015) with the same data splits.</cell></row><row><cell cols="2">METHOD</cell><cell>RECALL@k</cell></row><row><cell cols="2">*  FEAT</cell><cell>0.349</cell></row><row><cell cols="2">MANIREG</cell><cell>0.477</cell></row><row><cell cols="2">SEMIEMB</cell><cell>0.486</cell></row><row><cell cols="2">PLANETOID-I</cell><cell>0.501</cell></row><row><cell cols="2">*  DIEL</cell><cell>0.405</cell></row><row><cell></cell><cell>*  LP</cell><cell>0.162</cell></row><row><cell cols="2">GRAPHEMB</cell><cell>0.258</cell></row><row><cell cols="2">PLANETOID-G</cell><cell>0.394</cell></row><row><cell cols="2">PLANETOID-T</cell><cell>0.500</cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note>* UPPER BOUND 0.617</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Accuracy on NELL entity classification with labeling rates of 0.1, 0.01, and 0.001. Upper rows are inductive methods and lower rows are transductive methods.</figDesc><table><row><cell>METHOD</cell><cell>0.1</cell><cell>0.01</cell><cell>0.001</cell></row><row><cell>FEAT</cell><cell cols="3">0.621 0.404 0.217</cell></row><row><cell>MANIREG</cell><cell cols="3">0.634 0.413 0.218</cell></row><row><cell>SEMIEMB</cell><cell cols="3">0.654 0.438 0.267</cell></row><row><cell>PLANETOID-I</cell><cell cols="3">0.702 0.598 0.454</cell></row><row><cell>LP</cell><cell cols="3">0.714 0.448 0.265</cell></row><row><cell>GRAPHEMB</cell><cell cols="3">0.795 0.725 0.581</cell></row><row><cell cols="4">PLANETOID-G/T 0.845 0.757 0.619</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We note that it is possible to develop other architectures for different applications, such as using a shared hidden layer for feature vectors and embeddings.3 https://github.com/phanein/deepwalk</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://svmlight.joachims.org/ 5 http://linqs.umiacs.umd.edu/projects//projects/lbc/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was funded by the NSF under grants CCF-1414030 and IIS-1250956, and by Google.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vikas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving distant supervision for information extraction using label propagation through lists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sneha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COMPSTAT</title>
		<imprint>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hruschka</forename><surname>Burr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estevam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>L?on</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical semisupervised classification with incomplete class hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavana</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-based clustering for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Handcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">M</forename><surname>Tantrum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series A</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="354" />
			<date type="published" when="2007" />
			<publisher>Statistics in Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph regularized transductive classification on heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yizhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danilevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="570" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Greg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Galileo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimation and prediction for stochastic blockmodels for graphs with latent block structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Snijders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Nowicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="100" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">New regularized algorithms for transductive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pratim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="442" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mingzhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tie-Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fr?d?ric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pidgin: ontology alignment using web text as interlingua</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Derry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pratim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="589" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multi-task cross-lingual sequence tagging from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06270</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="321" to="328" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
