<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pose and Joint-Aware Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Shah</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlok</forename><surname>Mishra</surname></persName>
							<email>shlokm@umd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Cheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Research Center for Information Technology Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
							<email>abhinav2@umd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pose and Joint-Aware Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress on action recognition has mainly focused on RGB and optical flow features. In this paper, we approach the problem of joint-based action recognition. Unlike other modalities, constellation of joints and their motion generate models with succinct human motion information for activity recognition. We present a new model for joint-based action recognition, which first extracts motion features from each joint separately through a shared motion encoder before performing collective reasoning. Our joint selector module re-weights the joint information to select the most discriminative joints for the task. We also propose a novel joint-contrastive loss that pulls together groups of joint features which convey the same action. We strengthen the joint-based representations by using a geometry-aware data augmentation technique which jitters pose heatmaps while retaining the dynamics of the action. We show large improvements over the current state-of-the-art joint-based approaches on JHMDB, HMDB, Charades, AVA action recognition datasets. A late fusion with RGB and Flowbased approaches yields additional improvements. Our model also outperforms the existing baseline on Mimetics, a dataset with out-of-context actions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of action recognition has seen a lot of advances in recent times with improved spatio-temporal modeling, faster models and longer range temporal understanding. Most of the recent approaches in this area make use of raw RGB or dense optical flow as input features to reason about actions. In this paper, we approach the task of action recognition using joints and their motion. Unlike the commonly used dense optical flow, joints convey motion information succinctly, relying only on a sparse set of keypoints. Johansson's seminal work <ref type="bibr" target="#b20">[21]</ref> on 'Moving Light Display' showed the importance of moving joints for human perception. In their experiments, bright spots were attached to joints of an actor dressed in black who was moving in front of a dark background. When the actor was not moving, the collection of spots was not helpful in discerning the action but when the person starts moving, the relative motion creates an impression of a person walking, dancing, etc. Further, many existing vision-based models tend to be biased by static context like scenes and objects <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b58">59]</ref>. Using joints helps to reduce these biases as it tends to be more robust to scene variations and explicitly captures only the human motion.</p><p>Recently, several interesting approaches for action recognition using human pose as a feature have been proposed. Approaches like PoTion <ref type="bibr" target="#b10">[11]</ref>, PA3D <ref type="bibr" target="#b62">[63]</ref>, SIP-Net <ref type="bibr" target="#b58">[59]</ref> have shown impressive results. But, these approaches have certain limitations like overfitting on small datasets <ref type="bibr" target="#b10">[11]</ref>, requirement for access to multiple posemodalities <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b58">59]</ref>, pose-tracking <ref type="bibr" target="#b58">[59]</ref>. Many of these approaches also require access to features from a pose extractor, making it difficult for the model to be applied to different off-the-shelf pose extractors. Most importantly, we note that all of these approaches do a collective joint-reasoning step right from the first layer. Reasoning about activities in videos using joints requires the model to integrate motion information from multiple joints. But we found that early fusion of joint information does not let the model learn good per-joint motion signatures and might make the model rely a lot on co-occurrence patterns which can lead to sub-optimal representations.</p><p>In this paper, we propose to alleviate the aforementioned issues through our approach Joint-Motion Reasoning Network (JMRN). First, in order to learn richer motion representations, we extract per joint motion information using a shared motion encoder. This is followed by a collective joint reasoning module to infer the final activity. Our approach of separating per-joint feature extraction and collective reasoning allows for additional constraints on the mo- <ref type="figure">Figure 1</ref>: The overall pipeline. We propose the joint-motion re-weighting network (JMRN) to better capture interdependencies between joints. This model captures motion information while learning to use the most discriminative joints. We first encode joint trajectories for each joint and augment them with the Pose-Aware augmentation to obtain the input motion representations p j , j ? [1, ? ? ? , J]. The joint-motion extractor module is a shared encoder that learns information about each of the J joints separately. This information is then forwarded to the inter-joint reasoning module, which learns a weight w j for each joint and reweighs the representation. In addition to the classification loss, we enforce a novel contrastive loss on the MLP-projected joint representations. The joint-contrastive loss is calculated for each joint separately. tion representations. We propose a novel joint-contrastive loss which pulls together features from the same action for each joint. This loss enforces that the per-joint motion signatures are expressive of the action. Different from the usual contrastive learning setup <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> where the loss is applied to the last layer features, the proposed loss operates on the 'mid-level' joint features. To improve the generalization capability of the model, we propose to use two geometry-aware data augmentation techniques. <ref type="figure">Fig. 1</ref> shows the schematic of our model. Our proposed approach is not tied to any specific pose-extractor and does not require any pose tracking, which is difficult for in-the-wild videos.</p><p>Using the proposed approach, we obtain an absolute accuracy improvement of 7.35% on JHMDB <ref type="bibr" target="#b19">[20]</ref>, 3.9% on HMDB <ref type="bibr" target="#b26">[27]</ref>, 2.4 mAP on Charades <ref type="bibr" target="#b44">[45]</ref>, and 1 mAP on AVA <ref type="bibr" target="#b16">[17]</ref> over approaches which use pose heatmaps alone. When combined with recent RGB and flow-based methods, our method yields additional improvements without retraining the backbone, demonstrating its effectiveness. We also evaluate our method on the Mimetics <ref type="bibr" target="#b58">[59]</ref> dataset. This dataset consists of out-of-context actions performed by mime artists. Evaluation on this dataset is meant to show the generalization capability of action recognition approaches. Standard RGB and flow-based models show dismal performance on this dataset. Our approach outperforms the baseline reported in <ref type="bibr" target="#b58">[59]</ref> by 1.5% on top 1 accuracy without performing any tracking and only relying on joint heatmaps. Extensive analysis illustrates the importance of each proposed technique, and we often find that even our individual techniques outperform previous approaches.</p><p>In summary, we make the following contributions:</p><p>? We propose a new model to improve reasoning over multiple human joints. This model learns motion rep-resentations for each joint independently, then uses an inter-joint reasoning module to combine all the motion cues from various joints to make a prediction.</p><p>? Our novel joint-contrastive loss guides the joint representations to extract richer features. To the best of our knowledge, a contrastive learning loss on 'mid-level' joint features has not been explored in the past.</p><p>? We propose a geometry-aware data augmentation method to deal with limited data and improve generalization.</p><p>? We obtain state-of-the-art results on JHMDB, HMDB, Charades and AVA when using the joints alone. The late fusion of our approach with RGB and flow-based models gives improvements showing the complementary nature of pose information. Our model also obtains an improvement on Mimetics <ref type="bibr" target="#b58">[59]</ref>, a dataset with out-of-context actions using only pose heatmaps and without any tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Action Recognition from Videos. <ref type="bibr" target="#b45">[46]</ref> was amongst the first to successfully apply deep networks for action recognition that relied on RGB and optical flow. <ref type="bibr" target="#b5">[6]</ref> improved performance on action recognition task by inflating weights from 2D CNNs. Some recent advances in learning improved spatio-temporal representations <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b14">15]</ref> have led to better performance in standard action recognition benchmarks. But, studies <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b58">59]</ref> show that RGB and flow-based models capture a lot of dataset biases. Unlike RGB and flow, human joints and their motion offer a succinct representation of the underlying activities and is less susceptible to dataset biases. To this end, we present an approach for action recognition using pose information.</p><p>Pose-based Action Classification. <ref type="bibr" target="#b19">[20]</ref> showed that pose features perform much better than low/mid features and thus can act as discriminative cues for action recognition. Strides of improvements in pose estimation from images and videos <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b36">37]</ref> have enabled extraction of pose information from in-thewild videos for the task of action recognition.</p><p>In P-CNN <ref type="bibr" target="#b9">[10]</ref>, the authors extract appearance and flow features conditioned on the human pose. Similarly, chained multi-stream networks <ref type="bibr" target="#b67">[68]</ref> combines the most important visual cues from pose, motion and RGB. A Multi-task deep learning framework was proposed in <ref type="bibr" target="#b33">[34]</ref> for using visual information and joints to classify activities. A recurrent pose-attention network was introduced in <ref type="bibr" target="#b11">[12]</ref> to learn the spatio-temporal features by attending to joints and extracting visual features. <ref type="bibr" target="#b1">[2]</ref> also extracts visual features near joints to obtain video descriptors. However, handling multiple people becomes difficult with such a method. These methods require access to ground-truth keypoints during training, making it difficult to train on videos without these annotations. A rank pooling-based approach was proposed in <ref type="bibr" target="#b30">[31]</ref> to pool information from the evolution of poseheatmaps.</p><p>Most related to our method, <ref type="bibr" target="#b10">[11]</ref> proposed to represent the pose evolution as a compact representation and used it in a deep framework to obtain improvements for action recognition in real-world videos. <ref type="bibr" target="#b62">[63]</ref> improved upon this by using Part Affinity Fields <ref type="bibr" target="#b3">[4]</ref> and visual features from the pose extraction pipeline along with a temporal pooling module. In contrast, we do not use these features and work on joint heatmaps alone, which is a common representation used in all pose extractors and shows improvements without using extra modalities. STAR-Net <ref type="bibr" target="#b35">[36]</ref>, reprojects pose-estimation features in space and time to build an end-to-end system for action recognition. While they use clips, we work with the entire video. Further, in addition to demonstrating better results, our approach is modular and can work with any pose estimation model. DynaMotion <ref type="bibr" target="#b0">[1]</ref> proposed using a dynamics-based encoder which encodes input sequences using a structured dictionary. Similar to <ref type="bibr" target="#b10">[11]</ref>, they reason on all joints together. By extracting motion information from each joint independently before performing inter-joint reasoning helps our model extract richer motion representation which leads to superior results. This way of modeling also naturally allows us to use additional supervision on joint motion representations which leads to additional gains.</p><p>Another line of work exploits the skeletal structure of humans to classify activities <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b42">43]</ref>. Accurate 3D poses can be obtained using depth cameras, but it is not viable in general settings and obtaining 3D pose from unconstrained videos is still a very difficult problem. Hence we do not focus on these approaches in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section we first describe how we obtain the input per-joint motion trajectories for our approach, followed by our proposed approach Pose extraction and encoding pose evolution In this paper, we focus on action recognition from videos using the evolution of human pose as a cue. Given a video of T frames, we first run a pose detector on each frame of the video to obtain heatmaps h t j ? R H?W , t = 1, . . . , T for the j th joint, j = 1, . . . , J. The heatmaps intuitively denote the location estimate of each joint for every frame. Following <ref type="bibr" target="#b10">[11]</ref>, we temporally aggregate the heatmaps using a weighted sum. Given the temporally stacked joint heatmaps C ? R T ?J?H?W , we aggregate the heatmaps to give a representation P ? = t C[t]o[t] using a (fixed) piece-wise linear weighing function o where P ? ? R J?H?W . We use three different weighting functions described in <ref type="bibr" target="#b10">[11]</ref>, giving the final motion representation of shape p j ? R 3?H?W . The advantage of using this approach is that it lets us encode the whole video without sampling, and also helps in reasoning about the global structure more effectively using CNNs. This module returns joint-motion trajectories which are used by our model for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Proposed Model and Training</head><p>Next, we introduce the structure of our JMRN which captures the joint correlations. Previous approaches used all joints concatenated together as input to the neural network. This can lead to the model not exploiting useful motion information from individual joints and relying on specific spatial arrangements of these found in the dataset. We argue that we can learn improved representations by independently extracting motion information from each joint using a shared module before combining the cues for inter-joint reasoning. Our next observation is that, for an activity, some joints might have more discriminative information than others. Some joints might act as 'distractors' to the training process and only provide redundant or noisy information. We can learn enhanced features by using information from more discriminative joints. Our model ( <ref type="figure">Fig. 1</ref>), JMRN is designed to solve these issues and learns effective representations for joint-based action recognition. The model consists of two modules: The first module extracts information from all joints separately. The second module is an inter-joint reasoning module that is conditional on the input and learns to weigh the features from the various joints before reasoning over altogether. The joint-selector module is similar in spirit to Squeeze-and-Excitation block <ref type="bibr" target="#b17">[18]</ref>, but with a key difference that instead of operating on higher-level representations of the entire input, our representations contain motion information about that joint alone. This makes our selector module naturally interpretable. Finally, we stack the weighted motion representations and feed them to the inter-joint reasoning module. This module generates final logits by performing collective reasoning over all joints. Joint-Motion Extraction. The joint-motion extraction module is a Siamese network that extracts information from each joint separately. The input to the network is the motion representation for the j th joint, p j ? R 3?H?W The module generates a representation r j ? R 256?H?W , j = 1, . . . , J which is then used by the joint-selector module. In addition, we also generate a compressed representation c j ? R c dim ?H?W which is obtained by passing r j through a 1 ? 1 convolution layer to reduce the number of channels.</p><p>Inter-Joint Reasoning. The joint-selector module pools information from all joint-motion representations r j to obtain weights that are used to modulate the representations c j . Specifically, we concatenate all the joint representations and apply a 1 ? 1 convolution. This is followed by an average pooling operation which generates a feature of dimension 256. A linear layer followed by sigmoid activation is used to generate the weights w j . Finally, we weigh the compressed representations c j by the obtained weights of w j and concatenate them. We use a 1 ? 1 convolution to reduce the large dimension of the input (J ? c dim ) and follow this with two convolutional layers and an FC layer to give the final class logits. Joint-Contrastive Loss The classification loss will naturally try to cluster final feature representations from the same class together. Extracting per-joint features before collective joint reasoning allows enforcing additional constraints on the joint features. Consider an instance of a person 'running'. While the standard classification loss enforces that concatenated features from instances of running lie close to each other, we can learn improved motion features by enforcing consistencies at the joint level. Specifically, our joint-contrastive loss ensures that per-joint features for instances of 'running' lie closer to each other than per-joint features of 'push-up'. We enforce this constraint by employing contrastive learning loss <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25]</ref>. Different from prior works, our loss operates on 'mid-level' joint motion features. Further, we apply this loss for each joint separately. Our positives come from augmented examples of the same instance and other instances of the same activity while the negatives are other remaining instances from the batch. We first project the joint features r j through an MLP and normalize them to obtain features z j which lie on a unit hypersphere. Let the corresponding label for the instance be denoted by y. The joint-contrastive loss is then defined by:</p><formula xml:id="formula_0">L cont = j=J j=1 L j cont (1) L j cont = i?B ?1 |P (i)| p?P (i) log exp z i j ? z p j /? a?B\i exp z i j ? z a j /? (2)</formula><p>where B are the instances in the batch, z i j denotes the projected features for the joint j of instance i and P (i) ? {p ? B \ i : y p = y i }, is the set of all instances which share the same label as instance i (positives). ? is the temperature parameter. To the best of our knowledge, a contrastive learning loss on joint features to improve representations has not been explored in the past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pose-Aware Data Augmentation (PAA)</head><p>Data augmentation, like flipping the representation, has been shown to be useful in previous approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b62">63]</ref>. We propose a data augmentation strategy which can use the geometric structure of the human joints. Our poseaware data augmentation is a cascade of two operations. The first operation is a random global jitter of the entire input representation. This helps the network to learn that the action is not strongly dependent on the global position of joint trajectories and ensures that the relative position among the different joints is not modified. Another benefit is that it regularizes training by sometimes pushing joints out of the frame which emulates joints not being visible in the scene. In the second operation, we divide the pose into six groups for joints corresponding to the Head: {Nose, REye, LEye, LEar, REar}, Torso: {RHip, LHip, Neck}, Left Hand: {LShoulder, LElbow, LWrist}, Right Hand: {RShoulder, RElbow, RWrist}, Left Leg: {LKnee, LAnkle}, and Right Leg: {RKnee, RAn-kle}. Each joint inside a group is randomly jittered by the same amount. As our first step involves a global motion, we do not jitter the joints in the torso group. This strategy amounts to adding random spatial noise but is pose-aware in the sense that all joints do not move by a different random amount. For example, the motion representations corresponding to eyes, ears and nose are jittered by the same amount. We represent our augmentation by parameters ?, ?, which represents the maximum amount of random jitter for each of the two strategies, respectively. The same amount of jitter is used for all frames of a video to preserve the geometric integrity of the action. Similar to pose-aware translation, we also experiment with pose-aware rotation which performs comparably. We include those experiments in the supplementary material. We note that while some of these data augmentation techniques have been explored for poseestimation, these have not been investigated for the task of action recognition which is a very different task conceptually and might require the model to learn different kind of invariances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present experimental results to show the effectiveness of the proposed approach. Following prior works, we use HMDB <ref type="bibr" target="#b26">[27]</ref>, JHMDB <ref type="bibr" target="#b19">[20]</ref>, Charades <ref type="bibr" target="#b44">[45]</ref>, AVA <ref type="bibr" target="#b16">[17]</ref> and Mimetics <ref type="bibr" target="#b58">[59]</ref> datasets for our experiments. HMDB <ref type="bibr" target="#b26">[27]</ref> consists of 6,766 video clips from fifty-one action classes. Each video clip is trimmed and corresponds to a single action. There are 3,570 training videos and 1,530 videos for validation in each of the three splits. JHMDB <ref type="bibr" target="#b19">[20]</ref> is a subset of HMDB that contains 928 short videos from twenty-one action classes. The dataset comprises of ?660 training videos and ?268 validation videos for each of the three splits. Charades <ref type="bibr" target="#b44">[45]</ref> is a more recent dataset that contains dailylife videos instead of videos from movies or Youtube. The dataset has 157 action classes with a total of 9,848 untrimmed videos, of which 7,981 videos are for training and 1,863 are for validation. AVA Actions <ref type="bibr" target="#b16">[17]</ref> is a large-scale dataset which evaluates video recognition models on spatio-temporal action localization. One frame every second is annotated with actors and each actor might be involved in multiple actions. We use the AVA v2.1 set which consists of 211k training segments and 57k validation segments. Following previous works, we evaluate mAP at frame-level IoU threshold of 0.5 on sixty actions which have at least twenty-five instances in the validation set. Mimetics <ref type="bibr" target="#b58">[59]</ref> is a test set for fifty actions from the Kinetics dataset. It consists of 713 videos collected from Youtube that have mimed actions. Due to the small size of this dataset, it is only used for testing a model that is trained with Kinetics or its subset. Unlike most common action recognition datasets, this dataset consists of minimal scene and object biases and hence can be used to evaluate a method for out-of-context actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>For all the datasets we extract pose using an off-the-shelf pose detector <ref type="bibr" target="#b2">[3]</ref> which gives heatmaps per frame for the video. We use the COCO trained pose extractor which returns 18 joints + background. We train the models using the Adam optimizer with a learning rate drop on accuracy plateau. We used cross-entropy for single label classification tasks and binary cross-entropy for multi-label tasks. Models are trained with the loss function L cls + ?L cont where ? is chosen empirically. For multi-label tasks, two instances are considered positives if they share any of the labels. We used our own implementation of PoTion (baseline) since the official implementation is not publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We show the results of our proposed approaches in Table 1. The baseline model is PoTion <ref type="bibr" target="#b10">[11]</ref>. The baseline model stacks heatmaps from all joints and uses the combined representation to recognize actions. The proposed model, JMRN first extracts motion information from each joint using a shared module before reweighing this information for inter-joint reasoning. This gives consistent gains on all datasets. Adding the novel joint-contrastive loss to enforce feature consistency at the joint-level brings in additional improvements. Our proposed pose-aware data augmentation gives a considerable improvement to the baseline model and our JMRN model over all datasets.</p><p>Comparison with prior Pose-based Methods. In <ref type="table" target="#tab_1">Tables  2 and 3</ref>, we compare our approach to other state-of-the-art approaches for the various datasets. The proposed approach performs better than other approaches that use only joint heatmaps. Our approach is more general, as unlike PAFs and CNN features, heatmaps can be obtained from any pose extractor. Further, unlike our method, PA3D's model uses additional inputs (PAF, CNN features, and temporal difference of features) and make use of ensembles of models. Our superior results without any of these bells and whistles show the benefit of our approach.</p><p>Fusion with State-of-the-Art Approaches. While joint motion is a strong cue for human activities, many of the current datasets have activities that are strongly dependent on objects and scenes. Our pose stream can still offer complementary motion information for such datasets and lead to an improved reasoning of activities. Na?vely averaging logits is sub-optimal for fusion since the logits are generated by models which are trained separately and might have a different effective range. We use a very simple learnt fusion scheme to combine the different modalities. Specifically, we learn a single scalar weighing parameter for each of the M modalities to be fused and M for each class for multi-label problems. This approach does not require backpropagation through the backbone and hence can be used as a quick post-training step with extracted logits. Fusing the scores with the best model, we obtain improvements on all datasets as shown in Tables 4 and 3. These results show that our model extracts joint information which is complementary to RGB and Flow. For HMDB and JH-MDB, we fuse with I3D <ref type="bibr" target="#b5">[6]</ref>, and additionally ResNeXt101-BERT <ref type="bibr" target="#b22">[23]</ref> which is the current state-of-the-art for HMDB. For Charades, we perform fusion with Long-Term Feature banks (LFB) <ref type="bibr" target="#b59">[60]</ref>. We see that our model enables complementary gains throughout. We found that on HMDB-1 test set, fusion of JMRN over ResNeXT101-BERT <ref type="bibr" target="#b22">[23]</ref> was particularly useful for jump (13%), turn (+7%), draw-sword (+7%) and run (+7%). On Charades, we reap the largest absolute improvements on standing up (+11%), sitting down (+10%) and washing a mirror (+10%) over the LFB model. These actions are strongly tied to human motion and the improvements justify the use of a pose stream in addition to RGB and optical <ref type="table">Table 1</ref>: Improvements using the proposed approach. We compare our model agains the baseline (PoTion <ref type="bibr" target="#b10">[11]</ref>). Our proposed model outperforms the baseline by a large margin. Use of the proposed data augmentation step (PAA) improves the JMRN model consistently across the three datasets. Finally, using the novel joint-contrastive loss in addition to the classification loss gives further improvements.  Models with * use difference features along with standard heatmaps and use an ensemble pose model. + are results on first split alone. P -pose heatmaps, PAF -Part affinity Fields <ref type="bibr" target="#b3">[4]</ref>, CF -Features from the pose extractor CNN.</p><p>flow. We believe that jointly training these models would lead to further improvements wherein each stream can specialize for different classes, but we leave that to future work.</p><p>Results on AVA Actions <ref type="bibr" target="#b16">[17]</ref> To show the advantage of using a pose-stream on large-scale datasets, we perform experiments on the AVA Actions dataset. We compare our approach with the best publicly available SlowFast <ref type="bibr" target="#b14">[15]</ref> model for this split. For fair comparison, we use bounding boxes provided by SlowFast <ref type="bibr" target="#b14">[15]</ref>. Pose is extracted for each frame in the clip and bounding box information is used to crop perperson pose information across the clip. These are then used to obtain the pose encoding. Since some actions involve multi-person context information, we also append the pose encoding of other people in the clip as separate channels.</p><p>Our results are shown in <ref type="table" target="#tab_4">Table 5</ref>. We obtain an improve-   <ref type="table" target="#tab_5">Table 6</ref> lists our results. We obtain improvements on all previously reported metrics over SIP-Net <ref type="bibr" target="#b58">[59]</ref> while using only pose heatmaps and without relying on any tracking -further justifying our approach. Our approach also outperforms the pose-only model of <ref type="bibr" target="#b37">[38]</ref> which uses a more accurate pose extractor and we obtain 2.7% improvement on top-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Additional Analyses</head><p>JMRN vs. Deeper Baseline. Our data augmentation strategy and model design helps us train deeper models and learn improved representations without over-fitting. To verify that our model leads to superior results compared to baseline due to design choices and not increased number of parameters, in <ref type="table" target="#tab_6">Table 7</ref> we experiment with a deeper version of the baseline model (PoTion). We add three convo-   lutional layers to the baseline model, increasing the number of parameters in the baseline to match that of our model. For a fair comparison, we make use of our data augmentation scheme in the deeper baseline model too. The deeper baseline still performs worse than our proposed model, thus validating our proposed approach.</p><p>Comparison with a Non-Siamese Network. Parameters of our motion extractor network are shared across all joints.</p><p>Here we show the results when we do not share these parameters and instead learn separate motion extractors for each joint. To maintain a similar number of parameters as the baseline, the motion extractors are made shallower than in JMRN. <ref type="table" target="#tab_7">Table 8</ref> shows the results of these experiments. This model performs worse than our proposed JMRN model and shows the importance of sharing the parameters.</p><p>What is the model learning? Since we first extract jointmotion signatures separately before fusing, we can attribute the weights in the selector to specific joints. In <ref type="figure">Fig. 2</ref> we show the average per-class weights for the HMDB-1 test set. We see some interesting, distinctive patterns. There are some joints like LHand and RHand which are used by the model for most inputs. Since none of the actions requires an explicit focus on keypoints on the face, motion features for keypoints like eyes and ears tend to be suppressed by the model for most cases. We also see a couple of cases where the model focuses on a very few joints like sword exercise. This might be due to biases underlying the dataset wherein the person performing the action is often in a similar pose and relatively few joints are enough to recognize the action. Further, they might not add new information than that given by other joints. We also see some usage of limbs that make intuitive sense. For example, as expected, the activity 'eat' and 'drink' predominantly depends on the upper limbs. The design of JMRN model allows us to see which joints were predominantly used by the model to infer activities.</p><p>Alternative Pose Extractors Our approach is modular and allows use of any off-the-shelf pose extractor. This is unlike approaches like PA3D and SIP-Net which make use of features from pose extractor and require access to features from pose extractor. We also use a more accurate pose extractor AlphaPose <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b61">62]</ref> on the Charades dataset. JMRN with AlphaPose gives an improvement of +0.5 mAP over JMRN with OpenPose. This demonstrates our approach's modular nature and applicability to other pose extractors. For fair comparison with previous approaches we used OpenPose for the experiments in the paper.</p><p>Benefit of joint-reweighting Apart from the benefit of visualizing the joints that the model focuses on, the jointselection helps improve performance too. We see that using the concatenated per-joint feature without reweighting drops the performance by 1% and 0.9 mAP on JHMDB and Charades respectively while having comparable performance on the HMDB dataset. <ref type="figure">Figure 2</ref>: Visualization of input conditional weights w i for HMDB-1 test set. We show the average weights for each class. Our approach of having a joint-selector as a part of the model allows for easy interpretation of joint importance for various actions. For example, we see that actions like 'eat' and 'drink' have a high dependence on hands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented an approach to learn better representations for pose-based action recognition. The proposed JMRN extracts useful per-joint motion information before reweighing the information for collective inter-joint reasoning. Our novel joint-contrastive loss further improves the results. This leads to improved performance on the downstream task of action recognition. The proposed pose-aware data augmentation step, applies a cascade of global and group-wise jitter. Our proposed approaches lead to improvement over state-of-the-art on JHMDB, HMDB, Charades, AVA and Mimetics using pose heatmaps alone. Fusion with state-ofthe-art RGB and flow-based model leads to further improvements showing complementary nature of the pose stream. Our proposed approach can be extended to explicitly handle missing joint information and people in the background. We leave these to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary</head><p>In this supplementary material, we provide: ? Additional implementation details for the approach. ? Additional details on modality fusion. ? Other variants for Joint-Contrastive loss. ? Analyses and experiments on data augmentations. ? Experiments on clip selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Additional Implementation Details</head><p>We use Adam with a learning rate of 1E-4 for JHMDB and HMDB and 5E-4 for Charades. The learning rate is reduced by 0.1 times after the validation loss plateaus. Use of dropout after convolutional layers, as employed in <ref type="bibr" target="#b10">[11]</ref> was beneficial for the Charades dataset. None of our experiments use the background joint in an attempt to force the network to extract richer per-joint motion information. Following <ref type="bibr" target="#b10">[11]</ref>, we additionally use flip augmentation for all experiments. Experiments on HMDB and Charades use a batch size of 128 while experiments on the much smaller JHMDB used a batch size of 16. Further, for JHMDB we activate the contrastive loss after 50 epochs. The dataset specific hyper-parameters that we used are: J-HMDB:  <ref type="bibr" target="#b10">[11]</ref> with three channels. This example is for a video of sixty frames. momentum of 0.9. We found ? = 0 to work well for this dataset. In addition, we found the use of Gumbel-Max trick <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b18">19]</ref> in the weight-selector to be beneficial. Patience of 2 was used for the scheduler. ?, ? = (3, 0) was used to train the model. We generate heatmaps by parsing the dataset provided by <ref type="bibr" target="#b63">[64]</ref> and adding Gaussian blobs around the keypoints. Our model gave 44.4% on Kinetics50 validation set. Experiments on AVA dataset Since AVA is annotated with multi-person multi-label actions, the bounding box information is used to crop per-person pose information across the clip. These are then used to obtain the pose encoding. For a fair comparison, we used the bounding boxes provided by SlowFast <ref type="bibr" target="#b14">[15]</ref>. To account for multi-person context information, we also append the pose encoding of other people in the clip as a separate channel. We use a batch size of 64 and train the model with a learning rate of 5E-4. We used ?, ? = (3, 0). Hyperparameters for joint-contrastive loss were set to ?, ? = 0.1, 0.5. Additional baseline details Since an official implementation of <ref type="bibr" target="#b10">[11]</ref> is not publicly available, we reproduce the results using our own implementation. As mentioned in the main paper, we make use of the same aggregation function for a fair comparison. For completeness, we include the aggregation function used in <ref type="figure" target="#fig_0">Fig. 3</ref>. Normalization: Normalization is essential before using a representation as an input to a neural network. The maxover-channel strategy chosen in <ref type="bibr" target="#b10">[11]</ref> normalizes the input to lie between 0 and 1. We posit that it is also important to encode the time spent by each joint at a location. The maxover-channel strategy assigns 1.0 as the maximum value in a channel irrespective of the actual duration that a joint spent at a particular spatial location which renders different representations incomparable. To solve this, we normalize by dividing the aggregated representation p j by the maximum possible value that a joint can accumulate at that location. Note that, in <ref type="bibr" target="#b10">[11]</ref> the authors mention that they tried this approach, but this did not improve their performance. However, we obtain considerable improvement upon using this normalization perhaps due to better modeling.   <ref type="table" target="#tab_8">Table 9</ref> shows the architecture of our motion extractor module which is shared among all joints to give r j . An additional 1 ? 1 convolution is used to generate c j . <ref type="table" target="#tab_9">Table 10</ref> shows the architecture of our joint selector module which generates w j using r j . The representation c j is weighed using the weights w j and passed to the classification module <ref type="table" target="#tab_10">(Table 11</ref>) to generate the final classification scores. ReLU activation is used after all layers except for the last layer of joint selector which uses sigmoid. The projection layers used with contrastive loss are implemented as two MLP layers which take spatially pooled per-joint features. The output from the MLP is normalized to lie on a unit hypersphere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Modality Fusion</head><p>Implementation Details As discussed in the main paper, we propose to use a very simple learnt fusion scheme to combine the different modalities. For single label classification tasks, we learn a single scalar weighing parameter for each of the M modalities to be fused and for a multi-label task (like Charades) with C classes, we learn M ? C parameters. We first use the pretrained RGB+Flow and pose models to extract per-modality logits for each clip. We then learn the parameters using the train set. During inference, the modalities are fused using the learnt logits. Specifically </p><formula xml:id="formula_1">l combined = ? RGB l RGB + ? f low l f low + ? pose l pose (3)</formula><p>Where l RGB , l f low , l pose ? R C are logits extracted from the pretrained network and ? RGB , ? f low , ? pose are optimized to minimize classification loss w.r.t l combined . For JHMDB and HMDB experiments, we L2 normalize the extracted logits. We train all models for 100 epochs. The Adam optimizer was used for all experiments and we chose learning rates of 1e-3, 1e-4, 5e-4 for JHMDB, HMDB, Charades respectively. Since we work with pre-extracted logits and there are a very few parameters to learn, these experiments can be performed very quickly and can be trained on a CPU. While there are a lot of approaches proposed for RGB and flow, we primarily choose models which have a high performance and are publicly available. We expect similar improvement over other models too. In <ref type="figure" target="#fig_1">Figures 4, and 5</ref> we visualize the improvements that we obtain on per-class metrics when we perform fusion with recent state-of-the-art methods. We see considerable improvements on classes where human motion plays a key role and this improvement is also seen in the overall performance. We believe that training schemes which involve back-propagation through the pose and RGB/flow backbones like <ref type="bibr" target="#b21">[22]</ref> will lead to further gains and can avoid the drop that we see in a few classes during fusion. We leave this to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Other variants for Joint-Contrastive loss</head><p>Here we experiment with different variants of our jointcontrastive loss. We first experiment with a variant that does not use label information. Specifically, only the instance and its augmented version are considered as positives. With this approach every other instance in the batch is considered as a negative for loss calculation. Note that this approach is commonly used in the self-supervised learning setting. We refer to this variant as only-aug. As discussed in the pa- <ref type="figure">Figure 4</ref>: Per-class AP difference on Charades validation set when JMRN is combined with R101-NL-LFB <ref type="bibr" target="#b59">[60]</ref>. We see that most classes see an improvement in per-class AP scores justifying the claim that pose has complementary information. per, for multi-label problems we consider an instance as a positive if it shares any label with the anchor. An alternative approach is to weigh the positive examples by the number of labels that they have in common. We call this variant multi-label-weighted. We refer to the variant used in the main paper as proposed. The results are shown in <ref type="table" target="#tab_1">Table 12</ref>. We see that the proposed variant uses the label information effectively during training and gives a better performance compared to the other variants. But, the other variants of joint-contrastive loss still outperform our model trained without any joint-contrastive loss thus showing the efficacy of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Analyses and experiments on data augmentation</head><p>Values of ? and ?. In <ref type="figure">Fig. 6</ref> we show the effect of ? and ? on the performance of our model on the three datasets. It can be seen that almost all values of ?, ? lead to improvements over (0, 0) making it easier to use this method on other datasets without an extensive hyper-parameter search. Further, relatively small values of ?, ? are enough to give a considerable performance improvement. To evaluate the contribution of augmentation scheme alone, we do not use contrastive loss in these experiments. Alternative Data Augmentation Technique. We experimented with a few other data augmentation strategies. We <ref type="table" target="#tab_1">Table 12</ref>: Experiments with variants of joint-contrastive loss. We experiment with three variants of the joint-contrastive loss to train our model. We see that the proposed variant outperforms the alternatives. Further, we see that across all datasets the use of any of the variants improves performance over model trained without the joint-contrastive loss. This shows the effectiveness of the proposed loss in learning good representations.  <ref type="figure">Figure 6</ref>: Effect of data augmentation hyperparameters ?, ?. We visualize the improvement in accuracy compared to a model which does not use the proposed data augmentation step. We observe that even small values of ?, ? lead to a significant improvement which shows the effectiveness of the approach. Further, we notice that the performance is not very sensitive to the hyperparameter values and all values lead to an improvement.</p><p>first use a technique that was inspired by CutMix <ref type="bibr" target="#b66">[67]</ref>. In the original paper, the authors generate new training examples by mixing patches from different images and changing the ground-truth labels based on the area of patches and the labels. We attempt to use this in our context. Specifically, during training, P joints from a pose-representation are replaced with pose-representation from another video. The ground truth labels are appropriately changed and the network is trained with the modified ground truth. In our experiments, this strategy performed considerably worse than the proposed PAA. We also experimented with a pose-aware rotation augmentation. Specifically, we rotate the obtained representation about the center by a random amount. This can be seen to be a rotation equivalent to the global translation jitter in which the entire representation is moved by the same amount. Similar to the groupwise translation, we experiment with groupwise rotation too. We found the augmentation to be helpful and it leads to improvements over the baseline, though it performs slightly worse than translation based augmentation that was shown in the main paper. Combining the two augmentation in novel ways could lead to improved performance but we leave that to future work.</p><p>It is to be noted that, unlike <ref type="bibr" target="#b10">[11]</ref>, which tried to randomly jitter each joint spatially, our approach of doing pose-aware augmentation retains the structure and hence leads to improvements. As observed in <ref type="bibr" target="#b10">[11]</ref>, we do not see any gain when we jitter or rotate each joint independently. PAA with baseline Our data augmentation step is general and can be applied to other models. To show this, we experimented with PAA applied to the baseline PoTion model. The results are shown in <ref type="table" target="#tab_2">Table 13</ref>. We see that the augmentation shows consistent improvement on the baseline but the results are still worse than our proposed model. Further, a joint-contrastive loss is not applicable to the baseline model since it does not extract per-joint motion features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Filtering Clips for Action recognition using Pose Information</head><p>Pose guided clip selection In this section we explore application of pose trajectories for clip selection. Human trajectory information can contain significant information about the underlying activity. We posit that pose information can give crucial cues to select representative clips in a video. The intuition behind this is that the pose trajectory encodes the movement of all the joints along with redundancies and time spent at each location. Properties of this trajectory, like points of high or low curvature, time spent at a location, etc., can give cues to select clips that are discriminative for the task of action recognition. We next describe our approach for clip selection. We assume a trained clip classifier f that takes as input clips c i ? R 3?H?W ?T for i ? 1, ? ? ? , N from a video with N clips and returns normalized logits z i that give the probability of occurrence of each class. The baseline model densely goes over each clip and uses a consensus function g(z 1 , ? ? ? , z N ) to give a single class-wise score for each video. Typically, max or avg is used as the consensus function.</p><p>Inspired by <ref type="bibr" target="#b25">[26]</ref>, we generate 'oracle' scores using the training data and the pre-trained clip classifier and then train a model to mimic the 'oracle'. For clip selection experiments, we train using pose-representation extracted from clips of sixteen frames. Oracle for the trained clip classifier is first obtained using O i = argtopK (f yi (c j )) which selects the K clips that give the highest confidence about the correct class. These Oracle clips are then used to generate ground truth labels: label[i, j] = 1 if c j ? O i and 0 otherwise. Thus instead of selection being guided by RGB data, we use features extracted from JMRN.</p><p>Next, we train a saliency ranker which ranks two clips during training time. In each batch, we sample an equal number of Oracle and non-oracle clips. During inference, we rank the clips according to the saliency given by the model for all clips in the video and select the topK.</p><p>In <ref type="table" target="#tab_3">Table 14</ref> we show the results of clip selection using our approach on average of 3 splits on HMDB dataset. Using JMRN to extract pose features helps select salient clips and gives improvements over all baselines. We also show the middle frames corresponding to the 5 most salient clips in <ref type="figure">Fig. 7</ref>. The selected clips are plausible for the activity of 'turn' and 'swing baseball' respectively. <ref type="figure">Figure 7</ref>: Here we show the middle frame corresponding to 5 most salient clips selected using our approach on the HMDB dataset. The upper row has frames from a video of 'turn' whereas the second row is from a video of 'swing baseball'. The frames are shown in order of their saliency scores. The selected clips are plausible for the underlying action.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>? c dim : 128 ? ?: 0.05 ? Epochs : 100 ? Training and evaluation on heatmaps of size 64 ? 86 ? ? = 6, ? = 3 ? ? = 0.3 HMDB: ? c dim : 64 ? ?: 0.5 ? Epochs : 200 ? Training on random crop of size 64?86 and evaluation on center crop of size 64 ? 86 ? ? = 4, ? = 0 ? ? = 0.1 Charades: ? c dim : 32 ? ?: 0.5 ? Epochs : 150 ? Training on random crop of size 64?64 and evaluation on center crop of size 64 ? 64 ? ? = 8, ? = 4 ? ? = 0.05 Experiments on the Mimetics dataset For experiments on Mimetics, we first train a model on Kinetics50 dataset which has the 50 classes common with Mimetics. Then, we evaluate this model on the Mimetics dataset. We used a batch size of 16, c dim = 256 and trained the model for 100 epochs,. We used Dropout with this model. SGD with momentum was used for training this model with lr of 0.01, Channel-time encoding o[t] used to encode the pose heatmaps. We use the same o[t] as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Per-class AP difference on AVA v2.1 validation set when JMRN is combined with SlowFast-R101-NL<ref type="bibr" target="#b14">[15]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art on J-HMDB and HMDB datasets. We report mean-per class accuracy averaged over 3 splits. We obtain an improvement of +7.35% on JHMDB and +3.9% on HMDB when using pose heatmaps alone. Note that the best results for<ref type="bibr" target="#b62">[63]</ref> make use of extra modalities and additional networks to process these.</figDesc><table><row><cell>Method</cell><cell>Features Used</cell><cell>HMDB</cell><cell>JHMDB</cell></row><row><cell>PA3D [63]</cell><cell>P + CF + PAF</cell><cell>55.3*</cell><cell>69.5*</cell></row><row><cell>Potion [11]</cell><cell>P</cell><cell>43.7</cell><cell>57.0</cell></row><row><cell>PA3D [63]</cell><cell>P</cell><cell>47.8</cell><cell>60.1</cell></row><row><cell>PA3D [63]</cell><cell>P</cell><cell>50.3*</cell><cell>61.2*</cell></row><row><cell>STAR-Net [36]</cell><cell>CF</cell><cell>-</cell><cell>64.3</cell></row><row><cell>EHPI [32]</cell><cell>P</cell><cell>-</cell><cell>60.5</cell></row><row><cell>DynaMotion [1]</cell><cell>P</cell><cell>49.1 +</cell><cell>60.2 +</cell></row><row><cell>SIP-Net [59]</cell><cell>CF</cell><cell>51.2</cell><cell>62.4</cell></row><row><cell>Ours</cell><cell>P</cell><cell>54.2</cell><cell>68.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">Classification mAP on the evaluation set of Cha-</cell></row><row><cell cols="3">rades dataset. We obtain an absolute improvement of 1.55</cell></row><row><cell cols="3">mAP over PA3D which uses PAF [4] and convolutional fea-</cell></row><row><cell cols="3">tures apart from heatmaps. We also show results on combi-</cell></row><row><cell cols="3">nation with RGB (R) and Flow-based (F) approaches which</cell></row><row><cell>gives further gains.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Features</cell><cell>mAP</cell></row><row><cell>2Stream [46]</cell><cell>R + F</cell><cell>11.9</cell></row><row><cell>Asyn-TF [44]</cell><cell>R + F</cell><cell>22.4</cell></row><row><cell>I3D [6]</cell><cell>R</cell><cell>32.72  ?</cell></row><row><cell>GCN [54]</cell><cell>R</cell><cell>36.2</cell></row><row><cell>NL I3D [56]</cell><cell>R</cell><cell>37.5</cell></row><row><cell>R50-I3D-NL [60]</cell><cell>R</cell><cell>38.29 #</cell></row><row><cell>R101-I3D-NL LFB [60]</cell><cell>R</cell><cell>42.5 #</cell></row><row><cell>PoTion [11]</cell><cell>P</cell><cell>13.54  ?</cell></row><row><cell>Potion + R101-NL-LFB</cell><cell>P + R</cell><cell>42.84</cell></row><row><cell>PA3D [63]</cell><cell>P + CF + PAF</cell><cell>13.8</cell></row><row><cell>PA3D [63] + GCN + I3D + NL-I3D</cell><cell>P + CF + PAF + R</cell><cell>41.0</cell></row><row><cell>Ours</cell><cell>P</cell><cell>16.2</cell></row><row><cell>Ours + R101-NL-LFB</cell><cell>P + R</cell><cell>43.23</cell></row><row><cell cols="3">? denotes results we have reproduced. # using official</cell></row><row><cell cols="3">implementation. P -pose heatmaps, PAF -Part affinity Fields [4],</cell></row><row><cell cols="2">CF -Features from the pose extractor CNN.</cell><cell></cell></row><row><cell cols="3">ment over the strong RGB-based baseline using late fusion</cell></row><row><cell cols="3">with JMRN's pose stream. It is particularly noteworthy that,</cell></row><row><cell cols="3">unlike the SlowFast model which uses Kinetics-600 to pre-</cell></row><row><cell cols="3">train the model, we train our model from scratch and our</cell></row><row><cell cols="3">pose-only model has a comparable performance to the I3D</cell></row><row><cell>RGB model.</cell><cell></cell><cell></cell></row><row><cell cols="3">Results on Mimetics dataset Mimetics [59] introduced a</cell></row><row><cell cols="3">test set for a subset of fifty actions found in the Kinetics</cell></row><row><cell cols="3">dataset. The proposed dataset is collected from Youtube but</cell></row><row><cell cols="3">consists of mimed actions and can be used to evaluate the</cell></row><row><cell cols="3">performance of models on out-of-context actions and thus</cell></row></table><note>evaluate their generalization capability. Pose can be a strong cue in such cases as the videos have minimal scene and object biases evident from the observation that RGB/Flow- based models have poor performance on this dataset [59].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison with fusion over recent state-of-the-art on HMDB and JHMDB dataset. We report mean-per class accuracy averaged over 3 splits. The improvement through model fusion shows that our approach is complementary to RGB (R) and flow-based (F) models.</figDesc><table><row><cell>Method</cell><cell>Modality</cell><cell>HMDB</cell></row><row><cell>2Stream [46]</cell><cell>R + F</cell><cell>59.4</cell></row><row><cell>TSN [55]</cell><cell>R + F</cell><cell>69.4</cell></row><row><cell>S3D [61]</cell><cell>R + F</cell><cell>75.9</cell></row><row><cell>R(2+1)D [52]</cell><cell>R + F</cell><cell>78.7</cell></row><row><cell>I3D [6]</cell><cell>R + F</cell><cell>81.09  ?</cell></row><row><cell>EvaNet [42]</cell><cell>R + F</cell><cell>82.3</cell></row><row><cell>PA3D [63] + I3D [6]</cell><cell>P + PAF + CF + R + F</cell><cell>82.1  *</cell></row><row><cell>ResNext101 BERT [23]</cell><cell>R + F</cell><cell>83.76  ?</cell></row><row><cell>Ours + I3D</cell><cell>P + R + F</cell><cell>82.33</cell></row><row><cell>Ours + ResNext101 BERT</cell><cell>P + R + F</cell><cell>84.53</cell></row><row><cell>Method</cell><cell>Modality</cell><cell>JHMDB</cell></row><row><cell>I3D [6]</cell><cell>R + F</cell><cell>86.8  ?</cell></row><row><cell>P-CNN [10]</cell><cell>R + F</cell><cell>61.1</cell></row><row><cell>P-CNN + IDT [10]</cell><cell>R + F</cell><cell>71.4</cell></row><row><cell>Action Tubes [16]</cell><cell>R + F</cell><cell>62.5</cell></row><row><cell>MR-TS R-CNN [40]</cell><cell>R + F</cell><cell>71.1</cell></row><row><cell>GRP + IDT [8]</cell><cell>R + F</cell><cell>74.3</cell></row><row><cell>KRP + IDT [9]</cell><cell>R + F</cell><cell>74.2</cell></row><row><cell>Chained MultiStream [68]</cell><cell>P + R + F</cell><cell>76.1</cell></row><row><cell>PoTion[63] + I3D [6]</cell><cell>P + R + F</cell><cell>85.5</cell></row><row><cell>PA3D[63] + RPAN [13]</cell><cell>P + PAF + CF + R + F</cell><cell>86.1</cell></row><row><cell>Ours + I3D</cell><cell>P + R + F</cell><cell>88.36</cell></row><row><cell cols="3">? denotes results we have reproduced. Models with * use</cell></row><row><cell cols="3">difference features along with standard heatmaps and use separate</cell></row><row><cell cols="3">models for all. P -pose heatmaps, PAF -Part affinity Fields [4],</cell></row><row><cell cols="2">CF -Features from the pose extractor CNN.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results on the validation set of the AVA v2.1 dataset. Results on this dataset demonstrate that the approach works on a large scale spatio-temporal action localization dataset. We see a significant improvement over the reproduced baseline. Further, we see that JMRN trained from scratch gives performance close to I3D-RGB model which was pretrained using large-scale Kinetics-400 dataset justifying the use of joint trajectories for action recognition. Fusion with a RGB-based model gives further gains.</figDesc><table><row><cell>Method</cell><cell>Pretraining</cell><cell>Modality</cell><cell>mAP</cell></row><row><cell>I3D [6]</cell><cell>Kinetics-400</cell><cell>RGB</cell><cell>14.5</cell></row><row><cell>I3D [6]</cell><cell cols="2">Kinetics-400 RGB + Flow</cell><cell>15.6</cell></row><row><cell>ACRN [48]</cell><cell cols="2">Kinetics-400 RGB + Flow</cell><cell>17.4</cell></row><row><cell>I3D [6]</cell><cell>Kinetics-600</cell><cell>RGB</cell><cell>21.9</cell></row><row><cell>SlowFast-R101-NL [15]</cell><cell>Kinetics-600</cell><cell>RGB</cell><cell>28.06  ?</cell></row><row><cell>PoTion[11]</cell><cell>None</cell><cell>Pose</cell><cell>13.1</cell></row><row><cell>Ours</cell><cell>None</cell><cell>Pose</cell><cell>14.1</cell></row><row><cell>Ours + SlowFast-R101-NL</cell><cell></cell><cell>Pose + RGB</cell><cell>28.4</cell></row><row><cell cols="4">? denotes results we have reproduced using the publicly</cell></row><row><cell cols="2">available model.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Mimetics experiments. Results when models are trained on the 50 Kinetics classes that intersect with Mimetics. We obtain an improvement over SIP-Net<ref type="bibr" target="#b58">[59]</ref> while not requiring any tracking and using pose heatmaps alone.</figDesc><table><row><cell>Method</cell><cell cols="3">top 1 top 5 mAP</cell></row><row><cell cols="2">SIP-Net [59] 25.1</cell><cell>51.4</cell><cell>38.3</cell></row><row><cell>Ours</cell><cell>26.6</cell><cell>52.7</cell><cell>40.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>JMRN vs. Deeper baseline. Our model outperforms a deeper version of the model proposed in<ref type="bibr" target="#b10">[11]</ref> thus showing the effectiveness of our approach.</figDesc><table><row><cell></cell><cell cols="3">JHMDB-1 HMDB-1 Charades</cell></row><row><cell>Deeper Baseline</cell><cell>66.38</cell><cell>49.25</cell><cell>14.88</cell></row><row><cell>Ours</cell><cell>71.08</cell><cell>54.05</cell><cell>16.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Effect of sharing parameters of the motion extractor module. The non-Siamese model performs worse than our proposed model.</figDesc><table><row><cell></cell><cell cols="3">JHMDB-1 HMDB-1 Charades</cell></row><row><cell>Non-Siamese</cell><cell>65.98</cell><cell>51.66</cell><cell>14.93</cell></row><row><cell>Ours</cell><cell>71.08</cell><cell>54.05</cell><cell>16.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Architecture details of the Motion Extractor. This module extracts motion information from each joint separately. In our proposed approach JMRN, parameters of this module are shared amongst all joints except the initial BatchNorm layer.</figDesc><table><row><cell>Layer</cell><cell cols="2">Kernel/Stride Output of Layer</cell></row><row><cell>BatchNorm</cell><cell>-</cell><cell>3 ? 64 ? 86</cell></row><row><cell>Convolution</cell><cell>3/2</cell><cell>128 ? 32 ? 43</cell></row><row><cell>Convolution</cell><cell>3/1</cell><cell>128 ? 32 ? 43</cell></row><row><cell>Convolution</cell><cell>3/2</cell><cell>256 ? 16 ? 22</cell></row><row><cell>Convolution</cell><cell>3/1</cell><cell>256 ? 16 ? 22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Architecture of the joint-selector module. This module uses the motion representation from all joints to reweight the joints most useful for the task.</figDesc><table><row><cell>Layer</cell><cell>Kernel/Stride</cell><cell>Output Size</cell></row><row><cell>Convolution</cell><cell>1/1</cell><cell>256 ? 16 ? 22</cell></row><row><cell>Average Pool</cell><cell>-</cell><cell>256 ? 1 ? 1</cell></row><row><cell>FC</cell><cell>3/1</cell><cell>J</cell></row><row><cell>Architecture Details</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Architecture of the classification module. The reweighed joints are the input to the module. This module performs the final classification.</figDesc><table><row><cell>Layer</cell><cell>Kernel/Stride</cell><cell>Output Size</cell></row><row><cell>Convolution</cell><cell>1/1</cell><cell>512 ? 16 ? 22</cell></row><row><cell>Convolution</cell><cell>3/2</cell><cell>512 ? 8 ? 11</cell></row><row><cell>BatchNorm</cell><cell>-</cell><cell>512 ? 8 ? 11</cell></row><row><cell>Convolution</cell><cell>3/1</cell><cell>512 ? 8 ? 11</cell></row><row><cell>BatchNorm</cell><cell>-</cell><cell>512 ? 8 ? 11</cell></row><row><cell>Global Average Pool</cell><cell>-</cell><cell>512</cell></row><row><cell>FC</cell><cell>-</cell><cell>C</cell></row><row><cell cols="2">for a single label classification task,</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>PAA with baseline. The proposed augmentation step is not tied to JMRN but can give performance improvements to other methods. The baseline shows consistent improvement when used with PAA. Our proposed model still outperforms Baseline + PAA</figDesc><table><row><cell cols="4">Approach JHMDB-1 HMDB-1 Charades</cell></row><row><cell>PoTion</cell><cell>59.44</cell><cell>42.04</cell><cell>13.54</cell></row><row><cell>+ PAA</cell><cell>65.92</cell><cell>49.76</cell><cell>15.16</cell></row><row><cell>Ours</cell><cell>71.08</cell><cell>54.05</cell><cell>16.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors would like to thank Alex Hanson, Ketul Shah, Susmija Reddy and Anoop Cherian for their helpful suggestions. This research is partially supported by the ONR MURI Grant N00014-20-1-2787.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Table 14: Clip selection comparison average of 3 splits on HMDB. Our model outperforms the dense clip selector baseline by 0.7% while utilizing only 60% of clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadjad</forename><surname>Asghari-Esfeden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Sznaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavia</forename><surname>Camps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Dynamic motion representation for human action recognition. Method Clips used Accuracy(%)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Action recognition with joints-pooled 3D deep convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congqi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Openpose: realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generalized rank pooling for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3222" to="3231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Non-linear temporal subspace representations for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">P-CNN: Pose-based CNN features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilhem</forename><surname>Ch?ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">RPAN: an end-toend recurrent pose-attention network for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rpan: An end-toend recurrent pose-attention network for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6047" to="6056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; psychophysics</title>
		<imprint>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mmtm: Multimodal transfer module for cnn fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Hamid Reza Vaezi Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhito</forename><surname>Iuzzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13289" to="13299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Late temporal modeling in 3D CNN architectures with bert for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kalfaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aydin</forename><surname>Alatan</surname></persName>
		</author>
		<idno>abs/2008.01232</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipeng</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the european conference on computer vision (ECCV)</title>
		<meeting>the european conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="713" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hmdb: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crowdpose</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00324</idno>
		<title level="m">Efficient crowded scenes pose estimation and a new benchmark</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Resound: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Repair: Removing representation bias by dataset resampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simple yet efficient real-time pose-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Ludl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gulde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Crist?bal</forename><surname>Curio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">LSTM pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">2D/3D pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Diogo C Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Star-net: Action recognition using spatio-temporal activation reprojection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Mcnally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcphee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Conference on Computer and Robot Vision</title>
		<meeting><address><addrLine>Kingston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Posefix: Model-agnostic general human pose refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7773" to="7781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Integralaction: Pose-driven feature integration for robust human action recognition in videos. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-region twostream R-CNN for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Evolving space-time neural architectures for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1793" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Twostream adaptive graph convolutional networks for skeletonbased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Asynchronous temporal fields for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtyfirst AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Actor-centric relation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="318" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3D skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Video representation learning using discriminative pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">3D single-person concurrent activity detection using stacked relation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linghan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Mimetics: Towards understanding human actions out of context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1675" to="1690" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pose Flow: Efficient online pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">PA3D: Poseaction 3D machine for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1281" to="1290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Compact generalized non-local network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Chained multi-stream networks exploiting pose, motion, and appearance for action classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
