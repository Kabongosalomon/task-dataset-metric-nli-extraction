<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-13">13 Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chakkrit</forename><surname>Termritthikun</surname></persName>
							<email>chakkritt60@nu.ac.th</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeshi</forename><surname>Jamtsho</surname></persName>
							<email>yjamtsho.cst@rub.edu.bt</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirarat</forename><surname>Ieamsaard</surname></persName>
							<email>jirarati@nu.ac.th</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paisarn</forename><surname>Muneesawang</surname></persName>
							<email>paisarnmu@nu.ac.th</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Lee</surname></persName>
							<email>ivan.lee@unisa.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">STEM</orgName>
								<orgName type="institution" key="instit2">University of South Australia Adelaide</orgName>
								<address>
									<postCode>5095</postCode>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Science and Technology Royal</orgName>
								<orgName type="institution">University of Bhutan Phuentsholing</orgName>
								<address>
									<postCode>21101</postCode>
									<country key="BT">Bhutan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engineering Faculty of Engineering</orgName>
								<orgName type="institution">Naresuan University Phitsanulok</orgName>
								<address>
									<postCode>65000</postCode>
									<country key="TH">Thailand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical and Computer Engineering Faculty of Engineering</orgName>
								<orgName type="institution">Naresuan University Phitsanulok</orgName>
								<address>
									<postCode>65000</postCode>
									<country key="TH">Thailand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">STEM</orgName>
								<orgName type="institution" key="instit2">University of South Australia Adelaide</orgName>
								<address>
									<postCode>5095</postCode>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-13">13 Aug 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.engappai.2021.104397</idno>
					<note>EEEA-NET: AN EARLY EXIT EVOLUTIONARY NEURAL ARCHITECTURE SEARCH PUBLISHED AT ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE. DOI: * This work is done when Chakkrit Termritthikun works as a visiting research student in University of South Australia EEEA: Early Exit Evolutionary Algorithm Network by Chakkrit Termritthikun, et al.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Deep learning ? Neural Architecture Search ? Multi-Objective Evolutionary Algorithms ? Image classification</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goals of this research were to search for Convolutional Neural Network (CNN) architectures, suitable for an on-device processor with limited computing resources, performing at substantially lower Network Architecture Search (NAS) costs. A new algorithm entitled an Early Exit Population Initialisation (EE-PI) for Evolutionary Algorithm (EA) was developed to achieve both goals. The EE-PI reduces the total number of parameters in the search process by filtering the models with fewer parameters than the maximum threshold. It will look for a new model to replace those models with parameters more than the threshold. Thereby, reducing the number of parameters, memory usage for model storage and processing time while maintaining the same performance or accuracy. The search time was reduced to 0.52 GPU day. This is a huge and significant achievement compared to the NAS of 4 GPU days achieved using NSGA-Net, 3,150 GPU days by the AmoebaNet model, and the 2,000 GPU days by the NASNet model. As well, Early Exit Evolutionary Algorithm networks (EEEA-Nets) yield network architectures with minimal error and computational cost suitable for a given dataset as a class of network algorithms. Using EEEA-Net on CIFAR-10, CIFAR-100, and ImageNet datasets, our experiments showed that EEEA-Net achieved the lowest error rate among state-of-the-art NAS models, with 2.46% for CIFAR-10, 15.02% for CIFAR-100, and 23.8% for ImageNet dataset. Further, we implemented this image recognition architecture for other tasks, such as object detection, semantic segmentation, and keypoint detection tasks, and, in our experiments, EEEA-Net-C2 outperformed MobileNet-V3 on all of these various tasks. (The algorithm code is available at https://github.com/chakkritte/EEEA-Net).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b68">[2019]</ref> <p>network architecture (right).</p><p>? We conduct extensive experiments to evaluate the effectiveness of an EEEA-Net by outperforming MobileNet-V3 for all of the image recognition, object detection, semantic segmentation, and keypoint detection. Also, the EEEA-Net was widely tested on standard CIFAR-10 Krizhevsky <ref type="bibr">[2009]</ref>, CIFAR-100 <ref type="bibr" target="#b15">Krizhevsky [2009]</ref>, ImageNet <ref type="bibr" target="#b0">Russakovsky et al. [2015]</ref>, PASCAL VOC <ref type="bibr" target="#b16">Everingham et al. [2010]</ref>, Cityscapes <ref type="bibr" target="#b17">Cordts et al. [2016]</ref>, and MS <ref type="bibr">COCO Lin et al. [2014]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>NAS was designed to search and design the model structure that suits the best to the applied dataset. Thus, the model obtained by NAS has a small number of parameters with high performance. NAS can find models suitable for both small and large datasets. The NAS can be of single-objective NAS and multi-objective NAS: a single-objective NAS considers models from a single objective such as error rate, number of parameters, or FLOPS. The multi-objective NAS considers models considering more than one objective, which we have adopted in this paper to optimise the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network Architecture Search</head><p>Setting optimised parameters in each layer, such as kernel size, kernel scroll position (stride), zero paddings, as well as the output size, is the main challenge in creating CNN architectures efficiently for a given dataset. The total parameters are directly proportional to the number of layers. Manually designing a model takes too long and requires considerable experimentation to achieve optimal performance, which is why an automated model discovery is essential.</p><p>The ability of NASs to find automated models suitable for datasets has proved a popular area of experimentation. Deep learning also is gaining popularity and is now being widely used. The NAS model has also been designed and expanded to enable applications in new tasks such as a NAS for semantic image segmentation, NAS for object detection, and NAS for skin lesion classification. The approaches used in NAS are <ref type="bibr">RL Zoph and Le [2016]</ref>, <ref type="bibr">EA Real et al. [2019]</ref>, <ref type="bibr" target="#b12">Liu et al. [2018a]</ref>, <ref type="bibr" target="#b19">Lu et al. [2019]</ref>, and relaxation . The three critical steps for NAS are:</p><p>Step 1: Search the CNN model's search space to find the suitable architecture. A CNN architecture search model contains many search spaces which are optimised in the image classification application. Google Brain has launched a search space for the NASNet model. It is a feed-forward network in which layers are broken into subgroups called cells. The normal cell can learn from images, and the normal cell maintains an image output equal to the input size. However, the kernel stride in each reduction cell is 2, halving the input image size. Normal and reduction cells are linked. Each cell is stacked during modelling, where N normal cells are connected. Reduction cells are added between the N normal cells, as shown in <ref type="figure" target="#fig_0">Fig. 1 (left)</ref>, to halve the image size, helping the next normal cell to process faster.</p><p>The output from the search space thereby includes normal cells and reduction cells used in the evaluation. NAS has directed acyclic graphs (DAGs) connection between input X1 and X2 of cells, as shown in <ref type="figure" target="#fig_0">Fig. 1 (right)</ref>. In each normal cell, there are two input activations and one output activation. In the first normal cell, input X1 and X2 are copied from the input (image). The next normal cell uses, as input, both the X1 from the last normal cell, and the X2 from the second to last normal cell. All cells are connected the same way until the end of the model. Also, each cell has a greater number of cell output channels in each layer.</p><p>Step 2: Evaluate the CNN model on a standard dataset for benchmarks. These benchmarks include number of errors, number of parameters, and search cost. The normal cells and reduction cells that are found in the search space are evaluated to measure the error rate, the number of parameters, and the computing costs, using CIFAR-10. Due to limited processor resources and GPU memory, parameters such as cell count (N ), number of epochs, initial channel, and channel increment, are different for each search space and evaluation.</p><p>Step 3: Evaluate with a large-scale dataset. When the model from the search space has been identified, it is evaluated with a larger dataset. Model evaluation with the CIFAR-10 dataset cannot be compared with other models because the CIFAR-10 dataset contains only ten classes. Given this constraint, CIFAR-100 datasets with 100 classes are required.</p><p>The search space of NASNet used RL and was tested with the CIFAR-10 dataset, which takes up to 2,000 GPU days to model. The AmoebaNet Real et al. <ref type="bibr" target="#b68">[2019]</ref>, based on an evolutionary algorithm model, takes up to 3,150 GPU days for the same dataset. Also, the search space of NASNet was designed to use shorter search times. However, the sequential model-based optimisation (SMBO) method <ref type="bibr" target="#b20">Liu et al. [2018c]</ref> takes 335 GPU days, the gradient descent method  takes just 4 GPU days, whereas weight-sharing across different structures  takes only 0.5 GPU days.</p><p>As indicated, the AmoebaNet takes 3,150 GPU days, whereas the NSGA-Net Lu et al. <ref type="bibr" target="#b68">[2019]</ref>, which uses a multiobjective evolutionary algorithm to find models, takes 4 GPU days. However, although the error rate of NSGA-Net is higher than that of AmoebaNet, based on a standard CIFAR-10 evaluation, the main focus of this area of research has been the reduction of search costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-objective Network Architecture Search</head><p>NAS aims to minimise errors, hyper-parameters, FLOPS, and delays, making it challenging to identify a network architecture suitable for each objective simultaneously. Thus, the best network architecture should reduce or minimise all of these dimensions. For the evolution-based NASs, NSGA-Nets Lu et al. <ref type="bibr" target="#b68">[2019]</ref> considers FLOPS and error count, <ref type="bibr">CARS Yang et al. [2020]</ref> and <ref type="bibr">LEMONADE Elsken et al. [2018]</ref> consider device-agnostic and device-aware objectives. In our work, however, we sought the achievement of the three goals; minimising errors, and reducing parameters and FLOPS, simultaneously.</p><p>The NASs mostly focus on creating a network architecture for image recognition, then transferring that architecture to other tasks. However, for object detection and semantic segmentation, the same network architecture can be used as a backbone.</p><p>Many network architectures, such as the EfficientNet Tan and Le <ref type="bibr" target="#b68">[2019]</ref>, FBNetV2 <ref type="bibr" target="#b25">Wan et al. [2020]</ref>, <ref type="bibr" target="#b67">DARTS Liu et al. [2018b]</ref>, P-DARTS Chen et al. <ref type="bibr" target="#b68">[2019]</ref>, <ref type="bibr">CDARTS Yu and Peng [2020]</ref>, <ref type="bibr">CARS Yang et al. [2020]</ref>, <ref type="bibr">LEMONADE Elsken et al. [2018]</ref>, NSGA-Net Lu et al. <ref type="bibr" target="#b68">[2019]</ref> and NSGA-NetV2 <ref type="bibr" target="#b28">Lu et al. [2020a]</ref> were tested only on image recognition datasets. It is challenging to design and evaluate a network architecture for general purposes. <ref type="table" target="#tab_0">Table 1</ref> shows the research objectives of the various NASs, illustrating that the image identification architectures were, in some cases, transferred to object detection, with one, MobileNetV3 Howard et al. <ref type="bibr" target="#b68">[2019]</ref> also being applied to transfer specifically researched image identification architecture to both object detection and semantic segmentation.</p><p>Our objective was to extend this image identification architecture, using the ImageNet dataset, to object detection, semantic segmentation, as well the further purpose of keypoint detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inverted Residuals Network (IRN)</head><p>The Inverted Residuals Network (IRN) Tan et al. <ref type="bibr" target="#b68">[2019]</ref> concept is needed to reduce the Residuals Network (RN) parameters. In contrast, the RN concept integrates data from the previous layer into the last layer. <ref type="figure">Fig. 2 (left)</ref> shows that the RN structure has three layers: wide, narrow, and wide approach layers. The wide layers have N ? 16 output channels whereas the narrow layers have N ? 16 channels each. The wide approach layer has N ? 32 output channels (N is the input channels in each case). However, all the convolution layers used the standard convolution. Batch normalisation (BN) and activation functions (ReLU) were also added into each convolution layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search Method Multiple Objective Dataset Searched</head><p>Architecture transfer ? MobileNetV3 Howard et al. <ref type="bibr" target="#b68">[2019]</ref> RL + expert -ImageNet IR, OD, SS EfficientNet Tan and Le <ref type="bibr" target="#b68">[2019]</ref> RL + scaling -ImageNet IR FBNetV2 <ref type="bibr" target="#b25">Wan et al. [2020]</ref> gradient -ImageNet IR DARTS  gradient -CIFAR-10 IR P-DARTS Chen et al. <ref type="bibr" target="#b68">[2019]</ref> gradient -CIFAR-10, CIFAR-100 IR PC-DARTS  gradient -CIFAR-10, ImageNet IR, OD CDARTS <ref type="bibr" target="#b27">Yu and Peng [2020]</ref> gradient -CIFAR-10, ImageNet IR CARS  EA Yes CIFAR-10 IR LEMONADE <ref type="bibr" target="#b23">Elsken et al. [2018]</ref> EA Yes CIFAR-10, CIFAR-100, ImageNet64 IR NSGA-Net Lu et al. <ref type="bibr" target="#b68">[2019]</ref> EA Yes CIFAR-10 IR NSGA-NetV2 <ref type="bibr" target="#b28">Lu et al. [2020a]</ref> EA Yes ImageNet IR EEEA-Net (this paper) EA+ EE-PI Yes CIFAR-10, ImageNet IR, OD, SS, KD ? IR = Image Recognition, OD = Object Detection, SS = Semantic Segmentation, KD = Keypoint Detection.  <ref type="figure">Figure 2</ref>: The difference between the residual network (left) and the inverted residual network (right).</p><p>The RN structure is modified and reversed to obtain the IRN. The layers in IRN are defined as narrow layer, wide layer and narrow approach layer. In IRN, the number of output channels obtained is equal to the number of input channels, N , as shown in <ref type="figure">Fig. 2 (right)</ref>. When the data is fed into the 1 ? 1 convolution layer, the number of channels will be expanded to N ? 16. The wide layer changes to a 3 ? 3 depth-wise separable convolution instead of a 3 ? 3 standard convolution, reducing the FLOPS and number of parameters. There are N ? 16 channels in a wide layer, which is equal to the previous layer. Also, 1 ? 1 standard convolution is used in the narrow approach to reduce the channels' size to be equal to the input channel N . Then, the input data (Xi) is combined with the IRN's output to get data (Xi + 1).</p><p>Convolutional layers can be defined in various formats to find a model structure with EA. Convolutional layers are also called cells. The types of convolutional layers used in our experiment are presented in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The most commonly used methods to develop NAS are RL and gradient descent algorithms. However, these algorithms possess limitations in solving multi-objective problems. EA automates the model search process, is easier to implement, and enables discovery of solutions while considering multiple objectives.</p><p>A general description of an EA, including encoding, a presentation of the multi-objective genetic algorithm, and genetic operations used with NAS, is provided in Section 3.1. The Early Exit Population Initialisation concept and method, its simple, yet effective application in EA, mitigating the complexity and parameters used in the previous models, while maintaining the same accuracy, are described in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evolutionary Neural Architecture Search</head><p>The Genetic Algorithm (GA) is an algorithm based on Darwinian concepts of evolution. The GA is part of a randombased EA <ref type="bibr" target="#b32">Xie and Yuille [2017]</ref>, <ref type="bibr" target="#b33">Baldominos et al. [2017]</ref>, <ref type="bibr" target="#b34">Real et al. [2017]</ref>. GA's search-based solutions stem from the genetic selection of robust members that can survive. The population is integral to GA because GA solutions are  like organisms that evolve after the environment. The most suitable solutions need to rely on genetic diversity. Thus, a greater number of genetically diverse populations enables more effective GA solutions.</p><p>The initial phase of GA creates the first population for candidate solutions. The population is determined by population size, which describes total solutions. Each solution is called an individual, where an individual consists of a chromosome. The chromosome is a mix of genes. In the initial population, it is possible to provide unique information about each gene to all other genes, at random. A fitness function computes the fitness value for each individual. CNN's model structure is searched with the NAS search space, defining error rate as a fitness function where fitness value represents dataset error value. As shown in Equation 1, the fitness value is calculated where n is the number of individuals.</p><formula xml:id="formula_0">f itness(i) = f (x i ), i = 1, 2, 3, .., n<label>(1)</label></formula><p>Organisms consist of different phenotypes and genotypes. Appearances such as foreign features (such as eye colour) and internal features (such as blood types) are called phenotypes. Genes of different organisms, called genotypes, can be transferred from model to model by gene transfer.</p><p>The CNN model's architecture is represented as a genotype in the NAS search space. A subset of the NAS search space includes normal cells and reduction cells. The cells are stacked in the complete architecture. Normal cells or reduction cells consist of connected layers such as convolution layers, average pooling, max pooling, and skip connection. A complete model must connect cells to create a genotype for training or testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Encoding</head><p>The genotype of the NAS model consists of normal cells and reduction cells, called chromosomes. There are various genes linked to chromosomes. The number of genes is defined as LA 1 LA 2 , LB 1 LB 2 , LC 1 LC 2 , and LD 1 LD 2 as in Equation 2. The gene consists of operations (L) and indices of operations (A, B, C, D). Operation (L) can be considered a type of CNN layer, such as max pooling, average pooling, depth-wise separable convolution, dilated convolution, inverted residuals block, and skip connection.</p><formula xml:id="formula_1">chromosome(x) = LA 1 LA 2 , LB 1 LB 2 , LC 1 LC 2 , LD 1 LD 2<label>(2)</label></formula><p>For example, consider nine different operations (L) in the experiment, as <ref type="bibr">[0,</ref><ref type="bibr">1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">5,</ref><ref type="bibr">6,</ref><ref type="bibr">7,</ref><ref type="bibr">8]</ref>. Moreover, the operation index (A, B, C, D) refers to the operation location to be connected with other operations (L). From <ref type="figure" target="#fig_1">Fig.3 (left)</ref>, the index is defined as follows: A = [0], B = [0, 1], C = [0, 1, 2], and D = [0, 1, 2, 3]. The connection between operations (L) and the operation index (A, B, C, D) determines the location of the connection between operations. For example, LA's gene code, LA 1 LA 2 is 30,80, meaning output data processed in operation 3 and 8 will be linked to index 0.</p><p>Similarly, LB 1 LB 2 equals 01,21, meaning data processed by operation 0 and 1 are connected at index 1. However, in the genes of LC 1 LC 2 and LD 1 LD 2 , those genes are linked sequentially to the output of LA 1 LA 2 and LB 1 LB 2 , to help reduce the number of model parameters. If the LC 1 LC 2 and LD 1 LD 2 is connected to the same input as LA 1 LA 2 , and LB 1 LB 2 (parallel network) will increase the processing time and the parameters.</p><formula xml:id="formula_2">P reviousIndex = index ? 2<label>(3)</label></formula><p>The position of the previous index can be computed from Equation 3, where the index is greater than 1. If the index is an even number, it is linked to the even previous index; otherwise, it is linked to the odd previous index. Thus, the LC 1 LC 2 gene is 12-02, which has an index of 2, and the LC 1 LC 2 gene is linked from index 0. While the LD 1 LD 2 gene is 63-73, it has an index of 3. The LD 1 LD 2 gene is linked from index 1. However, if there are different indices in a gene, for example, a gene 63-72, operator 6 is connected from index 1 and operator 7 is from index 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Multi-objective Genetic Algorithm</head><p>Initially, GA was used for single-objective optimisation problems (SOOP) and, later, GA was developed to solve the multi-objective optimisation problem (MOOP) <ref type="bibr" target="#b35">Deb et al. [2002]</ref>, which has more than one objective function to minimise fitness values. The GA that can solve the MOOP problems Carrau et al.</p><p>[2017], Hasan et al. <ref type="bibr" target="#b68">[2019]</ref> is called a multi-objective genetic algorithm (MOGA).</p><formula xml:id="formula_3">min {f 1 (x) , f 2 (x) , ..., f k (x)} s.t. x ? X<label>(4)</label></formula><p>The optimisation of the CNN model is generally a problem with more than one objective. As illustrated in Equation 4, where f is fitness values, the integer k ? 2 is the number of objectives, x is individual, and X is the set of individuals.</p><p>All these objectives must be as small as possible.</p><p>Indicators used to measure CNNs model performance include model accuracy, model size, and processing speed. There are three objectives to consider during a model search: lowest validation error, minimum parameters, and computational cost.</p><formula xml:id="formula_4">min {Error (x) , F LOP S (x) , P arams (x)} s.t. x ? X w error + w f lops + w params = 1 w error , w f lops , w params &gt;= 0<label>(5)</label></formula><p>The evolutionary algorithm finds the most effective model for each objective by finding the lowest objective values of the entire population. We defined the three objective values as being equally important. Thus, it is necessary to set the weight of each of the three objective values to 1/3 to find the best model for each value. As illustrated in Equation <ref type="formula" target="#formula_4">5</ref>, where x is individual, X is the individual's set and w error , w f lops , w params weighs each objective's weight.</p><p>For the MOOP problem, it is almost impossible to find one solution that provides the optimal value for each objective function. For each solution given by the MOOP, the best solution group is called nondominated or Pareto optimal because these solutions are compared using the Pareto Domination principle. Many solutions can be obtained from the search using MOOP. These solutions will be reviewed again to find the best solution within the searched solution group.</p><p>The best solution group should not dominate when compared to other solutions. For example, any solution v overwhelming a better solution can be represented as v ? w. If no solution v is worse than solution w, then solutions v is better than w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Genetic Operations</head><p>The processes used to create offspring in the new generation are called genetic operations. A new population must replace an ancestor group that cannot survive. The population can be created in two ways, by crossover or mutation. Crossover is the creation of a new population by switching genes of different chromosomes from two populations. The genotype of the parent chromosomes will be recombined to create a novel chromosome, which can be done in various ways. For example, a point crossover that performs random cutting points or chromosomes to produce offspring is a crossover between two chromosomes with a random probability of 0.5. Crossover creates offspring using random genes from parents. <ref type="figure" target="#fig_2">Fig. 4</ref> (top) demonstrates the uniform crossover operation used in this implementation, requiring two-parent architectures. We visualised the architectures as follows: 40-30-61-31-00-60-42-13, as parent 1 and 40-30-21-61-22-72-53-11, as parent 2. Then, in the crossover operation, the random probability of 0.5 is defined. The fifty-fifty chance was used to cross the gene between the two parent architectures for child modelling (40-30-61-61-02-72-52-13). The common parent gene is coloured black, but if the gene derived from the first parent is red, then the gene derived from the second parent is represented by blue.</p><p>A mutation is an operation to reduce population uniformity and contribute to genetic diversity. The mutation changes data in the gene by randomly locating the gene and replacing the original gene with random new genes. The mutation causes offspring chromosomes to be different from parents. The individual being mutated is called a mutant. Input: The number of generations G, population size n, validation dataset D, objectives w. Output: A set of K individuals on the Pareto front. Initialisation: An Early Exit Population Initialisation P 1 and Q 1 .</p><formula xml:id="formula_5">for i = 1 to G do R i = P i ? Q i for all p ? R i do Train model p on D Evaluate model p on D end for M = tournament-selection(P i+1 ) F = non-dominated-sorting(R i )</formula><p>Pick n individuals to form P i+1 by ranks and the crowding distance weighted by w based on Equation 5 Q i+1 = crossover(M ) ? mutation(M ) end for Select K models at an equal distance near Pareto front from P G</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Early Exit Population Initialisation (EE-PI)</head><p>EA can find cell patterns by selecting the best model with the lowest error rate. However, the discovery process takes longer to search and select cells in each generation. Each population has to be trained and evaluated, which increases the time needed to find cell patterns. The single-objective EA uses the error value to find network architecture. However, the network architecture with the lowest error may have too many parameters. In our experiment, the maximum number of generations was set to 30 with 40 populations per generation due to the limited resources of a single GPU.</p><p>The population is the only factor affecting processing time. The evaluation must examine the entire population to select the population with the lowest error rate, thus obtaining an effective model structure. However, a longer search time is required to evaluate every population using a single processor unit. Therefore, the EE-PI method was introduced into the evolutionary algorithm to reduce the search time and control the number of parameters, as illustrated in <ref type="figure">Fig. 5</ref> and detailed in Algorithm 1.</p><p>The EE-PI method filters the CNN models based on the number of parameters in the network architecture, which is iteratively compared to a pre-set maximum value (?). The EE-PI obtains the CNN network architecture which has less parameters than the maximum number of parameters attached to the EA, as illustrated in <ref type="figure">Fig. 5</ref>, which shows the Early Exit as the dashed-line block.</p><formula xml:id="formula_6">EarlyExit(?, ?) 1, if ? ? ? 0, otherwise<label>(6)</label></formula><p>In Equation 6, where ? is the parameter of the model which is discovered, ? is the maximum number of parameters. If the number of parameters found in the model are less than or equal to the maximum number of parameters (? ? ?), then the model will be considered as a part of the first-generation population.</p><p>For example, to select a network architecture with a maximum of 3 million parameters (? = 3), EA selects the model by considering the number of parameters lower than the maximum number of parameters. Suppose network architecture is not considered, because it has more than maximum parameters (?). In this case, it chooses a new structure with less than 3 million parameters. Therefore, in conjunction with the EA in the selection process, Early Exit facilitates the filtering out of the model with the number of parameters greater than the maximum number of parameters. The best network architecture is also discovered using the EA with Early Exit by considering the error rate and the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>Experiments were carried out in three parts: First, finding and evaluating the network architecture with EEEA on CIFAR-10 and CIFAR-100. The second part was the finding and evaluation of the EEEA-Net using the ImageNet datasets. In the third part, the EEEA-Net obtained from the second part is applied for other tasks such as object detection, semantic segmentation, and keypoint detection. The PyTorch deep learning library was used in the experiments. The experiment was carried out on Intel(R) Xeon(R) W-3235 CPU @ 3.30GHz 12 Core CPU, 192 GB RAM and NVIDIA RTX 2080 Ti GPU, running on the Ubuntu 18.04.3 operating systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CIFAR-10 and CIFAR-100 datasets</head><p>This subsection searched for a model with the CIFAR-10 dataset; it was evaluated on CIFAR-10 and CIFAR-100 datasets. Both CIFAR-10 and CIFAR-100 datasets consisted of 60,000 images, with 50,000 images and 10,000 images in the training set and test set, respectively. The CIFAR-10 and CIFAR-100 have 10 and 100 classes, respectively, with 600 images in each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Architecture Search on CIFAR-10</head><p>Thirty generations with 40 populations in each generation were defined to locate the network architecture with EA. The first-generation populations were randomly generated, with subsequent populations in generations 2-30 being evolved with EA. Each population was defined with a depth of two normal cells instead of the usual six normal cells. Thus, it reduced the search time of the network architecture. The search and evolution process happens more rapidly when Early Exit is used in the initial populations' process. Early Exit selects the network architecture having less than the pre-specified maximum of parameters (?). Thus, population evolution will choose only network architectures that are efficient and have fewer parameters.</p><p>The hyper-parameters for the search process were defined as: the total number of cells (normal cells and reduce cells) equal to eight layers with 32 initial channels by training the network from scratch for one epoch on the CIFAR-10 dataset. The hyper-parameters used included a batch size of 128, with SGD optimiser with weight decay equal to 0.0003 and momentum equal to 0.9. The initial learning rate was 0.05. Using the cosine rule scheduler, the Cutout regularisation had a length set to 16, a drop-path of the probability of 0.2, and the maximum number of parameters equal to 3, 4, and 5 million.</p><p>The evolutionary algorithm (EA-Net, ? = 0) took 0.57 GPU days to find the network architecture with NVIDIA RTX 2080 Ti. However, the early exit evolutionary algorithm (EEEA-Net-A, ? = 3) took 0.38 GPU days, EEEA-Net-B (? = 4) took up to 0.36 GPU days, and EEEA-Net-C (? = 5) took up to 0.52 GPU days. These architectures are used for performance evaluation in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Architecture Evaluation on the CIFAR-10 dataset</head><p>The network architecture had to be changed to find the normal and reduced cells with the Early Exit evolutionary algorithms. The CIFAR-10 dataset was used for the evaluation. The hyper-parameters were defined with the number of all cells (normal and reduce cells) set to 20 layers with 32 initial channels, the network was trained from scratch with 600 epochs with a batch size of 96, SGD optimiser with weight decay was 0.0003 and momentum 0.9, and the initial learning rate set to 0.025. Using the cosine rule scheduler, the Cutout regularisation had a length set to 16, a drop-path of the probability of 0.2, and auxiliary towers of weight equal to 0.4. <ref type="table">Table 3</ref> shows the comparisons and evaluations of EEEA-Net with other state-of-the-art models. EEEA-Net-C was evaluated with the test dataset, giving an error rate of 2.46% for CIFAR-10. It took 0.52 GPU days to find normal and reduce cells.</p><p>By comparison, our EEEA-Net-C model achieved a lower error rate and search time than all of those other models.</p><p>AmoebaNet-B was the lowest of the other state-of-the-art models: NASNet-A model, PNAS, both DARTS versions, and NSGA-Net. However, the AmoebaNet-B model required 3,150 GPU days to complete, according to . This is clearly a hugely greater amount of search resources than required in our model (0.52 GPU days).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Performance and Computational Complexity Analysis</head><p>The multi-objective search tested error rate, number of FLOPS, and parameters. Optimisation on the effectiveness of a multi-objective uses Hypervolume (HV) as a measure of performance that computes the dominated area, using a reference point (Nadir point) with the most significant objective value from the first-generation population. Then, the Pareto-frontier solution computes the area between the reference point and Pareto. The higher HV shows that a multi-objective solution performs better in all objectives.</p><p>After the model search, the HV for all the solutions obtained from the search is calculated to compare the performance of two variants: a model that uses Early Exit (EA-Net) and a model that does not use Early Exit. In <ref type="figure" target="#fig_4">Fig. 6</ref>, the values shown in the vertical axis are normalised HV, and the horizontal axis is generations. When we closely look at the HV value, it was found that the search using the EA-Net model yielded HV values greater than the three EEEA-Net models.  <ref type="bibr" target="#b68">[2019]</ref> 3.12 18.93 3.1 3,150 evolution AmoebaNet-B + CO Real et al. <ref type="bibr" target="#b68">[2019]</ref> 2.55 -2.8 3,150 evolution LEMONADE <ref type="bibr" target="#b23">Elsken et al. [2018]</ref> 3.05 -4.7 80 evolution NSGA-Net + CO Lu et al. <ref type="bibr" target="#b68">[2019]</ref> 2  However, considering only the model with an early exit, it was found that searches using ? equal to 5 performed better than ? equal to 3 and 4, since ? is a parameter that determines the model size by the number of parameters. Consequently, creating larger models by increasing the size of ?, gave superior performance.</p><p>In addition, when considering a model without an Early Exit (EA-Net) and a model that used an Early Exit (?=5, EEEA-Net-C), it was found that the search efficiency of the EEEA-Net-C model was nearly similar to that of EA-Net because the EA-Net search does not control the model size while searching for the model. Therefore, the model obtained by EA-Net's search may is likely to obtain a model of a large parameter size. On the other hand, the model with an Early Exit better controls the model size, and the resulting model provides similar performance than achievable in an uncontrolled search.    We present progress trade-offs after each generation of the EA-Net and EEEA-Nets search through <ref type="figure" target="#fig_8">Fig. 7</ref>. The whole population is demonstrated by two-dimensional coordinates such as CIFAR-10 accuracy vs. Generations and CIFAR-10 accuracy vs. FLOPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Data augmentation</head><p>The results from the evaluation of EEEA-Net are represented in precision floating-point (FP32). Our experimental goals were to create the most effective EEEA-Net without modifying the model structure. In the evaluation, we added the AutoAugment (AA) Cubuk et al. <ref type="bibr" target="#b68">[2019]</ref> technique to the data augmentation process. AA created a more diverse set of data, making the model more effective. When Cutout DeVries and Taylor <ref type="bibr">[2017]</ref> and AA Cubuk et al. <ref type="bibr" target="#b68">[2019]</ref> were used, we observed that the error rate of EEEA-Net-C was reduced to 2.42%. Without AA, however, an error rate of 2.46% occurred, as shown in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Architecture Evaluation on CIFAR-100 dataset</head><p>The AA technique was used to optimise the search and evaluation process of the EEEA-Net. The EEEA-Net-C was trained using the CIFAR-10 dataset, which was not sufficient for our purposes. Consequently, the EEEA-Net architectures obtained from CIFAR-10 dataset were used with CIFAR-100.</p><p>The hyper-parameters used in the training process were changed to evaluate the EEEA-Net with the CIFAR-100 dataset, where the number of all cells (normal and reduce cells) was set to 20 layers with 36 initial channels. This was the outcome of training the network from scratch in 600 epochs with a batch size of 128, setting the SGD optimiser with a weight decay of 0.0003 and momentum of 0.9, and the initial learning rate set to 0.025 and running with the cosine rule scheduler. The Cutout regularisation length was equal to 16, and the drop-path of probability was 0.2, with auxiliary towers of the weight of 0.4.</p><p>When the EEEA-Net-C (same model structure) was evaluated with CIFAR-100 datasets, it showed an error rate of 15.02%, as shown in <ref type="table">Table 3</ref>. Further, this evaluation, with 3.6 million parameters, resulted in the lowest error rate of all the state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ImageNet dataset</head><p>In this subsection, we used the ImageNet dataset for the search and model evaluation. The ImageNet dataset is a large-scale standard dataset for benchmarking performance for image recognition for 1,000 classes with 1,281,167 images for the training set, 50,000 images for the test set, divided into 1,000 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Architecture Search on ImageNet</head><p>Early Exit was used to discover a network architecture using the CIFAR-10 dataset. However, this network architecture was constructed from a multi-path NAS, which requires considerable memory. Given this, we used a single-path NAS to find the network architecture on ImageNet to reduce this search time, which also allows a multi-objective search with early exit population initialisation to be used on the OnceForAll <ref type="bibr" target="#b46">Cai et al. [2020]</ref> super-network (called Supernet) to discover all network architectures that offer the best trade-off. Supernet can also search for the four dimensions of the network architecture, including kernel size, width (number of channels), depth (number of layers), and input resolution resize. We set all hyper-parameters for our architecture searches following the process in NSGA-NetV2 <ref type="bibr" target="#b28">Lu et al. [2020a]</ref>.</p><p>The two objectives of accuracy and FLOPS were the criteria for searching for 300 high accuracy samples with low FLOPS. However, these sample architectures have a diverse number of parameters.  <ref type="table">Table 4</ref>: Results of CIFAR-10 using Cutout (CO) and AutoAugment (AA). architecture size when running on devices that may have memory constraints. Thus, to prevent the architecture from having too many parameters, we appended the Early Exit to create the first population with limited parameters.</p><p>In this experiment, we compiled the number of architecture parameters shown in <ref type="table" target="#tab_7">Table 5</ref> to calculate the average number of parameters equal to 5. Thus, the maximum number of parameters (?) where ? equals 5, 6 or 7, was defined as follows: EEEA-Net-A (? = 5), EEEA-Net-B (? = 6), EEEA-Net-C (? = 7). For a fair comparison, we set ? qual to 0, and called that EA-Net-N (? = 0). We categorised our networks using the number of MobilenetV3 FLOPS to define the network size architectures of EEEA-Net-A1, EEEA-Net-B1, EEEA-Net-C1, and EEEA-Net-N1 as smallscale architectures (&lt;155 FLOPS). The large-scale architectures (&lt;219 FLOPS) are EEEA-Net-A2, EEEA-Net-B2 EEEA-Net-C2, and EEEA-Net-N2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Architecture Evaluation on ImageNet dataset</head><p>The discovery of architecture from Supernet is the separation of some layers from Supernet called subnets. Since Supernet and the subnets have different network architectures, the accuracy of the subnets, with pre-trained weight from Supernet, is very low when they were tested on the validation dataset. So, the subnets have calibrated the statistics of batch normalisation (BN) after searching on Supernet. The new BN statistics from the subnets were calculated using the validation dataset and updating the BN of all of the subnets. Thus, BN calibration can improve test accuracy value efficiency with the ImageNet dataset. <ref type="table" target="#tab_7">Table 5</ref> shows the comparison of EEEA-Net performance with other models, using three main comparison factors: error rate, number of parameters, and FLOPS. We classify models using architectural search methods such as auto, manual, or a combination. When comparing our small-scale architectures (EEEA-Net-A1, EEEA-Net-B1, EEEA-Net-C1, and EEEA-Net-N1) with GhostNet 1.0 <ref type="bibr" target="#b46">Han et al. [2020]</ref>, we found that all our architectures outperform GhostNet 1.0. Also, EEEA-Net-A1, EEEA-Net-B1, EEEA-Net-C1, and EEEA-Net-N1 provide lower error and FLOPS counts than MobileNetsV3 Large 0.75 Howard et al. <ref type="bibr" target="#b68">[2019]</ref>. However, the MobileNetsV3 Large 0.75 has fewer parameters than our models.</p><p>Similarly, when we compared our large-scale architectures with other architectures, we found that EEEA-Net-C2 (? = 7) has a Top-1 error, and FLOPS were lower than all other architectures, as shown in <ref type="table">Table 4</ref>. When we compare our architecture with MobileNetsV3 Large 1.0, EEEA-Net-C2 provides a 1% less error value than MobileNetsV3 <ref type="bibr">[28]</ref>, and the FLOPS count of EEEA-Net-C2 is reduced by 2 Million from MobileNetsV3. However, EEEA-Net-C2 had 0.6 Million more parameters than MobileNetsV3.  <ref type="bibr" target="#b68">[2019]</ref> 24.8 7.5 3.9 312 auto FBNet-C Wu et al. <ref type="bibr" target="#b68">[2019]</ref> 25.   We chose MobileNetV3 and GhostNet, including both small and large versions, to compare with our architecture, as shown in <ref type="figure" target="#fig_9">Fig. 8</ref>. Overall, we observed that EEEA-Net-C (? = 7) significantly outperforms MobileNetV3 and GhostNet for Top-1 accuracy and FLOPS. Furthermore, EEEA-Net-C (? = 7) achieves lower parameters than GhostNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Architecture Transfer</head><p>After searching and evaluating the model using the ImageNet dataset for image recognition, the models trained with the ImageNet dataset can be further developed and applied to object detection, semantic segmentation and human keypoint detection applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Object detection</head><p>EEEA-Net-C2 (? = 7) was used as the backbone for the object detection task to compare the effectiveness of our architecture on a real-world application. We utilised the same architecture trained with ImageNet datasets on the firmware called Single-Shot Detectors (SSD) <ref type="bibr" target="#b47">Liu et al. [2016]</ref> and You Only Look Once version four (YOLOv4) <ref type="bibr" target="#b48">Bochkovskiy et al. [2020]</ref>.</p><p>PASCAL VOC is a standard set of data used to measure an architecture's performance with object detection datasets. It consists of 20 classes, with bottles and plants being small objects with the lowest Average Precision (AP) of all classes. We used the SSDLite framework <ref type="bibr" target="#b41">Sandler et al. [2018]</ref> for fast and optimised processing on mobile devices. We also used the YOLOv4 framework <ref type="bibr" target="#b48">Bochkovskiy et al. [2020]</ref> for high precision object detection.</p><p>All models were trained on the PASCAL VOC 2007 and VOC 2012 <ref type="bibr" target="#b16">Everingham et al. [2010]</ref> train set by training the network for 200 epochs with a batch size of 32, SGD optimiser with weight decay equal to 0.0005 and momentum equal to 0.9, the initial learning rate is 0.01. It uses the scheduler with the cosine rule without a restart. All input images are resized to 320 ? 320 pixels, and these models were used to evaluate the PASCAL VOC test set.</p><p>For YOLOv4, we adopted MobileNet-V2 <ref type="bibr" target="#b41">Sandler et al. [2018]</ref>, MobileNet-V3 Howard et al. <ref type="bibr" target="#b68">[2019]</ref> and EEEA-Net-C2 as the backbone of YOLOv4. All models were trained with 140 epochs and a batch size of 4. All inputs are random scales with multi-scale images ranging from 320 to 640 pixels. The label smoothing is 0.1, SGD optimiser with weight decay equal to 0.0005 and momentum equal to 0.9. The initial learning rate was set to 0.01 for a scheduler with a cosine rule with the warm-up strategy performed twice. <ref type="table" target="#tab_9">Table 6</ref> shows the performance of our architecture for object detection. EEEA-Net-C2 achieved a higher AP than NAS-Net, DARTS, ShuffleNet-V2, MobileNet-V2, MobileNet-V3, and MnasNet for the SSDLite framework. Nonetheless, EEEA-Net-C2 has 152 more million FLOPS than MobileNet-V3. For fairness, we used MobileNet-V2, MobileNet-V3 and EEEA-Net-C2 for training and evaluated these models using the PASCAL VOC test dataset via the YOLOv4 framework. The EEEA-Net-C2 significantly outperformed both MobileNet-V2 and MobileNet-V3.   <ref type="bibr" target="#b68">[2019]</ref> 5.60 27.09 75.9 MnasNet Tan et al. <ref type="bibr" target="#b68">[2019]</ref> 6.12 29.50 76.8 EEEA-Net-C2 (ours) 7.34 28.65 76.8 <ref type="table">Table 7</ref>: Results of BiSeNet with different backbones on Cityscapes validation set. (single scale and no flipping).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Semantic Segmentation</head><p>The cityscape dataset <ref type="bibr" target="#b17">Cordts et al. [2016]</ref> was chosen to experiment with semantic segmentation. It is a large-scale dataset of street scenes in 50 cities. Cityscapes provide dense pixel annotations of 5,000 images. These images were divided into three groups of 2,975, 500, 1,525 images for training, validation, and testing. We used BiSeNet <ref type="bibr" target="#b49">Yu et al. [2018]</ref> with different backbones to evaluate our architecture's performance for semantic segmentation on the Cityscapes dataset. NASNet, DARTS, ShuffleNet-V2, MobileNet-V2, MobileNet-V3, MnasNet, and EEEA-Net-C2 have trained 80,000 iterations with a poly learning scheduler at an initial learning rate of 0.01 and batch size equals 16. All training images were resized to 1024 ? 1024 pixels using image augmentation using colour jitter, random scale, and random horizontal flip. <ref type="table">Table 7</ref> shows that ShuffleNet-V2 achieved a smaller number of parameters and lower FLOPS than other architectures. However, MobileNet-V2 achieved a greater Mean Intersection over Union (mIoU) than ShuffleNet-V2, MobileNet-V3, MnasNet, and EEEA-Net-C2. The mIoU of EEEA-Net-C2 is the same as MnasNet. It is better than ShuffleNet-V2 and MobileNet-V3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Keypoint Detection</head><p>Human keypoint detection, also known as human pose estimation, is the visual sensing of human gestures from keypoints such as the head, hips, or ankles. MS <ref type="bibr">COCO Lin et al. [2014]</ref> is a comprehensive dataset to measure keypoint detection performance, consisting of data for 250,000 persons, with the data labelle at 17 keypoints. SimpleBaseline <ref type="bibr" target="#b50">Xiao et al. [2018]</ref> is a framework for keypoint detection, enabling easier changes to backbones. Given this, it allowed us to adapt to other architectures more simply.</p><p>All architectures were trained on the MS COCO train2017 set by training the network for 140 epochs with a batch size of 128, Adam optimiser, the initial learning rate is 0.001, which is reduced to 0.0001 at 90th epoch and reduced to 0.00001 at 120th epoch. The training set is resized to 256 ? 192 pixels using random rotation, scale, and flipping data. <ref type="table">Table 8</ref> shows the experimental result of SimpleBaseline with different backbones. Our EEEA-Net-C2 performed better than other backbones in the number of parameters. As well, EEEA-Net-C2 outperforms small architectures (excluding NASNet and DARTS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Params (M) FLOPS (M) AP (%) NASNet <ref type="bibr" target="#b10">Zoph and Le [2016]</ref> 10.66 569.11 67.9 DARTS  9.20 531.77 66.9 ShuffleNet-V2 <ref type="bibr" target="#b49">Ma et al. [2018]</ref> 7.55 154.37 60.4 MobileNet-V2 <ref type="bibr" target="#b41">Sandler et al. [2018]</ref> 9.57 306.80 64.9 MobileNet-V3 Howard et al. <ref type="bibr" target="#b68">[2019]</ref> 9.01 223.16 65.3 MnasNet Tan et al. <ref type="bibr" target="#b68">[2019]</ref> 10.45 320.17 62.5 EEEA-Net-C2 (ours) 7.47 297.49 66.7 <ref type="table">Table 8</ref>: Results of SimpleBaseline with different backbone settings on MS COCO2017 validation set. Flip is used during validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Limitations</head><p>The development of a NAS search with only one GPU processor was a challenge for the reasons set out below. Setting the appropriate number of populations, the number of generations of each population, and the number of search epochs suitable for one GPU processor presents considerable difficulties. All of these parameters affect the model search time.</p><p>Increasing the number of generations increases the computing cost, but increasing the number of generations provides an opportunity for greater recombination of populations, thereby maximising the efficiency of discovering new populations. Moreover, an increased number of search epoch helps improve each population's error fitness value.</p><p>All these restrictions help to improve the NAS search. However, increasing these numbers influences search time. For example, increasing the number of search epochs from 1 epoch to 10 epochs results in a 10? increase in search time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We achieved our research goals by successfully developing a CNN architecture suitable for an on-device processor with limited computing resources and applying it in real-world applications.</p><p>This outcome was achieved by significantly reducing the computational cost of a neural architecture search. We introduced the Early Exit Population Initialisation (EE-PI) for Evolutionary Algorithm method to create the EEEA-Nets model. Our method achieved a massive reduction in search time on CIFAR-10 dataset; 0.34 to 0.52 GPU days. This must be seen as an outstanding outcome compared against other state-of-the-art models, such as the NSGA-Net model, which required 4 GPU days, the 2,000 GPU days of the NASNet model and the 3,150 GPU days of the AmoebaNet model.</p><p>In the EEEA-Nets architecture, our emphasis was on reducing the number of parameters, the error rate and the computing cost. We were able to achieve this by introducing an Early Exit step into the Evolutionary Algorithm.</p><p>Our EEEA-Nets architectures were searched on image recognition task, transferring architectures to other tasks. Experimentally, EEEA-Net-C2 is significantly better than MobileNet-V3 on image recognition, object detection, semantic segmentation, and keypoint detection tasks. Addressing this latter task had not been achieved or even attempted in any other CNN model. Therefore, our architectures can be deployed on devices with limited memory and processing capacity by achieving these significant reductions, allowing real-time processing on smartphones or on-device systems.</p><p>The task of optimising the search for multi-objective evolutionary algorithms shall be continued as our future work to find better-performing models. In addition, we will consider applying a multi-objective evolutionary algorithm with EE-PI to find mobile-suitable models in other applications such as marine detection or pest detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Architecture Visualisation</head><p>This section visualises the architectures obtained by searching for EEEA-Nets with CIFAR-10 datasets, as shown in <ref type="figure" target="#fig_11">Fig. 9</ref>, and EEEA-Nets with ImageNet datasets shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. These architectures are the most reliable, minimising three goals.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EEEA-Net-C2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MobileNetv3</head><p>MobileNetv2 <ref type="figure" target="#fig_0">Figure 11</ref>: An example of object detection results of MobileNetv2, MobileNetv3, and EEEA-Net-C2 models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Error Analysis of EEEA-Net-C2</head><p>Our experiment applied EEEA-Net-C2 for detection, semantic segmentation, and human keypoint detection, where we concluded that the EEEA-Net-C2 model was better than the MobileNet-V3 model. For error analysis of the EEEA-Net-C2 model, images for each application were processed to check the correct results. In this appendix, error analysis of the EEEA-Net-C2 model is divided into three parts: error analysis of object detection, error analysis of semantic segmentation errors and error analysis of human keypoint detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Object detection</head><p>Object detection results from MobileNetv2, MobileNetv3 and EEEA-Net-C2 models are shown in <ref type="figure" target="#fig_0">Fig. 11</ref>. Given the error from the images in the first column, the MobileNetv2 model was found to have mistakenly identified "bird" as "person", while the MobileNetv3 model was unable to find "bird". However, the EEEA-Net-C2 model detected the location of all "birds".</p><p>As with the second column in <ref type="figure" target="#fig_0">Fig. 11</ref>, the EEEA-Net-C2 model can identify all "persons" positions. However, only the EEEA-Net-C2 model could not locate the hidden "person" behind the middle woman in the third column image. Additionally, in the fourth column pictures, the EEEA-Net-C2 model was able to identify more "plant pots" than the MobileNetv2 and MobileNetv3 models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MobileNetv3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EEEA-Net-C2</head><p>Input <ref type="figure" target="#fig_0">Figure 12</ref>: An example of semantic segmentation results of MobileNetv2, MobileNetv3 and EEEA-Net-C2 models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MobileNetv3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EEEA-Net-C2</head><p>MobileNetv2 <ref type="figure" target="#fig_0">Figure 13</ref>: An example of human keypoint detection results of MobileNetv2, MobileNetv3 and EEEA-Net-C2 models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Semantic segmentation</head><p>The results of visual image segmentation using MobileNetv3 and EEEA-Net-C2 models are shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. The error of the semantic segmentation results can be determined from the pictures of the first column. It was found that MobileNetv3 models could segment only "Traffic sign pole". However, the MobileNetv3 model cannot segment the left "traffic sign", while the EEEA-Net-C2 model can segment both "pole and sign".</p><p>The pictures in the second column from <ref type="figure" target="#fig_0">Fig. 12</ref> depicts that the EEEA-Net-C2 model segmented from the "traffic island" was less than the MobileNetv3 model. Next, in the third column, the EEEA-Net-C2 model segmented the "footpath" more precisely than the MobileNetv3 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Human keypoint detection</head><p>The human keypoint detection results from the MobileNetv2, MobileNetv3 and EEEA-Net-C2 models are shown in <ref type="figure" target="#fig_0">Fig. 13</ref>. When considering the error from the pictures in the first column, the MobileNetv3 model was found to indicate the "left arm" position, while the MobileNetv2 and EEEA-Net-C2 models were able to locate. This section has implemented an Early Exit method to model search from the NAS-Bench datasets, which avoids unfair comparisons and provides a uniform benchmark for NAS algorithms. The dataset used in this experiment was NAS-Bench-101, NAS-Bench-1Shot1 and NAS-Bench-201.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">NAS-Bench-101</head><p>The NAS-Bench-101 Ying et al. <ref type="bibr" target="#b68">[2019]</ref> provides a table dataset of 423,624 unique architectures. These architectures have trained and evaluated the CIFAR-10 dataset to allow our work to search and query the mapped performance in the dataset in a few milliseconds.</p><p>We have re-implemented model search from the NAS-Bench-101 dataset by using random search, regularised evolution and Early Exit evolution algorithms to search and query the performance of the resulting models. We used a reimplemented regularised evolution with the Early Exit method by taking population size of 100, a tournament size of 10, and maximum parameters' Early Exit (?) is 25 million.</p><p>The results in <ref type="figure" target="#fig_0">Fig. 15</ref> show that our early exit evolution algorithm tends to be higher in accuracy than the regularised evolution from 2 million seconds to 5 million seconds. Overall, the regularised evolution algorithm appears to perform better than the random search algorithm. However, our early exit evolution tends to outperform both random search and regularised evolution algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">NAS-Bench-1Shot1</head><p>NAS-Bench-1Shot1 Zela et al. <ref type="bibr">[2020]</ref> is the benchmark for a one-shot neural architecture search, developed from the NAS-Bench-101 search space by tracking the trajectory and performance of the obtained architectures for three search spaces: 6,240 architectures for search space 1, 29,160 architectures for search space 2, and 363,648 architectures for search space 3. <ref type="figure" target="#fig_0">Fig. 16</ref> shows the mean performance on validation regret of architectures obtained by random search, regularised evolution and Early Exit evolution algorithms. For search space 1, our algorithm achieves validation regret close to the regularised evolution algorithm. For search space 2, our algorithm converges better than the regularised evolution algorithm. Our algorithm outperforms random search and regularised evolution algorithms for search space 3, the most significant (100? more architectures than space 1, and 10? more than space 2).   <ref type="table">Table 9</ref>: Comparison of a single objective and multi-objective evolution algorithm with the 8 NAS methods provided by NAS-Bench-201 benchmark. Optimal shows the best architecture in the search space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>NASNet Zoph and Le [2016]  network architecture (left) and NSGA-Net Lu et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The chromosome structure of the Evolutionary Algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Crossover operation (top): the parent has two different network architectures; the chromosome of each parent architecture can be visualised as a digits string. Child architecture was mixed with a chromosome index from their parent's chromosome. Mutation operation (bottom): The location of a normal chromosome was randomly selected. Then this pair of genes will be replaced by a new random pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 (Figure 5 :</head><label>45</label><figDesc>bottom) shows the mutation operation used during implementation; the location of a chromosome of architecture was determined by randomly selecting only one pair of gene locations (72, orange). Then it was replaced with a random pair of gene value (33, magenta) of the newly mutated gene. An Early Exit Evolutionary Algorithm.Algorithm 1 Multi-objective evolutionary algorithm with an Early Exit Population Initialisation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Performance Metric of EA-Net and EEEA-Nets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>CIFAR-10 Accuracy vs Generations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>CIFAR-10 Accuracy vs FLOPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Progress of trade-offs after each generation of EA-Net and EEEA-Nets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of Top-1 accuracy, FLOPS (left), and parameters (right) between EEEA-Nets and MobileNetV3 [28] and GhostNet [40] on ImageNet dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>EA-Net (? = 0) Normal Cell. EA-Net (? = 0) Reduction Cell. EEEA-Net-C (?=5.0) Normal Cell. EEEA-Net-C (?=5.0) Reduction Cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Normal and Reduction cells learned on CIFAR-10: EA-Net (?=0.0), EEEA-Net-A (?=3.0), EEEA-Net-B (?=4.0), and EEEA-Net-C (?=5.0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>Comparison of accuracy between random search, regularised evolution and Early Exit evolution algorithms on NAS-Bench-101.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 :</head><label>16</label><figDesc>Comparison of accuracy between random search (RS), regularised evolution (RE) and Early Exit evolution (EE) algorithms on NAS-Bench-1Shot1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :Residuals Network Input narrow Add Conv 1x1, f=N BN narrow approach wide Depthwise 3x3 BN ReLU Conv 1x1, f=Nx16 BN ReLUResidual Network Input wide Add Conv 1x1, f=Nx32 BN ReLU wide approach narrow Conv 3x3, f=Nx16 BN ReLU Conv 1x1, f=Nx16 BN ReLU</head><label>1</label><figDesc>Comparison of different NAS search method with multi-objectives.</figDesc><table><row><cell>Xi</cell><cell>channels = N</cell><cell>Xi</cell><cell>Inverted channels = N</cell></row><row><cell></cell><cell>channels = Nx16</cell><cell></cell><cell>channels = Nx16</cell></row><row><cell></cell><cell>channels = Nx16</cell><cell></cell><cell>channels = Nx16</cell></row><row><cell></cell><cell>channels = Nx32</cell><cell></cell><cell>channels = N</cell></row><row><cell cols="2">Xi+1</cell><cell cols="2">Xi+1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Comparing EEEA-Net with other architectures from manual, combined, and auto search method on ImageNet datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Result of Object detection with different backbones on PASCAL VOC 2007 test set.</figDesc><table><row><cell>Model</cell><cell cols="3">Params (M) FLOPS (G) mIoU (%)</cell></row><row><cell>NASNet Zoph and Le [2016]</cell><cell>7.46</cell><cell>36.51</cell><cell>77.9</cell></row><row><cell>DARTS Liu et al. [2018b]</cell><cell>6.64</cell><cell>34.77</cell><cell>77.5</cell></row><row><cell>ShuffleNet-V2 Ma et al. [2018]</cell><cell>4.10</cell><cell>26.30</cell><cell>73.0</cell></row><row><cell>MobileNet-V2 Sandler et al. [2018]</cell><cell>5.24</cell><cell>29.21</cell><cell>77.1</cell></row><row><cell>MobileNet-V3 Howard et al.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Figure 10: EA-Nets and EEEA-Nets architectures were searched from ImageNet datasets. The stem and tail layers in all architectures are the same.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Legend</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>K=3 E=3</cell><cell>K=3 E=4</cell><cell>K=3 E=6</cell><cell>K=5 E=3</cell><cell>K=5 E=4</cell><cell>K=5 E=6</cell><cell>K=7 E=3</cell><cell>K=7 E=4</cell><cell>K=7 E=6</cell><cell>Skip</cell></row><row><cell></cell><cell>Stage 1</cell><cell>Stage 2</cell><cell>Stage 3</cell><cell>Stage 4</cell><cell>Stage 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stage 1</cell><cell>Stage 2</cell><cell>Stage 3</cell><cell>Stage 4</cell><cell>Stage 5</cell></row><row><cell>EA-Net-N1</cell><cell>Stem</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tail</cell><cell>EA-Net-N2</cell><cell></cell><cell>Stem</cell><cell>Tail</cell></row><row><cell>EEEA-Net-A1 ( = 5)</cell><cell>Stem</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tail</cell><cell cols="2">EEEA-Net-A2 ( = 5)</cell><cell>Stem</cell><cell>Tail</cell></row><row><cell>EEEA-Net-B1 ( = 6)</cell><cell>Stem</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tail</cell><cell cols="2">EEEA-Net-B2 ( = 6)</cell><cell>Stem</cell><cell>Tail</cell></row><row><cell>EEEA-Net-C1 ( = 7)</cell><cell>Stem</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tail</cell><cell cols="2">EEEA-Net-C2 ( = 7)</cell><cell>Stem</cell><cell>Tail</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Talwalkar [2019] 84.16?1.69 87.66?1.69 45.78?6.33 46.60?6.57 31.09?5.65 30.78?6.12 Reinforce Williams [1992] 91.09?0.37 93.85?0.37 70.05?1.67 70.17?1.61 43.04?2.18 43.16?2.28 ENAS Pham et al. [2018] 39.77?0.00 54.30?0.00 10.23?0.12 10.62?0.27 16.43?0.00 16.32?0.00 DARTS Liu et al. [2018b] 39.77?0.00 54.30?0.00 38.57?0.00 38.97?0.00 18.87?0.00 18.41?0.00 GDAS Dong and Yang [2019] 90.01?0.46 93.23?0.23 24.05?8.12 24.20?8.08 40.66?0.00 41.02?0.00 SNAS Xie et al. [2018] 90.10?1.04 92.77?0.83 69.69?2.39 69.34?1.98 42.84?1.79 43.16?2.64 DSNAS Hu et al. [2020] 89.66?0.29 93.08?0.13 30.87?16.40 31.01?16.38 40.61?0.09 41.07?0.09 PC-DARTS Xu et al. [2020] 89.96?0.15 93.41?0.30 67.12?0.39 67.48?0.89 40.83?0.08 41.31?0.22 EA-Net (SO) 91.53?0.00 94.22?0.00 73.13?0.00 73.17?0.00 46.32?0.00 46.48?0.00 EA-Net (?= 0) 88.97?2.48 91.54?2.69 66.84?5.08 67.00?4.90 39.93?5.54 39.27?6.21 EEEA-Net (?= 0.3) 87.07?1.59 89.76?1.87 64.04?3.21 64.31?3.21 35.42?3.81 34.98?4.13 EEEA-Net (?= 0.4) 89.91?0.77 92.68?0.69 68.70?1.50 68.65?1.51 41.71?1.58 41.25?1.61 EEEA-Net (?=0.5) 90.21?0.58 92.83?0.46 69.15?1.36 68.95?1.25 42.14?1.14 41.98?1.22</figDesc><table><row><cell>Method</cell><cell cols="2">CIFAR-10 validation test</cell><cell cols="2">CIFAR-100 validation test</cell><cell cols="2">ImageNet16-120 validation test</cell></row><row><cell>ResNet He et al. [2016]</cell><cell>90.83</cell><cell>93.97</cell><cell>70.42</cell><cell>70.86</cell><cell>44.53</cell><cell>43.63</cell></row><row><cell>RSPS Li and Optimal</cell><cell>91.61</cell><cell>94.37</cell><cell>73.49</cell><cell>73.51</cell><cell>46.77</cell><cell>47.31</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to acknowledge the Thailand Research Fund's financial support through the Royal Golden Jubilee PhD. Program (Grant No. PHD/0101/2559). The study was undertaken using the National Computational Infrastructure (NCI) in Australia under the National Computational Merit Allocation Scheme (NCMAS). Further, we would like to extend our appreciation to Mr Roy I. Morien of the Naresuan University Graduate School for his assistance in editing the English grammar, syntax, and expression in the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of The ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;0.5mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On-device facial verification using nuf-net model of deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chakkrit</forename><surname>Termritthikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeshi</forename><surname>Jamtsho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paisarn</forename><surname>Muneesawang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="579" to="589" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An improved residual network model for image recognition using a combination of snapshot ensembles and the cutout technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chakkrit</forename><surname>Termritthikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeshi</forename><surname>Jamtsho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paisarn</forename><surname>Muneesawang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1475" to="1495" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single-path nas: Designing hardware-efficient convnets in less than 4 hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhou</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodhi</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="481" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nsga-net: neural architecture search using multi-objective genetic algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashesh</forename><surname>Dhebar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference on</title>
		<meeting>the Genetic and Evolutionary Computation Conference on</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="419" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cars: Continuous evolution for efficient neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1829" to="1838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient multi-objective neural architecture search via lamarckian evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12965" to="12974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Cyclic differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10724</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nsganetv2: Evolutionary multi-objective surrogate-assisted neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu Naresh</forename><surname>Boddeti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pc-darts: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2020 : Eighth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Genetic cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Evolutionary convolutional neural networks: An application to handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Baldominos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yago</forename><surname>Saez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Isasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">283</biblScope>
			<biblScope unit="page" from="38" to="52" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;17 Proceedings of the 34th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A fast and elitist multiobjective genetic algorithm: Nsga-ii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meyarivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="197" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Enhancing controller&apos;s tuning reliability with multi-objective optimisation: From model in the loop to hardware in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes?s</forename><surname>Velasco Carrau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilberto</forename><surname>Reynoso-Meza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Garc?a-Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Blasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="52" to="66" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic multi-objective optimisation using deep reinforcement learning: benchmark, algorithm and an application to identify vulnerable zones based on water quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khin</forename><surname>Lwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Imani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antesar</forename><forename type="middle">M</forename><surname>Shabut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz</forename><forename type="middle">F</forename><surname>Bittencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed Alamgir</forename><surname>Hossain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="107" to="135" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ghostnet: More features from cheap operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1580" to="1589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10734" to="10742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Moga: Searching beyond mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4042" to="4046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-objective evolutionary design of deep convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashesh</forename><surname>Dhebar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu Naresh</forename><surname>Boddeti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2020 : Eighth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th European Conference on Computer Vision, ECCV 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun ; Changqian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
	<note>Proceedings of the European Conference on Computer Vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="472" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Arber Zela, Julien Siems, and Frank Hutter. Nas-bench-1shot1: Benchmarking and dissecting one-shot neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2020 : Eighth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7105" to="7114" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Snas: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dsnas: Direct neural architecture search without parameter retraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoukang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comparison of latency between EEEA-Net and other state-of-the-art models on non-GPU processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="12084" to="12092" />
		</imprint>
	</monogr>
	<note>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Only the MobileNetv3 model can pinpoint the &quot;leg&quot; of the person sitting in the second column pictures. However, in the third column pictures, the EEEA-Net-C2 model can locate the middle person&apos;s &quot;arms and legs</title>
		<imprint/>
	</monogr>
	<note>while the MobileNetv3 model identifies the person&apos;s wrong location. Additionally, in the fourth column pictures, the EEEA-Net-C2 model could locate the &quot;arm and leg&quot; more accurately than the MobileNetv2 and MobileNetv3 models</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The EEEA-Net-C2 model was designed and searched with the ImageNet dataset. Thus, the EEEA-Net-C2 model may have errors when used with other dataset or tasks. However, the EEEA-Net-C2 model has a performance higher than the MobileNetv2 and MobileNetv3 models on the same dataset and framework</title>
	</analytic>
	<monogr>
		<title level="m">The above data shows that the EEEA-Net-C2 model is accurate as well as inaccurate</title>
		<imprint/>
	</monogr>
	<note>used in the three applications</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Mobile Processing This appendix measures the performance of our EEEA-Net-C2 model and other state-of-the-art models on the smartphone and only CPU. All trained models with the ImageNet dataset are converted to the PyTorch JIT version to enable easy implementation on different platforms</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Fig. 14 shows the latency performance with 100 images with 224x224 pixels on the Google Pixel 3 XL smartphone (blue bars) and Intel i7-6700HQ CPU (red bars) devices with non-GPU resources by DARTSv2</title>
		<meeting><address><addrLine>NASNet, ReletiveNAS, ShuffleNetV2</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>MNASNet 1.0, MobileNetV2, MobileNetV3, and EEEA-Net-C2 models</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">On the Google Pixel 3 XL, the EEEA-Net-C2 model processed each image in 86 milliseconds per image, whereas the MobileNetV2 and MobileNetV3 models took 90 and 88 milliseconds, respectively. The EEEA-Net-C2 model has a shorter latency time than state-of-the-art models (including DARTSv2, P-DARTS, NASNet, and ReletiveNAS), and MobileNets models</title>
		<imprint/>
	</monogr>
	<note>primarily models for smartphones, are compared to EEEA-Net-C2</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<title level="m">the Intel i7-6700HQ CPU, the latency time of the EEEA-Net-C2 model has shorter latency than state-of-the-art models and lightweight models</title>
		<imprint/>
	</monogr>
	<note>including MNASNet 1.0, MobileNetV2, and MobileNetV3)</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The results obtained from different methods are achieved with different settings such as hyperparameters (e.g., learning rate and batch size), data augmentation</title>
	</analytic>
	<monogr>
		<title level="m">Experimental results with CIFAR-10, CIFAR-100, and ImageNet datasets were compared between NAS methods</title>
		<imprint/>
	</monogr>
	<note>Cutout and AutoAugment). Thus, the comparison may not be fair</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">extension of NAS-Bench-101, which extends different search spaces, and it has a wide range of datasets, including CIFAR-10, CIFAR-100, and ImageNet-16-120. It contains 15,625 architectures by five operations, and 6-dimensional vectors indicate the operation in the cell. All architecture evaluated performance by validation and test sets on CIFAR-10</title>
		<idno>CIFAR-100</idno>
		<imprint/>
	</monogr>
	<note>and ImageNet-16-120</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">) with the single objective evolution algorithms (SO) and multi-objective evolution algorithms (? = 0). The hyper-parameters for this search process were defined as the generations of EA equal to 10 generations with 100 populations by retaining a probability of 0.5, a mutation probability of 0.1, and Early Exit is the maximum number of parameters equal</title>
		<idno>or EEEA-Net (? = 0.3, 0.4 and 0.5</idno>
	</analytic>
	<monogr>
		<title level="m">We compare our Early Exit Evolution Algorithm</title>
		<imprint/>
	</monogr>
	<note>to 0.3, 0.4 and 0.5 million. The results are shown in Table 9, our EEEA-Net (? = 0.4 and 0.5) outperforms EEEA-Net (? = 0 and 0.3). However, the EA-Net (SO) using accuracy as the optimisation objective performed better than all EEEA-Nets</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Furthermore, when we compared our EEEA-Net (? = 0.5) with 8 NAS methods, including RSPS Li and Talwalkar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Reinforce Williams [1992</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">we found that EEEA-Net (? = 0.5) has an accuracy was higher than all other NAS method except for the Reinforce method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
