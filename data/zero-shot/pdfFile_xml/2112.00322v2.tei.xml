<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danila</forename><surname>Rukhovich</surname></persName>
							<email>d.rukhovich@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Vorontsova</surname></persName>
							<email>a.vorontsova@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
							<email>a.konushin@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D object detection</term>
					<term>anchor-free object detection</term>
					<term>sparse convolutional networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, promising applications in robotics and augmented reality have attracted considerable attention to 3D object detection from point clouds. In this paper, we present FCAF3D -a first-in-class fully convolutional anchor-free indoor 3D object detection method. It is a simple yet effective method that uses a voxel representation of a point cloud and processes voxels with sparse convolutions. FCAF3D can handle large-scale scenes with minimal runtime through a single fully convolutional feed-forward pass. Existing 3D object detection methods make prior assumptions on the geometry of objects, and we argue that it limits their generalization ability. To eliminate prior assumptions, we propose a novel parametrization of oriented bounding boxes that allows obtaining better results in a purely data-driven way. The proposed method achieves state-of-the-art 3D object detection results in terms of mAP@0.5 on ScanNet V2 (+4.5), SUN RGB-D (+3.5), and S3DIS (+20.5) datasets. The code and models are available at https://github.com/samsunglabs/fcaf3d.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D object detection from point clouds aims at simultaneous localization and recognition of 3D objects given a 3D point set. As a core technique for 3D scene understanding, it is widely applied in autonomous driving, robotics, and AR.</p><p>While 2D methods ( <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b32">[33]</ref>) work with dense fixed-size arrays, 3D methods are challenged by irregular unstructured 3D data of arbitrary volume. Consequently, the 2D data processing techniques are not directly applicable for 3D object detection, so 3D object detection methods ( <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b18">[19]</ref>) employ inventive approaches to 3D data processing.</p><p>Convolutional 3D object detection methods have scalability issues: large-scale scenes either require an impractical amount of computational resources or take too much time to process. Other methods opt for voxel data representation and employ sparse convolutions; however, these methods solve scalability problems at the cost of detection accuracy. In other words, there is no 3D object detection method that provides precise estimates and scales well. Besides being scalable and accurate, an ideal 3D object detection method should handle objects of arbitrary shapes and sizes without additional hacks and hand-tuned hyperparameters. We argue that prior assumptions on 3D object bounding boxes (e.g. aspect ratios or absolute sizes) restrict generalization and increase the number of hyperparameters and trainable parameters.</p><p>On the contrary, we do not want to rely on prior assumptions. We propose an anchor-free method that does not impose priors on objects and addresses 3D object detection with a purely data-driven approach. Moreover, we introduce a novel oriented bounding box (OBB) parametrization inspired by a Mobius strip that reduces the number of hyperparameters. To prove the effectiveness of our parametrization, we conduct experiments on SUN RGB-D with several 3D object detection methods and report improved results for all these methods.</p><p>In this paper, we present FCAF3D -a simple, effective, and scalable method for detecting 3D objects from point clouds. We evaluate the proposed method on ScanNet <ref type="bibr" target="#b6">[7]</ref>, SUN RGB-D <ref type="bibr" target="#b25">[26]</ref>, and S3DIS <ref type="bibr" target="#b0">[1]</ref>, demonstrating the solid superiority over the previous state-of-the-art on all benchmarks. On SUN RGB-D and ScanNet, our method surpasses other methods by at least 3.5% mAP@0.5. On S3DIS, FCAF3D outperforms the competitors by a huge margin.</p><p>Overall, our contribution is three-fold:</p><p>1. To our knowledge, we propose a first-in-class fully convolutional anchor-free 3D object detection method (FCAF3D) for indoor scenes. 2. We present a novel OBB parametrization and prove it to boost the accuracy of several existing 3D object detection methods on SUN RGB-D.</p><p>3. Our method significantly outperforms the previous state-of-the-art on challenging large-scale indoor ScanNet, SUN RGB-D, and S3DIS datasets in terms of mAP while being faster on inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent 3D object detection methods are designed to be either indoor or outdoor. Indoor and outdoor methods have been developing almost independently, applying domain-specific data processing techniques. Many modern outdoor methods <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b35">[36]</ref> project 3D points onto a bird-eye-view plane, thus reducing the task of 3D object detection to 2D object detection. Naturally, these methods take advantage of the fast-evolving algorithms for 2D object detection. Given a birdeye-view projection, <ref type="bibr" target="#b13">[14]</ref> processes it in a fully convolutional manner, while <ref type="bibr" target="#b31">[32]</ref> exploits 2D anchor-free approach. Unfortunately, the approaches that proved to be effective for both 2D object detection and 3D outdoor object detection cannot be trivially adapted to indoor, as it would require an impracticable amount of memory and computing resources. To address performance issues, different 3D data processing strategies have been proposed. Currently, three approaches dominate the field of 3D object detection -voting-based, transformer-based, and 3D convolutional. Below we discuss each of these approaches in detail; we also provide a brief overview of anchor-free methods.</p><p>Voting-based methods. VoteNet <ref type="bibr" target="#b21">[22]</ref> was the first method that introduced points voting for 3D object detection. VoteNet processes 3D points with Point-Net <ref type="bibr" target="#b22">[23]</ref>, assigns a group of points to each object candidate according to their voted center, and computes object features from each point group. Among the numerous successors of VoteNet, the major progress is associated with advanced grouping and voting strategies applied to the PointNet features. BRNet <ref type="bibr" target="#b3">[4]</ref> refines voting results with the representative points from the vote centers, which improves capturing the fine local structural features. MLCVNet <ref type="bibr" target="#b29">[30]</ref> introduces three context modules into the voting and classifying stages of VoteNet to encode contextual information at different levels. H3DNet <ref type="bibr" target="#b33">[34]</ref> improves the point group generation procedure by predicting a hybrid set of geometric primitives. VENet <ref type="bibr" target="#b28">[29]</ref> incorporates an attention mechanism and introduces a vote weighting module trained via a novel vote attraction loss.</p><p>All VoteNet-like voting-based methods are limited by design. First, they show poor scalability: as their performance depends on the amount of input data, they tend to slow down if given larger scenes. Moreover, many voting-based methods implement voting and grouping strategies as custom layers, making it difficult to reproduce or debug these methods or port them to mobile devices.</p><p>Transformer-based methods. The recently emerged transformer-based methods use end-to-end learning and forward pass on inference instead of heuristics and optimization, which makes them less domain-specific. GroupFree <ref type="bibr" target="#b15">[16]</ref> replaces VoteNet head with a transformer module, updating object query locations iteratively and ensembling intermediate detection results. 3DETR <ref type="bibr" target="#b18">[19]</ref> was the first method of 3D object detection implemented as an end-to-end trainable transformer. However, more advanced transformer-based methods still experience scalability issues similar to early voting-based methods. Differently, our method is fully-convolutional, thus being faster and significantly easier to implement than both voting-based and transformer-based methods.</p><p>3D convolutional methods. Voxel representation allows handling cubically growing sparse 3D data efficiently. Voxel-based 3D object detection methods ( <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b24">[25]</ref>) convert points into voxels and process them with 3D convolutional networks. However, dense volumetric features still consume much memory, and 3D convolutions are computationally expensive. Overall, processing large scenes requires a lot of resources and cannot be done within a single pass.</p><p>GSDN <ref type="bibr" target="#b9">[10]</ref> tackles performance issues with sparse 3D convolutions. It has encoder-decoder architecture, with both encoder and decoder parts built from sparse 3D convolutional blocks. Compared to the standard convolutional votingbased and transformer-based approaches, GSDN is significantly more memoryefficient and scales to large scenes without sacrificing point density. The major weakness of GSDN is its accuracy: this method is comparable to VoteNet in terms of quality, being significantly inferior to the current state-of-the-art <ref type="bibr" target="#b15">[16]</ref>.  GSDN uses 15 aspect ratios for 3D object bounding boxes as anchors. If GSDN is trained in an anchor-free setting with a single aspect ratio, the accuracy decreases by 12%. Unlike GSDN, our method is anchor-free while taking advantage of sparse 3D convolutions.</p><p>RGB-based anchor-free object detection. In 2D object detection, anchorfree methods are solid competitors for the standard anchor-based methods. FCOS <ref type="bibr" target="#b26">[27]</ref> addresses 2D object detection in a per-pixel prediction manner and shows a robust improvement over its anchor-based predecessor RetinaNet <ref type="bibr" target="#b14">[15]</ref>. FCOS3D <ref type="bibr" target="#b27">[28]</ref> trivially adapts FCOS by adding extra targets for monocular 3D object detection. ImVoxelNet <ref type="bibr" target="#b23">[24]</ref> solves the same problem with an FCOS-like head built from standard (non-sparse) 3D convolutional blocks. We adapt the ideas from mentioned anchor-free methods to process sparse irregular data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Following the standard 3D detection problem statement, FCAF3D accepts N pts RGB-colored points and outputs a set of 3D object bounding boxes. The FCAF3D architecture consists of a backbone, a neck, and a head (depicted in <ref type="figure" target="#fig_2">Fig. 2</ref>).</p><p>While designing FCAF3D, we aim for scalability, so we opt for a GSDN-like sparse convolutional network. For better generalization, we reduce the number of hyperparameters in this network that need to be manually tuned; specifically, we simplify sparsity pruning in the neck. Furthermore, we introduce an anchorfree head with a simple multi-level location assignment. Finally, we discuss the limitations of existing 3D bounding box parametrizations and propose a novel parametrization that improves both accuracy and generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sparse Neural Network</head><p>Backbone. The backbone in FCAF3D is a sparse modification of ResNet <ref type="bibr" target="#b10">[11]</ref> where all 2D convolutions are replaced with sparse 3D convolutions. The family of sparse high-dimensional versions of ResNet was first introduced in <ref type="bibr" target="#b4">[5]</ref>; for brevity, we refer to them as to HDResNet.</p><p>Neck. Our neck is a simplified GSDN decoder. Features on each level are processed with one sparse transposed 3D convolution and one sparse 3D convolution. Each transposed sparse 3D convolution with a kernel size of 2 might increase the number of non-zero values by 2 3 times. To prevent rapid memory growth, GSDN uses the pruning layer that filters input with a probability mask.</p><p>In GSDN, feature level-wise probabilities are calculated with an additional convolutional scoring layer. This layer is trained with a special loss encouraging consistency between the predicted sparsity and anchors. Specifically, voxel sparsity is set to be positive if any of the subsequent anchors associated with the current voxel is positive. However, using this loss may be suboptimal, as distant voxels of an object might get assigned with a low probability.</p><p>For simplicity, we remove the scoring layer with the corresponding loss and use probabilities from the classification layer in the head instead. We do not tune the probability threshold but keep at most N vox voxels to control the sparsity level, where N vox equals the number of input points N pts . We claim this to be a simple yet elegant way to prevent sparsity growth since reusing the same hyperparameter makes the process more transparent and consistent.</p><p>Head. The anchor-free FCAF3D head consists of three parallel sparse convolutional layers with weights shared across feature levels. For each location (x,?,?), these layers output classification probabilitiesp, bounding box regression parameters ?, and centerness?, respectively. This design is similar to the simple and light-weight head of FCOS <ref type="bibr" target="#b26">[27]</ref> but adapted to 3D data.</p><p>Multi-level location assignment. During training, FCAF3D outputs locations {(x,?,?)} for different feature levels, which should be assigned to ground truth boxes {b}. For each location, FCOS <ref type="bibr" target="#b26">[27]</ref> and ImVoxelNet <ref type="bibr" target="#b23">[24]</ref> consider ground truth bounding boxes covering this location, whose faces are all within distance threshold, select the bounding box with the least volume, and assign it to this location. Such a strategy is suboptimal, and its alterations are widely explored in 2D object detection <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b8">[9]</ref>. ImVoxelNet <ref type="bibr" target="#b23">[24]</ref> uses a modified strategy that requires hand-tuning the face distance threshold for each feature level.</p><p>We propose a simplified strategy for sparse data that does not require tuning dataset-specific hyperparameters. For each bounding box, we select the last feature level at which this bounding box covers at least N loc locations. If there is no such a feature level, we opt for the first one. We also filter locations via center sampling <ref type="bibr" target="#b26">[27]</ref>, considering only the points near the bounding box center as positive matches. More details are presented in Sec. 5.3.</p><p>Through assignment, some locations {(x,?,?)} are matched with ground truth bounding boxes bx ,?,? . Accordingly, these locations get associated with ground truth labels px ,?,? and 3D centerness values cx ,?,? . During inference, the scoresp are multiplied by 3D centerness? just before NMS as proposed in <ref type="bibr" target="#b23">[24]</ref>.</p><p>Loss function. The overall loss function is formulated as follows:</p><formula xml:id="formula_0">L = 1 N pos x,?,? (L cls (p, p) + 1 {px ,?,? ? =0} L reg (b, b) + 1 {px ,?,? ? =0} L cntr (?, c)). (1)</formula><p>Here, the number of matched locations N pos is x,?,? 1 {px ,?,? ? =0} . Classification loss L cls is a focal loss, regression loss L reg is IoU, and centerness loss L cntr is binary cross-entropy. For each loss, predicted values are denoted with a hat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bounding Box Parametrization</head><p>The 3D object bounding boxes can be axis-aligned (AABB) or oriented (OBB). An AABB can be described as b AABB = (x, y, z, w, l, h), while the definition of an OBB includes a heading angle ?: b OBB = (x, y, z, w, l, h, ?). In both formulas, x, y, z denote the coordinates of the center of a bounding box, while w, l, h are its width, length, and height, respectively. AABB parametrization. For AABBs, we follow the parametrization proposed in <ref type="bibr" target="#b23">[24]</ref>. Specifically, for a ground truth AABB (x, y, z, w, l, h) and a location (x,?,?), ? can be formulated as a 6-tuple:</p><formula xml:id="formula_1">? 1 = x + w 2 ?x, ? 2 =x ? x + w 2 , ? 3 = y + l 2 ??, ? 4 =? ? y + l 2 , ? 5 = z + h 2 ??, ? 6 =? ? z + h 2 .<label>(2)</label></formula><p>The predicted AABBb can be trivially obtained from ?. Heading angle estimation. All state-of-the-art 3D object detection methods from point clouds address the heading angle estimation task as classification followed by regression. The heading angle is classified into bins; then, the precise heading angle is regressed within a bin. For indoor scenes, the range from 0 to 2? is typically divided into 12 equal bins <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b18">[19]</ref>. For outdoor scenes, there are usually only two bins <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b12">[13]</ref>, as the objects on the road can be either parallel or perpendicular to the road.</p><p>When a heading angle bin is chosen, the heading angle value is estimated through regression. VoteNet and other voting-based methods estimate the value of ? directly. Outdoor methods explore more elaborate approaches, e.g. predicting the values of trigonometric functions. For instance, SMOKE <ref type="bibr" target="#b16">[17]</ref> estimates sin ? and cos ? and uses the predicted values to recover the heading angle.  <ref type="figure" target="#fig_3">Fig. 3</ref> depicts indoor objects where the heading angle is unambiguous. Accordingly, ground truth angle annotations can be chosen randomly for these objects, making heading angle bin classification meaningless. To avoid penalizing the correct predictions that do not coincide with annotations, we use rotated IoU loss, as its value is the same for all possible choices of heading angle. Thus, we propose OBB parametrization that considers the rotation ambiguity.</p><p>Proposed Mobius OBB parametrization. Considering the OBB with parameters (x, y, z, w, l, h, ?), let us denote q = w l . If x, y, z, w + l, h are fixed, it turns out that the OBBs with</p><formula xml:id="formula_2">(q, ?) , 1 q , ? + ? 2 , (q, ? + ?) , 1 q , ? + 3? 2<label>(3)</label></formula><p>define the same bounding box. We notice that the set of (q, ?), where ? ? (0, 2?], q ? (0, + inf) is topologically equivalent to a Mobius strip <ref type="bibr" target="#b19">[20]</ref> up to this equivalence relation. Hence, we can reformulate the task of estimating (q, ?) as a task of predicting a point on a Mobius strip. A natural way to embed a Mobius strip being a two-dimensional manifold to Euclidean space is the following:</p><p>(q, ?) ? (ln(q) sin(2?), ln(q) cos(2?), sin(4?), cos(4?)).</p><p>It is easy to verify that 4 points from Eq. 3 are mapped into a single point in Euclidean space (see Supplementary for details). However, the experiments reveal that predicting only ln(q) sin(2?) and ln(q) cos(2?) provides better results than predicting all four values. Thereby, we opt for a pseudo embedding of a Mobius strip to R 2 . We call it pseudo since it maps the entire center circle of a Mobius strip defined by ln(q) = 0 to (0, 0). Accordingly, we cannot distinguish points with ln q = 0. However, ln(q) = 0 implies strict equality of w and l, which is rare in real-world scenarios. Moreover, the choice of an angle has a minor effect on the IoU if w = l; thereby, we ignore this rare case for the sake of detection accuracy and simplicity of the method. Overall, we obtain a novel OBB parametrization:</p><formula xml:id="formula_4">? 7 = ln w l sin(2?), ? 8 = ln w l cos(2?).<label>(5)</label></formula><p>In the standard parametrization 2,b is trivially derived from ?. In the proposed parametrization, w, l, ? are non-trivial and can be obtained as follows:</p><formula xml:id="formula_5">w = sq 1 + q , l = s 1 + q , ? = 1 2 arctan ? 7 ? 8 ,<label>(6)</label></formula><p>where ratio q = e ? ? 2 7 +? 2 8 and size s = ? 1 + ? 2 + ? 3 + ? 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our method on three 3D object detection benchmarks: ScanNet V2 <ref type="bibr" target="#b6">[7]</ref>, SUN RGB-D <ref type="bibr" target="#b25">[26]</ref>, and S3DIS <ref type="bibr" target="#b0">[1]</ref>. For all datasets, we use mean average precision (mAP) under IoU thresholds of 0.25 and 0.5 as a metric.</p><p>ScanNet. The ScanNet dataset contains 1513 reconstructed 3D indoor scans with per-point instance and semantic labels of 18 object categories. Given this annotation, we calculate AABBs via the standard approach <ref type="bibr" target="#b21">[22]</ref>. The training subset is comprised of 1201 scans, while 312 scans are left for validation.</p><p>SUN RGB-D. SUN RGB-D is a monocular 3D scene understanding dataset containing more than 10,000 indoor RGB-D images. The annotation consists of per-point semantic labels and OBBs of 37 object categories. As proposed in <ref type="bibr" target="#b21">[22]</ref>, we run experiments with objects of the 10 most common categories. The training and validation splits contain 5285 and 5050 point clouds, respectively.</p><p>S3DIS. Stanford Large-Scale 3D Indoor Spaces dataset contains 3D scans of 272 rooms from 6 buildings, with 3D instance and semantic annotation. Following <ref type="bibr" target="#b9">[10]</ref>, we evaluate our method on furniture categories. AABBs are derived from 3D semantics. We use the official split, where 68 rooms from Area 5 are intended for validation, while the remaining 204 rooms comprise the training subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Hyperparameters. For all datasets, we use the same hyperparameters except for the following. First, the size of output classification layer equals the number of object categories, which is 18, 10, and 5 for ScanNet, SUN RGB-D, and S3DIS. Second, SUN RGB-D contains OBBs, so we predict additional targets ? 7 and ? 8 for this dataset; note that the loss function is not affected. Last, ScanNet, SUN RGB-D, and S3DIS contain different numbers of scenes, so we repeat each scene 10, 3, and 13 times per epoch, respectively. Similar to GSDN <ref type="bibr" target="#b9">[10]</ref>, we use the sparse 3D modification of ResNet34 named HDResNet34 as a backbone. The neck and the head use the outputs of the backbone at all feature levels. In initial point cloud voxelization, we set the voxel size to 0.01m and the number of points N pts to 100,000. Respectively, N vox equals to 100,000. Both ATSS <ref type="bibr" target="#b32">[33]</ref> and FCOS <ref type="bibr" target="#b26">[27]</ref> set N loc to 3 2 for 2D object detection. Accordingly, we select a feature level so bounding box covers at least N loc = 3 3 locations. We select 18 locations by center sampling. The NMS IoU threshold is 0.5.</p><p>Training. We implement FCAF3D using the MMdetection3D <ref type="bibr" target="#b5">[6]</ref> framework. The training procedure follows the default MMdetection <ref type="bibr" target="#b2">[3]</ref> scheme: training takes 12 epochs and the learning rate decreases on the 8th and the 11th epochs. We employ the Adam optimizer with an initial learning rate of 0.001 and weight decay of 0.0001. All models are trained on two NVidia V100 with a batch size of 8. Evaluation and performance tests are run on a single NVidia GTX1080Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Presented  <ref type="table">Table 1</ref>. Results of FCAF3D and existing indoor 3D object detection methods that accept point clouds. The best metric values are marked bold. FCAF3D outperforms previous state-of-the-art methods: GroupFree (on ScanNet and SUN RGB-D) and GSDN (on S3DIS). The reported metric value is the best one across 25 trials; the average value is given in brackets.</p><p>Evaluation. We follow the evaluation protocol introduced in <ref type="bibr" target="#b15">[16]</ref>. Both training and evaluation are randomized, as the input N pts are randomly sampled from the point cloud. To obtain statistically significant results, we run training 5 times and test each trained model 5 times independently. We report both the best and average metrics across 5 ? 5 trials: this allows comparing FCAF3D to the 3D object detection methods that report either a single best or an average value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with State-of-the-art Methods</head><p>We compare FCAF3D with previous state-of-the-arts on three indoor benchmarks in Tab. 1. As one might observe, FCAF3D achieves the best results on all benchmarks. The performance gap is especially tangible in terms of mAP@0.5: our method surpasses previous state-of-the-art by 4.5% on ScanNet and 3.7% on SUN RGB-D. On S3DIS, FCAF3D outperforms weak state-of-the-art by a huge margin. Overall, the proposed method is consistently better than existing methods, setting a new state-of-the-art for indoor 3D object detection. The examples of ScanNet, SUN RGB-D, and S3DIS point clouds with predicted bounding boxes are depicted in <ref type="figure" target="#fig_4">Fig. 4</ref>, 5, 6.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Object Geometry Priors</head><p>To study geometry priors, we train and evaluate existing methods with proposed modifications. We experiment with 3D object detection methods accepting data of different modalities: point clouds, RGB images, or both, to see whether the effect is data-specific or universal. VoteNet and ImVoteNet have the same head and are trained with the same losses. Among them, there are 4 prior losses: size classification loss, size regression loss, direction classification loss, and direction regression loss. Both classification losses correspond to targets parametrized using priors (per-category mean object sizes and a set of angle bins). Similar to FCAF3D, we replace the aforementioned losses with a rotated IoU loss with Mobius parametrization 5. To give a complete picture, we also try a sin-cos parametrization used in the outdoor 3D object detection method SMOKE <ref type="bibr" target="#b16">[17]</ref>.</p><p>The rotated IoU loss decreases the number of trainable parameters and hyperparameters, including geometry priors and loss weights. This loss has already been used in outdoor 3D object detection <ref type="bibr" target="#b34">[35]</ref>. Recently, <ref type="bibr" target="#b5">[6]</ref> reported results of VoteNet trained with axis-aligned IoU loss on ScanNet.</p><p>Tab. 2 shows that replacing the standard parametrization with Mobius one boosts VoteNet and ImVoteNet mAP@0.5 by approximately 4%.</p><p>ImVoxelNet does not use a classification+regression scheme to estimate heading angle but predicts its value directly in a single step. Since the original ImVox-elNet uses the rotated IoU loss, we do not need to remove redundant losses, only to change the parametrization. Again, the Mobius parametrization helps to obtain the best results, even though the superiority is minor.</p><p>GSDN anchors. In this study, we provide a more comprehensive comparison against GSDN and report the results in Tab. 3. A fair comparison implies that we should test our method in the most similar scenario with the same set of hyperparameters. Accordingly, we use a voxel size of 0.05m, ensuring we operate the same inputs and do not benefit from using more detailed and informative spatial information. With the same input voxel size, the voxel sizes at different feature levels of the decoder are also of the same sizes (0.2, 0.4, 0.8, 1.6).</p><p>Moreover, we introduce a minor modification to our FCAF3D network. The first 3D convolution in the network has the stride of 2 in the original FCAF3D, but in GSDN, it equals to 1. With the same stride of 1, the same voxel size and the same voxel sizes at different feature levels, FCAF3D slightly outperforms GSDN in terms of mAP@0. <ref type="bibr" target="#b24">25</ref>   <ref type="table">Table 2</ref>. Results of several 3D object detection methods that accept inputs of different modalities, with different OBB parametrization on SUN RGB-D. The FCAF3D metric value is the best across 25 trials; the average value is given in brackets. For other methods, we report results from the original papers and also the results obtained through our experiments with MMdetection3D-based re-implementations (marked as Reimpl). PC stands for point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone  <ref type="table">Table 3</ref>. Results of fully convolutional 3D object detection methods that accept point clouds on ScanNet. The FCAF3D results better than the results of the original GSDN (with anchors) are marked bold. The all-best results are underlined.</p><p>notable accuracy gain in mAP@0.5 (46.2 against 34.8). The number of scenes processed in a second by both these methods is comparable: it equals to 17 and 20, respectively. This minor difference in speed between FCAF3D based on HDResNet34 and GSDN is attributed to the different sparsity pruning strategies: GSDN employs anchor-based strategy with the corresponding anchor-based loss, but in our anchor-free method, we cannot use the anchor-based sparsity pruning. However, the balanced FCAF3D with a more lightweight backbone with three feature levels, the voxel size of 0.05m, and the stride of 1 outperforms GSDN in both accuracy and speed. Overall, we argue that FCAF3D addresses the 3D object detection in a more efficient way and thus should be preferred. As can be observed, the all-best results are obtained by the original accurate FCAF3D with the HDResNet34 backbone, the voxel size of 0.01m, and the default stride of 2: in this setting, FCAF3D outperforms GSDN by a huge margin (mAP@0.25 of 70.7 against 62.8, mAP@0.5 of 56.0 against 34.8).</p><p>Finally, we address the speed issues with the most lightweight HDResNet34:2 backbone having only two feature levels. According to the reported values, the fast FCAF3D modification with HDResNet34:2 processes 30 scenes per second, while GSDN is able to handle only 20 scenes. While improving the inference speed, we do not sacrifice the superior accuracy: with the voxel size of 0.02m, FCAF3D based on the HDResNet34:2 backbone still outperforms GSDN in both mAP@0.25 and mAP@0.5.  <ref type="table">Table 4</ref>. Results of ablation studies on the voxel size, the number of points (which equals the number of voxels Nvox in pruning), centerness, and center sampling in FCAF3D. The better options are marked bold (actually, these are the default options used to obtain the results in Tab. 1 above). The reported metric value is the best across 25 trials; the average value is given in brackets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>In this section, we discuss the FCAF3D design choices and investigate how they affect metrics when applied independently in ablation studies. We run experiments with varying voxel size, the number of points in a point cloud N pts , the number of locations selected by center sampling, and with and without centerness. The results of ablation studies are aggregated in Tab. 4 for all benchmarks.</p><p>Voxel size. Expectedly, with an increasing voxel size, accuracy goes down. We try voxels of 0.03, 0.02, and 0.01 m. We do not experiment with smaller values since inference would take too much time. We attribute the notable gap in mAP between voxel sizes of 0.01 and 0.02 m to the presence of almost flat objects, such as doors, pictures, and whiteboards. Namely, with a voxel size of 2 cm, the head would output locations with 16 cm tolerance, but the almost flat objects could be less than 16 cm by one of the dimensions. Accordingly, we observe a decrease in accuracy for larger voxel sizes.</p><p>Number of points. Similar to 2D images, subsampled point clouds are sometimes referred to as low-resolution ones. Accordingly, they contain less information than their high-resolution versions. As can be expected, the fewer the points, the lower is detection accuracy. In this series of experiments, we sample 20k, 40k, and 100k points from the entire point cloud, and the obtained metric values revealed a clear dependency between the number of points and mAP. We do not consider larger N pts values to be on a par with the existing methods (specifically, GSDN <ref type="bibr" target="#b9">[10]</ref> uses all points in a point cloud, GroupFree <ref type="bibr" target="#b15">[16]</ref> samples 50k points, VoteNet <ref type="bibr" target="#b21">[22]</ref> selects 40k points for ScanNet and 20k for SUN RGB-D). We use N vox = N pts to guide pruning in the neck. When N vox exceeds 100k, the inference time increases due to growing sparsity in the neck, while the accuracy improvement is negligible. So we restrict our grid search for N pts with 100k and use it as a default value regarding the obtained results.</p><p>Centerness. Using centerness improves mAP for the ScanNet and SUN RGB-D datasets. For S3DIS, the results are controversial: the better mAP@0.5 is balanced with a minor decrease of mAP@0.25. Nevertheless, we analyze the results altogether, so we can consider centerness a helpful feature with a small positive effect on the mAP, almost reaching 1% of mAP@0.5 on ScanNet.</p><p>Center sampling. Finally, we study the number of locations selected in center sampling. We select 9 locations, as proposed in FCOS <ref type="bibr" target="#b26">[27]</ref>, the entire set of 27 locations, as in ImVoxelNet <ref type="bibr" target="#b23">[24]</ref>, and 18 locations. The latter appeared to be the best choice according to mAP on all the benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Inference Speed</head><p>Compared to standard convolutions, sparse convolutions are time-and memoryefficient. GSDN authors claim that with sparse convolutions, they process a scene with 78M points covering about 14,000 m 3 within a single fully convolutional feed-forward pass, using only 5G of GPU memory. FCAF3D uses the same sparse convolutions and the same backbone as GSDN. However, as can be seen in Tab. 3, the default FCAF3D is slower than GSDN. This is due to the smaller voxel size: we use 0.01m for a proper multi-level assignment while GSDN uses 0.05m.</p><p>To build the fastest method, we use HDResNet34:3 and HDResNet34:2 backbones with only three and two feature levels, respectively. With these modifications, FCAF3D is faster on inference than GSDN <ref type="figure" target="#fig_0">(Fig. 1)</ref>.</p><p>For a fair comparison, we re-measure inference speed for GSDN and votingbased methods, as point grouping operation and sparse convolutions have become much faster since the initial release of these methods. In performance tests, we opt for implementations based on the MMdetection3D <ref type="bibr" target="#b5">[6]</ref> framework to mitigate codebase differences. The reported inference speed for all methods is measured on the same single GPU so they can be directly compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented FCAF3D, a first-in-class fully convolutional anchor-free 3D object detection method for indoor scenes. Our method significantly outperforms the previous state-of-the-art on the challenging indoor SUN RGB-D, ScanNet, and S3DIS benchmarks in terms of both mAP and inference speed. We also proposed a novel oriented bounding box parametrization and showed that it improves accuracy for several 3D object detection methods. Moreover, the proposed parametrization allows avoiding any prior assumptions about objects, thus reducing the number of hyperparameters. Overall, FCAF3D with our bounding box parametrization is accurate, scalable, and generalizable at the same time. B Metric values for <ref type="figure" target="#fig_0">Fig. 1</ref> We report inference speed for different methods on ScanNet dataset in Tab. 5. The inference speed is measured on the same single NVidia GTX1080Ti. In terms of AP@0.5, FCAF3D outperforms GSDN by a large margin for each category. Similar to AP@0.25, the accuracy gap for the sofa category is the most dramatic: with an AP@0.25 of 70.1, FCAF3D is an order of magnitude more accurate than GSDN, which has only 6.1. Accordingly, FCAF3D has an approximately 1.8 times larger mAP compared to GSDN.  <ref type="table">Table 11</ref>. AP@0.5 scores for 5 object categories from the S3DIS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Visualization</head><p>This section contains additional visualizations of the results of 3D object detection for all three benchmarks. The ground truth and estimated 3D object bounding boxes are drawn over the corresponding point clouds. Objects of different categories are marked with different colors.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>mAP@0.5 scores on ScanNet against scenes per second. FCAF3D modifications (marked red) have different number of backbone feature levels. For each existing method, there is a FCAF3D modification surpassing this method in both detection accuracy and inference speed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The general scheme of the proposed FCAF3D. All convolutions and transposed convolutions are three-dimensional and sparse. This design allows processing the input point cloud in a single forward pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of objects with an ambiguous heading angle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The point cloud from ScanNet with AABBs. The color of a bounding box denotes the object category. Left: estimated with FCAF3D, right: ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The point cloud from SUN RGB-D with OBBs. The color of a bounding box denotes the object category. Left: estimated with FCAF3D, right: ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>The point cloud from S3DIS with AABBs. The color of a bounding box denotes the object category. Left: estimated with FCAF3D, right: ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>71.5 (70.7) 57.3 (56.0) 64.2 (63.8) 48.9 (48.2) 66.7 (64.9) 45.9 (43.8) 0.02 66.3 (65.8) 49.4 (48.6) 62.3 (62.0) 46.3 (45.5) 61.0 (58.5) 43.8 (38.5) 0.03 59.6 (59.2) 42.6 (41.6) 60.4 (59.7) 41.6 (41.0) 55.4 (53.3) 38.6 (35.0) Number of points 20k 69.0 (68.1) 52.8 (52.0) 63.0 (62.5) 46.9 (46.5) 60.1 (58.8) 45.1 (40.1) 40k 67.6 (66.7) 53.6 (52.2) 63.4 (63.1) 47.2 (46.6) 63.7 (61.2) 44.8 (42.2) 100k 71.5 (70.7) 57.3 (56.0) 64.2 (63.8) 48.9 (48.2) 66.7 (64.9) 45.9 (43.8) Centerness No 71.0 (70.4) 56.1 (55.1) 63.8 (63.3) 48.2 (47.5) 67.9 (65.5) 46.0 (43.5) Yes 71.5 (70.7) 57.3 (56.0) 64.2 (63.8) 48.9 (48.2) 66.7 (64.9) 45.9 70.1) 55.7 (55.0) 63.8 (63.3) 48.6 (48.2) 66.5 (63.6) 44.4 (42.5) 18 71.5 (70.7) 57.3 (56.0) 64.2 (63.8) 48.9 (48.2) 66.7 (64.9) 45.9 (43.8) 27 70.2 (69.7) 55.7 (54.1) 64.3 (63.8) 48.7 (47.9) 65.1 (63.2) 43.6 (41.7)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>The point cloud from SUN RGB-D with OBBs. The color of a bounding box denotes the object category: bed, chair, desk, dresser, table (only categories that are present in the pictures are listed). Left: estimated with FCAF3D, right: ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>The point cloud from ScanNet with AABBs. The color of a bounding box denotes the object category: cabinet, chair, sofa, table, door, window, bookshelf, picture, counter, desk, shower curtain, toilet, sink, bathtub, other furniture (only categories that are present in the pictures are listed). Left: estimated with FCAF3D, right: ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>The point cloud from S3DIS with AABBs. The color of a bounding box denotes the object category: table, chair, sofa, bookcase, whiteboard. Left: estimated with FCAF3D, right: ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>(64.2 against 62.8), while demonstrating a</figDesc><table><row><cell>Method</cell><cell cols="3">Input mAP@0.25 mAP@0.5</cell></row><row><cell>VoteNet[22]</cell><cell></cell><cell>57.7</cell><cell>-</cell></row><row><cell>Reimpl.[6]</cell><cell></cell><cell>59.1</cell><cell>35.8</cell></row><row><cell>Reimpl. w/ IoU loss w/ naive param.</cell><cell>PC</cell><cell cols="2">61.1 (60.3) 38.4 (37.7)</cell></row><row><cell>w/ sin-cos param.</cell><cell></cell><cell cols="2">60.7 (59.8) 37.1 (36.4)</cell></row><row><cell>w/ Mobius param.</cell><cell></cell><cell cols="2">61.1 (60.5) 40.4 (39.5)</cell></row><row><cell>ImVoteNet[21]</cell><cell></cell><cell>63.4</cell><cell>-</cell></row><row><cell>Reimpl.[6]</cell><cell></cell><cell>64.0</cell><cell>37.8</cell></row><row><cell>Reimpl. w/ IoU loss</cell><cell>RGB</cell><cell></cell></row><row><cell>w/ naive param.</cell><cell>+PC</cell><cell cols="2">64.2 (63.9) 39.1 (38.3)</cell></row><row><cell>w/ sin-cos param.</cell><cell></cell><cell cols="2">64.6 (64.0) 39.9 (37.8)</cell></row><row><cell>w/ Mobius param.</cell><cell></cell><cell cols="2">64.6 (64.1) 40.8 (39.8)</cell></row><row><cell>ImVoxelNet[24]</cell><cell></cell><cell>40.7</cell><cell>-</cell></row><row><cell>w/ naive param. w/ sin-cos param.</cell><cell>RGB</cell><cell cols="2">41.3 (40.4) 13.8 (13.0) 41.3 (40.5) 13.2 (12.8)</cell></row><row><cell>w/ Mobius param.</cell><cell></cell><cell cols="2">41.5 (40.6) 14.6 (14.0)</cell></row><row><cell>FCAF3D</cell><cell></cell><cell></cell></row><row><cell>w/ naive param. w/ sin-cos param.</cell><cell>PC</cell><cell cols="2">63.8 (63.5) 46.8 (46.2) 63.9 (63.6) 48.2 (47.3)</cell></row><row><cell>w/ Mobius param.</cell><cell></cell><cell cols="2">64.2 (63.8) 48.9 (48.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Verification of Eq. 4. Here, we prove that four different representations of the same OBB from Eq. 3 map to the same point on a Mobius strip by Eq. 4.</figDesc><table><row><cell></cell><cell></cell><cell cols="7">(q, ?) ? (ln(q) sin(2?), ln(q) cos(2?), sin(4?), cos(4?))</cell></row><row><cell cols="2">1 q</cell><cell cols="2">, ? +</cell><cell>? 2</cell><cell>? (ln(</cell><cell>1 q</cell><cell>) sin(2? + ?), ln(</cell><cell>1 q</cell><cell>) cos(2? + ?), sin(4? + 2?), cos(4? + 2?))</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">= (ln(q) sin(2?), ln(q) cos(2?), sin(4?), cos(4?))</cell></row><row><cell></cell><cell cols="8">(q, ? + ?) ? (ln(q) sin(2? + 2?), ln(q) cos(2? + 2?), sin(4? + 4?), cos(4? + 4?))</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">= (ln(q) sin(2?), ln(q) cos(2?), sin(4?), cos(4?))</cell></row><row><cell>1 q</cell><cell cols="2">, ? +</cell><cell cols="2">3? 2</cell><cell>? (ln(</cell><cell>1 q</cell><cell cols="2">) sin(2? + 3?), ln(</cell><cell>1 q</cell><cell>) cos(2? + 3?), sin(4? + 6?), cos(4? + 6?))</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">= (ln(q) sin(2?), ln(q) cos(2?), sin(4?), cos(4?))</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 9 .Table 10 .</head><label>5910</label><figDesc>Results of 3D object detection methods that accept point clouds on ScanNet. C Per-category results ScanNet. Tab. 6 contains per-category AP@0.25 scores for 18 object categories for the ScanNet dataset. For 12 out of 18 categories, FCAF3D outperforms other methods. The largest quality gap can be observed for window (60.2 against 53.7), picture (29.9 against 18.6), and other furniture (65.4 against 56.4) categories. Tab. 7 shows per-category AP@0.5 scores. According to the reported values, FCAF3D is the best at detecting objects of 13 out of 18 categories. The most significant improvement is achieved for cabinet (35.8 against 26.0), sofa (85.2 Method bath bed bkshf chair desk dresser nstand sofa table toilet mAP H3DNet[34] 47.6 52.9 8.6 60.1 8.4 20.6 45.6 50.4 27.1 69.1 39.0 GroupFree[16] 64.0 67.1 12.4 62.6 14.5 21.9 49.8 58.2 29.2 72.2 45.2 FCAF3D 66.2 69.8 11.6 68.8 14.8 30.1 59.8 58.2 35.5 74.5 48.9 AP@0.5 scores for 10 object categories from the SUN RGB-D dataset. Method table chair sofa bkcase board mAP GSDN[10] 73.7 98.1 20.8 33.4 12.9 47.8 FCAF3D 69.7 97.4 92.4 36.7 37.3 66.7 Per-category AP@0.25 scores for 5 object categories from the S3DIS dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Scenes per sec. 0.25 0.5 mAP</cell></row><row><cell>VoteNet[22]</cell><cell cols="2">11.8 58.6 33.5</cell></row><row><cell>GSDN[10]</cell><cell cols="2">20.1 62.8 34.8</cell></row><row><cell>H3DNet[34]</cell><cell>4.9</cell><cell>67.2 48.1</cell></row><row><cell>BRNet[4]</cell><cell cols="2">10.3 66.1 50.9</cell></row><row><cell>3DETR[19]</cell><cell>3.1</cell><cell>62.7 37.5</cell></row><row><cell>3DETR-m[19]</cell><cell>3.1</cell><cell>65.0 47.0</cell></row><row><cell>GroupFree[16]</cell><cell>6.6</cell><cell>69.1 52.8</cell></row><row><cell>FCAF3D</cell><cell>8.0</cell><cell>71.5 57.3</cell></row><row><cell>w/ 3 levels</cell><cell cols="2">12.2 69.8 53.6</cell></row><row><cell>w/ 2 levels</cell><cell cols="2">31.5 63.1 46.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. We would like to thank Alexey Rukhovich for useful discussions on topology.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Comments on Mobius Parametrization</head><p>Comments on Eq. 3. The OBB heading angle ? is typically defined as an angle between x-axis and a vector towards a center of one of OBB faces. If a frontal face exists, then ? is defined unambiguously; however, this is not the case for some indoor objects. If a frontal face cannot be chosen unequivocally, there are four possible representations for a single OBB. The heading angle describes a rotation within the xy plane around z-axis w.r.t. the OBB center. Therefore, the OBB center (x, y, z), height h, and the OBB size s = w + l are the same for all representations. Meanwhile, the ratio q = w l of the frontal and lateral OBB faces and the heading angle ? do vary. Specifically, there are four options for the heading angle: ?, ? + ? 2 , ? + ?, ? + 3? 2 . Swapping frontal and lateral faces gives two ratio options: q and 1 q . Overall, there are four different tuples (q, ?) for the same OBB:</p><p>(q, ?) , 1 q , ? + ? 2 , (q, ? + ?) , 1 q , ? + 3? 2 .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A hierarchical graph network for 3d object detection on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Back-tracing representative points for voting-based 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">MMDetection3D: OpenMMLab next-generation platform for general 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmdetection3d" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d-mpa: Multiproposal aggregation for 3d semantic instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ota: Optimal transport assignment for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yoshie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative sparse detection networks for 3d single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part IV 16</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d-sis: 3d semantic instance segmentation of rgbd scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d fully convolutional network for vehicle detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Group-free 3d object detection via transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Smoke: Single-stage monocular 3d object detection via keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>T?th</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for realtime object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An end-to-end transformer model for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Munkres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Topology</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imvotenet: Boosting 3d object detection in point clouds with image votes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Imvoxelnet: Image to voxels projection for monocular and multi-view general-purpose 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rukhovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorontsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01178</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Frustum voxnet for 3d object detection from rgb-d or depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stamos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10956</idno>
		<title level="m">Fcos3d: Fully convolutional one-stage monocular 3d object detection</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Venet: Voting enhancement network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mlcvnet: Multilevel context votenet for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Center-based 3d object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">H3dnet: 3d object detection using hybrid geometric primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Iou loss for 2d/3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Method cab bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn mAP VoteNet</title>
		<imprint/>
	</monogr>
	<note>22] 36.3 87.9 88.7 89.6 58.8 47.3 38.1 44.6 7.8 56.1 71.7 47.2 45.4 57.1 94.9 54.7 92.1 37.2 58.7 GSDN[10] 41.6 82.5 92.1 87.0 61.1 42.4 40.7 51.5 10.2 64.2 71.1 54.9 40.0 70.5 100 75.5 93.2 53.1 62.8</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Groupfree</surname></persName>
		</author>
		<idno>16] 52</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Method cab bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn mAP VoteNet</title>
	</analytic>
	<monogr>
		<title level="m">Table 6. Per-category AP@0.25 scores for 18 object categories from the ScanNet dataset</title>
		<imprint/>
	</monogr>
	<note>22] 8.1 76.1 67.2 68.8 42.4 15.3 6.4 28.0 1.3 9.5 37.5 11.6 27.8 10.0 86.5 16.8 78.9 11.7 33.5 GSDN[10] 13.2 74.9 75.8 60.3 39.5 8.5 11.6 27.6 1.5 3.2 37.5 14.1 25.9 1.4 87.0 37.5 76.9 30.5 34.8</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Groupfree</surname></persName>
		</author>
		<idno>16] 26.0 81.3 82.9 70.7 62.2 41.7</idno>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>26.5 55.8 7.8 34.7 67.2 43.9 44.3 44.1 92.8 37.4 89.7 40.6 52.8 FCAF3D 35.8 81.5 89.8 85.0 62.0 44.1 30.7 58.4 17.9 31.3 53.4 44.2 46.8 64.2 91.6 52.6 84.5 57.1</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<title level="m">Table 7. AP@0.5 scores for 18 object categories from the ScanNet dataset</title>
		<imprint/>
	</monogr>
	<note>against 70.7), picture (17.9 against 7.8), shower (64.2 against 44.1), and sink (52.6 against 37.4)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Per-category AP@0.25 scores for the 10 most common object categories for the SUN RGB-D benchmark are reported in Tab. 8. Compared to other methods, FCAF3D is more accurate at detecting objects of 7 out of 10 categories. In this experiment, the quality gap is not so dramatic: it equals 4.1 % for desk and 5.2 % for night stand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun Rgb-D</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>for the rest categories, it does not exceed 2 %</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">For SUN RGB-D, the superiority of the proposed method is more noticeable when analyzing on per-category AP@0.5. As shown in Tab. 9, FCAF3D outperforms the competitors for 9 out of 10 object categories. For some categories, there is a significant margin: e.g., 30.1 against 21.9 for dresser, 59.8 against 49.8 for night stand, and 35.5 against 29.2 for table. Respectively, FCAF3D surpasses other methods by more than 3.5 % in terms of mAP@0.5. S3DIS. The results of the proposed method in comparison with GSDN are presented in Tab. 10 and Tab. 11. In terms of AP@0.25, FCAF3D is far more accurate when detecting sofas, bookcases, and whiteboards</title>
	</analytic>
	<monogr>
		<title level="m">Table 8. AP@0.25 scores for 10 object categories from the SUN RGB-D dataset</title>
		<imprint/>
	</monogr>
	<note>Most notably, FCAF3D achieves an impressive AP@0.25 of 92.4 for the sofa category, leaving GSDN with</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
