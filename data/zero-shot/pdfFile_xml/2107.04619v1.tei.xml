<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DIVERSE VIDEO GENERATION USING A GAUSSIAN PROCESS TRIGGER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Shrivastava</surname></persName>
							<email>gauravsh@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
							<email>abhinav@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DIVERSE VIDEO GENERATION USING A GAUSSIAN PROCESS TRIGGER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos and multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a Gaussian Process (GP) to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. In addition, we leverage the changes in this distribution overtime to control the sampling of diverse future states by estimating the end of on-going sequences. That is, we use the variance of GP over the output function space to trigger a change in an action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Humans are often able to imagine multiple possible ways that the scene can change over time.</p><p>Modeling and generating diverse futures is an incredibly challenging problem. The challenge stems from the inherent multi-modality of the task, i.e., given a sequence of past frames, there can be multiple possible outcomes of the future frames. For example, given the image of a "person holding a cup" in <ref type="figure">Figure.</ref> 1, most would predict that the next few frames correspond to either the action "drinking from the cup" or "keeping the cup on the table." This challenge is exacerbated by the lack of real training data with diverse outputs -all real-world training videos come with a single real future and no "other" potential futures. Similar looking past frames can have completely different futures (e.g., <ref type="figure">Figure.</ref> 1). In the absence of any priors or explicit supervision, the current methods struggle with modeling this diversity. Given similar looking past frames, with different futures in the training data, variational methods, which commonly utilize <ref type="bibr" target="#b21">(Kingma &amp; Welling, 2013)</ref>, tend to average the results to better match to all different futures <ref type="bibr" target="#b7">(Denton &amp; Fergus, 2018;</ref><ref type="bibr" target="#b1">Babaeizadeh et al., 2017;</ref><ref type="bibr" target="#b12">Gao et al., 2018;</ref><ref type="bibr" target="#b27">Oliu et al., 2017)</ref>. We hypothesize that explicit modeling of future diversity is essential for high-quality, diverse future frame generation.</p><p>In this paper, we model the diversity of the future states, given past context, using Gaussian Processes (GP) <ref type="bibr" target="#b28">(Rasmussen, 2006)</ref>, which have several desirable properties. They learn a prior on potential future given past context, in a Bayesian formulation. This allows us to update the distribution of possible futures as more context frames are provided as evidence and maintain a list of potential futures (underlying functions in GP). Finally, our formulation provides an interesting property that is crucial to generating future frames -the ability to estimate when to generate a diverse output vs. continue an on-going action, and a way to control the predicted futures.</p><p>In particular, we utilize the variance of the GP at any specific time step as an indicator of whether an action sequence is on-going or finished. An illustration of this mechanism is presented in <ref type="figure">Figure.</ref> 2. When we observe a frame (say at time t) that can have several possible futures, the variance of the GP model is high <ref type="figure" target="#fig_2">(Figure. 2 (left)</ref>). Different functions represent potential action sequences that can be generated, starting from this particular frame. Once we select the next frame (at t+2), the GP variance of the future states is relatively low <ref type="figure" target="#fig_2">(Figure. 2 (center)</ref>), indicating that an action sequence <ref type="figure">Figure 1</ref>: Given "person holding cup," humans can often predict multiple possible futures (e.g.,"drinking from the cup" or "keeping the cup on the table.").</p><p>is on-going, and the model should continue it as opposed to trying to sample a diverse sample. After the completion of the on-going sequence, the GP variance over potential future states becomes high again. This implies that we can continue this action (i.e., pick the mean function represented by the black line in <ref type="figure">Figure.</ref> 2 (center)) or try and sample a potentially diverse sample (i.e., one of the functions that contributes to high-variance). This illustrates how we can use GP to decide when to trigger diverse actions. An example of using GP trigger is shown in <ref type="figure">Figure.</ref> 2 (right), where after every few frames, we trigger a different action. Now that we have a good way to model diversity, the next step is to generate future frames. Even after tremendous advances in the field of generative models for image synthesis <ref type="bibr" target="#b7">(Denton &amp; Fergus, 2018;</ref><ref type="bibr" target="#b1">Babaeizadeh et al., 2017;</ref><ref type="bibr" target="#b42">Vondrick &amp; Torralba, 2017;</ref><ref type="bibr" target="#b24">Lu et al., 2017;</ref><ref type="bibr" target="#b43">Vondrick et al., 2016;</ref><ref type="bibr" target="#b29">Saito et al., 2017;</ref><ref type="bibr" target="#b37">Tulyakov et al., 2018;</ref><ref type="bibr" target="#b17">Hu &amp; Wang, 2019)</ref>, the task of generating future frames (not necessarily diverse) conditioned on past frames is still hard. As opposed to independent images, the future frames need to obey potential video dynamics that might be on-going in the past frames, follow world knowledge (e.g., how humans and objects interact), etc.. We utilize a fairly straightforward process to generate future frames, which utilizes two modules: a frame auto-encoder and a dynamics encoder. The frame auto-encoder learns to encode a frame in a latent representation and utilizes it to generate the original frame back. The dynamics encoder learns to model dynamics between past and future frames. We learn two independent dynamics encoders: an LSTM encoder, utilized to model on-going actions and the GP encoder (similar to <ref type="bibr" target="#b35">(Srivastava et al., 2015)</ref>), and a GP encoder, utilized to model transitions to new actions. The variance of this GP encoder can be used as a trigger to decide when to sample new actions. We train this framework end-to-end. We first provide an overview of GP formulation and scalable training techniques in ?3, and then describe our approach in ?4.</p><p>Comprehensively evaluating diverse future frames generation is still an open research problem. Following recent state-of-the-art, we will evaluate different aspects of the approach independently. The quality of generated frames is quantified using image synthesis/reconstruction per-frame metrics: SSIM <ref type="bibr" target="#b31">Sampat et al., 2009)</ref>, PSNR, and LPIPS <ref type="bibr" target="#b8">Dosovitskiy &amp; Brox, 2016;</ref><ref type="bibr" target="#b20">Johnson et al., 2016)</ref>. The temporal coherence and quality of a short video clip (16 neighboring frames) are jointly evaluated using the FVD <ref type="bibr" target="#b38">(Unterthiner et al., 2018)</ref> metric. However, high-quality, temporarily coherent frame synthesis does not evaluate diversity in predicted frames. Therefore, to evaluate diversity, since there are no multiple ground-truth futures, we propose an alternative evaluation strategy, inspired by <ref type="bibr" target="#b40">(Villegas et al., 2017b)</ref>: utilizing action classifiers to evaluate whether an action switch has occurred or not. A change in action indicates that the method was able to sample a diverse future. Together, these metrics can evaluate if an approach can generate multiple high-quality frames that temporally coherent and diverse. Details of these metrics and baselines, and extensive quantitative and qualitative results are provided in ?5.</p><p>To summarize, our contributions are: (a) modeling the diversity of future states using a GP, which maintains priors on future states given the past frames using a Bayesian formulation (b) leveraging the changing GP distribution over time (given new observed evidence) to estimate when an on-going action sequence completes and using GP variance to control the triggering of a diverse future state. This results in state-of-the-art results on future frame generation. We also quantify the diversity of the generated sequences using action classifiers as a proxy metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Understanding and predicting the future, given the observed past, is a fundamental problem in video understanding. The future states are inherently multi-modal and capturing their diversity finds direct applications in many safety-critical applications (e.g., autonomous vehicles), where it is critical  <ref type="figure" target="#fig_2">Figure 2</ref>: An illustration of using GP variance to control sampling on-going actions vs. new actions.</p><p>to model different future modes. Earlier techniques for future prediction <ref type="bibr" target="#b53">(Yuen &amp; Torralba, 2010;</ref><ref type="bibr" target="#b44">Walker et al., 2014)</ref> relied on searching for matches of past frames in a given dataset and transferring the future states from these matches. However, the predictions were limited to symbolic trajectories or retrieved future frames. Given the modeling capabilities of deep representations, the field of future frame prediction tremendous progress in recent years. One of the first works on video generation <ref type="bibr" target="#b35">(Srivastava et al., 2015)</ref> used a multi-layer LSTM network to learn representations of video sequences in a deterministic way. Since then, a wide range of papers <ref type="bibr" target="#b27">(Oliu et al., 2017;</ref><ref type="bibr" target="#b5">Cricri et al., 2016;</ref><ref type="bibr" target="#b39">Villegas et al., 2017a;</ref><ref type="bibr" target="#b10">Elsayed et al., 2019;</ref><ref type="bibr" target="#b41">Villegas et al., 2019;</ref><ref type="bibr" target="#b4">Castrej?n et al., 2019)</ref> have built models that try to incorporate stochasticity of future states. Generally, this stochasticity lacks diverse high-level semantic actions.</p><p>Recently, several video generation models have utilized generative models, like variational autoencoders (VAEs) <ref type="bibr" target="#b21">(Kingma &amp; Welling, 2013)</ref> and generative adversarial networks (GANs) <ref type="bibr" target="#b14">(Goodfellow et al., 2014)</ref>, for this task. One of the first works by <ref type="bibr" target="#b52">Xue et al. (2016)</ref> utilized a conditional VAE (cVAE) formulation to learn video dynamics. Similar to our approach, their goal was to model the frame prediction problem in a probabilistic way and synthesizing many possible future frames from a single image. Since then, several works have utilized the cVAE for future generation <ref type="bibr" target="#b1">(Babaeizadeh et al., 2017;</ref><ref type="bibr" target="#b7">Denton &amp; Fergus, 2018)</ref>. The major drawback of using the cVAE approach is that its objective function marginalizes over the multi-modal future, limiting the diversity in the generated samples <ref type="bibr" target="#b2">(Bhattacharyya et al., 2018)</ref>. GAN-based models are another important class of synthesis models used for future frame prediction or video generation <ref type="bibr" target="#b42">(Vondrick &amp; Torralba, 2017;</ref><ref type="bibr" target="#b24">Lu et al., 2017;</ref><ref type="bibr" target="#b43">Vondrick et al., 2016;</ref><ref type="bibr" target="#b29">Saito et al., 2017;</ref><ref type="bibr" target="#b37">Tulyakov et al., 2018;</ref><ref type="bibr" target="#b17">Hu &amp; Wang, 2019)</ref>. However, these models are very susceptible to mode collapse <ref type="bibr" target="#b30">(Salimans et al., 2016)</ref>, i.e., the model outputs only one or a few modes instead of generating a wide range of diverse output. The problem of mode collapse is quite severe for conditional settings, as demonstrated by <ref type="bibr" target="#b19">(Isola et al., 2016;</ref><ref type="bibr" target="#b25">Mathieu et al., 2015)</ref>. This problem is worse in the case of diverse future frame generation due to the inherent multi-modality of the output space and lack of training data.</p><p>Another class of popular video generation models is hierarchical prediction <ref type="bibr" target="#b45">(Walker et al., 2017;</ref><ref type="bibr" target="#b40">Villegas et al., 2017b;</ref><ref type="bibr" target="#b48">Wichers et al., 2018;</ref>. These models decompose the problems into two steps. They first predict a high-level structure of a video, like a human pose, and then leverage that structure to make predictions at the pixel level. These models generally require additional annotation for the high-level structure for training.</p><p>Unlike these approaches, Our approach explicitly focuses on learning the distribution of diverse futures using a GP prior on the future states using a Bayesian formulation. Moreover, such GP approaches have been used in the past for modeling the human dynamics as demonstrated by <ref type="bibr" target="#b46">(Wang et al., 2008;</ref><ref type="bibr" target="#b15">Hardy et al., 2014;</ref><ref type="bibr" target="#b26">Moon &amp; Pavlovic, 2008)</ref>. However, due to the scalability issues in GP, these models were limited to handling low dimensional data, like human pose, lane switching, path planning, etc. To the best of our knowledge, ours is the first approach that can process video sequences to predict when an on-going action sequence completes and control the generation of a diverse state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REVIEW OF GAUSSIAN PROCESS</head><p>A Gaussian process (GP) <ref type="bibr" target="#b28">(Rasmussen, 2006</ref>) is a Bayesian non-parametric approach that learns a joint distribution over functions that are sampled from a multi-variate normal distribution. Consider a data set consisting of n data-points {inputs, targets} n 1 , abbreviated as {X, Y } n 1 , where the inputs are denoted by X = {x 1 , . . . , x n }, and targets by Y = {y 1 , . . . , y n }. The goal of GP is to learn an unknown function f that maps elements from input space to a target space. A GP regression formulates the functional form of f by drawing random variables from a multi-variate normal distribution given by</p><formula xml:id="formula_0">[f (x 1 ), f (x 2 ), . . . , f (x n )] ? N (?, K X,X ), with mean ?, such that ? i = ?(x i ), and K X,X is a covariance matrix. (K X,X ) ij = k(x i , x j ), where k(?)</formula><p>is a kernel function of the GP. Assuming the GP prior on f (X) with some additive Gaussian white noise y i = f (x i ) + , the conditional distribution at any unseen points X * is given by:</p><formula xml:id="formula_1">f * |X * , X, Y ? N (E[f * ], Cov[f * ]), where (1) E[f * ] = ? X * + K X * ,X [K X,X + ? 2 I] ?1 Y Cov[f * ] = K X * ,X * ? K X * ,X [K X,X + ? 2 I] ?1 K X,X *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LEARNING AND MODEL SELECTION</head><p>We can derive the marginal likelihood for the GP by integrating out the f (x) as a function of kernel parameters alone. Its logarithm can be defined analytically as:</p><formula xml:id="formula_2">log (p (Y |X)) = ? 1 2 Y T K ? + ? 2 I ?1 Y + log K ? + ? 2 I + const,<label>(2)</label></formula><p>where ? denotes the parameters of the covariance function of kernel K X,X . Notice that the marginal likelihood involves a matrix inversion and evaluating a determinant for n ? n matrix. A na?ve implementation would require cubic order of computations O(n 3 ) and O(n 2 ) of storage, which hinders the use of GP for a large dataset. However, recent researches have tried to ease these constraints under some assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SCALABLE GP</head><p>The model selection and inference of GP requires a cubic order of computations O(n 3 ) and O(n 2 ) of storage which hinders the use of GP for a large dataset. Titsias (2009) proposed a new variational approach for sparse approximation of the standard GP which jointly infers the inducing inputs and kernel hyperparameters by optimizing a lower bound of the true log marginal likelihood, resulting in O(nm 2 ) computation, where m &lt; n. <ref type="bibr" target="#b16">Hensman et al. (2013)</ref> proposed a new variational formulation of true log marginal likelihood that resulted in a tighter bound. Another advantage of this formulation is that it can be optimized in a stochastic <ref type="bibr" target="#b16">(Hensman et al., 2013)</ref> or distributed <ref type="bibr" target="#b6">(Dai et al., 2014;</ref><ref type="bibr" target="#b11">Gal et al., 2014)</ref> manner, which is well suited for our frameworks which use stochastic gradient descent. Further, recent works <ref type="bibr" target="#b50">(Wilson &amp; Nickisch, 2015;</ref><ref type="bibr" target="#b50">Wilson et al., 2015;</ref> have improved the scalability by reducing the learning to O(n) and test prediction to O(1) under some assumptions.</p><p>In this work, for scalability, we leverage the SVGP approach proposed by <ref type="bibr" target="#b16">Hensman et al. (2013)</ref>. It proposes a tighter bound for the sparse GP introduced by Titsias <ref type="formula" target="#formula_2">(2009)</ref>, which uses pseudo inputs u to lower bound the log joint probability over targets and pseudo inputs. SVGP introduces a multivariate normal variational distribution q(u) = N (m, S), where the parameters m and S are optimized using the evidence lower bound or ELBO (eq. 3) of true marginal likelihood (eq. 2). The pseudo inputs, u, depend on variational parameters</p><formula xml:id="formula_3">{z m } M m=1 , where M = dim(u) N . Therefore, the ELBO for SVGP is L svgp (Y, X) = E q(u) [log p (Y, u|X, Z)] + H[q(u)],<label>(3)</label></formula><p>where the first term was proposed by <ref type="bibr" target="#b36">Titsias (2009)</ref>, and the modification by <ref type="bibr" target="#b16">Hensman et al. (2013)</ref> introduces the second term. For details about the pseudo inputs u and variational parameters z i , we refer the readers to <ref type="bibr" target="#b36">(Titsias, 2009;</ref><ref type="bibr" target="#b16">Hensman et al., 2013)</ref>.</p><p>In this work, we build on the sparse GP approach from GPytorch (Gardner et al., 2018), which implements <ref type="bibr" target="#b16">(Hensman et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OUR APPROACH</head><p>Given a set of observed frames, our goal is to generate a diverse set of future frames. Our model has three modules: (a) a frame auto-encoder (or encoder-generator), (b) an LSTM temporal dynamics encoder, and (c) a GP temporal dynamics encoder to model priors and probabilities over diverse potential future states.</p><formula xml:id="formula_4">Training Frame Auto-Encoder E D ? Training Dynamics Encoders LSTM Encoder ? +1 ? ? +1 ? ?1 LSTM GP Encoder ? +1 ? ?1 ? +2 +1 Inference Diverse Video Generator (DVG) with Trigger-Switch Switch between ? +1 and ? +1 E D ? +1 LSTM ? +1 ? +1</formula><p>The frame encoder maps the frames onto a latent space, that is later utilized by temporal dynamics encoders and frame generator to synthesize the future frames. For inference, we use all three modules together to generate future frames, and use the GP as a trigger to switch to diverse future states. Below we describe each module in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">FRAME AUTO-ENCODER</head><p>The frame encoder network is a convolution encoder which takes frame x t ? R H?W and maps them to the latent space z t ? R d , where H ? W is the input frame size and d is the dimension of latent space respectively. Therefore, f enc :</p><formula xml:id="formula_5">R H?W ? R d , i.e., z t = f enc (x t )</formula><p>. Similarly, the decoder or generator network, utilizes the latent feature to generate the image. Therefore, f gen :</p><formula xml:id="formula_6">R d ? R H?W , i.e.,x t = f gen (z t ).</formula><p>We borrow the architectures for both encoder and generator networks from <ref type="bibr" target="#b7">(Denton &amp; Fergus, 2018)</ref>, where the frame encoder is convolutional layers from VGG16 network <ref type="bibr" target="#b33">(Simonyan &amp; Zisserman, 2015)</ref> and the generator is a mirrored version of the encoder with pooling layers replaced with spatial up-sampling, a sigmoid output layer, and skip connections from the encoder network to reconstruct image. L gen (x t ,x t ) = x t ?x t 2 is the reconstruction loss for frame auto-encoder. This auto-encoder is illustrated in <ref type="figure">Figure.</ref> 3 (stage 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LSTM TEMPORAL DYNAMICS ENCODER</head><p>The first dynamics we want to encode is of an on-going action sequence, i.e., if an action sequence is not completed yet, we want to continue generating future frames of the same sequence till it finishes. This module f LSTM : R d ? R d , has a fully-connected layer, followed by two LSTM layers with 256 cells each, and a final output fully-connected layer. The output fully-connected layer takes the last hidden state from LSTM (h t+1 ) and outputs? t+1 after tanh(?) activation. Therefore, z t+1 = f LSTM (z t ). The training loss is given by</p><formula xml:id="formula_7">L LSTM = T t=1 z t+1 ?? t+1</formula><p>2 , where T are the total number of frames (both past and future) used for training. This simple dynamics encoder, inspired by <ref type="bibr" target="#b35">(Srivastava et al., 2015)</ref> and illustrated in <ref type="figure">Figure.</ref> 3 (stage 2), is effective and performs well on standard metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">GP TEMPORAL DYNAMICS ENCODER</head><p>Next, we want to learn the priors for potential future states by modeling the correlation between past and future states using a GP layer. Given a past state, this temporal dynamics encoder captures the distribution over future states. This enables us to use the predictive variance to decide when to sample diverse outputs, and provides us with a mechanism to sample diverse future states. The input to the GP layer is z t and the output is a mean and variance, which can be used to samplez t+1 . The loss function for the GP dynamics encoder follows from eq. 3, L GP = ?L svgp (z t+1 , z t ).</p><p>Unlike LSTM, the GP layer does not have hidden or transition states; it only models pair-wise correlations between past and future frames (illustrated in <ref type="figure">Figure.</ref> 3 (stage 2)). In this work, we use the automatic relevance determination (ARD) kernel, denoted by k(z, z ) = ? 2 ARD exp ?0.5 d j=1 ? j (z j ? z j ) 2 , parameterized by learnable parameters ? ARD and {? 1 , . . . , ? d }. This GP layer is implemented using GPyTorch (Gardner et al., 2018) (refer to ?3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">TRAINING OBJECTIVE</head><p>All three modules, frame auto-encoder and the LSTM and GP temporal encoders, are jointly trained using the following objective function:</p><formula xml:id="formula_8">L DVG = T t=1 Frame Auto-Encoder ? 1 L gen (x t ,x t ) + LSTM Frame Generation ? 2 L gen (x t , f gen (? t )) + GP Frame Generation ? 3 L gen (x t , f gen (z t )) + ? 4 L LSTM (z t+1 ,? t+1 ) LSTM Dynamics Encoder + ? 5 L GP (z t+1 , z t )</formula><p>GP Dynamics Encoder <ref type="formula">(4)</ref> where [? 1 , . . . , ? 5 ] are hyperparameters. There are three frame generation losses, each utilizing different latent code: z t from frame encoder,? t from LSTM encoder, andz t from GP encoder. In additional, there are two dynamics encoder losses, one each for LSTM and GP modules. Empirically, we observed that the model trains better with higher values for ? 1 , ? 2 , ? 4 , possibly because GP is only used to sample a diverse state and all other states are sampled from the LSTM encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">INFERENCE MODEL OF DIVERSE VIDEO GENERATOR (DVG)</head><p>During inference, we put together the three modules described above as follows. The output of the frame encoder z t is given as input to both LSTM and GP encoders. The LSTM outputs? t+1 and the GP outputs a mean and a variance. The variance of GP can be used to decide if we want to continue an on-going action or generate new diverse output, a process we call trigger switch. If we decide to stay with the on-going action, LSTM's output? t+1 is provided to the decoder to generate the next frame. If we decide to switch, we samplez t+1 from the GP and provide that as input to the decoder. This process is illustrated in <ref type="figure" target="#fig_0">Figure. 3 (stage 3)</ref>. The generated future frame is used as input to the encoder to output the next z t+1 ; this process is repeated till we want to generate frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">TRIGGER SWITCH HEURISTICS</head><p>An important decision for a diverse future generation is when to continue the current action and when to switch to a new action. We use the GP to switch to new states. We use two heuristics to decide when to generate diverse actions: a deterministic switch and a GP trigger switch. For the deterministic switch, we do not use the variance of the GP as a trigger, and switch every 15 frames. Each switch uses the sample from GP as the next future state. This enables us to have a consistent sampling strategy across generated samples. For the GP trigger switch, we compare the current GP variance with the mean of the variance of the last 10 states. If the current variance is larger than two standard deviations, we trigger a switch. This variable threshold allows the diverse video generator to trigger switches based on evidence, which can vary widely across samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Next, we describe the experimental setup, datasets we use, qualitative and quantitative results.</p><p>We evaluate our models on four datasets and compare it with the state-of-the-art baselines. All models use 5 frames as context (past) during training and learn to predict the next 10 frames. However, our model is not limited to generating just 10 frames. All our models are trained using Adam optimizer.</p><p>KTH Action Recognition Dataset. The KTH action dataset <ref type="bibr" target="#b32">(Schuldt et al., 2004)</ref> consists of video sequences of 25 people performing six different actions: walking, jogging, running, boxing, handwaving, and hand-clapping. The background is uniform, and a single person is performing actions in the foreground. Foreground motion of the person in the frame is fairly regular.</p><p>BAIR pushing Dataset. The BAIR robot pushing dataset <ref type="bibr" target="#b9">(Ebert et al., 2017)</ref> contains the videos of table mounted sawyer robotic arm pushing various objects around. The BAIR dataset consists of different actions given to the robotic arm to perform.</p><p>Human3.6M Dataset. Human3.6M <ref type="bibr" target="#b18">(Ionescu et al., 2014)</ref> dataset consists of 10 subjects performing 15 different actions. We did not use the pose information from the dataset.  <ref type="figure">Figure 4</ref>: LPIPS Quantitative Results on KTH, BAIR, and Human3.6M datasets. All methods use the best matching sample out of 100 random samples. We used fixed trigger heuristic to keep trigger point for each sample the same for our approach.</p><p>UCF Dataset. This dataset <ref type="bibr" target="#b34">(Soomro et al., 2012)</ref> consists of 13,320 videos belonging to 101 different action classes. We sub-sample a small dataset for qualitative evaluation on complex videos. Our subset consists of 7 classes: Bench press, Bodyweight squats, Clean and Jerk, Pull-ups, Push-ups, Shotput, Tennis-Swing, Lunges and Fencing. The background of this dataset can bias our diversity evaluation metric. Therefore, we only include this dataset only for qualitative evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">BASELINES</head><p>We compared our method with the following prior works. Further, wherever available, we used either the official implementation or the pre-trained models for the baselines uploaded by their respective authors.</p><p>SVG-LP <ref type="bibr" target="#b7">(Denton &amp; Fergus, 2018)</ref>: Stochastic Video Generation with a Learned Prior (SVG-LP) is a VAE-based method that uses a latent space prior for generating video sequences. It outperforms other VAE-based approaches (e.g., SV2P <ref type="bibr" target="#b1">(Babaeizadeh et al., 2017)</ref>). It also uses an LSTM-based dynamics model. This model similar to ours except that we use a GP to model the prior on future states to aid with multi-modal outputs.</p><p>SAVP :Stochastic Adversarial Video Prediction (SAVP) is a VAE-GAN based approach that combines the best of both families of approaches. It also uses the LSTM dynamics model.</p><p>Conditional VRNN <ref type="bibr" target="#b4">(Castrej?n et al., 2019)</ref>: Condition variational RNN leverages the flexibility of hierarchical latent variable models to increase the expressiveness of the latent space.</p><p>VidFlow <ref type="bibr" target="#b22">(Kumar et al., 2019)</ref>: VideoFlow model uses a normalizing flow approach that enables direct optimization of the data likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GP-LSTM:</head><p>We train a model inspired by <ref type="bibr" target="#b0">(Al-Shedivat et al., 2016)</ref>, where the dynamics model is a GP, which uses recurrent kernels modeled by an LSTM. This method is closest to ours since it utilizes the constructs of both GP and LSTM. However, they train a single dynamics model and have no way to control the generation of future states.</p><p>We refer to our model as Diverse Video Generator (DVG), which uses a GP trigger switch. We also study heuristic switching at 15 and 35 frames for ablation analysis. We provide additional ablation analysis in the supplementary material, for models with RNNs and GRUs instead of LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">METRICS</head><p>Evaluation of generated videos in itself is an open research problem with new emerging metrics.</p><p>In this work, we tried our best to cover all published metrics which have been used for evaluating future frame generation models.</p><p>Accuracy of Reconstruction. One way to evaluate a video generation model is to check how close the generated frames are to the ground-truth. Since the models are intended to be stochastic or diverse for variety in prediction, this is achieved by sampling a finite number of future sequences from a model and evaluating the similarity between the best matching sequence to the ground-truth and the ground truth sequence. Previous works <ref type="bibr" target="#b7">(Denton &amp; Fergus, 2018;</ref><ref type="bibr" target="#b1">Babaeizadeh et al., 2017;</ref> used traditional image reconstruction metrics, SSIM and PSNR, to measure the similarity between the generated samples and ground-truth. As shown by <ref type="bibr" target="#b8">Dosovitskiy &amp; Brox, 2016;</ref><ref type="bibr" target="#b20">Johnson et al., 2016;</ref><ref type="bibr" target="#b38">Unterthiner et al., 2018)</ref>, these metrics are not suited for video evaluation because of their susceptibility towards perturbation like blurring, structural distortion, etc.. We include these metrics in our supplementary material for the sake of completeness. We also evaluate the similarity of our generated sequences using recently proposed perceptual metrics, VGG cosine similarity (LPIPS), and Frechet Video Distance (FVD) score. LPIPS or Learned Perceptual Image Patch Similarity is a metric developed to quantify the perceptual distance between two images using deep features. Several works <ref type="bibr" target="#b8">Dosovitskiy &amp; Brox, 2016;</ref><ref type="bibr" target="#b20">Johnson et al., 2016)</ref> show that this metric is much more robust to perturbation like distortion, blurriness, warping, color shift, lightness shift, etc. Frechet Video Distance (FVD score) <ref type="bibr" target="#b38">(Unterthiner et al., 2018)</ref> is a deep metric designed to evaluate the generated video sequences. As is standard practice, all methods use the best matching sample out of 100 randomly generated samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diversity of Sequences.</head><p>Reconstruction accuracy of generated samples only implies that there is at least one generated sequence close to the ground-truth. However, these metrics do not capture the inherent multi-modal nature of the task. Aside from being able to generate samples close to ground truth, a video generation model should be able to represent diversity in its generated video sequences. Therefore, we propose a metric inspired by <ref type="bibr" target="#b40">(Villegas et al., 2017b</ref>) that utilizes a video classifier to quantify the diversity of generated sequences. The action classifier is trained on the respective datasets (KTH and Humans3.6M). Note that we cannot utilize this metric for the BAIR dataset since we do not have corresponding action labels.</p><p>For the diversity metric, we use 500 starting sequences of 5 frames, sample 50 future sequences of 40 frames. We ignore the first 5 generated frames since they are likely correlated with the groundtruth and correspond to the on-going sequences. Then, we evaluate the next two clips, made of frames <ref type="bibr">[10,</ref><ref type="bibr">25]</ref> and <ref type="bibr">[25,</ref><ref type="bibr">40]</ref>. Ideally, a method that can generate diverse sequences will generate more diverse clips as time progresses. For the Diversity Score, we compute the mean number of generated clips that changed from the on-going action as classified by the classifier. More concretely, if N is the total number of generated clips (N = 25000 for us ), c i is the ground-truth class, c i is the predicted class, and 1(?) is the indicator function if the the parameter is correct, then Diversity Score = 1 N N 1(c i =? i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">QUANTITATIVE RESULTS (RECONSTRUCTION)</head><p>We report the quantitative results on KTH, BAIR, and Human3.6M datasets using the LPIPS metric in <ref type="figure">Figure.</ref> 4, and FVD metric in <ref type="table" target="#tab_2">Table 1</ref>. For comparisons with baselines in <ref type="figure">Figure.</ref> 4, we see that on the KTH and Human3.6M dataset, our approach generally performs on-par or better than the baselines. In fact, except for SAVP, all methods are very close to each other. For the Human3.6M dataset, the GP-LSTM baseline performs poorly, and all other methods are similar, with ours being better than others. On the BAIR dataset, we notice that our GP-LSTM baseline performs better, closely followed by our approach. Again, SAVP performs worse on all metrics. For the FVD metric <ref type="table" target="#tab_2">(Table 1)</ref>, variants of our approach achieve state-of-the-art results on all datasets. Note that using a fixed trigger at frames 15 and 35 leads to better FVD scores for KTH and Human3.6M dataset, while GP trigger performs better for the BAIR dataset. All settings, except one, of our approach perform better than the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">QUANTITATIVE RESULTS (DIVERSITY)</head><p>We report the quantitative results using the proposed diversity score in <ref type="table" target="#tab_2">Table 1</ref>. We notice for the KTH dataset that SVG-LP/SAVP baseline change actions 20.1%/26.6% of the time in the first clip and 21.2%/24.5% of the time in the second clip. In comparison, our approach gives the diversity score of 48.53% and 48.10% for the first and second clips, respectively. As can be observed, the GP trigger results in considerably higher diversity as the sequence progresses. On the Human3.6M dataset, the difference between the scores of baselines and our methods is ?6%. The overall score drop between the KTH and the Human3.6M datasets on diversity metric can be accounted to actions performed in the videos are very distinct. Besides, cameras are placed far off from the person performing actions making it harder for the models to generate diverse samples.</p><p>We further analyze common action triggers for the GP trigger and notice that it separates the moving (walk, jog, run) and still (clap, wave, box) actions, and common action switches are within each cluster; e.g., walk ? jog, wave ? clap. More analysis is provided in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">QUALITATIVE RESULTS</head><p>Qualitative Results are shown in <ref type="figure">Figure.</ref> 5. For KTH results in <ref type="figure">Figure.</ref> 5, we plot a randomly selected sample for all methods. As we can see, SAVP and SVG-LP output average or blurry images after 20-30 frames, and our method is able to switch between action classes (for diverse sample using GP trigger). In supplementary, we show results on KTH with more than 100 sampled frames and best matching samples for baselines and ours. For the BAIR dataset, we show the best LPIPS results for all approaches; where we can see that our method generates samples much closer to the ground-truth. We also included a random sample with a fixed trigger at 15 th frame, where we can see a change in the action. For the Human3.6M dataset (after digital zoom), we can see that our best LPIPS sample matches the ground-truth person's pose closely as opposed to SVG-LP, demonstrating the effectiveness of our approach. Similar results are observed for the UCF example. Note that due to manuscript length constraints, we have provided more qualitative results in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We propose a method for diverse future video generation. We model the diversity in the potential future states using a GP, which maintains priors on future states given the past and a probability distribution over possible futures given a particular sample. Since this distribution changes with more evidence, we leverage its variance to estimate when to generate from an on-going action and when to switch to a new and diverse future state. We achieve state-of-the-art results for both reconstruction quality and diversity of generated sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 ABLATION STUDIES FOR TEMPORAL DYNAMICS ENCODER</head><p>We perform ablation studies on our model by trying different variants of recurrent modules for our temporal dynamics encoder networks. These models are: DVG-RNN, our model with an RNN dynamics encoder; DVG-GRU, with an RNN dynamics encoder with GRU units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure.</head><p>A.1 shows ablation analysis for different variants of our approach. On the KTH dataset, different dynamics models (RNN, GRU, LSTM) all perform the same. On the BAIR dataset, RNN perform poorly and LSTM performs the best among the three. On Human3.6M dataset RNN performs higher than our LSTM and GRU models. On the FVD metric in  <ref type="figure">Figure A</ref>.1: Ablation results on KTH, Human3.6M and BAIR dataset using variants of temporal dynamics model in our method. We report best LPIPS metric. All methods use the best matching sample out of 100 random samples. We used fixed trigger to keep trigger point for each sample the same. On KTH, all temporal dynamics models have similar performance; and on BAIR, our LSTM model have best performance.  KTH action dataset comprises of 6 action classes: walking, running, jogging, waving, clapping, and boxing. On an abstract level, we can cluster these actions into two categories moving actions and still actions. From the <ref type="figure">Figure.</ref> A.2, it is interesting to observe that our GP triggering model captures the future trajectories of the videos and clusters them into these two categories moving actions and still actions. Common action switches that are to be expected can be observed from the <ref type="figure">Figure.</ref> A.2; for example, walk and jog, wave and clap interchange frequently after triggering. Still actions seldom change to moving actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 SSIM AND PSNR RESULTS</head><p>We evaluated our generated video sequences using the tradition metrics like structural similarity index (SSIM) and peak signal-to-noise ratio (PSNR) for comparison with previous baselines which reported these metrics. We trained all models on 64 ? 64-size frames from the KTH, Human3.6M, and BAIR datasets. We used the standard training practice of using 5 frames as context (or past) and the model have to predict the next 10 frames. For all methods, SSIM and PSNR is computed by drawing 100 samples from the model for each test sequence and picking the best score with respect to the ground truth. We emphasize that these results are only for completeness and we hope that the community will stop relying on such reconstruction metrics for video prediction.</p><p>Results are reported in <ref type="figure">Figure.</ref> A.3 represent the evaluation plots for traditional metrics on KTH, BAIR, and Human3.6M dataset. We follow the experimental setups from the baseline papers.  <ref type="figure">Figure A</ref>.3: Quantitative results on KTH, BAIR and Human3.6M dataset. We report average SSIM and PSNR metrics. All methods use the best matching sample out of 100 random samples. We used fixed trigger to keep trigger point for each sample the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 QUALITATIVE RESULTS</head><p>It can be observed from <ref type="figure">Figure.</ref> A.4 that after 15th frame SVG-LP is stuck in the same pose while after 35th frame SAVP starts distorting the human. However, our method (DVG) consistently generates frames that are diverse and distortion free for longer period of time. Similarly, in <ref type="figure">Figure.</ref> A.9 it can be observed that after 30th frame SVG-LP and SAVP start generating subpar frames while our method is able to generate visually acceptable sequences for longer term. Few additional qualitative results on the BAIR dataset are provided in <ref type="figure" target="#fig_1">Figures. A.5</ref>  <ref type="table" target="#tab_2">GT   T = 5  10  15  100  105  85  75  65  55  45  35  25  95  20  30  40  50  60  70</ref>  First row is the ground-truth video (with last frame of the provided 5 frames is shown)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 GAUSSIAN LAYER SPECIFICS</head><p>As mentioned in the paper, GPytorch was used for our GP layer implementation. We utilized a large-scale variational GP implementation of GPytorch for our multi-dimensional GP regression problem of learning to predict the variance over the future frames in the latent space. For variational GP implementation, 40 inducing points were randomly initialized and learned during the training of GP. We used a RBF kernel along with gaussian likelihood for our GP layer. For optimization of our GPLayer, we employed stochastic optimization technique (Adam optimizer) to minimize the variational ELBO for a GP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 I3D NETWORK ARCHITECTURE FOR ACTION CLASSIFIER</head><p>For our diversity metric mentioned in ?5, we utilized the standard kinetics-pretrained I3D action recognition classifier. The input to the action classifier is a 15 frames clip and each frame has a size of 64 ? 64. The action classifier attains accuracy close to 100% for KTH dataset and is above 90% accuracy for human3.6m dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>An overview of the proposed Diverse Video Generator (DVG).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative Results on BAIR (left), KTH (center), Human3.6M (right, top), and UCF (right, bottom) datasets. First row is the ground-truth video in each figure (with the last frame of the provided 5 frames is shown as 'GT'). Every 5 th frame is shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure A. 2 :</head><label>2</label><figDesc>Changes in action from past frames to future frames on KTH dataset. Total of 25,000 generated videos were used to calculate percentage change shown in the above figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>80 90 Figure A. 4 :Figure A. 9 :</head><label>9049</label><figDesc>KTH dataset: Qualitative comparison of the generated video sequences (every 5 th frame shown). First row is the ground-truth video (with last frame of the provided 5 frames is shown) Qualitative results on BAIR dataset. We show the best LPIPS samples out of 100 samples for all methods. Qualitative results on BAIR dataset. We show the best LPIPS samples out of 100 samples for all methods. 7: Human3.6M dataset: Qualitative comparison of the generated video sequences (every 5 th frame shown). First row is the ground-truth video (with last frame of the provided 5 frames is shown) 8: Human3.6M dataset: Qualitative comparison of the generated video sequences (every 5 th frame shown). First row is the ground-truth video (with last frame of the provided 5 frames is shown) KTH dataset: Qualitative comparison of the generated video sequences (every 5 th frame shown).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Original Sequence Modified Sequence Modified Sequence</head><label></label><figDesc></figDesc><table><row><cell>New Sequence Trigger</cell><cell></cell><cell cols="2">Sequence Complete,</cell><cell></cell><cell></cell></row><row><cell>(Future Variance High)</cell><cell></cell><cell cols="2">New Sequence Trigger (Future Variance High)</cell><cell></cell><cell>New Sequence Triggered Sequence On-going</cell><cell>Modified</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sequence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>New Action Sequence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Potential</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Diverse</cell><cell></cell></row><row><cell>T</cell><cell></cell><cell cols="2">Sequence on-going (Future Variance Low)</cell><cell>T Sample</cell><cell>Original Action Sequence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Continued On-going Sequence</cell></row><row><cell>t</cell><cell>t</cell><cell>t+2</cell><cell>t+k</cell><cell>t+n</cell><cell>T</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results on KTH, BAIR, Human3.6M datasets. For the FVD Score, all methods use the best matching sample out of 100 random samples and lower numbers are better. For the Diversity Score, we compute the score across 50 generated samples, for 500 starting sequences, and higher numbers are better.</figDesc><table><row><cell>Model</cell><cell>Trigger</cell><cell></cell><cell cols="2">FVD Score (?)</cell><cell cols="2">Diversity Score (?) (frames: [10,25])</cell><cell cols="2">Diversity Score (?) (frames: [25,40])</cell></row><row><cell></cell><cell></cell><cell>KTH</cell><cell>BAIR</cell><cell>Human3.6M</cell><cell>KTH</cell><cell>Human3.6M</cell><cell>KTH</cell><cell>Human3.6M</cell></row><row><cell>SVG-LP</cell><cell>-</cell><cell>156.35</cell><cell>270.04</cell><cell>718.04</cell><cell>20.10</cell><cell>4.8</cell><cell>21.20</cell><cell>4.6</cell></row><row><cell>SAVP</cell><cell>-</cell><cell>65.98</cell><cell>126.75</cell><cell>-</cell><cell>26.60</cell><cell>-</cell><cell>24.50</cell><cell>-</cell></row><row><cell>GP-LSTM</cell><cell>-</cell><cell>92.34</cell><cell>197.49</cell><cell>604.75</cell><cell>31.40</cell><cell>5.4</cell><cell>30.90</cell><cell>6.0</cell></row><row><cell>VidFlow</cell><cell>-</cell><cell>-</cell><cell>124.81</cell><cell>-</cell><cell>. -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VRNN</cell><cell>-</cell><cell>67.26</cell><cell>134.81</cell><cell>523.45</cell><cell>32.50</cell><cell>5.6</cell><cell>31.80</cell><cell>5.9</cell></row><row><cell>DVG [ours]</cell><cell>@15,35</cell><cell>65.69</cell><cell>123.08</cell><cell>479.43</cell><cell>48.30</cell><cell>9.3</cell><cell>46.20</cell><cell>9.0</cell></row><row><cell>DVG [ours]</cell><cell>GP</cell><cell>69.63</cell><cell>120.03</cell><cell>496.89</cell><cell>47.71</cell><cell>10.8</cell><cell>48.10</cell><cell>10.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">.1, all variants of our</cell></row><row><cell cols="16">approach perform better than all baselines. In approaches, GRU dynamics model performs better on</cell></row><row><cell cols="11">KTH and LSTM performs better on Human3.6M and BAIR dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Average LPIPS on KTH</cell><cell></cell><cell></cell><cell cols="4">Average LPIPS on BAIR</cell><cell></cell><cell cols="5">Average LPIPS on HUMAN 3.6M</cell></row><row><cell>0.900 0.925 0.950</cell><cell></cell><cell></cell><cell>Ours RNN Ours GRU Ours LSTM</cell><cell>0.88 0.90 0.92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours RNN Ours GRU Ours LSTM</cell><cell>0.985 0.980</cell><cell></cell><cell></cell><cell></cell><cell>Ours RNN Ours GRU Ours LSTM</cell><cell></cell></row><row><cell>0.875</cell><cell></cell><cell></cell><cell></cell><cell>0.86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.975</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.850</cell><cell></cell><cell></cell><cell></cell><cell>0.84</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.970</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.825</cell><cell></cell><cell></cell><cell></cell><cell>0.82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.965</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.800</cell><cell></cell><cell></cell><cell></cell><cell>0.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.960</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.955</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell></row><row><cell></cell><cell></cell><cell>Frames #</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Frames #</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Frames #</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table A .</head><label>A</label><figDesc>1: Quantitative results on KTH, BAIR, Human3.6M datasets. For the FVD Score, all the ablation methods use the best matching sample out of 100 random samples and lower numbers are better. For the Diversity Score, we compute the score across 50 generated samples, for 500 starting sequences, and higher numbers are better.</figDesc><table><row><cell>Model</cell><cell>Dynamics</cell><cell>Trigger</cell><cell></cell><cell cols="2">FVD Score (?)</cell><cell cols="4">Diversity Score (?) (frames: [10,25])</cell><cell>Diversity Score (?) (frames: [25,40])</cell></row><row><cell></cell><cell></cell><cell></cell><cell>KTH</cell><cell>BAIR</cell><cell>Human3.6M</cell><cell>KTH</cell><cell cols="3">Human3.6M</cell><cell>KTH</cell><cell>Human3.6M</cell></row><row><cell>DVG [ours]</cell><cell>LSTM</cell><cell>@15,35</cell><cell>65.69</cell><cell>123.08</cell><cell>479.43</cell><cell>48.30</cell><cell></cell><cell>9.3</cell><cell></cell><cell>46.20</cell><cell>9.0</cell></row><row><cell>DVG [ours]</cell><cell>GRU</cell><cell>@15,35</cell><cell>64.89</cell><cell>124.38</cell><cell>485.96</cell><cell>48.53</cell><cell></cell><cell>8.5</cell><cell></cell><cell>44.23</cell><cell>9.1</cell></row><row><cell>DVG [ours]</cell><cell>RNN</cell><cell>@15,35</cell><cell>66.84</cell><cell>126.07</cell><cell>503.64</cell><cell>46.60</cell><cell></cell><cell>7.6</cell><cell></cell><cell>41.50</cell><cell>8.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>box</cell><cell cols="2">86 2.9 3.3 2.3 2.4 3.4</cell></row><row><cell cols="8">A.2 ANALYSIS: CHANGES IN ACTION AFTER GP TRIGGERING</cell><cell>clap wave jog run Past frames</cell><cell cols="2">2.6 74 19 0.77 2.3 0.85 7.1 27 60 1.7 2 2.1 0.73 0.25 0.27 25 68 5.8 0.24 0 0.24 25 72 3.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>walk</cell><cell cols="2">Future -&gt; box clap wave jog run walk 0.63 0.084 0.42 31 60 7.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>-A.6, and on the Human3.6M dataset in Figures. A.7-A.8.</figDesc><table><row><cell>SAVP</cell></row><row><cell>SVG-LP</cell></row><row><cell>Ours</cell></row><row><cell>Best Matching</cell></row><row><cell>Ours</cell></row><row><cell>Diverse Sample1</cell></row><row><cell>Ours</cell></row><row><cell>Diverse Sample2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was partially funded by independent grants from Facebook AI, Office of Naval Research (N000141612713), and Defense Advanced Research Projects Agency (DARPA) SAIL-ON program (W911NF2020009). We also thank Harsh Shrivastava, Pulkit Kumar, Max Ehrlich, and Pallabi Ghosh for providing feedback on the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning scalable deep kernels with recurrent structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maruan</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunus</forename><surname>Saatchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accurate and diverse sampling of sequences based on a &quot;best of many&quot; sample objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apratim</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00885</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2018.00885.3" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep video generation, prediction and completion of human action sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<idno>1611-3349. 3</idno>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="374" to="390" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improved conditional vrnns for video prediction. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llu?s</forename><surname>Castrej?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.12165.3" />
		<imprint>
			<date type="published" when="1904" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Cricri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Aksu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moncef</forename><surname>Gabbouj</surname></persName>
		</author>
		<idno>abs/1612.01756</idno>
		<ptr target="http://arxiv.org/abs/1612.01756.3" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gaussian process models with parallelization and gpu acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwen</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Stochastic video generation with a learned prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Self-supervised visual planning with temporal skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reduced-gate convolutional lstm architecture for nextframe video prediction using predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Maida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bayoumi</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2019.8852480.3</idno>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Distributed variational inference in sparse gaussian process regression and latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Van Der Wilk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Disentangling propagation and generation for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1812.00452</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bindel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial networks</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple-step prediction using a two stage gaussian process model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Havlak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACC.2014.6859020.3</idno>
	</analytic>
	<monogr>
		<title level="m">2014 American Control Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3443" to="3449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Gaussian processes for big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolo</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel adversarial inference framework for video prediction with action control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>1611-3349</idno>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="694" to="711" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes. CoRR, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6114</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Videoflow: A conditional flow-based model for stochastic video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Durk</forename><surname>Kingma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Stochastic adversarial video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01523</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Flexible spatio-temporal networks for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaochao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d human motion tracking using dynamic probabilistic latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<idno type="DOI">10.1109/CRV.2008.45.3</idno>
	</analytic>
	<monogr>
		<title level="j">Canadian Conference on Computer and Robot Vision</title>
		<imprint>
			<biblScope unit="page" from="155" to="162" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Folded recurrent neural networks for future video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Oliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Selva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
		<idno>abs/1712.00311</idno>
		<ptr target="http://arxiv.org/abs/1712.00311" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.308</idno>
		<ptr target="http://dx.doi.org/10.1109/ICCV.2017.308" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Complex wavelet structural similarity: A new image similarity index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Sampat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Markey</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2009.2025923.2</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2385" to="2401" />
			<date type="published" when="2009-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local svm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPR.2004.1334462.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Pattern Recognition</title>
		<meeting>the 17th International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004-08" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3045118.3045209.2" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Variational learning of inducing variables in sparse gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Titsias</surname></persName>
		</author>
		<idno>PMLR. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics</title>
		<editor>David van Dyk and Max Welling</editor>
		<meeting>the Twelth International Conference on Artificial Intelligence and Statistics<address><addrLine>Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-04" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="16" to="18" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mocogan: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00165</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2018.00165" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Towards accurate generative models of video: A new metric and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Decomposing motion and content for natural video sequence prediction. CoRR, abs/1706.08033</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.08033.3" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning to generate long-term future via hierarchical prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungryull</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">High fidelity video prediction with large stochastic recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkanath</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harini</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generating the future with adversarial transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/1609.02612</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Patch to the future: Unsupervised visual prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3302" to="3309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The pose knows: Video forecasting by generating pose futures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2007.1167.3</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="298" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Eidetic 3d LSTM: A model for video prediction and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1lKS2AqtX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Hierarchical long-term video prediction without supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevan</forename><surname>Wichers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Kernel interpolation for scalable structured gaussian processes (kiss-gp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Nickisch</surname></persName>
		</author>
		<idno>2015. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Thoughts on massively scalable gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Andrew Gordon Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Dann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nickisch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Stochastic variational deep kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Andrew Gordon Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">L</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A data-driven approach for event prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00068</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2018.00068.2,8" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
