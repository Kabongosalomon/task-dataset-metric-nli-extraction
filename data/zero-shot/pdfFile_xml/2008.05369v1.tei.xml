<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anomaly localization by modeling perceptual features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dehaene</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">AnotherBrain</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Eline</surname></persName>
							<email>pierre@anotherbrain.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">AnotherBrain</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Anomaly localization by modeling perceptual features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although unsupervised generative modeling of an image dataset using a Variational AutoEncoder (VAE) has been used to detect anomalous images, or anomalous regions in images, recent works have shown that this method often identifies images or regions that do not concur with human perception, even questioning the usability of generative models for robust anomaly detection. Here, we argue that those issues can emerge from having a simplistic model of the anomaly distribution and we propose a new VAE-based model expressing a more complex anomaly model that is also closer to human perception. This Feature-Augmented VAE is trained by not only reconstructing the input image in pixel space, but also in several different feature spaces, which are computed by a convolutional neural network trained beforehand on a large image dataset. It achieves clear improvement over state-of-the-art methods on the MVTec anomaly detection and localization datasets. * Equal contributions.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The use of large databases of labeled data has been extensively leveraged in the past decade to automate industrial tasks such as digitalization of handwritten letters or identification of cancerous tissue. In the domain of quality control, in which a given image is either identified as normal or anomalous, labels would need to cover the entirety of the possible anomalies range (stain, crack, ...) for such an approach to be used on a production line. When anomalies are rare, the constitution of such an exhaustive dataset can be time-consuming. The unsupervised anomaly detection setting is a two-class classification task in which the training set is only comprised of non-defective samples, from the normal class, whereas samples from the second, anomalous class are only given at test time. For quality control, this task can also be extended to a pixel-wise segmentation of anomalous regions on otherwise normal samples, which we will call anomaly localization.</p><p>Variational AutoEncoders (VAE, Kingma and Welling <ref type="bibr" target="#b0">[1]</ref>) are especially good models for this task, since they approximately model the probability density of the distribution of their training dataset. Anomaly detection can then be seen as thresholding the approximated sample probability: a sample with low probability under the evaluated normal data distribution is seen as an anomaly. Moreover, they are also useful for anomaly localization. Indeed, autoencoders learn to reconstruct training samples after compressing them to a low dimensional space, training encoder (compressive) and decoder (decompressive) models jointly with a distance-based metric between an original sample and its reconstruction. When used on an anomalous sample, a VAE will reconstruct correctly normal regions of the image, whereas anomalous regions may contain never-before-seen features that the decoder will struggle to imitate. A number of works thus compared the input sample with its reconstruction pixel-wise to localize anomalies in an image <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>.</p><p>Nevertheless, on the one hand Nalisnick et al. <ref type="bibr" target="#b5">[6]</ref> questioned the feasibility of detecting anomalies using generative models, showing that a VAE trained on the CIFAR10 dataset <ref type="bibr" target="#b6">[7]</ref> assigned higher probability values to samples from the fairly different SVHN dataset <ref type="bibr" target="#b7">[8]</ref> than to test samples from CIFAR10. On the other hand, Bergmann et al. <ref type="bibr" target="#b8">[9]</ref> identified issues of accordance with human perception of anomalies with a pixel-wise reconstruction error, and used the Structural SIMilarity (SSIM, Wang et al. <ref type="bibr" target="#b9">[10]</ref>) to identify larger zones of perceptual anomalies.</p><p>In this paper, we argue that these two problems are linked and we contribute two new limitations of the standard reconstruction based framework:</p><p>? Training on normal data only can make the model "blind" to anomalous features present in defective data whereas humans are very sensitive to some of those features. ? High likelihood of a sample under the normal data distribution does not necessarily mean high probability of being normal. A comparison with the likelihood of the sample under the (unknown) anomalous data distribution is required.</p><p>We illustrate those limitations with a toy dataset. By leveraging pre-trained networks on large image datasets, we contribute a new reconstruction-based model on both pixel and perceptual features spaces. We demonstrate that it outperforms state-of-the-art results on all subsets of the MVTec AD dataset <ref type="bibr" target="#b1">[2]</ref>.</p><p>2 Related work 2.1 Anomaly localization using generative models Generative models are trained to approximate the distribution of the training set by learning to generate samples from that distribution. Generative Adversarial Networks (GAN, Goodfellow et al. <ref type="bibr" target="#b10">[11]</ref>) do so by jointly training a generator network to generate credible samples from noise, and a discriminator network to classify the samples as either generated or real. The two models implicitly define the training data manifold as the union of samples that the generator can generate and samples that the discriminator labels as real. Schlegl et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> use GAN for anomaly detection and localization by searching for the closest neighbor of a given sample on the manifold. They compare the sample with this normal closest neighbor to identify the presence and location of an anomaly.</p><p>We will note ? ? (z) the mean of the decoded distribution p ? (x|z) and z ? (x) the mean of the encoded distribution q ? (z|x). The VAE reconstruction of a given sample x is defined as r ?,? (x) = ? ? (z ? (x)). As with GAN, VAE are mainly used for anomaly detection and localization by comparing a sample and its reconstruction <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Bergmann et al. <ref type="bibr" target="#b1">[2]</ref> show that VAE achieves better results than GAN on a variety of anomaly localization datasets. Indeed, it is a known GAN flaw <ref type="bibr" target="#b13">[14]</ref> that even though they typically generate images of better quality than VAE, they have a tendency to only be able to generate a subset of the training distribution. This is especially problematic for anomaly detection, in which a whole subset of the normal distribution could be flagged as anomalous.</p><p>An and Cho <ref type="bibr" target="#b2">[3]</ref> proposed the ELBO as an anomaly score to classify samples. Matsubara et al. <ref type="bibr" target="#b4">[5]</ref> used only the reconstruction term of the ELBO to localize anomalies in images. Baur et al. <ref type="bibr" target="#b3">[4]</ref> used several variants of VAE and GAN like architectures to localize the differences between a sample and its reconstruction. Bergmann et al. <ref type="bibr" target="#b8">[9]</ref> replaced the pixel-wise L2 reconstruction error with the SSIM <ref type="bibr" target="#b9">[10]</ref>, then Bergmann et al. <ref type="bibr" target="#b1">[2]</ref> showed that autoencoder-based models were the best architectures for anomaly localization on the MVTec AD dataset, an anomaly localization dataset that they submitted, but also that depending on the dataset, standard autoencoders could achieve better performance than SSIM autoencoders. Dehaene et al. <ref type="bibr" target="#b14">[15]</ref> showed that a local image anomaly could deteriorate the VAE reconstruction of a whole image, reducing the effectiveness of reconstructionbased anomaly localization, and proposed a gradient-based method similar to AnoGAN to refine anomaly segmentations given by autoencoder-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Anomaly detection using transfer learning</head><p>In anomaly detection, the training dataset is only comprised of normal samples. Some features that are present during test time were never seen by the model during training but may be crucial for normal/abnormal classification. The transfer of discriminative features from pre-trained models, while common in computer vision, has only recently been used for anomaly localization.</p><p>Napoletano et al. <ref type="bibr" target="#b15">[16]</ref> used the distance to the nearest neighbor in a set of trained normal prototypes in a pre-trained feature space as an anomaly score. Nazare et al. <ref type="bibr" target="#b16">[17]</ref> modeled these feature distributions using K-Means. They both used high-level features in their pre-trained models, so that to use their models for anomaly localization, one has to treat images as overlapping patches. Sabokrou et al. <ref type="bibr" target="#b17">[18]</ref> enabled fast localization by using the convolutional features of a pre-trained network and modeled them as one-dimensional normal distributions. Andrews et al. <ref type="bibr" target="#b18">[19]</ref>, Burlina et al. <ref type="bibr" target="#b19">[20]</ref> used the features extracted from pre-trained models to train a One-Class Support Vector Machine (OC SVM) to achieve better results than a pixel-wise OC SVM for anomaly detection.</p><p>Very recently, Bergmann et al. <ref type="bibr" target="#b20">[21]</ref> compared those previous models as well as autoencoder reconstruction-based methods. They also proposed a method based on an ensemble of regressors, trained to imitate a pre-trained CNN (the teacher) on the training dataset. The regressors are supposed to reproduce the teacher's features well on the training dataset, but to diverge from the teacher, as well as from the other regressors, on anomalous patches, so that an anomaly map can be derived from the ensemble.</p><p>In this paper, we merge the transfer learning approach with the anomaly localization framework given by the VAE.</p><p>3 Implicit biases in previous approaches</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The example distribution</head><p>To understand the problem we are trying to tackle, in this section we will study a toy dataset of grayscale images. Formal equations defining the normal and anomalous distributions are in Appendix 1, and samples from the three distributions are shown in figure 1 (a). Normal images are composed of pixel-wise white noise on a random monochrome background. This is a very simple latent variable model that a VAE can learn to approximate perfectly, so that D KL (q n (x) p ? (x)) = 0 (Proof in Appendix 2). Anomalous images are composed of a background of horizontal stripes, with added white noise of an amplitude smaller than the noise amplitude of the normal data distribution. Finally we introduce a third distribution, which we call the shuffled anomaly distribution. To sample from this distribution, we first sample from the anomaly distribution, then we randomly shuffle the pixels in the sample.</p><p>We use a VAE with the decoder variance ? trained as a global parameter: p ? (x|z) ? N (? ? (z), ? 2 ) as described in Dai and Wipf <ref type="bibr" target="#b21">[22]</ref>. Even though it is obvious for the human reader which samples are anomalies, we can see in figure 1 (b) that in this setting the VAE affects higher likelihoods to anomalous samples than to normal samples. This is even more surprising when we note that the normal distribution is simple enough for the VAE to model perfectly. The result of figure 1 (b) is in fact inherent to the two chosen distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Typical set score</head><p>In this subsection we show that previous approaches solely based on statistical tests are not sufficient if they do not align with human perception. Nalisnick et al. <ref type="bibr" target="#b22">[23]</ref>, Choi et al. <ref type="bibr" target="#b23">[24]</ref> studied a similar thought experiment as in section 3.1, with constant ? and f = 0, and proposed a typicality test based on the notion of typical sets as a solution.</p><p>Given a set x 1 ..., x N of samples to classify,</p><formula xml:id="formula_0">?| 1 N log p ? (x 1 , ..., x N ) + H[p ? (x)]</formula><p>| is proposed as an anomaly score for the set. The intuition behind the use of typical sets is to accumulate evidence over several independent realizations of a random process to determine if the random process follows a given distribution. The drawback of this accumulation is that the score is no more associated with a single sample, but to the whole set. To use this idea, while still getting a single-image analysis, we will consider the realizations k = [ x k ? ? ? (z) k ] over all pixels k, which are i.i.d. with distribution N (0, ? 2 ), to accumulate evidence across all pixels and generate an image-wide diagnostic:</p><formula xml:id="formula_1">Typical score = ?| 1 d d k=1 log p ? ([ x k ? ? ? (z) k ]|z) + H[N (0, ? 2 )]|<label>(2)</label></formula><p>We can see in figure 1 (c) that the test indeed reorders the normal and anomaly distribution in accordance to human perception. Nevertheless, since the typicality test is invariant to a shuffle in pixel ordering, it cannot separate the shuffled anomaly distribution and the anomaly distribution. This can be problematic because for a human assessor, samples from the shuffled anomaly distribution are similar to normal samples. This statistical test, while theoretically sound to reject atypical events, is not able to yield the perceptive ordering of the three distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implicit models of anomalies</head><p>How can we have a perfect approximation of the normal distribution and reject different abnormal samples than a human on such a simple case ? What seems to be a paradox here should not really be one: it is not generally possible to discriminate normal from anomalous data using only the normal distribution's density q n (x) = q(x|n), since we need to compare it to the abnormal distribution's density q a (x) = q(x|a). The optimal normal/abnormal classifier C * a (x) uses both distributions:</p><formula xml:id="formula_2">C * a (x) = q(a|x) q(n|x) = q a (x)q(a) q n (x)q(n)<label>(3)</label></formula><p>with q(a) = 1 ? q(n) = x q(a|x). Here we will explicit the anomaly models implicitly used by classic approaches in order to introduce our anomaly model.</p><p>The standard VAE approach to anomaly localization is only based on the quality of the pixel-wise reconstruction: one classically defines a pixel-wise anomaly classifier C a k (x) using a threshold T so that</p><formula xml:id="formula_3">?k, (C a k (x) &gt; 0.5) ?? (p ? (x k |z ? (x)) &lt; T )<label>(4)</label></formula><p>This can explicitly be obtained (in Appendix 3) by seeing the tested image as the composition of a sample from a more restrictive normal distribution ? ? (z) and white pixel noise e from (for example) a Gaussian distribution. This restrictive normal distribution is obtained from deterministically decoding latent variables and assigning them the mean of the decoded distribution. This gives us an anomalous sample x a as:</p><formula xml:id="formula_4">x a = ? ? (z) + e z ? p(z), e ? p a (e) ? N (0, ? 2 a )<label>(5)</label></formula><p>with ? a a constant.</p><p>This models anomalies as independent noise over the pixels, which is not a very informative prior for image analysis.</p><p>4 Proposed method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Transferable features for anomaly modeling</head><p>We want to modify the anomaly model from equation 12 to model more complex and perceptual anomalies. This may be done by transforming the anomalies in a feature space in which we could model our test images as</p><formula xml:id="formula_5">x a = x n + e f (x a ) = f (x n ) + e f x n ? q n (x n ), e f ? N (0, ? f a )<label>(6)</label></formula><p>with f some transformation of the pixels.</p><p>It is now the features of the image that are disturbed with white noise in an abstract feature space. We nevertheless have to find the right feature space f (R d ) so that this model is useful for the detection of anomalies. We characterize its usefulness with the following properties:</p><p>? It should be local to enable a precise localization of anomalies.</p><p>? It should be robust to a large variety of possible anomalies, of different sizes and complexities. ? It should be as close as possible to the human perception, i.e. enable the detection and localization of the same kind of anomalies as human assessors.</p><p>Concerning the last point, convolutional neural networks trained beforehand on large image datasets are often used to define similarity metrics that are close to human perception <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, so their feature maps are great answers to that problem. To make the localization robust and precise, we will use L feature maps, taken at different layers of a pre-trained CNN, such as VGG16 <ref type="bibr" target="#b26">[27]</ref>, that we will call the feature extractor. Going up through the layers, we will trade precision in the localization for a more complex structure of the anomalies. Now that we have selected our f transforms, we need to have a model of the normal distribution in these feature spaces, an approximation of the f (x n ) term in equation 6. This can be done as in equation 12, using VAEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Augmented VAE</head><p>For each feature extractor layer f i , i ? [1.</p><p>.L] we use a VAE to approximate its distribution on the normal dataset:</p><formula xml:id="formula_6">E qn(x) log p ?0 (x) ? ELBO ?0,?0 (x) ?i ? [1..L], E qn(x) log p ?i (y i ) ? ELBO ?i,?i (y i ) with y i = f i (x)<label>(7)</label></formula><p>Each channel of each feature space is given an independent decoder variance parameter ? 2 i,c , which is trained globally and is independent from the latent space.</p><p>To reduce the number of parameters used by the model, we propose to use only one encoder and decoder for all the features and pixels to reconstruct. The features of higher level in the feature extractor will be decoded in early layers of the decoder, whereas its output remains a distribution over the pixels. A 1x1 kernel convolution followed by a spatial bilinear interpolation are used if the number of channels and spatial resolution in the decoder and feature extractor feature map do not match. The architecture of the model is drawn in <ref type="figure" target="#fig_1">figure 2a</ref>. The time and space complexity of this model is roughly the same as the complexity of the underlying VAE model added to the complexity of the feature extractor. The latter is not trained: its weights are frozen during the VAE's training.</p><p>For training as well as for anomaly detection, the merging of the different layers' errors is done by spatially upsampling the error maps of each layers to the original image resolution, denoted by a hat:</p><formula xml:id="formula_7">y i ? R d .</formula><p>The L error maps are then summed over the layers dimension to get a single error map the size of the original image. This process is illustrated in <ref type="figure" target="#fig_1">figure 2b</ref>. The full loss function for the Feature Augmented VAE (FAVAE) is: The anomaly score for a pixel is computed by calculating a log probability map for each layer (summing only on channels), then by upsampling each map to the size of the original image, and finally by summing over the layers, as in <ref type="figure" target="#fig_1">figure 2b</ref>.</p><formula xml:id="formula_8">L = ? L i=0 E q ? (z|x) log p ? (y i |z) + D KL (q ? (z|x) p(z))<label>(8)</label></formula><p>We define the anomaly score for the whole image as the sum of this map over every pixel and channel.</p><p>To conclude this section, <ref type="figure" target="#fig_0">figure 1d</ref> shows that using FAVAE, a form of perceptual anomaly detection has been reached on the toy dataset: the three datasets are ranked in a perceptually conform order.  <ref type="bibr" target="#b14">[15]</ref>. For a given dataset and metric, green cell indicates that FAVAE performs better than vanilla VAE (on average) and best result among all models is written in bold. Each experiment has been run 5 times. We report mean and standard deviation for each experiment. In order to evaluate the proposed method for the task of anomaly segmentation, we perform experiments with the recently proposed MVTec AD dataset <ref type="bibr" target="#b1">[2]</ref>. This collection of datasets comprises 5 categories of textures and 10 categories of objects in the context of industrial inspection, with 73 differents types of defects.   Unless specified we used the same architecture for the encoder and decoder (detailed in Appendix 4) for all our experiments as well as the same hyperparameters when it applies. We did not do any hyperparameter optimization for any model. Experiments were run 5 times per dataset and model.Each experiment takes about one hour (100 epochs) on a GTX 1080 Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MVTec Anomaly Detection dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>In Appendix 5, we test several feature extractors. Here we report the results for FAVAE augmented with VGG16 features. We model the input of the 2nd, 3rd and 4th max pooling (right before activation) Results are listed in table 3. The Vanilla VAE is our baseline model that targets only pixel reconstruction. We also added scores from Dehaene et al. <ref type="bibr" target="#b14">[15]</ref> who tested several anomaly localization algorithms. For each dataset we reported the best score reported in their work. The corresponding model is either an autoencoder, a SSIM-autoencoder <ref type="bibr" target="#b8">[9]</ref>, a VAE or a ?-VAE <ref type="bibr" target="#b21">[22]</ref>, or either one of these augmented with AE-grad <ref type="bibr" target="#b14">[15]</ref>.</p><p>On the localization task (Pixel AUROC), FAVAE improves the Ref scores from Dehaene et al. <ref type="bibr" target="#b14">[15]</ref>, which involve 8 different models and a comparatively slow gradient-based method, on all subset of MVTec AD. On both detection and localization tasks the improvement over vanilla VAE and Ref are significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Study on perceptual features</head><p>In the next experiment, we make several changes to the architecture to assess the benefits of the FAVAE approach. For example, does FAVAE work because we add structured, multi scale features or because those features really bias the model towards human perception ? <ref type="figure" target="#fig_2">Figure 3b</ref>  FAVAE (M1) with VGG16 as a feature extractor is the winning architecture both for localization and detection, which means that we both need high level features and perceptual features coming from ImageNet training for visual anomaly detection. Interestingly (M2) with a randomly initialized VGG backbone and (M3) decoding its own encoder's features perform better than the vanilla VAE (M4) in localization but not in detection. This may hint that we could better merge the anomaly maps of different layers. The performance of (M2) and (M3) for localization also shows that adding multi scale features helps for anomaly localization. Finally, (M6) and (M5) offer poor performances as their feature extractors are allowed to generate useless and easy to predict features.</p><p>This second experiment comforts our hypothesis. It helps to model not only pixels but also higherlevel feature (even random) for the task of anomaly localization ((M1), (M2) and (M3) performs better that vanilla VAE). Classifiers trained on ImageNet learn features that transfer well ((M1) performs better than (M2) and (M3)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The difference between an outlier and an anomaly with respect to a normal distribution is that an outlier is supposed to come from the distribution but is a very rare event. An anomaly, on the other hand, is a sample that is more probably coming from another distribution than the normal one. In this paper we have shown on our toy dataset the interest of going beyond outlier detection for anomaly detection: in some cases we have to represent models of the anomalies to yield useful results. Next, we have implicitly modeled richer and more perceptual anomalies by transferring features from pre-trained models and modeling these features' distributions using VAEs. We have demonstrated that the features not only need to be of higher level than pixels, but also come from models pre-trained on large image datasets in order to gain maximum performance. Finally, we showed that by simply augmenting a VAE with features from the VGG16 model trained on ImageNet, we outperformed state-of-the-art methods for anomaly detection and localization.</p><p>Future work could focus on refining the method for merging errors from different layers, and on the extension of this work to more complicated video datasets, using spatio-temporal perceptual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Formal definition of the toy dataset</head><p>The normal distribution is defined as the following model: <ref type="figure" target="#fig_0">N (0, 1)</ref> where k indexes all pixels and ? n is a constant.</p><formula xml:id="formula_9">q n (?, x) = q n (?) d k=1 q n (x k |?) ?k, q n (x k |?) ? N (?, ? 2 n ), q n (?) ?</formula><p>The anomaly distribution is defined as: <ref type="figure" target="#fig_0">N (0, 1)</ref>, q a (G) ? N (0, ? 2 a ), q a (?) ? U(0, 2?) where i, j respectively index pixels by columns and rows, ? and ? a are constants, ? a s.t. Var[q a (x i,j |?, G, ?)] &lt; ? n . This distribution has an underlying structure of horizontal stripes, with added white noise of an amplitude smaller than the noise amplitude of the normal distribution.</p><formula xml:id="formula_10">?i, j, x i,j = ? + G sin (2??j + ?) q a (?, ?, G) = q a (?) q a (?) q a (G) q a (?) ?</formula><p>For the experiment we chose ? n = 0.0285, ? a = 0.0570, ? = 5, d = 128 ? 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof that VAE is optimal on the toy dataset</head><p>We want to prove that the VAE will learn ? so that p ? (x) = q n (x) by optimising the ELBO. A known <ref type="bibr" target="#b0">[1]</ref> form of the ELBO is:</p><formula xml:id="formula_11">ELBO ?,? = ?H(q n (x)) ? D KL (q n (x) p ? (x)) ? E qn(x) D KL (q ? (z|x) p ? (z|x))<label>(9)</label></formula><p>with H the entropy. The KL divergence is positive and the entropy term is constant with respect to ?, ?, so that iff D KL (q n (x) p ? (x)) = 0 and E qn(x) D KL (q ? (z|x) p ? (z|x)) = 0, the ELBO is maximized with respect to ?, ?.</p><p>If</p><formula xml:id="formula_12">p ? (x|z) = d k=1 p ? (x k |z)</formula><p>with ?k, p ? (x k |z) ? N (Id(z), ? 2 n ) and with the standard VAE prior p(z) = N (0, 1), then we have p ? (x) = q n (x).</p><p>The decoder simply has to broadcast the latent variable z to all pixels: ?k, ? ?,k (z) = z and the learnable parameter of the decoder variance must reach: ? = ? n so that D KL (q n (x) p ? (x)) = 0 in equation 9. We suppose the decoder architecture capable of implementing this very simple solution, and we call ? * its corresponding parameters.</p><p>We can now prove that there exists ? so that D KL (q ? (z|x) p ? * (z|x)) = 0. The Gaussian family is self-conjugate with respect to a Gaussian likelihood function <ref type="bibr" target="#b27">[28]</ref>, which means, considering the pixels x 1 , ..x d as a set of observations of the likelihood function p ? * (x k |z), that the posterior p ? * (z|x) is Gaussian and that:</p><formula xml:id="formula_13">p ? * (z|x) = N d k=1 x k ? 2 n + d , ? 2 n ? 2 n + d<label>(10)</label></formula><p>We suppose the encoder architecture able to implement this simple solution, and we call ? * its corresponding parameters.</p><p>We have thus found ? * , ? * maximizing the ELBO and so that D KL (q n (x) p ? * (x)) = 0. By optimizing the ELBO, the VAE can find the optimal generative model p ? * (x) = q n (x).</p><p>C Implicit anomaly models in standard approaches</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Models based on encoder-decoder reconstruction</head><p>We will show how to get the standard approach pixel-wise normal/anomalous classifier ?k, (C a k (x) &gt; 0.5) ?? (p ? (x k |z ? (x)) &lt; T ) (11) from the following anomaly sampling process:</p><formula xml:id="formula_14">x a = ? ? (z) + e z ? p(z), e ? p a (e) ? N (0, ? 2 a )<label>(12)</label></formula><p>with ? a a constant.</p><p>We will first need to explicit approximations that are commonly used with the VAE. We suppose that since from equation 9 D KL (q ? (z|x) p ? (z|x)) is minimised on the training set, we can approximate p ? (z|x) by q ? (z|x) for x ? q n (x). We also suppose that the encoded distributions q ? (z|x) are sufficiently narrow, so that we can only decode the mean of the encoded distribution, z ? (x). These two approximations correspond to only taking into account the quality of the VAE reconstruction. Another common approximation that is typical to the standard VAE-based anomaly detection framework is to suppose that p ? (z|x) ? q ? (z|x) remains true even for x sampled from another distribution than the normal distribution. This corresponds to using an encoder trained on the normal distribution on the anomalous distribution even though we know they are different. These approximations enable us to write:</p><formula xml:id="formula_15">p ? (a|x) = p ? (a|x, z) p ? (z|x) dz ? p ? (a|x, z) q ? (z|x) dz ? p ? (a|x, z ? (x)) (13) p ? (n|x) = p ? (n|x, z) p ? (z|x) dz ? p ? (n|x, z) q ? (z|x) dz ? p ? (n|x, z ? (x))<label>(14)</label></formula><p>Then</p><formula xml:id="formula_16">C a k (x) = p ? (a|x) p ? (n|x) ? p ? (a|x k , z ? (x)) p ? (n|x k , z ? (x)) ? p a,? (x k |z ? (x))p(a) p n,? (x k |z ? (x))p(n)<label>(15)</label></formula><p>Since p a (x k |z) is a Gaussian of same mean but different scale as p ? (x k |z), they intersect at two points that are symmetric with respect to their mean value, and so we get equation 11 as a result.</p><p>C.2 The recent model from Dehaene et al. <ref type="bibr" target="#b14">[15]</ref> We showed that classical VAE-based anomaly detection models implicitly used simplistic, pixel-wise independent anomaly models that could hinder their practicality. In this section we will show as another example that the more recent approach of Dehaene et al. <ref type="bibr" target="#b14">[15]</ref>, while it does not need some aforementioned approximations, also uses the same kind of pixel-wise anomaly models.</p><p>They propose to search for an optimal "corrected" image x t on the normal data manifold using gradient descent on the objective L r (x t ) + ?||x t ? x|| 1 with L r (x t ) the VAE reconstruction loss. They derive this objective from an adversarial examples framework, however we show here that it can also emerge from an explicit model of the anomaly distribution. Let us assume that the test image is the composition of a sample from the normal distribution and white pixel noise from a Laplace distribution x = x n + e x n ? q n (x n ) e ? q e (e) ? Lap(0, 1/?) we search for the most probable pair of realizations (x, x n ) according to that model, i.e. the x n that minimizes ? log q(x, x n ) = ? log( q(x n ) q(x|x n ) ) = ? log q n (x n ) ? log q e (e) = ? log q n (x n ) ? log q e (x ? x n ) ? ?ELBO(x n ) + ?||x n ? x|| 1 + K ? L r (x n ) + ?||x n ? x|| 1 + K</p><p>K is as constant with respect to x n . We can thus see that even though they add a more flexible model, anomalies are still considered as independent noise over pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Architecture</head><p>This section presents the architecture of the modules used to implement FAVAE. We reproduce here the Pytorch string representation of the main modules.</p><p>The encoder is a Fully Convolutional Network. The output of the encoder is a 200x1x1 tensor for a 3x128x128 input image. The output tensor is split into two parts (mean and sigma of the encoded distribution). Thus the latent space dimension is 100. The decoder is the symmetric of the encoder, using transposed convolutions. s t r i d e = ( 1 , 1 ) ) ) ( 1 ) : S e q u e n t i a l ( ( 0 ) : Conv2d ( 2 5 6 , 2 5 6 , k e r n e l _ s i z e = ( 1 , 1 ) , s t r i d e = ( 1 , 1 ) ) ( 1 ) : ReLU ( ) ( 2 ) : Conv2d ( 2 5 6 , 2 5 6 , k e r n e l _ s i z e = ( 1 , 1 ) , s t r i d e = ( 1 , 1 ) ) ) ( 2 ) : S e q u e n t i a l ( ( 0 ) : Conv2d ( 5 1 2 , 5 1 2 , k e r n e l _ s i z e = ( 1 , 1 ) , s t r i d e = ( 1 , 1 ) ) ( 1 ) : ReLU ( ) ( 2 ) : Conv2d ( 5 1 2 , 5 1 2 , k e r n e l _ s i z e = ( 1 , 1 ) , s t r i d e = ( 1 , 1 ) ) ) ) Finally    <ref type="bibr" target="#b14">[15]</ref>.</p><p>A green cell indicates that the VGG16 FAVAE model performs better than vanilla VAE (on average) for a given dataset and metric. Best result for a given dataset and metric is written in bold. Each experiment has been run 5 times. We report mean and standard deviation for each experiment. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Line by line, respectively: samples from normal dataset, samples from anomaly dataset, samples from shuffled anomaly dataset obtained by shuffling anomaly pixel wise. (b) The classic VAE approach, despite a model learned to perfectly represent the normal distribution, ranks the anomalous samples as more likely to be drawn from the normal distribution than the normal samples. (c) On a classic VAE, the typicality test rejects the anomalous samples in accordance to human perception, but the same test would reject the shuffled anomalous samples, whereas a human would probably not. (d) Our model's log probability separates classes in accordance to human perception.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustrations of the model (a) and the anomaly map computations (b) for L = 2 with y 0 = x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) FAVAE reconstruction and anomaly localization on MVTec AD. Columns from left to right:<ref type="bibr" target="#b0">[1]</ref> input image,<ref type="bibr" target="#b1">[2]</ref> VAE reconstruction,<ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> {pixel, VGG 1st, VGG 2nd, VGG 3rd} layer log reconstruction probability,<ref type="bibr" target="#b6">[7]</ref> merged log probability of layers,<ref type="bibr" target="#b7">[8]</ref> input image with anomaly map overlay,<ref type="bibr" target="#b8">[9]</ref> anomaly map with ground truth defect segmentation (white mask) overlay. (b) FAVAE Perceptual feature study on MVTec AD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>presents qualitative results on MVTec AD. The anomaly map is obtained by equalizing the anomaly score histogram over the entire test set of each dataset. After equalization, a jet colormap is applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>presents the performance of 6 variants, averaged over all MVTec AD datasets. (M1) is the standard pre-trained VGG16 FAVAE. (M2)'s feature extractor is a VGG16 backbone with randomly initialized weights. (M3) uses its own encoder as a feature extractor with a gradient stop to prevent degenerate solutions. (M4) is the vanilla VAE (no feature extractor). (M5) is a VGG16 FAVAE like (M1) with unfrozen weights in the feature extractor. (M6) is the same as (M3) without the gradient stop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>MVTec results for FAVAE, Vanilla VAE and state of the art algorithms. Ref are the best results from Dehaene et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The quality of the samples are very close to what can be acquired in an industrial setting, and the diversity of the subsets allows to test a model on a large range of real life applications. For each category, we dispose of a training dataset containing normal samples and a test dataset containing normal and anomalous samples, with a labeled segmentation of the defects.Our models are trained on normal training samples and tested on both normal and anomalous test samples to evaluate the anomaly segmentation performance.As in Dehaene et al.<ref type="bibr" target="#b14">[15]</ref>, for the textures datasets, we first subsample the original dataset images to 512 ? 512 and then crop random patches of size 128 ? 128 which are used to train and test the different models. For the object datasets, we subsample the original dataset images to 128 ? 128 then we perform rotation and translation data augmentations. For all datasets we train on 10000 images.</figDesc><table><row><cell cols="6">Anomaly segmentation is then computed by reconstructing the anomalous image and its features and</cell></row><row><cell cols="6">comparing those reconstructions with the original image augmented with its real features. Given</cell></row><row><cell cols="6">the ground truth segmentation from the test set, we compute the AUROC (Area Under the Receiver</cell></row><row><cell cols="6">Operating Characteristics) at image (anomaly detection) and pixel level (anomaly localization). Note</cell></row><row><cell cols="6">that an AUROC of 1 expresses the best possible classification in terms of normal and anomalous</cell></row><row><cell>images/pixels.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Model Feature Extractor</cell><cell>Pixel AUROC</cell><cell>Image AUROC</cell></row><row><cell></cell><cell></cell><cell>Vgg</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Enc</cell><cell>Z</cell><cell>Dec</cell><cell>(1) VGG16</cell><cell>0.953</cell><cell>0.883</cell></row><row><cell></cell><cell></cell><cell>Random</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Enc</cell><cell>Z</cell><cell>Dec</cell><cell>(2) random</cell><cell>0.899</cell><cell>0.764</cell></row><row><cell cols="2">Z = ? Enc</cell><cell>Dec Enc</cell><cell>(3) encoder with gradient stop</cell><cell>0.890</cell><cell>0.779</cell></row><row><cell>Enc</cell><cell>Z</cell><cell>Dec</cell><cell>(4) none</cell><cell>0.824</cell><cell>0.791</cell></row><row><cell>Enc</cell><cell>Z</cell><cell>Dec Vgg</cell><cell>(5) unfrozen VGG16</cell><cell>0.762</cell><cell>0.686</cell></row><row><cell>=</cell><cell></cell><cell>Enc</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Enc</cell><cell>Z</cell><cell>Dec</cell><cell>(6) encoder</cell><cell>0.700</cell><cell>0.604</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>table 2</head><label>2</label><figDesc>summarizes the connections between the decoder and the feature extractor.</figDesc><table><row><cell>Feature</cell><cell>Feature</cell><cell>Decoder</cell></row><row><cell>Extractor</cell><cell>Layers</cell><cell>Layers</cell></row><row><cell>VGG16</cell><cell>(7, 14, 21)</cell><cell>(22, 10, 16)</cell></row><row><cell>Resnet18</cell><cell cols="2">(conv2x, conv3x, conv4x) (22, 10, 16)</cell></row><row><cell>YOLOv3</cell><cell>20</cell><cell>10</cell></row><row><cell>auto feature extractor</cell><cell>(3,9,15)</cell><cell>(22, 10, 16)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Reconstruction mapping between decoder layers and feature extractor layers.In this experiment we study the performances of various ImageNet classifiers as feature extractors for FAVAE. Results are listed in table 3. The Vanilla VAE is our baseline model that targets only pixel reconstruction. We also added scores from Dehaene et al.<ref type="bibr" target="#b14">[15]</ref> who tested several anomaly localization algorithms. For each dataset we reported the best score reported in their work. For VGG16 we model the input of the 2nd, 3rd and 4th max pooling (right before activation). For ResNet18 we model features from conv2x, conv3x and conv4x. For YOLOv3 we model a single layer of feature -the one with the greatest resolution. YOLOv3 is an interesting model since it has been trained with supervision to upsample and merge high semantic features with low semantic features. This could be an alternative to the handcrafted upsampling and sum routine that we use to merge layers when using other feature extractors.For anomaly localization (Pixel AUROC), we observe that the VGG16 version is better or similar on every dataset except tile. FAVAE based on VGG16 improves the Ref scores from Dehaene et al.<ref type="bibr" target="#b14">[15]</ref>, which involve 8 different models and a comparatively slow gradient-based method. On average the reference score (0.908 ? 0.088) and vanilla VAE score (0.823 ? 0.120) are improved by VGG16 (0.953 ? 0.068).For anomaly detection (Image AUROC) VGG16 is still the best solution on average but is superior to the other feature extractors on only 11 over 15 datasets. On average vanilla score (0.775 ? 0.159) is improved by VGG16 (0.879 ? 0.108). On both detection and localization tasks the improvement over vanilla VAE and Ref are significant.</figDesc><table><row><cell>E Which feature extractor ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>MVTec result for several feature extractors. Ref are the best results from Dehaene et al.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>S e q u e n t i a l ( ( 0 ) : D e F l a t t e n ( s h a p e = ( 1 0 0 , 1 , 1 ) ) ( 1 ) : C o n v T r a n s p o s e 2 d ( 1 0 0 , <ref type="bibr" target="#b2">3</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mvtec ad -a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep autoencoding models for unsupervised anomaly segmentation in brain MR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shadi</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<idno>abs/1804.04488</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Anomaly machine component detection by deep generative model with unregularized score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Tachibana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Uehara</surname></persName>
		</author>
		<idno>abs/1807.05800</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Do deep generative models know what they don&apos;t know?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilan</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cifar-10 (canadian institute for advanced research)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sindy</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
		<idno>abs/1807.02011</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><surname>Simoncelli</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2003.819861</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-05" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">f-anogan: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2019.01.010</idno>
		<idno>1361-8415. doi</idno>
		<ptr target="https://doi.org/10.1016/j.media.2019.01.010" />
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m">Nips 2016 tutorial: Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Iterative energy-based projection on a normal data manifold for anomaly localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dehaene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriel</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Combrexelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Eline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anomaly detection in nanofibrous materials by cnn-based self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavio</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raimondo</forename><surname>Schettini</surname></persName>
		</author>
		<idno>doi: 10.3390/ s18010209</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Are pre-trained cnns good feature extractors for anomaly detection in surveillance videos?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">F</forename><surname>Nazare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moacir</forename><forename type="middle">A</forename><surname>De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep-anomaly: Fully convolutional neural network for fast anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zahra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Moayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klette</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2018.02.006</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Transfer representation-learning for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerone Theodore Alexander</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><forename type="middle">D</forename><surname>Griffin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Where&apos;s wally now? deep generative and discriminative embeddings for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Jeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Diagnosing and enhancing VAE models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">P</forename><surname>Wipf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Detecting out-of-distribution inputs to deep generative models using typicality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Waic, but why? generative ensembles for robust anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00068</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep feature consistent variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianxu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.1109/wacv.2017.131</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<ptr target="https://en.wikipedia.org/wiki/Conjugate_prior" />
		<title level="m">Conjugate prior -Wikipedia, the free encyclopedia</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
	<note>Online; accessed 05</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
