<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RODD: A Self-Supervised Approach for Robust Out-of-Distribution Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Khalid</surname></persName>
							<email>umarkhalid@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashkan</forename><surname>Esmaeili</surname></persName>
							<email>ashkan.esmaeili@ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazmul</forename><surname>Karim</surname></persName>
							<email>nazmul.karim18@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Rahnavard</surname></persName>
							<email>nazanin.rahnavard@ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RODD: A Self-Supervised Approach for Robust Out-of-Distribution Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies have started to address the concern of detecting and rejecting the out-of-distribution (OOD) samples as a major challenge in the safe deployment of deep learning (DL) models. It is desired that the DL model should only be confident about the in-distribution (ID) data which reinforces the driving principle of the OOD detection. In this paper, we propose a simple yet effective generalized OOD detection method independent of out-of-distribution datasets. Our approach relies on self-supervised feature learning of the training samples, where the embeddings lie on a compact low-dimensional space. Motivated by the recent studies that show self-supervised adversarial contrastive learning helps robustify the model, we empirically show that a pre-trained model with self-supervised contrastive learning yields a better model for uni-dimensional feature learning in the latent space. The method proposed in this work, referred to as RODD, outperforms SOTA detection performance on extensive suite of benchmark datasets on OOD detection tasks. On the CIFAR-100 benchmarks, RODD achieves a 26.97 % lower false positive rate (FPR@95) compared to SOTA methods. Our code is publicly available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In a real-world deployment, machine learning models are generally exposed to the out-of-distribution (OOD) objects that they have not experienced during the training. Detecting such OOD samples is of paramount importance in safety-critical applications such as health-care and autonomous driving <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. Therefore, the researchers have started to address the issue of OOD detection more recently <ref type="bibr">[1, 2, 13-15, 26, 32, 39]</ref>. Most of the recent studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38]</ref> on OOD detection use OOD data for the model regularization such that some distance metric between the ID and OOD distributions is maximized. In recent studies <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>, generative models and auto-encoders have 1 https://github.com/UmarKhalidcs/RODD been proposed to tackle OOD detection. However, they require OOD samples for hyper-parameter tuning. In the realworld scenarios, OOD detectors are distribution-agnostic. To overcome this limitation, some other methods that are independent of OOD data during the training process have been proposed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref>. Such methods either use the membership probabilities <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31]</ref> or a feature embedding <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39]</ref> to calculate an uncertainty score. In <ref type="bibr" target="#b35">[36]</ref>, the authors proposed to reconstruct the samples to produce a discriminate feature space. Similarly, <ref type="bibr" target="#b5">[6]</ref> proposed synthesizing virtual outliers to regularize the model's decision boundary during training. Nevertheless, the performance of the methods that rely on either reconstruction or generation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref> degrades on large-scale datasets or video classification scenarios.</p><p>In this work, we claim that if the feature vectors belonging to each known class lie on a low-dimensional subspace, a representative singular vector can be calculated for each class that can be used to calculate uncertainty scores <ref type="bibr" target="#b38">[39]</ref>. In order to achieve such a compact representation of the features belonging to each class, we have leveraged contrastive learning as a pre-training tool that has improved the performance of the proposed robust out-of-distribution detector (RODD) as it has helped the better feature mapping in the latent space during the downstream fine-tuning stage <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34]</ref>. Self-supervised pre-training, where we use adversaries as a form of data augmentation, helps to raise the RODD's performance in the settings with corrupted samples. This concept has been established by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35</ref>] that a selfsupervised contrastive adversarial learning can generate an adversarially robust model during the fine-tuning. The overall architecture of the RODD is shown in <ref type="figure">Fig. 1</ref>.</p><p>In summary, we make the following contributions in this study. First, we propose that OOD detection test can be designed using the features extracted by self-supervised contrastive learning that reinforce the uni-dimensional projections of the ID set. Second, we have theoretically proved that such uni-dimensional projections, boosted by the contrastive learning, can be characterized by the prominent first  <ref type="figure">Figure 1</ref>. Overall architecture of the proposed OOD detection method. (a) In the first step, self-supervised adversarial contrastive learning is performed.</p><p>(b) Secondly, the encoder is fine-tuned by freezing the weights (W) of the penultimate layer. The columns of W are initialized to be orthonormal.(c) Thirdly, employing singular value decomposition (SVD), we calculate the first singular vector of each class using its features. (d) The final step is the OOD detection, where an uncertainty score is estimated using cosine similarity between the feature vector (Ft) representing the test sample t and first singular vector of each ID class. Here, BN represents Batch Normalization, L is the number of classes, and ? th is the threshold for the uncertainty score. singular vector that represents its corresponding class attributes. Furthermore, the robustness of the proposed OOD detector has been evaluated by introducing corruptions in both OOD and ID datasets. Extensive experiments illustrate that the proposed OOD detection method outperforms the state-of-the-art (SOTA) algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach</head><p>Our proposed OOD detection approach builds upon employing a self-supervised training block to extract robust features from the ID dataset. This is carried out by training a contrastive loss on ID data as shown in <ref type="figure">Fig. 1 (a)</ref>. Next, we utilize the concept of union of one-dimensionalembeddings to project the deep features of different classes onto one-dimensional and mutually orthogonal predefined vectors representing each class to obtain logits. At the final layer's output, we evaluate the cross-entropy between the logit probability output and the labels to form the supervised loss as shown in <ref type="figure">Fig. 1 (b)</ref>. The uni-dimensional mapping is carried out to guarantee that intra-class distribution consists of samples aligning the most with the uni-dimensional vector characterizing its samples. To this end, the penultimate layer of the model is modified by using cosine similarity and introducing a sharpening layer as shown in <ref type="figure">Fig. 1 (b)</ref>, where output logits are calculated as, P (F n ) = Z(Fn) G(Fn) , where</p><formula xml:id="formula_0">Z(Fn) = W T Fn Fn , G(Fn) = ?(BN (W T g Fn))<label>(1)</label></formula><p>Here, F n represents the encoder output for the training sample n, ? is the sigmoid function, and W g is the weight matrix for the sharpening layer, represented by G(F n ), which essentially maps F n to a scalar value. In the sharpening layer, batch normalization (BN) is used for faster convergence as proposed by <ref type="bibr" target="#b12">[13]</ref>. It is worth mentioning that during the fine-tuning stage, we do not calculate the bias vector for the penultimate and sharpening layers.</p><p>The orthogonality comes with wide angles between the uni-dimensional embeddings of separates classes creating a large and expanded rejection region for the OOD samples if they lie in the vast inter-class space. To achieve this, we initialize the weight matrix W = [w l w 2 . . . w l ] of the penultimate layer with orthonormal vectors as in <ref type="bibr" target="#b28">[29]</ref> and then freeze it during the fine-tuning stage. Here, w l represents the weights of the last fully connected layer corresponding to class l. During fine-tuning, the features are projected onto the predefined set of orthogonal vectors w l for l = 1, 2, . . . , L, where L is the number of ID classes.</p><p>After training, OOD testing can be done by evaluating the inner products between the calculated first singular vectors (U 1 , U 2 , . . . , U L ) representing their corresponding classes as shown in <ref type="figure">Fig. 1 (c)</ref>, and the extracted feature for the sample of interest. To perform OOD inspection on the test sample t ? S t , where S t is the test set, the uncertainty score is calculated as,</p><formula xml:id="formula_1">?t = min(arccos F T t U l Ft ), ? l ? {1, 2, . . . , L} (2)</formula><p>Here, F t is the output of the encoder for the test sample t. The measured uncertainty is then used to calculate the probability that if t belongs to ID or OOD using the probability function p(? t ? ? T h |t ? S t ) as RODD is a probalistic approach where sampling is performed during the test time.</p><p>In an ideal scenario, features of ID class l have to be aligned with the corresponding w l , where w l is the l th column of matrix W. In that case, ? T h = 0. However, in practice, all class features are not exactly aligned with their respective column in W, that further strengthens the idea of using the first singular vector of each class feature matrix, separately.</p><p>Next, we will explain how the contrastive learning pretraining and sharpening module, G(F n ), boosts the performance of our approach. Firstly, contrastive learning has been beneficial because we do not freeze the weights of the encoder after the self-supervised learning and keep finetuning them along the training procedure using the crossentropy loss. In other words, the features are warm-started with initialized values derived from the contrastive loss pretraining, yet the final objective function to optimize is composed of two terms L CL + ?L LL , where L CL and L LL denote the contrastive and cross-entropy losses, respectively. In addition, the cross-entropy loss imposes the orthogonality assumption infused by the choice of orthogonal matrix containing union of w l ? l ? {1, 2, . . . , L} each of which represent one class. By feeding the inner products of features with W into L LL , the features are endorsed to get reshaped to satisfy orthogonality and rotate to align w l .</p><p>Furthermore, augmenting the data of each class with the adversarial perturbations can improve classification perfromance on ID perturbed data while still detecting the OOD data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>. Moreover, prior to feeding the optimizer with the inner products for supervised training, we modify the uni-dimensional mappings using G(F n ) to optimally benefit from the self-supervised learned features. To compensate for the uni-dimensional confinement which can downgrade the classifier's performance, we use the sharpening concept, where we enhance the confidence of the obtained logit vector by scaling the inner products with a factor denoted with the sharpening function G(F n ) explained above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Theoretical Analysis</head><p>In this section, we provide theoretical analyses on how pre-training with contrastive loss promotes the unidimensional embeddings approach utilized in RODD by promoting one prominent singular vector (with a dominant singular value) in the deep feature extraction layer.</p><p>The objective function used in our optimization is composed of a contrastive loss and a softmax cross entropy. For simplicity, we use a least squared loss measuring the distance between linear prediction on a sample's extracted feature to its label vector W T F n ? y n 2 2 as a surrogate for the softmax cross entropy (L LL ) 2 . This is justified in <ref type="bibr" target="#b33">[34]</ref>.</p><p>Let A = [a i,j ] denote the adjacency matrix for the augmentation graph of training data formally defined as in <ref type="bibr" target="#b33">[34]</ref>. In general, two samples are connected through an edge on this graph if they are believed to be generated from the same class distribution. Without loss of generality, we assume that the adjacency matrix is block-diagonal, i.e., different classes are well-distinguished. Therefore, the problem can be partitioned into data specific to each class. Let F and Y denote the matrix of all features and label vectors, i.e., F n and y n , where n denotes the n th sample, respectively. The training loss including one term for contrastive learning loss and one for the supervised uni-dimensional embedding matching can be written as: 3</p><formula xml:id="formula_2">L(F) = A ? FF T 2 F L CL (F) +? W T F ? Y 2 F L LL (F)</formula><p>.</p><p>(</p><p>Y and A are given matrices, and W is fixed to some orthonormal predefined matrix. The optimization variable is therefore the matrix F. Thus, the optimization problem can be written as:</p><formula xml:id="formula_4">min F A ? FF T 2 F + ? W T F ? Y 2 F .<label>(4)</label></formula><p>Before bringing the main theorem, two assumptions are made on the structure of the adjacency matrix arising from its properties <ref type="bibr" target="#b33">[34]</ref>: 1: For a triple of images x i , x j , x s , we have ai,j aj,s ? [ 1 1+? , 1 + ?] for small ?, i.e., samples of the same class are similar. 2: For a quadruple of images x i , x j , x s , x t , where x i , x j are from different classes and x s , x t are from the same classes, ai,j as,t ? ? for small ?. Lemma 1. Let F * denote the solution to min F L CL (first loss term in (4)). Assume F * can be decomposed as F * = U?V T . Under Assumptions 1,2 (above), for F * with sin-</p><formula xml:id="formula_5">gular values ? i , we have N l i=2 ? 2 i ? 6 (1 + ?) 3 2 ? 1 for some small ?, where ? i = ? ii ,</formula><p>and N l is the number training samples of class l. Proof. In <ref type="bibr" target="#b33">[34]</ref>, it is shown that</p><formula xml:id="formula_6">N l i=2 ? 4 i ? 2 (1+?) 3 2 ?1) . The proof is straightforward powering N l i=2 ? 2</formula><p>i by two and applying Cauchy-Schwartz inequality. Theorem 1. Let F * denote the solution to <ref type="bibr" target="#b3">(4)</ref>. Assume F * is decomposed as F * = U?V T . There exist a ? min such that</p><formula xml:id="formula_7">N l i=2 ? 4 i ? 2 (1 + ?) 3 2 ? 1)</formula><p>, if ? &lt; ? min in P (4). <ref type="bibr" target="#b1">2</ref> The least squared loss (L LL ) measures the distance of the final layer predictions (assuming linear predictor in the deep feature space) from the one-hot encoded vector (alternatively logits if available) <ref type="bibr" target="#b2">3</ref> It is shown in <ref type="bibr" target="#b7">[8]</ref> that the solution to the contrastive learning loss can be written as the following Cholesky decomposition problem, min F A? FF T 2 F , which constitutes the first term of the loss in Eq. (3).</p><p>The purpose is to show that treating corrupted or adversarial ID data vs. OOD data, the uni-dimensional embedding is robust in OOD rejection. This mandates invariance and stability of the first singular vector for the features extracted for samples generated from each class. The goal of this theorem is to show that using the contrastive loss along certain values of ? regularizing the logit loss, the dominance of the first eigenvector of the adjacency matrix is also inherited to the first singular vector of the F and this is inline with the mechanism of proposed approach whose functionality depends on the stability and dominance of the first singular vector because we desire most of the information included in the samples belonging to each class can be reflected in uni-dimensional projections.</p><p>Assuming the dominance is held for the first singular value of each class data, the contrastive learning can therefore split them by summarizing the class-wise data into unidimensional separate representations. The V matrix is used to orthogonalize and rotate the uni-dimensional vectors obtained by contrastive learning to match the pre-defined orthogonal set of vectors w l as much as possible.</p><p>Now the proof for the main theorem is provided.</p><p>Proof. A is Hermitian. Therefore, it can be decomposed</p><formula xml:id="formula_8">as A = Q?Q T . The solution set to minimize L CL is S = {Q? 1 2 V T : ? orthonormal matrix V} (? i = ? ii = ? 2 i )</formula><p>. Let L 1 and L 2 be the minima for (4) obtained on the sets S and S c , i.e., the complementary set of S. L 1 equals ? min F?S L LL (F) as the first loss is 0 for elements in S. Now, we consider L 2 . S c can be partitioned into two sets S c 1 and S c 2 , where elements in S c 1 set L LL to zero and elements in S c 2 yield non-zero values for L LL . Therefore, L 2 is the minimum of the two partition's minima.</p><formula xml:id="formula_9">L 2 = min min F?S c 1 L CL (F) LHS , min F?S c 2 L CL (F) + ?L LL (F) RHS (5)</formula><p>It is obvious that for a small enough ?, L 2 equals the RHS above. This can be reasoned as follows. Let the LHS value be denoted with m 1 . m 1 &gt; 0 since S and S c 1 are disjoint sets with no sharing boundaries. The RHS in <ref type="formula">(5)</ref> is composed of two parts. The first part can be arbitrarily small because although S and S c 2 are disjoint, they are connected sets with sharing boundaries. (For instance any small perturbation in ? eigenvalues drags a matrix from S into S c 2 . However, they are infinitesimally close due to the continuity property). The second term can also be shrunk with an arbitrarily small choice of ? = ? min = m1 L LL (F) that guarantees the RHS takes the minimum in Eq. (5), whereF = arg min F?S c 2 L CL (F) <ref type="bibr" target="#b3">4</ref> . Therefore, for ? &lt; ? min , <ref type="bibr" target="#b3">4</ref> (As discussed,F makes the first term arbitrarily approach 0 due to continuity property holding between S and S c 2 and there is an element in the minimum objective value in Eq. (4) (min{L 1 , L 2 }) is, min min F?S c 2 L CL (F)+?L LL (F), min F?S ?L LL (F) . The final aim is to show that ? can be chosen such that F * inherits the dominance of first eigenvalue from A. This is straightforward if the solution is RHS in (5) because the solution lies on S in that case and therefore, can be expressed as Q? 1 2 V T inheriting the property in Lemma 1. Thus, we first consider cases where min{L 1 , L 2 } is obtained by the RHS by explicitly writing when LHS&gt;RHS. We assume the minimizers for the RHS and LHS differ in a matrix R. Let F * denote the minimizer for RHS. Then, the minimizer of LHS is F * + R. We have</p><formula xml:id="formula_10">LHS = A ? (F * + R)(F * + R) T 2 F + ? W T F * + W T R ? Y 2 F = A ? F * F * T 0 ? (F * R T + RF * T + RR T ) E 2 F + ? W T F * ? Y + W T R 2 F = E 2 F + ? W T F * ? Y 2 F + ? W T R 2 F + 2? W T F * ? Y, W T R , where the inner product of two matrices A, B ( A, B ) is defined as T r(AB T ).</formula><p>The RHS in <ref type="bibr" target="#b4">(5)</ref> </p><formula xml:id="formula_11">equates ? W T F * ? Y 2</formula><p>F since F * is its minimizer and the loss has only the logit loss term.</p><p>Thus, the condition LHS &gt; RHS reduces to</p><formula xml:id="formula_12">E 2 F + ? W T R 2 F + 2? W T F * ? Y, W T R &gt; 0.</formula><p>Using the fact that the matrix W is predefined to be an orthonormal matrix, multiplying it by R does not change the Frobenius norm. Hence, the condition reduces to</p><formula xml:id="formula_13">E 2 F + ? R 2 F &gt; 2? Y ? W T F * , W T R .</formula><p>To establish this bound, the Cauchy-Schwartz inequality (C-S) and the Inequality of Arithmetic and Geometric Means (AM-GM) are used to obtain the upper bound for the inner product. The sufficient condition holds true if it is established for the obtained upper bound (tighter inequality). Applying (C-S) and (AM-GM) inequalities we have</p><formula xml:id="formula_14">Y ? W T F * , W T R C?S ? ? Y ? W T F * F W T R F = Y ? W T F * F R F AM ?GM ? ? 1 2 Y ? W T F * 2 F + 1 2 R 2 F</formula><p>Substituting this for the inner product to establish a tighter inequality, we get</p><formula xml:id="formula_15">E 2 F + ? R 2 F &gt; ? Y ? W T F * 2 F + ? R 2 F reducing to E 2 F &gt; ? Y ? W T F * 2 F .</formula><p>As the matrix of all zeros, i.e., [0] ? S, inserting [0] for F leads to a trivial upper bound for the minimum ob-</p><formula xml:id="formula_16">tained over F ? S, i.e., Y ? W T F * 2 F is upper bounded with Y 2 F . Finding a condition for E 2 F &gt; ? min Y 2 F guarantees the desired condition is satisfied. If E 2 F &gt; ? min Y 2</formula><p>F is met, the solution lies in S and RHS obtains the minimum, validating Lemma 1 for F * . Otherwise, if the solution lies in S c 2 and is attained from the LHS such that it contravenes the dominance of the first pricinpal component of A, we will show by contradiction that the proper choice for ? avoids LHS to be less than the S c 2 arbitrarily close toF) RHS in <ref type="bibr" target="#b4">(5)</ref>. To this end, we take a more profound look into E 2 F . If R is to perturb the solution F * such that the first principal component is not prominent, for R + F * , we shall have N l i=2 ? 2 i &gt; ? + ? for some positive ? violating the condition stated in the Theorem. This means there is at least one singular value of F * + R, for which we</p><formula xml:id="formula_17">have ? r &gt; ?+? N l ?1 = ? N l ?1 + O( 4 ? ?)</formula><p>. As F * inherits the square root of eigenvalues of A, according to Lemma 1 and using Taylor series expansion, ? r (</p><formula xml:id="formula_18">F * ) = O( 4 ? ?). This yields ? r (R) &gt; ? N l ?1 + O( 4 ? ?)</formula><p>. E is a symmetric matrix and therefore it has eigenvalue decomposition.</p><formula xml:id="formula_19">E 2 F ? ? 2 r (E) = ? 2 r (RR T + RF * T + F * R T ) = ? 2 r (RR T ) + O(?) &gt; ? 2 (N l ?1) 2 + O(?). Knowing that Y 2 F = N 2 l , if ? &lt; ? 2 N 4 l</formula><p>, the condition for RHS&lt;LHS is met. According to Lemma 1 and the previous bound found for ? min , if ? min &lt; min{ ? 2</p><formula xml:id="formula_20">N 4 l , m1</formula><p>L LL (F) }, the solution should be F * = Q? 1 2 V T . Hence, for certain range of values for ?, the solution takes the form Q? 1 2 V obeying the dominance of ? 1 in A and this concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section, we evaluate our proposed OOD detection method through extensive experimentation on different ID and OOD datasets with multiple architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets and Architecture</head><p>In our experiments, we used CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b20">[21]</ref> as ID datasets and 7 OOD datasets. OOD datasets utilized are TinyImageNet-crop (TINc), TinyImageNetresize(TINr) <ref type="bibr" target="#b4">[5]</ref>, LSUN-resize (LSUN-r) <ref type="bibr" target="#b36">[37]</ref>, Places <ref type="bibr" target="#b40">[41]</ref>, Textures <ref type="bibr" target="#b3">[4]</ref>, SVHN <ref type="bibr" target="#b26">[27]</ref> and iSUN <ref type="bibr" target="#b32">[33]</ref>. For an architecture, we deployed WideResNet <ref type="bibr" target="#b39">[40]</ref> with depth and width equal to 40 and 2, respectively, as an encoder in our experiments. However, the penultimate layer has been modified as compared to the baseline architecture as shown in <ref type="figure">Fig. 1.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation Metrics and Inference Criterion</head><p>As in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31]</ref>, the OOD detection performance of RODD is evaluated using the following metrics: (i) FPR95 indicates the false positive rate (FPR) at 95% true positive rate (TPR) and (ii) AUROC, which is defined as the Area Under the Receiver Operating Characteristic curve. As RODD is a probabilistic approach, sampling is preformed on the ID and OOD data during the test time to ensure the probabilistic settings. We employ Monte Carlo sampling to estimate p(? t ? ? T h ) for OOD detection, where ? T h is the uncertainty score threshold calculated using training samples. During inference, 50 samples are drawn for a given sample, t. The evaluation metrics are then applied on ID test data and OOD data using the estimated ? T h to calculate the difference in the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head><p>We show the performance of RODD in <ref type="table" target="#tab_1">Tables 1 and 2</ref> for CIFAR-10 and CIFAR-100, respectively. Our method achieves an FPR95 improvement of 21.66%, compared to the most recently reported SOTA <ref type="bibr" target="#b5">[6]</ref>, on CIFAR-10. We obtain similar performance gains for CIFAR-100 dataset as well. For RODD, the model is first pre-trained using selfsupervised adversarial contrastive learning <ref type="bibr" target="#b15">[16]</ref>. We finetune the model following the training settings in <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Ablation Studies</head><p>In this section, we conduct extensive ablation studies to evaluate the robustness of RODD against corrupted ID and OOD test samples. Firstly, we apply the 14 corruptions in <ref type="bibr" target="#b8">[9]</ref> on OOD data to generate corrupted OOD (OOD-C). Corruptions introduced can be benign or destructive based on thier intensity which is defined by their severity level. To do comprehensive evaluations, 5 severity levels of the corruptions are infused. By introducing such corruptions in OOD datasets, the calculated mean detection error for both CIFAR-10 and CIFAR-100 is 0%, which highlights the inherit property of RODD that it shifts perturbed OOD features further away from the ID as shown in t-SNE plots in <ref type="figure" target="#fig_0">Fig.  2</ref> which shows that perturbing OOD improves the RODD's performance. Secondly, we introduced corruptions <ref type="bibr" target="#b8">[9]</ref> in the ID test data while keeping OOD data clean during testing. The performance of RODD on corrupted CIFAR-100 (CIFAR100-C) has been compared with VOS <ref type="bibr" target="#b5">[6]</ref> in <ref type="table" target="#tab_2">Table 3</ref>. Lastly, we compared the classification accuracy of our proposed method with the baseline WideResNet model <ref type="bibr" target="#b39">[40]</ref> on clean and corrupted ID test samples in <ref type="table" target="#tab_3">Table 4</ref>. RODD has improved accuracy on corrupted ID test data as compared to the baseline with a negligible drop on classification accuracy of clean ID test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have proposed that in-distribution features can be aligned in a narrow region of the latent space using constrastive pre-training and uni-dimensional feature mapping. With such compact mapping, a representative first singular vector can be calculated from the features for each in-distribution class. The cosine similarity between these computed singular vectors and an extracted feature vector of the test sample is then estimated to perform OOD test. We have shown through extensive experimentation that our method achieves SOTA OOD detection results on CIFAR-10 and CIFAR-100 image classification benchmarks.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>t-SNE representation of features extracted by introducing Gaussian noise on OOD dataset. 10,000 samples each of TINc and LSUNc while 1,000 sample of each class from ID CIFAR-10 test set are used to generate 2D t-SNE plot. (a) Features extracted from the baseline model with severity level 1. (b) Features extracted using RODD with corruption severity level 1. (c) Features extracted from the RODD with corruption severity level 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2204.02553v3 [cs.CV] 15 Oct 2022</figDesc><table><row><cell></cell><cell cols="4">(a) Contrastive Pre-Training Stage</cell><cell></cell><cell></cell><cell cols="2">(b) Fine-Tuning Stage</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sharpening</cell></row><row><cell></cell><cell>PGD Attack</cell><cell>Perturbed Data</cell><cell></cell><cell>Maximize Agreement</cell><cell></cell><cell></cell><cell></cell><cell>Output Logits</cell></row><row><cell>Training Data</cell><cell>Augmentation Generator</cell><cell>Augmented Data</cell><cell>Encoder</cell><cell>Projector</cell><cell>Training Data</cell><cell>Encoder</cell><cell></cell><cell>?</cell></row><row><cell></cell><cell></cell><cell cols="2">(c) SVD Stage</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(d) OOD Detection Stage</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Singular Vectors</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>SVD</cell><cell></cell><cell>......</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Block</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Test Samples (S t )</cell><cell>Uncertainty Score</cell></row><row><cell>Training Data</cell><cell>Encoder</cell><cell></cell><cell cols="2">First Singular Vectors</cell><cell></cell><cell>Encoder</cell><cell cols="2">OOD Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ID</cell><cell>OOD</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>OOD detection results of RODD and comparison with competitive baselines trained on CIFAR-10 as ID dataset. All values are shown in percentages. ? indicates larger values are better and ? indicates smaller values are better. Table 2. OOD detection results of RODD and comparison with competitive baselines trained on CIFAR-100 as ID dataset. All values are shown in percentages. ? indicates larger values are better and ? indicates smaller values are better.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">OOD Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="14">SVHN FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC iSUN LSUNr TINc TINr Places Textures</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>MSP [10]</cell><cell>48.49</cell><cell>91.89</cell><cell>56.03</cell><cell>89.83</cell><cell>52.15</cell><cell>91.37</cell><cell>53.15</cell><cell>87.33</cell><cell>54.24</cell><cell>79.35</cell><cell>59.48</cell><cell>88.20</cell><cell>59.28</cell><cell>88.50</cell></row><row><cell>ODIN [24]</cell><cell>33.55</cell><cell>91.96</cell><cell>32.05</cell><cell>93.50</cell><cell>26.52</cell><cell>94.57</cell><cell>36.75</cell><cell>89.20</cell><cell>49.15</cell><cell>81.64</cell><cell>57.40</cell><cell>84.49</cell><cell>49.12</cell><cell>84.97</cell></row><row><cell cols="2">Mahalanobis [23] 12.89</cell><cell>97.62</cell><cell>44.18</cell><cell>92.66</cell><cell>42.62</cell><cell>93.23</cell><cell>42.75</cell><cell>88.85</cell><cell>52.25</cell><cell>80.33</cell><cell>92.38</cell><cell>33.06</cell><cell>15.00</cell><cell>97.33</cell></row><row><cell>Energy [25]</cell><cell>35.59</cell><cell>90.96</cell><cell>33.68</cell><cell>92.62</cell><cell>27.58</cell><cell>94.24</cell><cell>35.69</cell><cell>89.05</cell><cell>50.45</cell><cell>81.33</cell><cell>40.14</cell><cell>89.89</cell><cell>52.79</cell><cell>85.22</cell></row><row><cell>OE [11]</cell><cell>4.36</cell><cell>98.63</cell><cell>6.32</cell><cell>98.85</cell><cell>5.59</cell><cell>98.94</cell><cell>13.45</cell><cell>96.44</cell><cell>15.67</cell><cell>96.78</cell><cell>19.07</cell><cell>96.16</cell><cell>12.94</cell><cell>97.73</cell></row><row><cell>VOS [6]</cell><cell>8.65</cell><cell>98.51</cell><cell>7.56</cell><cell>98.71</cell><cell>14.62</cell><cell>97.18</cell><cell>11.76</cell><cell>97.58</cell><cell>28.08</cell><cell>94.26</cell><cell>37.61</cell><cell>90.42</cell><cell>47.09</cell><cell>86.64</cell></row><row><cell>FS [39]</cell><cell>24.71</cell><cell>95.31</cell><cell>17.41</cell><cell>96.61</cell><cell>4.84</cell><cell>96.28</cell><cell>12.45</cell><cell>97.83</cell><cell>9.65</cell><cell>97.95</cell><cell>11.56</cell><cell>96.42</cell><cell>5.55</cell><cell>98.64</cell></row><row><cell>RODD (Ours)</cell><cell>1.82</cell><cell>99.63</cell><cell>4.07</cell><cell>99.32</cell><cell>4.49</cell><cell>99.25</cell><cell>10.29</cell><cell>98.10</cell><cell>6.30</cell><cell>99.0</cell><cell>9.59</cell><cell>98.47</cell><cell>3.87</cell><cell>99.43</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">OOD Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="14">SVHN FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC iSUN LSUNr TINc TINr Places Textures</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>MSP [10]</cell><cell>84.59</cell><cell>71.44</cell><cell>82.80</cell><cell>75.46</cell><cell>82.42</cell><cell>75.38</cell><cell>69.82</cell><cell>79.77</cell><cell>79.95</cell><cell>72.36</cell><cell>82.84</cell><cell>73.78</cell><cell>83.29</cell><cell>73.34</cell></row><row><cell>ODIN [24]</cell><cell>84.66</cell><cell>67.26</cell><cell>68.51</cell><cell>82.69</cell><cell>71.96</cell><cell>81.82</cell><cell>45.55</cell><cell>87.77</cell><cell>57.34</cell><cell>80.88</cell><cell>87.88</cell><cell>71.63</cell><cell>49.12</cell><cell>84.97</cell></row><row><cell cols="2">Mahalanobis [23] 57.52</cell><cell>86.01</cell><cell>26.10</cell><cell>94.58</cell><cell>21.23</cell><cell>96.00</cell><cell>43.45</cell><cell>86.65</cell><cell>44.45</cell><cell>85.68</cell><cell>88.83</cell><cell>67.87</cell><cell>39.39</cell><cell>90.57</cell></row><row><cell>Energy [25]</cell><cell>85.52</cell><cell>73.99</cell><cell>81.04</cell><cell>78.91</cell><cell>79.47</cell><cell>79.23</cell><cell>68.85</cell><cell>78.85</cell><cell>77.65</cell><cell>74.56</cell><cell>40.14</cell><cell>89.89</cell><cell>52.79</cell><cell>85.22</cell></row><row><cell>OE [11]</cell><cell>65.91</cell><cell>86.66</cell><cell>72.39</cell><cell>78.61</cell><cell>69.36</cell><cell>79.71</cell><cell>46.75</cell><cell>85.45</cell><cell>78.76</cell><cell>75.89</cell><cell>57.92</cell><cell>85.78</cell><cell>61.11</cell><cell>84.56</cell></row><row><cell>VOS [6]</cell><cell>65.56</cell><cell>87.86</cell><cell>74.65</cell><cell>82.12</cell><cell>70.58</cell><cell>83.76</cell><cell>47.16</cell><cell>90.98</cell><cell>73.78</cell><cell>81.58</cell><cell>84.45</cell><cell>72.20</cell><cell>82.43</cell><cell>76.95</cell></row><row><cell>FS [39]</cell><cell>22.75</cell><cell>94.33</cell><cell>45.45</cell><cell>85.61</cell><cell>40.52</cell><cell>87.21</cell><cell>11.76</cell><cell>97.58</cell><cell>44.08</cell><cell>86.26</cell><cell>47.61</cell><cell>88.42</cell><cell>47.09</cell><cell>86.64</cell></row><row><cell>RODD (Ours)</cell><cell>19.89</cell><cell>95.76</cell><cell>39.79</cell><cell>88.40</cell><cell>36.61</cell><cell>89.73</cell><cell>44.42</cell><cell>85.95</cell><cell>42.56</cell><cell>87.67</cell><cell>41.72</cell><cell>89.10</cell><cell>24.64</cell><cell>94.14</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Evaluation using corrupted ID test samples for CIFAR-100. All values are in % and averaged over 7 OOD datasets discussed in Section 3.1 whereas corruption severity is varied from 1-5 as in<ref type="bibr" target="#b8">[9]</ref>. ? indicates larger values are better and ? indicates smaller values are better.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Method Clean</cell><cell>Gauss</cell><cell cols="5">Noise Shot Impulse Defocus Motion Zoom Snow Frost Blur Weather Fog</cell><cell cols="3">Digital Bright Cont. Elastic Pixel</cell><cell>JPEG</cell></row><row><cell>?FPR95</cell><cell cols="4">VOS RODD 39.76 67.91 65.42 66.79 72.55 76.95</cell><cell>90.36 65.53</cell><cell>84.50 49.51</cell><cell>83.62 71.81</cell><cell cols="2">84.56 55.87 53.92 59.84 52.23 48.39 87.0 83.34 83.84 86.11 86.67 52.98</cell><cell>85.81 57.31</cell><cell>89.58 55.42 66.47 89.25</cell></row><row><cell>?AUROC</cell><cell>VOS RODD</cell><cell>81.9 88.1</cell><cell cols="2">74.26 72.90 77.18 78.40</cell><cell>60.00 78.41</cell><cell>68.35 84.70</cell><cell>69.83 74.64</cell><cell cols="2">68.55 65.31 68.14 68.50 66.54 66.82 82.42 83.50 80.60 83.85 85.54 83.44</cell><cell>66.98 81.91</cell><cell>61.18 83.11</cell><cell>62.38 78.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Clean and corruption accuracy (%) of RODD and Baseline on CIFAR10-C and CIFAR100-C.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Clean</cell><cell cols="5">Noise Gauss Shot Impulse Defocus Motion Zoom Snow Frost Blur Weather Fog</cell><cell>Digital Bright Cont. Elastic Pixel</cell><cell>JPEG</cell></row><row><cell>CIFAR10-C</cell><cell cols="2">Baseline 94.52 RODD 94.45</cell><cell>46.54 57.72 49.63 59.89</cell><cell>56.45 55.62</cell><cell>69.15 69.77</cell><cell>62.98 64.81</cell><cell cols="2">58.85 74.88 72.18 84.26 92.19 75.14 74.31 61.79 78.59 74.48 86.56 93.08 73.37 75.49 70.79 80.12 68.27 77.34</cell></row><row><cell>CIFAR100-C</cell><cell cols="2">Baseline 72.35 RODD 72.20</cell><cell>18.80 26.56 18.40 27.13</cell><cell>25.56 26.25</cell><cell>49.80 50.32</cell><cell>40.45 41.82</cell><cell cols="2">39.37 45.38 42.62 56.40 69.14 52.87 48.32 40.40 46.25 43.46 57.13 70.0 51.81 49.05</cell><cell>40.70 46.11 40.86 47.62</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research is based upon work supported by Leonardo DRS and partly by the National Science Foundation under Grant No. CCF-1718195 and ECCS-1810256.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Triggering failures: Out-of-distribution detection by learning from local adversarial attacks in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Besnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Briot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15701" to="15710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Atom: Robustifying out-of-distribution detection using outlier mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="430" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial robustness: From self-supervised pre-training to fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="699" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Vos: Learning what you don&apos;t know by virtual outlier synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.01197</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can autonomous vehicles identify, recover from, and adapt to distribution shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Filos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Tigkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3145" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Provable guarantees for self-supervised deep learning with spectral contrastive loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Jeff Z Haochen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04606</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Contrastive learning with adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hui</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Nvasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17081" to="17093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10951" to="10960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mos: Towards scaling out-ofdistribution detection for large semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8710" to="8719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ood-maml: Metalearning for few-shot out-of-distribution detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taewon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoung</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust pre-training by adversarial contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="16199" to="16210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Select to better learn: Fast and accurate deep learning using data selection from nonlinear manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Joneidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Vahidian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashkan</forename><surname>Esmaeili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Rahnavard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7819" to="7829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adversarial training for face recognition systems using contrastive adversarial learning and triplet loss fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazmul</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Meeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarinda</forename><surname>Samarasinghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Rf signal transformation and classification using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazmul</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Rahnavard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03564</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial self-supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2983" to="2994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Training confidence-calibrated classifiers for detecting outof-distribution samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09325</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayadurgam</forename><surname>Srikant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02690</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Energy-based out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weitang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Out-of-distribution detection for deep neural networks with isolation forest and local outlier factor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghua</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><forename type="middle">B</forename><surname>Freidovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingling</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="132980" to="132989" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generative probabilistic novelty detection with adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Pidhorskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranya</forename><surname>Almohsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Out-ofdistribution detection using multiple semantic label representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabi</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">React: Out-ofdistribution detection with rectified activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Out-ofdistribution detection using an ensemble of self supervised leave-out classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataraj</forename><surname>Jammalamadaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><forename type="middle">L</forename><surname>Willke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="550" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingmei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06755</idno>
		<title level="m">Turkergaze: Crowdsourcing saliency with webcam based eye tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Investigating why contrastive learning benefits robustness against label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Whitecross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baharan</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improved ood generalization via adversarial training and pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Ming</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Classificationreconstruction learning for open-set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Yoshihashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rei</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Naemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4016" to="4025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised out-ofdistribution detection by maximum classifier discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9518" to="9526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Outof-distribution detection using union of 1-dimensional subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zaeemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niccol?</forename><surname>Bisagno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Sambugaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Conci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Rahnavard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9452" to="9461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
