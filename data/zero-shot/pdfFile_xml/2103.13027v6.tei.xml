<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AutoMix: Unveiling the Power of Mixup for Stronger Classifiers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
							<email>liuzicheng@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310000</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<postCode>310000</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
							<email>lisiyuan@westlake.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<postCode>310000</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
							<email>wudi@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310000</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<postCode>310000</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Liu</surname></persName>
							<email>liuzihan@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310000</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<postCode>310000</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
							<email>chenzhiyuan@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310000</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<postCode>310000</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Wu</surname></persName>
							<email>wulirong@westlake.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<postCode>310000</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<email>stan.z.li@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310000</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<postCode>310000</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AutoMix: Unveiling the Power of Mixup for Stronger Classifiers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Equal contribution, Corresponding author</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data augmentation</term>
					<term>mixup</term>
					<term>image classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data mixing augmentation have proved to be effective in improving the generalization ability of deep neural networks. While early methods mix samples by hand-crafted policies (e.g., linear interpolation), recent methods utilize saliency information to match the mixed samples and labels via complex offline optimization. However, there arises a trade-off between precise mixing policies and optimization complexity. To address this challenge, we propose a novel automatic mixup (AutoMix) framework, where the mixup policy is parameterized and serves the ultimate classification goal directly. Specifically, AutoMix reformulates the mixup classification into two sub-tasks (i.e., mixed sample generation and mixup classification) with corresponding sub-networks and solves them in a bi-level optimization framework. For the generation, a learnable lightweight mixup generator, Mix Block, is designed to generate mixed samples by modeling patch-wise relationships under the direct supervision of the corresponding mixed labels. To prevent the degradation and instability of bi-level optimization, we further introduce a momentum pipeline to train AutoMix in an end-to-end manner. Extensive experiments on nine image benchmarks prove the superiority of AutoMix compared with state-of-the-art in various classification scenarios and downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have witnessed the great success of Deep Neural Networks (DNNs) in various tasks, such as image processing <ref type="bibr" target="#b50">[68,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b43">61,</ref><ref type="bibr">48,</ref><ref type="bibr">49]</ref>, graph learning <ref type="bibr" target="#b45">[63,</ref><ref type="bibr" target="#b42">60,</ref><ref type="bibr">5]</ref>, and video processing <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr">71]</ref>. Most of these successes can be attributed to the use of complex network architectures with numerous parameters and a sufficient amount of data. However, when the data is insufficient, models with high complexity, e.g., Transformer-based networks <ref type="bibr" target="#b11">[11,</ref><ref type="bibr">52]</ref>, are prone to over-fitting and overconfidence <ref type="bibr" target="#b16">[16]</ref>, resulting in poor generalization abilities <ref type="bibr" target="#b40">[58,</ref><ref type="bibr">47,</ref><ref type="bibr" target="#b2">2]</ref>.</p><p>To improve the generalization of DNNs, a series of data mixing augmentation techniques emerged. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> samples via a linear combination of corresponding data pairs; CutMix <ref type="bibr" target="#b48">[66]</ref> designs a patch replacement strategy that randomly replaces a patch in an image with patches from the other image. However, these hand-crafted methods <ref type="bibr" target="#b37">[55,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17]</ref> cannot guarantee mixed samples containing target objects and might cause the label mismatch problem. Subsequently, <ref type="bibr" target="#b39">[57,</ref><ref type="bibr" target="#b35">53,</ref><ref type="bibr">42]</ref> try to guide CutMix by saliency information to relieve this problem. Recently, optimization-based methods tried to solve the problem by searching an approximate mixing policy <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b25">25]</ref> based on portfolio optimization, e.g., maximizing the saliency regions to confirm the co-presence of the targets in the mixed samples. Although they design more precise mixing policies than hand-crafted methods, their indirect optimization and heavy computational overhead limit the algorithms' efficiency. Evidently, it is not efficient to transform the mixup policy from a random linear interpolation to a complex portfolio optimization problem.</p><p>This paper mainly discusses two questions: (1) how to design an accurate mixing policy and serve directly to the mixup classification objective; <ref type="bibr" target="#b2">(2)</ref> how to solve generation-classification optimization problems efficiently instead of portfolio optimizations. As a basis for solving these two issues, we first reformulate the mixup training into two sub-tasks, mixed sample generation and mixup classification. Then, we propose a novel automatic mixup framework (AutoMix) that generates accurate mixed samples by a generation subnetwork, Mix Block (MB), with a good complexity-accuracy trade-off. Specifically, MB is a cross-attention-based module that dynamically selects discriminative pixels based on feature maps of the sample pair to match the corresponding mixed labels. However, MB may collapse into trivial solutions when optimized jointly with the classification encoder due to a gradient entanglement problem. Thus, Momentum Pipeline (MP) is further introduced to stabilize AutoMix and decouple the training process of this bi-level optimization problem. Comprehensive experiments on eight classification benchmarks (CIFAR-10/100, Tiny-ImageNet, ImageNet-1k, CUB-200, FGVC-Aircraft, iNaturalist2017/2018, and Places205) and eight network architectures show that AutoMix consistently outperforms state-of-the-art mixup methods across different tasks. We further provide extensive analysis to verify the effectiveness of proposed components and the robustness of hyper-parameters. Our main contributions are three-fold:</p><p>-From a fresh perspective, we divide the mixup training into bi-level subtasks: mixed sample generation and mixup classification, and regard the generation as an auxiliary task to the classification. We unify them into a framework named AutoMix to optimize the mixup policy in an end-to-end manner. -A novel Mix Block is designed for mixed sample generation. The combination of Mix Block and Momentum Pipeline optimizes the two sub-tasks in a decoupled manner and improves mixup training accuracy and stability. -AutoMix surpasses counterparts significantly on various classification scenarios based on eight popular network architectures and downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Mixup training. We first consider the general image classification task with k different classes: given a finite set of n samples X = [x i ] n i=1 ? R n?W ?H?C and their ground-truth class labels Y = [y i ] n i=1 ? R n?k , encoded by a one-hot vector y i ? R k . We seek the mapping from the data x i to its class label y i modeled by a deep neural network f ? : x ?? y with network parameters ? by optimizing a classification loss (.), say the cross entropy (CE) loss,</p><formula xml:id="formula_0">CE (f ? (x), y) = ?y log f ? (x).<label>(1)</label></formula><p>Then we consider the mixup classification task: given a sample mixup function h, a label mixup function g, and a mixing ratio ? sampled from Beta(?, ?) distribution, we can generate the mixup data X mix with x mix = h(x i , x j , ?) and the mixup label Y mix with y mix = g(y i , y j , ?). Similarly, we learn f ? : x mix ?? y mix by mixup cross-entropy (MCE) loss,</p><formula xml:id="formula_1">M CE = ? CE (f ? (x mix ), y i ) + (1 ? ?) CE (f ? (x mix ), y j ).<label>(2)</label></formula><p>Mixup reformulation. Comparing Eq. 1 and Eq. 2, the mixup training has the following features: (1) extra mixup policies, g and h, are required to generate X mix and Y mix . (2) the classification performance of f ? depends on the generation policy of mixup. Naturally, we can split the mixup task into two complementary sub-tasks: (i) mixed sample generation and (ii) mixup classification. Notice that the sub-task (i) is subordinate to (ii) because the final goal is to obtain a stronger classifier. Therefore, from this perspective, we regard the mixup generation as an auxiliary task for the classification task. Since g is generally designed as a linear interpolation, i.e., g(y i , y j , ?) = ?y i + (1 ? ?)y j , h becomes the key function to determine the performance of the model. Generalizing previous offline methods, we define a parametric mixup policy h ? as the sub-task with another set of parameters ?. The final goal is to optimize M CE given ? and ? as below:</p><formula xml:id="formula_2">min ?, ? M CE f ? h ? (x i , x j , ?) , g(y i , y j , ?) .<label>(3)</label></formula><p>Offline mixup limits the power of mixup. Keep the reformulation in mind, the previous methods focus on manually designing h(?) in an offline and nonparametric manner based on their prior hypotheses, or arguably, such mixup  policies are separated from the ultimate optimization of the model, e.g., an optimization algorithm with the goal of maximizing saliency information. Specifically, they build an implicit connection between the two sub-tasks, as shown on the left of <ref type="figure" target="#fig_2">Figure 2</ref>. Therefore, the mixed samples generated from these offline mixup policies could be redundant or mislead the training. To address this, we propose AutoMix, which combines these two sub-tasks in a mutually beneficial manner and unveils the power of mixup. We build a bridge between the mixup generation and classification task with a unified optimization framework named as AutoMix to improve the mixup training efficiency. In this framework, the proposed Mix Block (MB) and Momentum Pipeline (MP) in Au-toMix not only can generate semantic mixed samples but reduces computational overhead significantly. A comparison overview with offline approaches is presented in <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Label Mismatch: MixBlock</head><p>In <ref type="figure">Figure 3</ref>, we further examined that offline approaches are incapable of addressing the label mismatch issue in mixup training. It is difficult for offline methods to preserve the discriminative features in the mixed sample if detached from the final optimization goal. As a result, the prediction of the accuracy of the mixed sample is limited (see the right of <ref type="figure" target="#fig_3">Figure 4</ref>). This paper presents a parametric mixup generation function named Mix Block (MB) M ? for learning a mixup policy without requiring extensive saliency computation. M ? generates a pixel-wise mixup mask s ? R H?W for the pairs of input images, where s w,h ? [0, 1]. We regard the mask-based mixup policy as an adaptive selection process in terms of ?, which can automatically select the discriminative patches from sample pairs to generate label-matched mixed samples. Thus, the core of M ? is the devised ? embedded cross-attention mechanism to learn the pixellevel proportional relationships in a given data pair. To do so, the deep feature maps z from f ? with rich spatial and semantic information can be utilized to bootstrap the two sub-tasks of mixup. Additionally, to facilitate the capture of task-relevant information in the generated mixed samples, the M ? training is directly supervised by the target loss, M CE , in an end-to-end manner.</p><p>Parametric mixup generation. The generation task can be formulated as a dynamic regression problem: given a sample pair (x i , x j ) and a mixing ratio ?, MB predicts the probability that each pixel (or patch) on x mix belongs to x i according to the feature map pair (z i , z j ) and mixing ratio ?. The overall parametric mixup function of AutoMix can be formulated as follows:</p><formula xml:id="formula_3">h ? (x i , x j , ?) = M ? (z l i,? , z l j,1?? ) x i + (1 ? M ? (z l i,? , z l j,1?? )) x j ,<label>(4)</label></formula><p>where denotes element-wise product; z l ? is ? embedded feature map at l-th layer. As shown in the right of <ref type="figure" target="#fig_4">Figure 5</ref>, we first embed ? with the l-th feature map in a simple and efficient way by concatenating, z l ? = concat(z, ?), whose effectiveness has been shown in the left of <ref type="figure" target="#fig_3">Figure 4</ref>. As we can see from Equation 4, our aim is to obtain a pixel-level mask s in the input space from M ? (?) based on ? embedded z l i,? and z l j,1?? to generate semantic mixed samples. In order to achieve this goal, a pair-wise similarity matrix P and an upsampling function U (?) is required. Due to the symmetry of mixup, i.e., the sum of the two masks used to generate a mixed sample is equal to 1, for x i of a pair (</p><formula xml:id="formula_4">x i , x j ), we can denote M ? : z l i,? , z l j,1?? ?? s i , s i = U ? P (z l i,? , z l j,1?? ) ? W Z z l i,? ,<label>(5)</label></formula><p>where W Z is a linear transformation matrix; ? is the Sigmoid activation function, which is used to probabilize the mask; and s i is the H ?W mask we are looking for. By multiplying P and the value embedding, W z z l i,? , the discriminative features in x i,? relative to x j,1?? are then selected. Symmetrically, the mask s j for x j can be calculated in this way, s j = 1 ? s i . Furthermore, the similarity matrix P has to consider both ? information and relative relationships in a sample pair; thus, the cross-attention mechanism is introduced to achieve this purpose. When x i in a sample pair (x i , x j ) is taken as the input, a mask can be generated dynamically from corresponding z l i,? and P matrix. Formally, our cross-attention can be formulated as:</p><formula xml:id="formula_5">P (z l i,? , z l j,1?? ) = softmax (W P z l i,? ) T ? W P z l j,1?? C(z l i,? , z l j,1?? ) ,<label>(6)</label></formula><p>where W P denotes shared linear transformation matrices (e.g., 1?1 convolution), ? denotes matrix multiplication, and C(z l i,? , z l j,1?? ) is a normalization factor. Notice that P is the row normalized pair-wise similarity matrix between every spatial position on z l i,? and z l j,1?? . Similarly, if we take z l j,1?? as the value, then the mask can be computed by transposing P and s i = 1 ? s j .</p><p>AutoMix in end-to-end training. The framework is shown in <ref type="figure" target="#fig_4">Figure 5</ref>, given a set of labeled data D = {(x i , y i )} n i=1 and the corresponding l-th layer feature map Z = {z l i } n i=1 , M ? is nested in encoder for optimization. Under the supervision of the same loss M CE , the encoder is trained using the mixed sample generated by M ? , which in turn uses the backbone's feature to generate the mixed sample. To enable M ? to find the ? correspondence between the x mix and y mix at the early stage of training, our auxiliary loss is proposed:</p><formula xml:id="formula_6">? = ? max ||? ? 1 HW h,w s i,h,w || ? , 0 ,<label>(7)</label></formula><p>where ? is a loss weight linearly decreased to 0 during training. We set the initial ? to 0.1 and = 0.1. Notice that AutoMix uses standard cross-entropy loss CE as default. CE loss facilitates the backbone to provide a stable feature map at the early stage so that speeds up M ? converges. To differentiate the function of M CE , cls denotes classification task for training encoder and gen denotes generation task for training M ? . AutoMix can be optimized by a joint loss:</p><formula xml:id="formula_7">L(?, ?) = CE + cls M CE classif ication + gen M CE + ? generation .<label>(8)</label></formula><p>Obviously, the purpose of the classification task is to optimize ? while the generation task is to optimize ?. Therefore, this is a typical bi-level optimization problem. Although M ? does not need extra computational overhead to maximize the saliency information, using SGD to directly update the nested ? and ? will lead to instability. To address this problem properly, we use the momentum pipeline to decouple the training of ? and ?. As indicated in Eq. 8, though the same M CE is used, the focus of each is different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bi-level Optimization: Momentum Pipeline</head><p>Image A Image B Although MB is designed to be lightweight and efficient, it also poses a bi-level optimization problem with gradient entanglement. Experiments demonstrate that the entanglement problem may cause M ? trapped into a trivial solution (degraded to MixUp, in <ref type="figure" target="#fig_5">Figure 6</ref>). M ? with much smaller parameters than the encoder will be disturbed by the classification task when optimizing both the two sub-tasks at the same time. MB thus cannot generate semantic mixed samples stably and eventually collapse. According to Eq. 3 and Eq. 8, for each iteration, the gradient entanglement problem of L cls in M ? can be formulated as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-trivial Trivial</head><formula xml:id="formula_8">? ? L cls M CE ? ? ? h ? (x i , x j , ?) f ? (h ? (x i , x j , ?)).<label>(9)</label></formula><p>It is notable that the instability of f ? may result in a vicious cycle of joint training. As a consequence, the primary goal of getting the Eq. 3 operating well is to ensure that f ? outputs stable features and, to the extent possible, that ? and ? can focus on their own tasks in the case of using the same loss. Inspired by methods in self-supervised learning <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b15">15]</ref>, they adopted momentum pipeline (MP) to avoid the feature collapse and realized that the teacher network f ? k of the Siamese network shows more stable performance than student network f ?q . Along this path, we designed a new MP for decoupling the nested bi-level optimization problem of AutoMix: the student network f ?q focuses on the classification task, while the stable teacher network f ? k is connected with M ? to perform generation task. Moreover, optimizing Eq. 8 with batch approach requires X mix generated by f ? k and M ? first and then using X mix to optimize f ?q . By analogy, referring to the Expectation-Maximization (EM) algorithm, the two sets of parameters ? and ? can be optimized in an alternating way by the designed MP, i.e., first fix one set of parameters optimizing the other:  where t is the iteration step, ? q and ? k represent the parameters of the student and teacher network, respectively. Note that f ?q and f ? k share the same network structure with the same initialized parameters, but f ? k is updated via an exponential moving average (EMA) strategy [41] from f ?q :</p><formula xml:id="formula_9">? t q ? argmin ? L(? t?1 q , ? t?1 ),<label>(10)</label></formula><formula xml:id="formula_10">? t ? argmin ? L(? t k , ? t?1 ),<label>(11)</label></formula><formula xml:id="formula_11">? k ? m? k + (1 ? m)? q ,<label>(12)</label></formula><p>where m ? [0, 1) is the momentum coefficient. It is worthy to notice that MP not only solves optimization instability but also significantly speeds up and stabilizes the convergence of AutoMix. In <ref type="figure" target="#fig_6">Figure 7</ref>, M ? gets close to convergence in the first few epochs and consistently delivers high-quality mixed samples to f ? . Moreover, detailed AutoMix architecture and pseudo code are provided in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate AutoMix in three aspects: (1) Image classification in various scenarios based on various network architectures, (2) Robustness against corruptions and adversarial samples, and <ref type="formula" target="#formula_2">(3)</ref> Transfer learning capacities to downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation on Image Classification</head><p>This subsection demonstrates performance gains of AutoMix for various classification tasks on eight classification benchmarks, including CIFAR-10/100 <ref type="bibr" target="#b27">[27]</ref>, Tiny-ImageNet <ref type="bibr">[</ref>  <ref type="bibr" target="#b25">[25]</ref>, and SuperMix <ref type="bibr" target="#b10">[10]</ref>. Notice that AugMix is reproduced by timm <ref type="bibr" target="#b41">[59]</ref>, * denotes open-source arXiv preprint work, and ? denotes the results reproduced by the official source code (Co-Mixup, AlignMix <ref type="bibr" target="#b36">[54]</ref>, and TransMix <ref type="bibr" target="#b4">[4]</ref>). All mixup augmentation methods use the optimal ? among {0.2, 0.5, 1, 2, 4}, while the rest of the hyper-parameters follow the original paper. AutoMix uses the same set of hyper-parameters in all experiments: ? = 2, the feature layer l = 3, and the momentum coefficient in MP starts from m = 0.999 and is increased to 1 in a cosine curve. As for all classification results, we report the mean performance of 3 trials where the median of top-1 test accuracy in the last 10 training epochs is recorded for each trial, and bold and blue denote the best and second best results.  <ref type="bibr" target="#b16">[16]</ref> on CIFAR-100, i.e., the absolute discrepancy between accuracy and confidence. As shown in <ref type="figure" target="#fig_8">Figure 8</ref>, AutoMix has the best calibration effect among all competitors, with the ECE error rate of 2.3%, closest to the red diagonal. We can see from the figure that the Cut series does not perform well on calibration, but may further aggravate the overconfidence; while MixUp and ManifoldMix calibrate the predictions, but cause the under-confidence problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet Datasets</head><p>Settings. In the more challenging large-scale classification scenarios, mixup methods are widely used, especially for recently proposed Transformer-based networks. We evaluate AutoMix and popular mixup variants on ImageNet-1k using three popular training procedures: (a) PyTorch-style setting trains 100 or 300 epochs by SGD optimizer with the batch size of 256, the basic learning rate of 0.1, the SGD weight decay of 0.0001, and the SGD momentum of 0.9, which is the standard benchmarks for mixup methods <ref type="bibr" target="#b48">[66,</ref><ref type="bibr">42]</ref>      <ref type="table" target="#tab_4">Table 2</ref> and <ref type="figure" target="#fig_0">Figure 1</ref> show regular image classification results using only one mixup methods: AutoMix consistently outperforms previous state-of-the-art methods with light/median/heavy ResNet architectures, e.g., +0.26?0.44% for 100 epochs and +0.22?0.34% for 300 epochs. <ref type="table" target="#tab_6">Table 3</ref> and <ref type="table" target="#tab_7">Table 4</ref> report results on more practical training settings: RSB and DeiT denote randomly combining Mixup and CutMix which produces competitive performs as previous state-of-the-art methods (e.g., PuzzleMix), while AutoMix still brings significantly gains over the original RSB (+0.32?1.30%) and DeiT (+0.18?0.98%). It is worth noticing that previous mixup variants yield little performance gain when adopted on lightweight ConvNets, while AutoMix achieves stable performance gains on these backbones. For example, AutoMix and previous methods improve Vanilla by +0.38% vs 0.08% based on ResNet-18 in <ref type="table" target="#tab_4">Table 2</ref>, and improve Vanilla by 0.32% vs -0.08% based on EfficientNet B0 in <ref type="table" target="#tab_6">Table 3</ref>). Moreover, AutoMix brings remarkable gains over the DeiT setting (0.12?0.98%) based on Transformer architectures. AutoMix also yields more competitive performances than the recently proposed Transformer-based mixup method, TransMix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation on Fine-grained and Scene Classification</head><p>Small-scale datasets. We first perform small-scale fine-grained classification following transfer learning settings on CUB-200 and Aircraft: training 200 epochs by SGD optimizer with the initial learning rate of 0.001, the weight decay of 0.0005, the batch size of 16, using the standard augmentations as in Sec. 4.1; the official PyTorch pre-trained models on ImageNet-1k are adopted as initialization. <ref type="table">Table 5</ref> shows that AutoMix achieves the best performance and noticeably improves Vanilla (2.19%/3.55% on CUB-200 and 1.14%/1.62% on Aircraft), which verifies that AutoMix has strong adaptability to more challenging scenarios. Since some specific attributes are more useful to distinguish similar classes in fine-grained scenarios, AutoMix generates mixed samples with discriminative patches (e.g., head and beak of birds) rather than a complete object. Large-scale datasets. Then, we adopt similar settings as (a) in Sec. 4.1 with the total epoch of 100 epochs (training from scratch) on large-scale datasets based on ResNet variants. As for the imbalanced and long-tail fine-grained recognition tasks on iNat2017/2018, <ref type="table">Table 5</ref> shows that AutoMix surpasses the previous best methods and improves Vanilla by large margins (2.74%/4.33% on iNat2017 and 2.20%/3.55% on iNat2018), which demonstrates that AutoMix can alleviate the long-tail and imbalance issues. As for scenic classification on Places205, AutoMix still sets state-of-the-art performances. Therefore, we can conclude that AutoMix can adapt to more challenging scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Robustness</head><p>We first evaluate robustness against corruptions on CIFAR-100-C <ref type="bibr" target="#b20">[20]</ref>, which is designed for evaluating the corruption robustness and provides 19 different corruptions (e.g., noise, blur, and digital corruption, etc). AugMix <ref type="bibr" target="#b21">[21]</ref> is proposed to improve robustness against natural corruptions by minimizing Jensen-Shannon divergence (JSD) between logits of a clean image and two AugMix images. However, the improvement of AugMix is very limited to clean data. In <ref type="table">Table 6</ref>, AutoMix shows a consistent top level in both clean and corruption data. We further study robustness against the FGSM <ref type="bibr" target="#b14">[14]</ref> white box attack of 8/255 ? epsilon ball following <ref type="bibr" target="#b51">[69]</ref>, and AutoMix outperforms previous methods in <ref type="table">Table 6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Transfer Learning</head><p>Weakly supervised object localization. Following CutMix, we also evaluate AutoMix on the weakly supervised object localization (WSOL) task on CUB-200, which aims to localize objects of interest without bounding box supervision. We use CAM to extract attention maps, and calculate the maximal box accuracy with a threshold ? ? {0.3, 0.5, 0.7}, following MaxBoxAccV2 <ref type="bibr" target="#b6">[6]</ref>. <ref type="table" target="#tab_10">Table 8</ref> shows that AutoMix achieves the best performance to localize semantic regions. Object detection. We then evaluate transferable abilities of the learned features to object detection task with Faster R-CNN [43] on PASCAL VOC train-val07+12 <ref type="bibr" target="#b12">[12]</ref> and COCO train2017 [32] based on Detectron2 <ref type="bibr" target="#b44">[62]</ref>. We fine-tune Faster R-CNN with R50-C4 pre-trained on ImageNet-1k with mixup methods on VOC (24k iterations) and COCO (2? schedule). <ref type="table">Table 7</ref> shows that AutoMix achieves better performances than previous cutting-based mixup variants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We conduct an ablation study to prove that each component of AutoMix plays an essential role to make the framework operate properly. Three main questions are answered here: (1) Are the modules in MB effective? (2) How many gains can MB bring without EMA and CE? (3) Is AutoMix robust to hyperparameters?</p><p>(1) The cross-attention mechanism enables MB to capture the task-relevant pixels between two samples, which is the core design of MB to generate useful mixed masks. Based on this, ? embedding and ? encourage MB to learn proportional correspondence on a different scale. Without these modules, the performance drops by almost 4% (66.83% vs. 70.72%), as shown in <ref type="figure" target="#fig_9">Figure 9</ref>. (2) In <ref type="table" target="#tab_3">Table 10</ref>, we show that the EMA and CE adopted in the MP improve the performance of MB by ensuring training stability, however, CE is not as effective for other mixup methods. Most importantly, without these them, i.e. EMA and CE, we show MB still delivers significant gains (e.g. +2.29% and +2.21% on CIFAR-100 and Tiny). Note that m = 0 indicates removing EMA, which means f ? k is a copy of f ?q with the same weights. Therefore, we can confirm the effectiveness of M ? .  (3) AutoMix has two core hyper-parameters, ? and l, which are fixed for all experiments. A larger ? facilitates MB to learn intra-class relationships. <ref type="figure" target="#fig_9">Figure 9</ref> shows that AutoMix with ? = 2 as default achieves the best performances on various datasets. The feature layer l 3 makes a good trade-off between the performance and complexity, as shown in <ref type="table" target="#tab_3">Table 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>MixUp <ref type="bibr" target="#b51">[69]</ref>, the first mixing-based data augmentation algorithm, was proposed to generate mixed samples with mixed labels by convex interpolations of any two samples and their unique one-hot labels. ManifoldMix <ref type="bibr" target="#b37">[55]</ref> extends MixUp to the hidden space of DNNs and <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b36">54]</ref> improves ManifoldMix. CutMix <ref type="bibr" target="#b48">[66]</ref> incorporates the Dropout strategy into the mixup strategy and proposes a mixing strategy based on the patch of the image, i.e., randomly replacing a local rectangular area in images. Based on CutMix, AttentiveMix <ref type="bibr" target="#b39">[57]</ref> and SaliencyMix <ref type="bibr" target="#b35">[53]</ref> guide mixing patches by saliency regions in the image (based on CAM or a saliency detector) to obtain mixed samples with more class-relevant information; ResizeMix [42] maintains the information integrity by replacing one resized image directly into a rectangular area of another image; FMix <ref type="bibr" target="#b17">[17]</ref> transforms images into the spectrum domain to generate binary masks by setting a threshold; other researchers design refined mixing strategies <ref type="bibr">[1,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b22">22]</ref>, Furthermore, PuzzleMix <ref type="bibr" target="#b26">[26]</ref> and Co-Mixup <ref type="bibr" target="#b25">[25]</ref> propose combinatorial optimization strategies to find optimal mixup masks by maximizing the saliency information. Compared with previous methods, AutoMix does not require a hand-crafted sample mixing strategy or saliency information but adaptively generates mixed samples based on mixing ratios and feature maps in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Limitations</head><p>In this paper, we propose an AutoMix framework, which optimizes both the mixed sample generation task and the mixup classification task in a momentum training pipeline. Without adding cost to inference, AutoMix can generate outof-manifold samples with adaptive masks. Extensive experiments have shown the effectiveness and excellent generalizability of the proposed AutoMix on CIFAR, ImageNet, and fine-grained datasets. On top of that, we also outperformed other mixup algorithms when comparing with robustness and localization tasks as well. Furthermore, the proposed momentum training pipeline serves as a significant improvement in convergence speed and overall performance. As for future work, we consider improving AutoMix in four aspects. (i) AutoMix is now learning the mixed policy by the proposed cross-attention-based module between only two samples, and it would be more efficient if it could be extended to multiple samples. (ii) Supervised labels are required to learn the online mixup policy in AutoMix, which limits the AutoMix to supervised tasks. It would be a general mixup strategy if we extend AutoMix to task-agnostic visual representation learning. (iii) Although the time complexity of AutoMix is faster than that of the combinatorial optimization-based methods, there is still a big gap with the hand-crafted methods. A pre-trained Mix Block will be a promising avenue in future research. (iv) Despite mixup augmentation techniques are widely studied and used on classification tasks, mixups applied in various downstream tasks are still limited to some variants of Mixup <ref type="bibr" target="#b51">[69]</ref> and CutMix <ref type="bibr" target="#b48">[66]</ref> (e.g., Yolo.V4 <ref type="bibr" target="#b3">[3]</ref> employs Mixup and CutMix for object detection). It would benefit downstream tasks if we can extend AutoMix to object detection and instance segmentation with limited training samples. For example, AutoMix might be used as PuzzleMix <ref type="bibr" target="#b26">[26]</ref>  More Experiments. We evaluate AutoMix for various training epochs on CIFAR-10/100 based on ResNet-18 (R-18) and ResNeXt-50 (RX-50), as shown in <ref type="table" target="#tab_3">Table 13</ref> and <ref type="table" target="#tab_3">Table 14</ref>. It is worth noting that some methods converge fast while suffering performance decay with longer train times, such as CutMix and SaliencyMix, and some methods perform better when train longer, such as ManifoldMix training 1200 epochs. Unlike these methods, AutoMix steadily outperforms them by a large margin regardless of the training time setting.</p><p>Hyperparameters for AutoMix. We further analyze the hyper-parameter setting for AutoMix with extra ablation studies conducted on Tiny-ImageNet and ImageNet-1k with various network architectures. As the same conclusion we provided in main body of experiment, the result in <ref type="figure" target="#fig_0">Figure 10</ref> also recommends the choice of l = 3, which reflects the hyper-parameter robustness of AutoMix. The detailed structure of AutoMix is illustrated in <ref type="figure" target="#fig_0">Figure 11</ref>. Similar to the flow chart in the method, the module colored as blue can be updated by backpropagation but not green. Furthermore, the dotted line means stopgradient. Notice that we use the encoder k for inference and drop M ? after training. The training process contains three steps: (1) using the momentum encoder k to generate the feature maps z for M ? ; (2) generating X q mix and X k mix based on two mixing factors ? q and ? k and the feature maps;</p><p>(3) training the active encoder q with mixed samples X q mix and optimizing M ? with X k mix separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Algorithm of AutoMix</head><p>We provide the pseudo code of AutoMix in Pytorch style:</p><p>Gradual change of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradual change of</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>, MixUp [69] generates augmented arXiv:2103.13027v6 [cs.CV] 21 Sep 2022 The plot of efficiency vs. accuracy on ImageNet-1k and visualization of mixup methods. AutoMix improves performance without the heavy computational overhead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The difference between AutoMix and offline approaches. Left: Offline mixup methods, where a fixed mixup policy generates mixed samples for the classifier to learn from. Right: AutoMix, where the mixup policy is trained with the feature map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Left: AutoMix samples with different ? (0, 0.3, 0.7, 1). Right: Top-1 accuracy of mixed data. Prediction is counted as correct if the top-1 prediction belongs to {yi, yj}; Top-2 accuracy is calculated by counting the top-2 predictions are equal to {yi, yj}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>The left diagram represents the five key steps of AutoMix. (1) Extract feature map Z from the frozen encoder k. (2) Mix Block M ? generates mixed samples by using Z and mixup ratio ? ? [0, 1]. (3) and (4) Decoupled training M ? and encoder q via stop gradient, the blue and green lines indicate the encoder training and the M ? training, correspondingly. (5) Update the k's parameters through momentum moving. The right diagram is the architecture of proposed M ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Accuracy on Tiny-ImageNet and different results of the mixed sample. Momentum pipeline decoupled mixup generation and classification, which mitigates the trivial solution problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Visualization of mixed samples generated by M ? with ? = 0.5 at different training periods on ImageNet-1k (100 epochs in total). It is worth noting that M ? is able to generate mixed samples stably and converge quickly with the addition of MP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>; (b) DeiT setting trains 300 epochs by AdamW optimizer[38]  with the batch size of 1024, the basic learning rate of 0.001, and the weight decay of 0.05; (c) timm<ref type="bibr" target="#b41">[59]</ref> RSB A2/A3 settings train 300/100 epochs by LAMB optimizer<ref type="bibr" target="#b47">[65]</ref> with the batch size of 2048, the basic learning rate of 0.005/0.008, and the weight decay of 0.02. More detailed ingredients and hyper-parameters are provided in Appendix. These three settings adopt the basic data augmentations (RandomResizedCrop</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Calibration plots of Mixup variants and AutoMix on CIFAR-100 using ResNet-18. The red line indicates the expected prediction tendency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Ablation of hyperparameter ? of Au-toMix on CIFAR-100 and Tiny-ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>The network architecture of Au-toMix. The parameters in blue modules (active) are updated by backpropagation while the green (freeze) using momentum update in Equation 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Visualization of mixed samples on ImageNet-1k. The upper part presents the plot of mixed samples from AutoMix (l = 3) for ? = 0.5; the lower shows the mixed samples when different ? values are taken.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Visualization of mixed samples on ImageNet-1k. The upper part presents the plot of mixed samples from AutoMix (l = 3) for ? = 0.5; the lower offers the mixed samples when different ? values are taken.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Top-1 accuracy (%)? of various algorithms based on ResNet variants for small-scale classification on CIFAR-10/100 and Tiny-ImageNet datasets. Gain +0.19 +0.32 +0.87 +0.76 +0.13 +1.41 +2.70 Mixup (+0.13?0.87%) based on various ResNet architectures on CIFAR-10/100. Moreover, AutoMix noticeably outperforms previous best algorithms by 1.41% and 2.70% on the more challenging Tiny-ImageNet. Calibration. DNNs tend to predict over-confidently in classification tasks [51], mixup methods can significantly alleviate this problem. To verify the calibration ability of AutoMix, we evaluate popular mixup algorithms by the expected calibration error (ECE)</figDesc><table><row><cell></cell><cell>CIFAR-10</cell><cell cols="2">CIFAR-100</cell><cell>Tiny-ImageNet</cell></row><row><cell>Method</cell><cell cols="4">R-18 RX-50 R-18 RX-50 WRN-28-8 R-18 RX-50</cell></row><row><cell>Vanilla</cell><cell cols="2">95.50 96.23 78.04 81.09</cell><cell>81.63</cell><cell>61.68 65.04</cell></row><row><cell>MixUp</cell><cell cols="2">96.62 97.30 79.12 82.10</cell><cell>82.82</cell><cell>63.86 66.36</cell></row><row><cell>CutMix</cell><cell cols="2">96.68 97.01 78.17 81.67</cell><cell>84.45</cell><cell>65.53 66.47</cell></row><row><cell cols="3">ManifoldMix 96.71 97.33 80.35 82.88</cell><cell>83.24</cell><cell>64.15 67.30</cell></row><row><cell cols="3">SaliencyMix 96.53 97.18 79.12 81.53</cell><cell>84.35</cell><cell>64.60 66.55</cell></row><row><cell>FMix  *</cell><cell cols="2">96.58 96.76 79.69 81.90</cell><cell>84.21</cell><cell>63.47 65.08</cell></row><row><cell>PuzzleMix</cell><cell cols="2">97.10 97.27 81.13 82.85</cell><cell>85.02</cell><cell>65.81 67.83</cell></row><row><cell>Co-Mixup  ?</cell><cell cols="2">97.15 97.32 81.17 82.91</cell><cell>85.05</cell><cell>65.92 68.02</cell></row><row><cell cols="3">ResizeMix  *  96.76 97.21 80.01 81.82</cell><cell>84.87</cell><cell>63.74 65.87</cell></row><row><cell>AutoMix</cell><cell cols="2">97.34 97.65 82.04 83.64</cell><cell>85.18</cell><cell>67.33 70.72</cell></row></table><note>Small-scale Datasets Settings. On CIFAR-10/100, RandomFlip and RandomCrop with 4 pixels padding for 32?32 resolutions are basic data augmentations, and we use the following training settings: SGD optimizer with SGD weight decay of 0.0001, the momen- tum of 0.9, the batch size of 100, and training 800 epochs; the basic learning rate is 0.1 adjusted by Cosine Scheduler [37]. On Tiny-ImageNet, the basic augmenta- tions include RandomFlip and RandomResizedCrop for 64?64 resolutions, and we use the similar training ingredients as CIFAR except for the basic learning rate of 0.2 and training 400 epochs. CIFAR version of ResNet variants [19] are used, i.e., replacing the 7 ? 7 convolution and MaxPooling by a 3 ? 3 convolution. Classification. Table 1 shows small-scale classification results on CIFAR-10/100 and Tiny datasets. Compared to the previous state-of-the-art methods, AutoMix consistently surpasses ManifoldMix (+0.32?1.94%), PuzzleMix (+0.16?0.91%), and Co-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Top-1 accuracy (%)? of image classification based on ResNet variants on ImageNet-1k using PyTorch-style 100-epoch and 300-epoch training procedures. .58 77.17 78.96 80.42 71.01 75.16 78.69 80.59 ManifoldMix 69.98 73.98 77.01 79.02 79.93 71.73 75.44 78.21 80.64 SaliencyMix 69.16 73.56 77.14 79.32 80.27 70.21 75.01 78.46 80.45 FMix * 69.96 74.08 77.19 79.09 80.06 70.30 75.12 78.51 80.20 PuzzleMix 70.12 74.26 77.54 79.43 80.53 71.64 75.84 78.86 80.67 ResizeMix * 69.50 73.88 77.42 79.27 80.55 71.32 75.64 78.91 80.52 AutoMix 70.50 74.52 77.91 79.87 80.89 72.05 76.10 79.25 80.98 Gain +0.38 +0.26 +0.37 +0.44 +0.34 +0.22 +0.26 +0.34 +0.31</figDesc><table><row><cell></cell><cell>PyTorch 100 epochs</cell><cell>PyTorch 300 epochs</cell></row><row><cell>Methods</cell><cell cols="2">R-18 R-34 R-50 R-101 RX-101 R-18 R-34 R-50 R-101</cell></row><row><cell>Vanilla</cell><cell cols="2">70.04 73.85 76.83 78.18 78.71 71.83 75.29 77.35 78.91</cell></row><row><cell>MixUp</cell><cell cols="2">69.98 73.97 77.12 78.97 79.98 71.72 75.73 78.44 80.60</cell></row><row><cell>CutMix</cell><cell>68.95 73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Top-1 accuracy (%)? on ImageNet-1k based on various ConvNets using RSB A2/A3 training settings.</figDesc><table><row><cell></cell><cell cols="5">R-50 EfficientNet B0 MobileNet.V2</cell></row><row><cell>Methods</cell><cell>A3</cell><cell>A2</cell><cell>A3</cell><cell>A2</cell><cell>A3</cell></row><row><cell>RSB</cell><cell cols="5">78.08 77.26 74.02 72.87 69.86</cell></row><row><cell>MixUp</cell><cell cols="5">77.66 77.19 73.87 72.78 70.17</cell></row><row><cell>CutMix</cell><cell cols="5">77.62 77.24 73.46 72.23 69.62</cell></row><row><cell cols="6">ManifoldMix 77.78 77.22 73.83 72.34 70.05</cell></row><row><cell cols="6">SaliencyMix 77.93 77.67 73.42 72.07 69.69</cell></row><row><cell>FMix  *</cell><cell cols="5">77.76 77.33 73.71 72.79 70.10</cell></row><row><cell>PuzzleMix</cell><cell cols="5">78.02 77.35 74.10 72.85 70.04</cell></row><row><cell cols="6">ResizeMix  *  77.85 77.27 73.67 72.50 69.94</cell></row><row><cell>AutoMix</cell><cell cols="5">78.44 77.58 74.61 73.19 71.16</cell></row><row><cell>Gain</cell><cell cols="5">+0.36 +0.23 +0.51 +0.32 +0.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Top-1 accuracy (%)? on ImageNet-1k based on ViTs and Con-vNeXt using DeiT training settings.</figDesc><table><row><cell>Methods</cell><cell cols="2">DeiT-S Swin-T ConvNeXt-T</cell></row><row><cell>DeiT</cell><cell>79.80 81.28</cell><cell>82.10</cell></row><row><cell>MixUp</cell><cell>79.65 81.01</cell><cell>80.88</cell></row><row><cell>CutMix</cell><cell>79.78 81.20</cell><cell>81.57</cell></row><row><cell cols="2">AttentiveMix 77.63 77.27</cell><cell>78.19</cell></row><row><cell cols="2">SaliencyMix 79.88 81.37</cell><cell>81.33</cell></row><row><cell>FMix  *</cell><cell>77.37 79.60</cell><cell>81.04</cell></row><row><cell>PuzzleMix</cell><cell>80.45 81.47</cell><cell>81.48</cell></row><row><cell>ResizeMix  *</cell><cell>78.61 81.36</cell><cell>81.64</cell></row><row><cell>TransMix  ?</cell><cell>80.70 81.80</cell><cell>-</cell></row><row><cell>AutoMix</cell><cell>80.78 81.80</cell><cell>82.28</cell></row><row><cell>Gain</cell><cell>+0.08 +0.00</cell><cell>+0.18</cell></row></table><note>and RandomFlip) for 224 ? 224 resolutions with Cosine Scheduler by default, (b) and (c) use RandAugment [8] for better performances. Classification.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .Table 6</head><label>56</label><figDesc>Top-1 accuracy (%)? of various algorithms based on ResNet variants on fine-grained and scenic classification datasets. 83.01 80.23 85.10 60.23 63.70 62.53 66.94 59.63 63.10 MixUp 78.39 84.58 79.52 85.18 61.22 66.27 62.69 67.56 59.33 63.01 CutMix 78.40 85.68 78.84 84.55 62.34 67.59 63.91 69.75 59.21 63.75 ManifoldMix 79.76 86.38 80.68 86.60 61.47 66.08 63.46 69.30 59.46 63.23 SaliencyMix 77.95 83.29 80.02 84.31 62.51 67.20 64.27 70.01 59.50 63.33 FMix * 77.28 84.06 79.36 86.23 61.90 66.64 63.71 69.46 59.51 63.63 PuzzleMix 78.63 84.51 80.76 86.23 62.66 67.72 64.36 70.12 59.62 63.91 ResizeMix * 78.50 84.77 78.10 84.08 62.29 66.82 64.12 69.30 59.66 63.88 AutoMix 79.87 86.56 81.37 86.72 63.08 68.03 64.73 70.49 59.74 64.06 Gain +0.11 +0.18 +0.61 +0.12 +0.42 +0.31 +0.37 +0.37 +0.08 +0.15</figDesc><table><row><cell></cell><cell></cell><cell cols="3">CUB-200 FGVC-Aircraft</cell><cell cols="2">iNat2017</cell><cell>iNat2018</cell><cell>Places205</cell></row><row><cell cols="2">Method</cell><cell cols="5">R-18 RX-50 R-18 RX-50 R-50 RX-101 R-50 RX-101 R-18 R-50</cell></row><row><cell>Vanilla</cell><cell cols="5">77.68 . Top-1 accuracy (%)? and FGSM er-</cell><cell>Table 7. Trasfer learning of object de-</cell></row><row><cell cols="6">ror (%)? on CIFAR-100 based on ResNeXt-50</cell><cell>tection task with Faster-RCNN on Pas-</cell></row><row><cell cols="4">(32x4d) trained 400 epochs.</cell><cell></cell><cell></cell><cell>cal VOC and COCO datasets.</cell></row><row><cell></cell><cell></cell><cell cols="4">Clean Corruption FGSM Acc(%)? Acc(%)? Error(%)?</cell><cell>VOC Methods mAP mAP AP bb COCO 50 AP bb 75</cell></row><row><cell cols="2">Vanilla</cell><cell>80.24</cell><cell>51.71</cell><cell>63.92</cell><cell></cell><cell>Vanilla</cell><cell>81.0 38.1 59.1 41.8</cell></row><row><cell cols="2">MixUp</cell><cell>82.44</cell><cell>58.10</cell><cell>56.60</cell><cell></cell><cell>Mixup</cell><cell>80.7 37.9 59.0 41.7</cell></row><row><cell cols="2">CutMix</cell><cell>81.09</cell><cell>49.32</cell><cell>76.84</cell><cell></cell><cell>CutMix</cell><cell>81.9 38.2 59.3 42.0</cell></row><row><cell cols="2">AugMix</cell><cell>81.18</cell><cell>66.54</cell><cell>55.59</cell><cell></cell><cell>PuzzleMix 81.9 38.3 59.3 42.1</cell></row><row><cell cols="3">PuzzleMix 82.76</cell><cell>57.82</cell><cell>63.71</cell><cell></cell><cell>ResizeMix 82.1 38.4 59.4 42.1</cell></row><row><cell cols="3">AutoMix 83.13</cell><cell>58.35</cell><cell>55.34</cell><cell></cell><cell>AutoMix 82.4 38.6 59.5 42.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .Table 10 .</head><label>910</label><figDesc>Ablation of modules in MixBlock. Ablation of the proposed momentum pipeline (MP) and the cross-entropy loss lCE (CE) based on ResNet-18. 79.57 81.93 66.02 65.72 67.19 70.13 70.02 70.45 +MP+CE 80.41 79.64 82.04 66.10 65.05 67.33 70.10 70.04 70.50</figDesc><table><row><cell></cell><cell>Tiny-ImageNet</cell><cell></cell><cell cols="3">CIFAR-100</cell><cell cols="3">Tiny-ImageNet</cell><cell></cell><cell cols="2">ImageNet-1k</cell></row><row><cell>module</cell><cell>R-18 RX-50</cell><cell>modules</cell><cell cols="9">MixUp CutMix M? MixUp CutMix M? MixUp CutMix M?</cell></row><row><cell cols="2">(random grids) 64.40 66.83</cell><cell>(none)</cell><cell cols="9">79.12 78.17 79.46 63.39 64.40 64.84 69.98 68.95 70.04</cell></row><row><cell cols="2">+cross attention 66.87 69.76</cell><cell>+MP(m=0)</cell><cell>-</cell><cell>-</cell><cell>81.75</cell><cell>-</cell><cell>-</cell><cell>67.05</cell><cell>-</cell><cell>-</cell><cell>70.41</cell></row><row><cell cols="2">+? embedding 67.15 70.41</cell><cell>+MP</cell><cell>80.82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ ?</cell><cell>67.33 70.72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>MaxBoxAcc (%)? for the WSOL task on CUB-200 based on ResNet variants.</figDesc><table><row><cell cols="4">Backbone Vanilla Mixup CutMix FMix  *  PuzzleMix Co-Mixup Ours</cell></row><row><cell>R-18</cell><cell>49.91 48.62 51.85 50.30</cell><cell>53.95</cell><cell>54.13 54.46</cell></row><row><cell>RX-50</cell><cell>53.38 50.27 57.16 59.80</cell><cell>59.34</cell><cell>59.76 61.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 .</head><label>11</label><figDesc>Ablation of feature layer</figDesc><table><row><cell cols="3">l on Tiny-ImageNet, reporting top-1</cell></row><row><cell cols="3">Acc (%)? vs. params (M)? vs. the</cell></row><row><cell cols="3">total training time (hours)?.</cell></row><row><cell></cell><cell>R-18</cell><cell>RX-50</cell></row><row><cell></cell><cell cols="2">Acc(%) Params Time Acc(%) Params Time</cell></row><row><cell cols="3">Mixup 63.86 11.27 20 66.36 23.38 113</cell></row><row><cell>l1</cell><cell cols="2">67.30 11.38 67 70.70 23.80 413</cell></row><row><cell>l2</cell><cell cols="2">67.27 11.39 41 70.43 23.86 252</cell></row><row><cell>l3</cell><cell cols="2">67.33 11.44 34 70.72 24.84 196</cell></row><row><cell>l4</cell><cell cols="2">67.32 11.64 28 70.67 27.99 174</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>according to the design of CycleMix [70] on medical image segmentation tasks. 35. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: International Conference on Computer Vision (ICCV) (2021) 8 36. Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv preprint arXiv:1506.01497 (2015) 13 44. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recognition challenge. International journal of computer vision pp. 211-252 (2015) 8 45. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Inverted residuals and linear bottlenecks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018) 8 46. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Gradcam: Visual explanations from deep networks via gradient-based localization. arXiv preprint arXiv:1610.02391 (2019) 4 47. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research 15(1), 1929-1958 (2014) 1 48. Tan, C., Gao, Z., Wu, L., Li, S., Li, S.Z.: Hyperspherical consistency regularization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 7244-7255 (2022) 1 49. Tan, C., Xia, J., Wu, L., Li, S.Z.: Co-learning: Learning from noisy labels with self-supervision. In: Proceedings of the 29th ACM International Conference on Multimedia. pp. 1405-1413 (2021) 1 50. Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neural networks. In: International Conference on Machine Learning (ICML) (2019) 8 51. Thulasidasan, S., Chennupati, G., Bilmes, J., Bhattacharya, T., Michalak, S.: On mixup training: Improved calibration and predictive uncertainty for deep neural networks. arXiv preprint arXiv:1905.11001 (2019) 9 52. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jegou, H.: Training data-efficient image transformers &amp; distillation through attention. In: International Conference on Machine Learning (ICML). pp. 10347-10357 (2021) 1, 8, 21 70. Zhang, K., Zhuang, X.: Cyclemix: A holistic strategy for medical image segmentation from scribble supervision. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2022) 15 71. Zhao, Z., Wu, Z., Zhuang, Y., Li, B., Jia, J.: Tracking objects as pixel-wise distributions (2022) 1 72. Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features for scene recognition using places database. In: Advances in Neural Information</figDesc><table><row><cell>the 2020s (2022) 8</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">37. Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts.</cell></row><row><cell cols="4">arXiv preprint arXiv:1608.03983 (2016) 9, 21 38. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International Conference on Learning Representations (ICLR) (2019) 10 39. Maji, S., Rahtu, E., Kannala, J., Blaschko, M., Vedaldi, A.: Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151 (2013) 8, 21 40. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K?pf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: An imperative style, high-performance deep learning library. In: Advances in Neural Information Processing Systems (NeurIPS) (2019) 21 41. Polyak, B.T., Juditsky, A.B.: Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization pp. 838-855 (1992) 8 42. Qin, J., Fang, J., Zhang, Q., Liu, W., Wang, X., Wang, X.: Resizemix: Mixing data with preserved object information and true labels. arXiv preprint arXiv:2012.11101 (2020) 2, 8, 10, 14, 21 224 224 224 Test crop ratio 0.875 0.875 0.95 0.95 Epochs 100/300 300 300 100 Batch size 256 1024 2048 2048 Optimizer SGD AdamW LAMB LAMB LR 0.1 1 ? 10 ?3 5 ? 10 ?3 8 ? 10 ?3 LR decay cosine cosine cosine cosine Weight decay 10 ?4 0.05 0.02 0.02 Warmup epochs 5 5 5 Label smoothing 0.1 Dropout Stoch. Depth 0.1 0.05 Repeated Aug Gradient Clip. 1.0 43. Processing Systems (NeurIPS). pp. 487-495 (2014) 8, 21 H. flip</cell></row><row><cell>RRC</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rand Augment</cell><cell>9/0.5</cell><cell>7/0.5</cell><cell>6/0.5</cell></row><row><cell>Auto Augment</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mixup alpha</cell><cell>0.8</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Cutmix alpha</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>Erasing prob.</cell><cell>0.25</cell><cell></cell><cell></cell></row><row><cell>ColorJitter</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EMA</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CE loss</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BCE loss</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mixed precision</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">A.2 More Experiments and Ablations</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/Westlake-AI/openmixup</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported by the Science and Technology Innovation 2030-Major Project (No. 2021ZD0150100) and the National Natural Science Foundation of China (No. U21A20427). This work was performed during the internship of Zhiyuan Chen at Westlake University. We thank Jianzhu Guo, Cheng Tan, and all reviewers for polishing the writing.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>Tiny-ImageNet </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gridmix: Strong regularization through local context mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.patcog.2020.107594</idno>
		<ptr target="https://doi.org/https://doi.org/10.1016/j.patcog.2020.10759414" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y M</forename><surname>Liao</surname></persName>
		</author>
		<idno>abs/2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<title level="m">Transmix: Attend to mix for vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Physical attack on monocular depth estimation with optimal adversarial patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluating weakly supervised object localization methods right</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the cifar datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08819</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tf-blender: Temporal feature blender for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supermix: Supervising the mixing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dabouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Taherkhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faramarzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Badrinaaraayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07794</idno>
		<title level="m">Patchup: A regularization technique for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P B J</forename><surname>Hare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12047</idno>
		<title level="m">Fmix: Enhancing mixed sample data augmentation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>4 (2020) 2, 8, 14</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<title level="m">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02781</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pixmix: Dreamlike pictures comprehensively improve safety measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stylemix: Separating content and style for enhanced data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03065</idno>
		<title level="m">Co-mixup: Saliency guided joint mixup with supermodular diversity</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Puzzle mix: Exploiting saliency and local statistics for optimal mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note>PMLR (2020) 2, 8, 14</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">OpenMixup: Open mixup toolbox and benchmark for visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://github.com/Westlake-AI/openmixup(2022)8" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Genurl: A general framework for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno>ArXiv abs/2110.14553</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tlpg-tracker: Joint learning of target localization and proposal generation for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the 29th International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sg-net: Spatial granularity network for onestage video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Densernet: Weakly supervised visual localization using multi-scale feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mousas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Bae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.01791</idno>
		<title level="m">Saliencymix: A saliency guided data augmentation strategy for better regularization</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Alignmix: Improving representation by interpolating aligned features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kijak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amsaleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022) 9</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Manifold mixup: Better representations by interpolating hidden states</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attentive cutmix: An enhanced data augmentation approach for deep learning based image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walawalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<title level="m">Resnet strikes back: An improved training procedure in timm (2021) 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-supervised learning on graphs: Contrastive, generative, or predictive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep clustering and visualization for end-to-end high-dimensional data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Pre-training graph neural networks for molecular representations: Retrospect and prospect. In: ICML 2022 2nd AI for Science Workshop</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training BERT in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Dlme: Deep local-flatness manifold embedding</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">We briefly introduce image datasets used in Section 4</title>
		<imprint/>
	</monogr>
	<note>Dataset information</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">) Small-scale fine-grained classification scenarios: CUB-200-2011 (CUB) [56] contains 11,788 images from 200 wild bird species for fine-grained classification. FGVC-Aircraft (Aircraft) [39] contains 10,000 images of 100 classes of aircrafts. (4) Large-scale fine-grained classification scenarios: iNaturalist2017 (iNat2017</title>
		<idno>Small scale classification benchmarks: CIFAR-10/100 [27] contains 50</idno>
	</analytic>
	<monogr>
		<title level="m">000 training images and 10,000 test images in 32?32 resolutions, with 10 and 100 classes settings</title>
		<imprint/>
	</monogr>
	<note>Large scale classification benchmarks: ImageNet-1k (IN-1k) [28] contrains 1,281,167 training images and 50,000 validation images of 1000 classes. Tiny-ImageNet (Tiny) [7] is a re-scale version of ImageNet-1k, which has 10,000 training images and 10,000 validation images of 200 classes in 64?64 resolutions. 24] contains a total of 5,089 categories with 579,184 training images and 95,986 validation images. iNaturalist2018 (iNat2018) [24] contains a total of 8,142 categories with 437,512 training images and 24,426 validation images</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Scenic classification dataset Places205 [72] contains around 2,500,000 images from 205 common scene categories. Notice that we use modified structures [19] of ResNet and ResNeXt for CIFAR-10/100 and Tiny-ImageNet experiments</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Notice that optimization-based methods adopt a consistent ? for all datasets, PuzzleMix adopts ? = 1, Co-Mixup and AutoMix adopts ? = 2. Hand-crafted methods use dataset-specific hyperparameter settings as follows: For CIFAR-10/100, Mixup and ResizeMix use ? = 1, and CutMix, FMix and SaliencyMix use ? = 0.2, and ManifoldMix uses ? = 2, respectively. For Tiny-ImageNet and ImageNet-1k using PyTorch-style training settings, ManifoldMix uses ? = 0.2, the rest methods use ? = 0.2 for ResNet-18 while adopt ? = 1 for median and large backbones (e.g., ResNet-50). For iNat2017 and iNat2018, Mixup and ManifoldMix use ? = 0.2, the rest methods adopt ? = 1 for ResNet-50 and ResNeXt-101 while use ? = 0.2 for ResNet-18. For ImageNet-1k using DeiT and RSB A2/A3 settings and Places205 using PyTorch-style settings, all these methods use ? = 0.2. For small-scale finegrained datasets</title>
	</analytic>
	<monogr>
		<title level="m">and RSB A2/A3 [59] on ImageNet-1k are provided in Table 12. Notice that we replace the step learning rate decay by Cosine Scheduler [37] and remove ColorJitter and PCA lighting in PyTorch training setting for better performances</title>
		<imprint/>
	</monogr>
	<note>Reproduction settings. We adopt OpenMixup 3 implemented in PyTorch [40] as the open-source codebase, where we implement AutoMix and reproduce most comparison methods. SaliencyMix and FMix use ? = 0.2, and ManifoldMix uses ? = 0.5, while the rest use ? = 1. As for other methods, we reproduce results of AugMix [21], Co-Mixup [25], and SuperMix [10] with their official implementations</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ingredients and hyper-parameters used for ImageNet-1k training settings</title>
	</analytic>
	<monogr>
		<title level="m">Procedure PyTorch DeiT RSB A2 RSB A3</title>
		<meeting>edure PyTorch DeiT RSB A2 RSB A3</meeting>
		<imprint/>
	</monogr>
	<note>Table 12</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<title level="m">Table 13. Top-1 accuracy (%)? on CIFAR-10 based on ResNet-18 and ResNeXt-50 (32x4d) trained with various epochs. * denotes unpublished open-source work on arxiv. Backbone ResNet</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<idno>95.10 95.50 95.59 95.92 95.81 96.23 96.26</idno>
		<title level="m">ResNeXt-50 Epoch 200ep 400ep 800ep 1200ep 200ep 400ep 800ep 1200ep Vanilla 94</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resizemix * 96</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<idno>76.42 77.73 78.04 78.55 79.37 80.24 81.09 81.32</idno>
		<title level="m">ResNeXt-50 Epoch 200ep 400ep 800ep 1200ep 200ep 400ep 800ep 1200ep Vanilla</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
