<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inpainting Transformer for Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Pirnay</surname></persName>
							<email>jonathan.pirnay@fujitsu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Digital Incubation</orgName>
								<orgName type="institution">Fujitsu Technology Solutions GmbH</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keng</forename><surname>Chai</surname></persName>
							<email>keng.chai@fujitsu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Digital Incubation</orgName>
								<orgName type="institution">Fujitsu Technology Solutions GmbH</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inpainting Transformer for Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Anomaly Detection ? Self-attention ? Transformer</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection in computer vision is the task of identifying images which deviate from a set of normal images. A common approach is to train deep convolutional autoencoders to inpaint covered parts of an image and compare the output with the original image. By training on anomaly-free samples only, the model is assumed to not being able to reconstruct anomalous regions properly. For anomaly detection by inpainting we suggest it to be beneficial to incorporate information from potentially distant regions. In particular we pose anomaly detection as a patch-inpainting problem and propose to solve it with a purely self-attention based approach discarding convolutions. The proposed Inpainting Transformer (InTra) is trained to inpaint covered patches in a large sequence of image patches, thereby integrating information across large regions of the input image. When training from scratch, in comparison to other methods not using extra training data, InTra achieves results on par with the current state-of-the-art on the MVTec AD dataset for detection and surpassing them on segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Anomaly detection and localization in vision describe the problem of deciding whether a given image is atypical with respect to a set of normal samples, and to identify the respective anomalous subregions within the image. Both problems have strong implications for industrial inspection <ref type="bibr" target="#b2">[3]</ref> and medical applications <ref type="bibr" target="#b7">[8]</ref>. In practical industrial applications, anomalies occur rarely. Due to the lack of sufficient anomalous samples, and as anomalies can be of unexpected shape and texture, it is hard to deal with this problem with supervised methods. Current approaches follow unsupervised methods and try to model the distribution of normal data only. At test time an anomaly score is given to each image to indicate how much it deviates from normal samples. For anomaly localization a similar score is assigned to subregions or individual pixels of the image.</p><p>A common approach following this paradigm is to use deep convolutional autoencoders or generative models such adversarial networks in order to model the manifold of normal training data. The difference between the input and reconstructed image is then used to compute the anomaly scores. In practice this approach often suffers from the drawback that convolutional autoencoders generalize strongly and anomalies are reconstructed well, leading to misdetection. Recent methods propose to mitigate this effect by posing the generative part as an inpainting problem: Parts of the input image are covered and the model is trained to reconstruct the covered parts in a self-supervised way <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b12">13]</ref>. By conditioning on the neighborhood of the excluded part only, small anomalies get effectively retouched. Due to their limited receptive field, fully convolutional neural networks (CNNs) are partially ineffective in modeling distant contextual information, which makes the removal of larger anomalous regions difficult. For inpainting in general settings, this can be effectively addressed by introducing contextual attention in the model <ref type="bibr" target="#b21">[21]</ref>. For inpainting in the context of anomaly detection we suggest it to be beneficial to learn the relevant patterns alone by combining information from large regions around the covered image part via attention. Inspired by the recent success of self-attention based models such as Transformers <ref type="bibr" target="#b17">[17]</ref> in image recognition <ref type="bibr" target="#b6">[7]</ref>, we pose anomaly detection as a patchinpainting problem and propose to solve it without convolutions: images are split into square patches, and a Transformer model is trained to reconstruct covered patches on the basis of a long sequence of neighboring patches. By recovering the whole image in this way, a full reconstructed image is obtained where the reconstruction of an individual patch incorporates a large context and not only the appearance of its immediate neighborhood. Thus patches are not reconstructed by simply mimicking the local neighborhood, leading to high anomaly scores even for spacious anomalous regions.</p><p>Our contributions enfold the modeling of anomaly detection as a patchsequence inpainting problem which we solve using a deep Transformer network consisting of a simple stack of multiheaded self-attention blocks. Within this network convolutional operations are removed entirely. Furthermore we propose to employ long residual connections between the Transformer blocks and to perform a nonlinear dimension reduction for keys and queries when computing self-attention in order to improve the network's reconstruction capabilities for difficult surfaces. By adding embeddings of the position of individual patches within an image to the sequence of patches, it is possible to perform the inpainting in a global context even if the sequence of patches does not cover the full image.</p><p>We evaluate our method on the challenging MVTec AD dataset <ref type="bibr" target="#b2">[3]</ref> for both detection and segmentation. Although Transformer networks are usually trained on huge amounts of data, we effectively train our networks with ?55M parameters from scratch only on the 60-400 images available for each category in MVTec AD. We compare our results to the current state-of-the-art not using any extra training data. Our proposed method InTra achieves on par results on the detection task and slightly better results on the segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In anomaly detection, reconstruction-based methods try to model only normal, defect-free samples. For this, deep CNN autoencoders are widely used to learn the manifold of defect-free images in a latent bottleneck. Given defective test data, these models should not be able to properly reconstruct the anomalous image since they only model normal data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2]</ref>. An anomaly map for segmentation is usually generated via pixel-wise difference or similarity measures between the input image and its model reconstruction, leading to noticeable anomalies.</p><p>Even though in reconstruction-based methods the models are trained on defect-free samples only, they often generalize well to anomalies in practice <ref type="bibr" target="#b8">[9]</ref>. An inpainting scheme can be used to effectively hide anomalous regions to further restrict a model's capability to reconstruct anomalies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b12">13]</ref>. By covering parts of the original image, the reconstruction method needs to have semantic understanding of the image to be able to generate a coherent and realistic image. Zavrtanik et al. propose to use a U-Net architecture <ref type="bibr" target="#b14">[15]</ref> taking advantage of long residual connections. Their reconstruction-based method randomly selects multiple parts of the image to inpaint, yielding the current state-of-the-art results for anomaly detection via inpainting for different benchmarks <ref type="bibr" target="#b22">[22]</ref>.</p><p>Anomalies which span over a large area may still cause problems as these will not be covered up sufficiently enough. As such we propose to add global context by replacing CNNs with a Transformer-based framework applied in vision.</p><p>Transformer models were originally introduced in natural language processing (NLP) and have since evolved to be the modern design for various sequence tasks like text translation, generation and document classification <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">20]</ref>. In a Transformer model, self-attention is used to relate elements of a sequence to each other. Based on the relative weighted importance a shared representation is calculated taking into account the relative dependencies between sequence elements. This is able to replace recurrent neural networks in sequence-to-sequence modeling because long-range dependencies are processed globally. The general architecture can be found in the original work <ref type="bibr" target="#b17">[17]</ref>.</p><p>While Transformer architectures have been widely studied in NLP and sequence modeling, convolutional architectures have been essentially the standard tool in recent years due to weight sharing, translation equivariance and locality.</p><p>Due to the induced bias in fully convolutional autoencoders, the restricted receptive field limits global context <ref type="bibr" target="#b21">[21]</ref>. Even though in theory the self-attention framework may mitigate this problem, running self-attention on the whole image without further simplifications is not feasible <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Recently Dosovitskiy et al. have proposed Vision Transformer <ref type="bibr" target="#b6">[7]</ref>, where the image data is split up into square non-overlapping uniform patches. Each patch and position gets embedded into a latent space and every image is treated as a sequence of these embedded patches. A Transformer architecture is applied on the restructured data achieving comparable results to state of the art CNNs and even surpassing them on some tasks while reducing model bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Inpainting Transformer for Anomaly Detection</head><p>Our approach is based on a simple stack of Transformer blocks which are trained to inpaint covered image patches based on neighboring patches. An overview of the method is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Patch Embeddings and Multihead Feature Self-attention</head><p>We use a similar notation as in <ref type="bibr" target="#b6">[7]</ref>. Let x ? R H?W ?C be an input image, where (H, W ) denotes the (height, width)-size and C the number of channels of the image. Let K be the desired side length of a square patch and N := H K , M := W K (the image is resized such that K divides H and W ). We split the image x into a N ? M grid of flattened square patches</p><formula xml:id="formula_0">x p ? R (N ? M ) ? (K 2 ? C) , where x (i,j) p ? R K 2 ?C</formula><p>is the patch in the i-th row and j-th column. Our aim is to choose square subgrids of some side length L in this patch grid and train a network to reconstruct any covered patch in the subgrid based on the rest of the subgrid's patches. Formally, this inpainting problem is as follows:</p><p>Let </p><formula xml:id="formula_1">x (i,j) p (i, j) ? S be</formula><formula xml:id="formula_2">(i,j) p (i,j)?S\{(t,u)} in the window.</formula><p>As by definition Transformers are invariant with respect to reorderings of the input, the one-dimensional positional information f (i, j) :</p><formula xml:id="formula_3">= (i ? 1) ? N + j of a patch x (i,j) p</formula><p>is used. To use as a sequence input to the Transformer model, we map the window of patches and their positional information into some latent space of dimension D, i.e. for each patch x with learnable weight matrix E ? R (K 2 ?C)?D , and where posemb denotes a standard learnable one-dimensional position embeddings.</p><formula xml:id="formula_4">(i,j) p with (i, j) ? S \ {(t, u)} we set y (i,j) := x (i,j) p E + posemb(f (i, j)) ? R D (1)</formula><p>To account for the patch at position (t, u) to inpaint, we add a single learnable embedding x inpaint ? R D to the position embedding via</p><formula xml:id="formula_5">z := x inpaint + posemb(f (t, u)) ? R D .<label>(2)</label></formula><p>The embedding x inpaint is comparable to the class token in <ref type="bibr" target="#b5">[6]</ref>. The vectors z and y (i,j) for all (i, j) ? S \ {(t, u)} build the final sequence of embedded patches which serves as an input sequence of length L 2 to the Inpainting Transformer model.</p><p>Applying multihead self-attention (MSA) to an input sequence forms the heart of a standard Transformer block as in <ref type="bibr" target="#b17">[17]</ref>. For this, queries q, keys k and values v are obtained by mapping the input sequence with learnable weight matrices W q , W k , W v ? R D?D . Self-attention is then computed over slices of q, k, v. In cases where the patches of the training images are very similar but indistinct the dot product of queries and keys are very close to each other, leading to an almost uniform softmax-weighted sum in the calculation of MSA. To mitigate this, we propose to perform a nonlinear dimension reduction when computing q and k. For this, W q and W k are swapped with multilayer perceptrons (MLP) with a single hidden layer. In all our models we used an output dimension of D 2 and a hidden layer dimension of 2 ? D with GELU non-linearity. We refer to this modified MSA as multihead feature self-attention (MFSA). We experienced improved detection results with MFSA (see Section 4.3). However, depending on the output and hidden dimension of the MLPs, the number of learnable parameters increases strongly with MFSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Architecture and Training</head><p>Our network architecture for inpainting is composed of a simple stack of n Transformer blocks. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the architecture. The structure of each Trans-former block mainly follows <ref type="bibr" target="#b6">[7]</ref> and consists of MFSA followed by a multilayer perceptron (MLP). Layer normalization is applied before ("pre-norm" <ref type="bibr" target="#b18">[18]</ref>), and residual connections after MFSA and MLP. Each MLP has a single hidden layer with GELU nonlinearity and maps R D ? R 4?D ? R D . In particular the input and output of each Transformer block is a sequence in R L 2 ?D (see <ref type="figure" target="#fig_2">Fig. 2)</ref>.</p><p>To obtain the inpainted patch, we average over the output sequence of the last Transformer block to get a single vector in R D which is mapped back to the pixel space of the flattened patches R K 2 ?C via a learnable affine transformation followed by a sigmoidal.</p><p>In early experiments an inspection of the attention weights showed that a large spatial context is present in earlier layers. In addition to that, Attention Rollout <ref type="bibr" target="#b0">[1]</ref> has been used in <ref type="bibr" target="#b6">[7]</ref> to illustrate that information across the entire input image is integrated already in the lowest layers. In order to carry this early information to deeper blocks of the network, we put additional long residual connections between early and late layers in a U-Net fashion <ref type="bibr" target="#b14">[15]</ref>. We found that the use of long residual connections leads to more structural detail in the overall reconstruction, slightly improving both detection and segmentation (see Section 4.3).</p><p>The network is trained by randomly sampling batches of patch windows with a fixed side length L from normal image data. In each window a random patch position (t, u) is chosen, which is inpainted by the network as described in the previous sections.</p><p>For the loss function, we compare the original and reconstructed patch with pixel-wise L 2 loss. To account for perceptual differences, we also include structural similarity <ref type="bibr" target="#b24">[23]</ref> and gradient magnitude similarity <ref type="bibr" target="#b19">[19]</ref>.</p><p>Given an original and reconstructed patch x p ,x p ? R K?K?C , the full loss function L is given by</p><formula xml:id="formula_6">L(x p ,x p ) = L 2 (x p ,x p ) + ? K 2 (i,j)?K?K (1 ? GMS avg (x p ,x p ) (i,j) ) + ? K 2 (i,j)?K?K (1 ? SSIM avg (x p ,x p ) (i,j) )<label>(3)</label></formula><p>where ?, ? are individual scaling parameters, 1 is a matrix of ones and GMS avg (resp. SSIM avg ) denotes the gradient magnitude similarity maps (resp. structural similarity maps) averaged over the color channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference and Anomaly Detection</head><p>The inferencing process is divided into two steps: First a complete inpainted image is generated, afterwards the difference between the reconstruction and original is used to compute a pixel-wise anomaly map. Let x ? R H?W ?C be an input image with an N ?M patch grid as introduced above. For each patch position (t, u) ? N ? M , we choose an appropriate patch window of side length L which is used as a basis to inpaint the patch at position x (t,u) p . In particular we define the window by its upper left patch x</p><formula xml:id="formula_7">(r,s) p via r =g(t) ? max(0, g(t) + L ? N ? 1),<label>(4)</label></formula><formula xml:id="formula_8">s =g(u) ? max(0, g(u) + L ? M ? 1),<label>(5)</label></formula><p>where the map g is given by g(c) := max 1, c ? L 2 . The above equations choose (r, s) such that (t, u) is as much centered in the L ? L patch-window as possible. Using this window, the patch x (t,u) p is reconstructed by the network as described. By reconstructing all patches in the N ? M grid, we obtain a full reconstructionx of the whole image.</p><p>For the generation of an expressive anomaly map from x andx we use a simplified variant of the GMS-based scheme proposed in <ref type="bibr" target="#b22">[22]</ref>. Denote by x l an image x resized to scale l. Now for original and reconstructed images x,x ? R H?W ?C and scale l ? { 1 2 , 1 4 }, we set m l (x,x) := bluravg l (1 ? GMS avg (x,x)) ? R l?H?l?W <ref type="bibr" target="#b5">(6)</ref> for a scaled and smoothed version of the gradient difference. To ease notation, we denote by bluravg l the application of an averaging filter followed by some Gaussian blur operation, both with a predefined kernel size and variance. As in <ref type="bibr" target="#b22">[22]</ref>, smoothing improves robustness with respect to small, poorly reconstructed anomalous regions. We resize the two-dimensional maps m 1 2 and m 1 4 back to the image's original size and take the pixel-wise mean which yields a difference map diff(x,x) ? R H?W .</p><p>To finally obtain an anomaly map for x during inference, we take the squared deviation of the difference map to the normal training data, i.e.</p><formula xml:id="formula_9">anomap(x) := diff(x,x) ? 1 |T | z?T diff(z,?) 2 ? R H?W ?0 ,<label>(7)</label></formula><p>where T is the set of normal training samples. The pixel-wise maximum of anomap(x) is taken as a scalar anomaly score for detection on the image level. An example of an anomaly map can be seen in <ref type="figure" target="#fig_0">Fig. 1b.</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our method on the MVTec AD dataset which contains high resolution samples of 5 texture and 10 object categories stemming from manufacturing <ref type="bibr" target="#b2">[3]</ref>. The dataset has been a widely used benchmark for anomaly detection and localization in the manufacturing domain. Each category consists of around 60 to 400 normal, defect-free samples for training. For each test image there is a ground-truth binary image labeled on pixel-level for segmentation of anomalous test images. Based on an image's anomaly score we report standard ROC AUC as a detection metric. For localisation, the image's anomaly map <ref type="bibr" target="#b6">(7)</ref> is used for an evaluation of pixel-wise ROC AUC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We train our model on each product category from scratch. We randomly choose 10% of images from the normal training data (however a maximum of 20) and use them as a validation set to control the quality of reconstructions. In each epoch 600 patch windows are sampled randomly per image. To augment the dataset, random rotation and flipping is used.</p><p>The choice of three parameters has an obvious significant impact on the performance: Side length K of square patches, side length L of a patch window and the choice of height H and width W (with H = W , as all images are square) to which the original image is resized during training and inference. The patch size determines how much of the image is covered, the size of the patch window determines the dilation of context we include during inpainting, the image size implicitly influences both. For all models we choose K = 16, L = 7. For the choice of image size in our pipeline, a balance needs to be struck between enlarging the image context of the 7?7 window, quality of patch reconstructions and computation time, as Transformer models usually take a long time to train. The heuristics is to choose the image as small as possible while keeping patch reconstructions at a high level of detail. Hence we train the model with image dimensions 256 ? 256, 320 ? 320, 512 ? 512 for 200 epochs and compare the best (epoch-wise) validation losses (averaged over ?5 epochs). If there is no significant improvement in the validation loss of at least 10 ?4 for an image dimension with the next in size, the smaller dimension is chosen. The rightmost column of <ref type="table" target="#tab_0">Table  1</ref> shows the resulting image sizes for each category. We note that in practice K, L, H, W could be tuned for the detection task at hand if prior knowledge about possible defects is present. The Inpainting Transformer model trained consists of 13 blocks with 8 attention heads each and a latent dimension of D = 512, using MFSA. In total this amounts to ?55M learnable parameters.</p><p>Given an image size of 512 ? 512, a kernel size of 21 (resp. 11) is used for averaging and Gaussian blur (with ? = 2) for bluravg 1 2 (resp. bluravg <ref type="bibr">1 4</ref> ) in <ref type="bibr" target="#b5">(6)</ref>. The kernel sizes are scaled linearly for smaller image sizes. Anomaly maps are resized back to their original high image resolution for proper segmentation comparison.</p><p>For the loss function L in <ref type="formula" target="#formula_6">(3)</ref> we set ? = ? = 0.01. The network is trained using the Adam optimizer with a learning rate of 0.0001 and a batch size of 256 until no improvement on the validation loss is observed for 50 consecutive epochs. The model weights at the epoch with the best validation loss are chosen for evaluation. Although common for training Transformers, we don't apply dropout at any point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head><p>The results for detection and segmentation are reported in <ref type="table" target="#tab_0">Table 1</ref>. We compare our method to RIAD <ref type="bibr" target="#b22">[22]</ref>, which also uses an inpainting reconstruction-based method and computes anomaly maps based on GMS. Furthermore we compare the results to CutPaste <ref type="bibr" target="#b11">[12]</ref> which uses a special data augmentation strategy to train a one-class classifier in a self-supervised way. CutPaste also offers results using pretrained representations, however we focus on the results without extra training data in accordance to our training procedure.</p><p>To our knowledge RIAD offers the current best performing model based on an inpainting scheme, whereas CutPaste is the best performing model on the MVTec AD benchmark not using extra training data. Our method outperforms RIAD on both detection and segmentation. On the detection task, CutPaste is superior to our method by 0.2%, however on the segmentation task, we can improve the result by 0.6%.</p><p>It is worth noting that there are two strongly underperforming categories: Cable contains many anomalous images where the defect lies in the overall constitution of the product (such as missing pieces). Combined with noise in large areas this makes these anomalies hard to detect via inpainting. Although the defects in capsule are per-se easily visible on the generated anomaly maps, our method does not learn the reconstruction of the typography sufficiently well, leading to high anomaly scores also on normal samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>We examine the influence of certain building blocks in the architecture. All categories except Leather are trained for 200 epochs with settings as described in Section 4.1, if not stated otherwise. Leather takes the longest until details start to show in the inpainted patches, so for comparability the network is trained for 700 epochs. It should be noted that training for only 200 epochs for most categories does not lead to results comparable to <ref type="table" target="#tab_0">Table 1</ref>. The average results of the ablation studies are reported in <ref type="table" target="#tab_1">Table 2</ref>. We observe a decline in both detection and segmentation when omitting long residual connections. We furthermore examine the effect of using normal multihead self-attention (MSA) instead of MFSA as described in Section 3.1. Segmentation results do not improve using MFSA over MSA, however the average detection results improve by 0.7% when using MFSA. Lastly we test how the side length L of a patch window influences the performance by training the network also with L ? {5, 9}. Detection and segmentation improve with growing patch windows, as more information from distant pixels can be used for the inpainting task. This comes with high computational cost however, as the computation of the dot product in self-attention is quadratic in the sequence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Inspired by the success of self-attention in vision tasks, we have used a Transformer model for visual anomaly detection by using an inpainting reconstruction approach. We argued that by discarding convolutions and using only selfattention to incorporate global context into reconstructions, anomalies can be successfully detected and localized. Hyperparameters such as the input image size, patch sequence length and patch dimension have a strong impact on the overall performance, and including the detection of good values for them in the training pipeline is paramount. With a simple pipeline as proposed, we have shown that InTra can reach state-of-the-art results on the popular MVTec AD dataset not using extra training data. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Schematic overview of the proposed method. a.) The image is split into square patches. An inpainting transformer model (InTra) is trained to reconstruct a covered patch (black) from a long sequence of surrounding patches (red). Positional embeddings are added to the patches to include spatial context. b.) Examples: By reconstruction of all patches of an input image (left), a full reconstruction is obtained (middle). Comparison of original and reconstruction yields a pixel-wise anomaly score (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>such a square subgrid ("window") of patches defined by some index set S = {r, . . . , r + L ? 1} ? {s, . . . , s + L ? 1}. Here L is the side length of the window, and (r, s) is the grid position of the window's upper left patch. If (t, u) ? S is the position of some patch, the formal task to inpaint (t, u) given S is to approximate the patch x(t,u) p using only the content and positions of all other patches x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of the proposed architecture. Left: Parts of an individual Transformer block. Right: A stack of Transformer blocks builds the full architecture. Long residual connections are used to add information from earlier blocks to later ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Examples of qualitative results continued from Figure 3. Categories from top to bottom: hazelnut, metal nut, pill, screw, toothbrush, transistor, zipper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Detection/Segmentation results for MVTec AD. Results are presented in ROC AUC % on image level for detection, and on pixel level for segmentation.</figDesc><table><row><cell>Category</cell><cell cols="4">RIAD [22] CutPaste [12] InTra (Ours) InTra Image Size</cell></row><row><cell></cell><cell cols="3">Det. / Seg. Det. / Seg. Det. / Seg.</cell></row><row><cell>Carpet</cell><cell>84.2 / 96.3</cell><cell cols="2">93.1 / 98.3 98.8 / 99.2</cell><cell>512 ? 512</cell></row><row><cell>Grid</cell><cell cols="3">99.6 / 98.8 99.9 / 97.5 100.0 / 98.8</cell><cell>256 ? 256</cell></row><row><cell>Leather</cell><cell cols="3">100.0 / 99.4 100.0 / 99.5 100.0 / 99.5</cell><cell>512 ? 512</cell></row><row><cell>Tile</cell><cell>93.4 / 89.1</cell><cell cols="2">93.4 / 90.5 98.2 / 94.4</cell><cell>512 ? 512</cell></row><row><cell>Wood</cell><cell cols="3">93.0 / 85.8 98.6 / 95.5 97.5 / 88.7</cell><cell>512 ? 512</cell></row><row><cell>avg. textures</cell><cell cols="3">95.1 / 93.9 97.0 / 96.3 98.9 / 96.1</cell></row><row><cell>Bottle</cell><cell cols="3">99.9 / 98.4 98.3 / 97.6 100.0 / 97.1</cell><cell>256 ? 256</cell></row><row><cell>Cable</cell><cell cols="2">81.9 / 94.2 80.6 / 90.0</cell><cell>70.3 / 91.0</cell><cell>256 ? 256</cell></row><row><cell>Capsule</cell><cell cols="3">88.4 / 92.8 96.2 / 97.4 86.5 / 97.7</cell><cell>320 ? 320</cell></row><row><cell>Hazelnut</cell><cell cols="3">83.3 / 96.1 97.3 / 97.3 95.7 / 98.3</cell><cell>256 ? 256</cell></row><row><cell>Metal Nut</cell><cell cols="3">88.5 / 92.5 99.3 / 93.1 96.9 / 93.3</cell><cell>256 ? 256</cell></row><row><cell>Pill</cell><cell cols="3">83.8 / 95.7 92.4 / 95.7 90.2 / 98.3</cell><cell>512 ? 512</cell></row><row><cell>Screw</cell><cell>84.5 / 98.8</cell><cell cols="2">86.3 / 96.7 95.7 / 99.5</cell><cell>320 ? 320</cell></row><row><cell>Toothbrush</cell><cell cols="3">100.0 / 98.9 98.3 / 98.1 100.0 / 98.9</cell><cell>256 ? 256</cell></row><row><cell>Transistor</cell><cell>90.9 / 87.7</cell><cell cols="2">95.5 / 93.0 95.8 / 96.1</cell><cell>256 ? 256</cell></row><row><cell>Zipper</cell><cell cols="3">98.1 / 97.8 99.4 / 99.3 99.4 / 99.2</cell><cell>512 ? 512</cell></row><row><cell>avg. objects</cell><cell cols="3">89.9 / 94.3 94.3 / 95.8 93.0 / 96.9</cell></row><row><cell cols="4">avg. all categories 91.7 / 94.2 95.2 / 96.0 95.0 / 96.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Detection/Segmentation results for the ablation studies. 'Regular' refers to the architecture as described in the previous sections, and 'NLR' to 'no long residual connections'.</figDesc><table><row><cell></cell><cell>Regular</cell><cell>NLR</cell><cell>MSA</cell><cell>L = 5</cell><cell>L = 9</cell></row><row><cell></cell><cell cols="2">Det. / Seg. Det. / Seg.</cell><cell>Det. / Seg.</cell><cell>Det. / Seg.</cell><cell>Det. / Seg.</cell></row><row><cell cols="2">avg. text. 98.2 / 95.3</cell><cell>98.6 / 95</cell><cell>98.4 / 95.5</cell><cell>98.4 / 95.8</cell><cell>98.3 / 96.2</cell></row><row><cell>avg. obj.</cell><cell cols="2">90.7 / 95.5 90.0 / 95.3</cell><cell>89.6 / 95.5</cell><cell>90.0 / 95.2</cell><cell>90.8 / 95.8</cell></row><row><cell>avg. all</cell><cell cols="2">93.2 / 95.4 92.9 / 95.2</cell><cell>92.5 / 95.5</cell><cell>92.8 / 95.4</cell><cell>93.3 / 95.9</cell></row><row><cell>diff. to Reg.</cell><cell></cell><cell cols="2">-0.3 / -0.2 -0.7 / +0.1</cell><cell>-0.5 / 0.0</cell><cell>+0.1 / +0.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.385</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.385" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4190" to="4197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep autoencoding models for unsupervised anomaly segmentation in brain mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11723-8_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-11723-816" />
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Com</title>
		<imprint>
			<biblScope unit="page" from="161" to="169" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mvtec ad -a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattleger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Detecting anomalous faces with &apos;no peeking&apos; autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhattad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<idno>abs/1802.05798</idno>
		<ptr target="http://arxiv.org/abs/1802.05798" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Anomaly detection of defects on concrete structures with the convolutional autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.aei.2020.101105</idno>
		<ptr target="https://doi.org/10.1016/j.aei.2020.101105" />
	</analytic>
	<monogr>
		<title level="j">Advanced Engineering Informatics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">101105</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep learning for medical anomaly detection -a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gammulle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<idno type="DOI">10.1145/3464423</idno>
		<ptr target="https://doi.org/10.1145/3464423" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anomaly detection using deep learning based image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haselmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tabatabai</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICMLA.2018.00201</idno>
		<ptr target="https://doi.org/10.1109/ICMLA.2018.00201" />
	</analytic>
	<monogr>
		<title level="m">17th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1237" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1912.12180" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cutpaste: Self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="9664" to="9674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised region-based anomaly detection in brain mri with adversarial image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bethapudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Willcocks</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISBI48211.2021.9434115</idno>
		<ptr target="https://doi.org/10.1109/ISBI48211.2021.9434115" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1127" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno>PMLR (10-15</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<meeting>the 35th International Conference on Machine Learning. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anomaly detection using autoencoders with nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yairi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis</title>
		<meeting>the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis</meeting>
		<imprint>
			<biblScope unit="page" from="4" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mlsda&amp;apos;14</surname></persName>
		</author>
		<idno type="DOI">10.1145/2689746.2689747</idno>
		<ptr target="https://doi.org/10.1145/2689746.2689747" />
		<imprint>
			<date type="published" when="2014" />
			<publisher>Association for Computing Machinery</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1176</idno>
		<ptr target="https://doi.org/10.18653/v1/P19-1176" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="1810" to="1822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient magnitude similarity deviation: A highly efficient perceptual image quality index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2013.2293423</idno>
		<ptr target="https://doi.org/10.1109/TIP.2013.2293423" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="684" to="695" />
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00577</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00577" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reconstruction by inpainting for visual anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zavrtanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">107706</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.patcog.2020.107706</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2020.107706" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2003.819861</idno>
		<ptr target="https://doi.org/10.1109/TIP.2003.819861" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
