<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NeoUNet: Towards accurate colon polyp segmentation and neoplasm detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><surname>Phan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hanoi University of Science and Technology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Sy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hanoi University of Science and Technology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dao</forename><forename type="middle">Viet</forename><surname>Hang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hanoi Medical University</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">The Institute of Gastroenterology and Hepatology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dao</forename><forename type="middle">Van</forename><surname>Long</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hanoi Medical University</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">The Institute of Gastroenterology and Hepatology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename><forename type="middle">Quang</forename><surname>Trung</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">University of Medicine and Pharmacy</orgName>
								<orgName type="institution" key="instit2">Hue University</orgName>
								<address>
									<settlement>Hue</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><forename type="middle">Thi</forename><surname>Thuy</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">National University of Agriculture</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country>Vietnam, Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><forename type="middle">Viet</forename><surname>Sang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hanoi University of Science and Technology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NeoUNet: Towards accurate colon polyp segmentation and neoplasm detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Polyp Segmentation ? Colonoscopy ? Deep Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic polyp segmentation has proven to be immensely helpful for endoscopy procedures, reducing the missing rate of adenoma detection for endoscopists while increasing efficiency. However, classifying a polyp as being neoplasm or not and segmenting it at the pixel level is still a challenging task for doctors to perform in a limited time. In this work, we propose a fine-grained formulation for the polyp segmentation problem. Our formulation aims to not only segment polyp regions, but also identify those at high risk of malignancy with high accuracy.</p><p>In addition, we present a UNet-based neural network architecture called NeoUNet, along with a hybrid loss function to solve this problem. Experiments show highly competitive results for NeoUNet on our benchmark dataset compared to existing polyp segmentation models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Colonoscopy is considered as the most effective procedure for colorectal polyp detection and removal <ref type="bibr" target="#b11">[12]</ref>. Among many histopathological types of precancerous polyps, adenomas with high-grade dysplasia carry the highest risks of developing into colorectal cancer (CRC) <ref type="bibr" target="#b6">[7]</ref>. With over 640,000 deaths each year <ref type="bibr" target="#b2">[3]</ref>, CRC is among the most common types of cancer. As such, the importance of performing colonoscopies to detect and remove high-risk polyps is undeniable.</p><p>However, a review by Leufkens et al. <ref type="bibr" target="#b18">[19]</ref> showed that 20 ? 47% of polyps might have been missed during colonoscopies. There are several factors contributing to this situation, including overloading healthcare systems with an overwhelming number of cases per day and reducing withdrawal time, low-quality endoscopy equipment, or personnel's lack of experience <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref>. Technologies such as image-enhanced endoscopies and novel accessories were invented and applied to solve this problem. However, cost-effectiveness is still a barrier, especially for limited-resource settings, while performance is not yet ideal. Thus, computeraided systems have a lot of potentials to improve colonoscopy quality. Recent years have seen very active research in this domain, mostly focused on automatic polyp segmentation and/or detection. Several such works have achieved very high accuracy on benchmark datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>While automatic segmentation can immensely improve endoscopic performance, it leaves out the difficult task of determining whether a polyp is neoplastic. Neoplastic polyps are precursor lesions to CRC. They require different approaches such as conventional polypectomies, endoscopic mucosal resection, endoscopic submucosal dissection, biopsy, marking, staging for further management such as surgery, or neoadjuvant chemo-radiotherapy. Non-neoplastic polyps, on the other hand, can be removed or left with/without following up during colonoscopies. Classifying neoplastic polyps could be very challenging, especially when the withdrawal time is under overwhelming pressure. The task typically requires well-trained endoscopists with many years of experience, who may still be unable to recognize and characterize some lesions due to tiredness. The critical nature of this task calls for a more fine-grained approach to segment polyps as well as to classify the lesions according to the risk of neoplasm with relative confidence. Several endoscopic systems such as CADEYE (Fujifilm), EndoBrain (Olympus) have integrated this functionality. However, these systems also require magnification and image-enhanced functions, making them costly and unapproachable in most developing countries.</p><p>Polyp segmentation has seen a number of approaches over the years. Traditional machine learning methods based on hand-crafted features <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref> rely on color, shape, texture, etc. . . to separate polyps from the surrounding mucosa. These approaches are generally limited, as polyps have very high intra-class diversity and low inter-class variation. The state-of-the-art methods for polyp segmentation in recent years have been deep neural networks. These networks can learn highly abstract and complex image representations to accurately find polyp areas. U-Net <ref type="bibr" target="#b24">[25]</ref> and related models are among the most successful and widely used, all of which feature an encoder-decoder architecture allowing the combination of low-level concrete features and high-level abstract features. Variants such as UNet++ <ref type="bibr" target="#b32">[33]</ref> and ResUNet++ <ref type="bibr" target="#b14">[15]</ref> improved on the original U-Net by adopting a nested architecture, while others (eg. DoubleUNet <ref type="bibr" target="#b13">[14]</ref>) went with a stacking approach. More general techniques in deep learning such as attention and deep supervision have also been incorporated in UNet, and have yielded promising results.</p><p>Fine-grained classification seeks to identify complex subclasses instead of simple coarse-grained classes. For example, an image classified as "Dog" may have more fine-grained classes such as "Golden Retriever" or "Pomeranian". While these problems may seem to be similar to multi-class classification, their high inter-class similarity can easily confuse the learning models, leading to low performance.</p><p>In this paper, we first restate the polyp segmentation problem by expanding it with a fine-grained classification aspect. We then propose a UNet-based network architecture to solve it. Specifically, our contributions are:</p><p>-To formally describe the polyp segmentation and neoplasm detection problem; -To propose NeoUNet, a deep neural network architecture designed for the stated problem; -To describe a new dataset, called NeoPolyp, our benchmark dataset for polyp segmentation and neoplasm detection; -To present experimental results for NeoUNet on the NeoPolyp dataset, including comparisons with existing models for polyp segmentation.</p><p>The rest of the paper is organized as follows. We provide a brief review of related works in Section 2. Section 3 describes the polyp segmentation and neoplasm detection problem in detail. The proposed NeoUNet is presented in Section 4. Section 5 showcases our experimental studies. Finally, we conclude the paper and highlight future works in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Convolutional neural networks (CNNs) have claimed state-of-the-art performance in almost every computer vision task in the last few years. Some of the earliest breakout models were AlexNet <ref type="bibr" target="#b15">[16]</ref> and VGG <ref type="bibr" target="#b27">[28]</ref>. These early architectures were still quite limited and suffered from degradation when increasing network depth. ResNet <ref type="bibr" target="#b7">[8]</ref> introduced skip connections that helped smooth out the loss landscape and combat gradient vanishing in very deep networks. GoogLeNet <ref type="bibr" target="#b28">[29]</ref> proposed a meticulously-designed multi-branch network with good performance. ResNeXt <ref type="bibr" target="#b31">[32]</ref> also took up a multi-branch approach and applied it to ResNet. EfficientNet <ref type="bibr" target="#b29">[30]</ref> is a family of networks designed using neural architecture search techniques that provides a range of tradeoffs between accuracy and latency. HarDNet <ref type="bibr" target="#b3">[4]</ref> focused on reducing inference latency by reducing memory traffic.</p><p>Semantic segmentation and segmentation for medical images, in particular, have seen a lot of interest in recent years. Long et al. <ref type="bibr" target="#b20">[21]</ref> adopted several well-known architectures for segmentation using transfer learning techniques. DeepLabV3 <ref type="bibr" target="#b4">[5]</ref> proposed the use of atrous convolutions for dense feature extraction with positive results. PraNet <ref type="bibr" target="#b5">[6]</ref> enhanced an FCN-like model with parallel partial decoder and reverse attention. HarDNet-MSEG <ref type="bibr" target="#b8">[9]</ref> adopts the HarDNet backbone as the encoder in an encoder-decoder structure. The network achieved state-of-the-art performance on the Kvasir-SEG dataset while also improving inference latency. U-Net <ref type="bibr" target="#b24">[25]</ref> was one of the first successful CNNs applied in medical imaging. The architecture features an encoder-decoder design, combining low-level features on the encoder branch with high-level features on the decoder branch. Numerous works have proposed improvements to the original UNet. UNet++ <ref type="bibr" target="#b32">[33]</ref> and ResUNet++ <ref type="bibr" target="#b14">[15]</ref> used a nested architecture, with multiple levels of crossconnections. DoubleUNet <ref type="bibr" target="#b13">[14]</ref> stacked two UNets sequentially, using VGG-16 as the encoder backbone, with squeeze and excitation units and ASPP modules.</p><p>While outperforming previous methods on several datasets, DoubleUNet is limited in terms of information flow between the two UNets. Tang et al. <ref type="bibr" target="#b30">[31]</ref> tackled this limitation with Coupled U-Net (CUNet), which adds skip connections between UNet blocks. Attention-UNet <ref type="bibr" target="#b21">[22]</ref> introduced attention gates which filters for useful salient features. Abraham et al. <ref type="bibr" target="#b0">[1]</ref> proposed a novel loss function in conjunction with the Attention-UNet architecture, combined with multi-scale input and deep supervision.</p><p>Due to the heterogeneous quality of different endoscopic systems, the lack of public datasets for polyp characterization is a challenge. Among the most similar research to ours is that of Ribeiro et al. <ref type="bibr" target="#b23">[24]</ref>, who presented several approaches -including CNNs and hand-crafted features -for polyp classification. The authors extracted a dataset of 100 polyp images from endoscopy videos, each containing exactly one polyp. Several CNN models were tested on this dataset, including VGG, AlexNet, GoogLeNet, etc. . . A primary drawback for this approach is that classification has to be done after detection or segmentation.</p><p>In other words, the problem is approached in two stages. While it is possible to combine the polyp detection modules with classification, this method can be inefficient and cumbersome, especially for systems with real-time requirements or running on embedded devices. In this paper, we present an end-to-end model for both segmentation and classification of polyps to overcome these limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Polyp segmentation and neoplasm detection</head><p>The problem presented in this work is an expansion of polyp segmentation, focusing on more fine-grained classification. In polyp segmentation, given an input image, we need to output a binary mask where each pixel's value is either 1 (the pixel is part of a polyp) or 0 (the pixel is part of the background).</p><p>The polyp segmentation and neoplasm detection problem (PSND) expects each pixel in the segmentation mask to have one of four values (see <ref type="figure" target="#fig_0">Fig. 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>for examples):</head><p>-0 if the pixel is part of the image background; -1 if the pixel is part of a non-neoplastic polyp; -2 if the pixel is part of a neoplastic polyp; -3 if the pixel is part of a polyp with unknown neoplasticity.</p><p>PSND presents several unique challenges. First, the surface pattern of a polyp could be homogenous or heterogeneous, with different areas of texture which requires experienced endoscopists to evaluate carefully. In one lesion, a neoplastic area may only take up a portion of the polyp's surface, causing further difficulties for machine learning models. This property is the primary distinction between our problem and generic multi-class segmentation. While datasets such as PASCAL-VOC have many more classes, they are primarily well-defined and highly distinguishable (e.g., car, bird, person, etc. . . ). Moreover, from the medical perspective, a misclassification could lead to biased decisions. False positives Polyps considered to have "unknown" neoplasticity also pose specific challenges to machine learning models. We generally do not want automatic systems to learn and output "unknown" pixels. Instead, it is much more beneficial to produce a classification between neoplastic and non-neoplastic, as this provides more insight to physicians while also reducing inter-class similarity. At the same time, "unknown" polyps are still helpful for learning how to segment without classification.</p><p>The challenges mentioned above serve as motivation for the proposed NeoUNet model, which we shall describe in detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NeoUNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture</head><p>NeoUNet is a U-Net architecture similar to the one introduced in [1], with several key differences. The encoder backbone uses the HarDNet68 architecture <ref type="bibr" target="#b3">[4]</ref>, comprising of Harmonic Dense blocks (HDB). Outputs from each encoder level are passed to corresponding decoder blocks through attention gate modules. All decoder blocks also have output layers producing multi-class segmentation masks at their corresponding scale level. These output layers allow us to train the network using deep supervision, which boosts the network's stability and convergence rate.</p><p>To take advantage of public ImageNet-trained HarDNet models, we do not make modifications to the backbone structure and thus cannot use joint multiscale inputs. However, the benefits of pretraining can outweigh this limitation.</p><p>The architecture's overview is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Encoder backbone HarDNet (Harmonic DenseNet) <ref type="bibr" target="#b3">[4]</ref> is a CNN architecture inspired by DenseNet <ref type="bibr" target="#b9">[10]</ref>. DenseNet's core principle is encouraging feature To create a better tradeoff between accuracy and memory, HarDNet sparsifies DenseNet by reducing the number of skip connections. Specifically, a layer k in a HarD Block (HDB) receives a feature map from layer k?2 n if 2 n divides k (n ? 0, k ? 2 n ? 0). In addition, layers whose indices are divisible to large powers of 2 are more "influential", as their feature maps are more frequently reused. Such layers are given more convolutional kernels (more output channels). Concretely, the number of output channels for layer l with a growth rate k is k ? m n , where n = max{? | l ... 2 ? }. m can be considered as the compression factor, where m = 2 means all layers have the same number of input feature maps. <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates the structure of a HDB. HarDNet68 further changes DenseNet architecture by removing the global dense connections, using Max Pooling for downsampling, and having dedicated growth rates for each HDB. HarDNet68 has 5 "strides" that reduce the feature map size by 2, 4, 8, 16, and 32 times compared to the original image. When incorporated in NeoUNet as the encoder backbone, each stride serves as an encoder block.</p><p>Attention Gate The information carried in skip connections between encoder and decoder blocks can generally be noisy, as finer feature maps contain more local features that may be irrelevant. Thus, we use additive attention gates <ref type="bibr" target="#b21">[22]</ref> on all such connections, except for the top-most blocks. An attention gate filters the input tensor using a set of attention coefficients ? i , calculated from the input x l and the gating signal g (see Eq. (1) and Eq. (2) <ref type="bibr" target="#b21">[22]</ref> for details).</p><formula xml:id="formula_0">q l att = ? T (? 1 (W T x x l i + W T g g i + b g )) + b ? (1) ? l i = ? 2 (q l att (x l i , g i ; ? att ))<label>(2)</label></formula><p>where ? 1 denotes the ReLU function, ? 2 denotes the sigmoid function. ? att = (W x , W g , b g , ?, b ? ) is the set of learnable parameters. The attention gate's output is the filtered tensorx l = x l .?. <ref type="figure">Fig. 4</ref> illustrates the attention gate's structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 4: Diagram of the additive attention gate module [22]</head><p>Decoder and output block Each decoder block consists of 2 sequential sets of {Convolution, Batch Norm, Leaky ReLU} layers. The output of the previous decoder block is concatenated with the attention gate output (carrying filtered information from the encoder) to form the current decoder's input. Each output block is a simple 1x1 convolution layer with two output channels, followed by the sigmoid activation. Each channel corresponds to the output mask for one class. Segmentation maps without classification can be inferred using the element-wise OR operator.</p><p>During training, we upsample output masks from all levels to the original image size and compute the loss values for each mask. These values are then summed to form the final loss for backpropagation. This form of deep supervision forces different levels of abstract features to make sense semantically, allowing faster convergence and improving stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Loss function</head><p>The loss function for training NeoUNet is a weighted sum of a segmentation loss and a multi-class loss, as shown in Eq. <ref type="formula" target="#formula_1">(3)</ref>.</p><formula xml:id="formula_1">L(M ct , M st , M cp , M sp ) = w c L c (M ct , M cp ) + w s L s (M st , M sp )<label>(3)</label></formula><p>The multi-class loss L c reflects the difference between the ground truth multiclass mask M ct and the predicted multi-class mask M cp . L c is averaged from Binary Cross Entropy loss and Focal Tversky loss <ref type="bibr" target="#b0">[1]</ref> (see Eq. <ref type="formula">(4)</ref>). This is the primary loss function that drives the model toward making accurate class-specific segmentation.</p><formula xml:id="formula_2">L c (M ct , M cp ) = BCE(M ct , M cp ) + F ocalT versky(M ct , M cp ) 2 (4)</formula><p>Tversky loss <ref type="bibr" target="#b25">[26]</ref> can be seen as a generalization of Dice loss, with two hyperparameters ? and ? that control the effect of false positives and false negatives. Focal Tversky loss <ref type="bibr" target="#b0">[1]</ref> extends Tversky loss by combining ideas from Focal loss <ref type="bibr" target="#b19">[20]</ref>, which adds a parameter ? that helps the model to focus on hard examples. For NeoUNet, we set ? = 1 ? ? = 0.3 for Tversky loss, meaning the model prioritizes recall over precision (since smaller polyps are likely to be missed), and ? = 4 3 to focus slightly more on hard examples. The segmentation loss L s is calculated using the ground truth binary segmentation mask M st , and the predicted binary segmentation mask M sp . M sp can be inferred from the multi-class mask M cp as in Eq. <ref type="bibr" target="#b4">(5)</ref>.</p><formula xml:id="formula_3">M sp (i, j) = 0 if M cp (i, j) = 0 1 otherwise (5)</formula><p>L s is averaged from Binary Cross Entropy loss and Tversky loss (see Eq. <ref type="formula">(6)</ref>). This is a secondary loss function that ensures the model maintains high segmentation accuracy. In addition, the segmentation loss allows NeoUNet to take advantage of training data marked as "Unknown."</p><formula xml:id="formula_4">L s (M st , M sp ) = BCE(M st , M sp ) + T versky(M st , M sp ) 2 (6)</formula><p>The parameters w c and w s controls the level of effect each loss has on the training process. We find that NeoUNet performs best when w c = 0.75 and w s = 0.25.</p><p>During training, pixels that are part of "unknown" polyps only contribute to the segmentation loss and are ignored by the multi-class loss. This mechanism allows the model to freely label "unknown" polyps based on existing features while still benefitting from partially labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benchmark dataset</head><p>A dataset of 7,466 annotated endoscopic images is curated in order to train and benchmark the proposed NeoUNet. These images are captured directly during endoscopic recording, including all four lighting modes: WLI (White Light Imaging), FICE (Flexible spectral Imaging Color Enhancement), BLI (Blue Light Imaging), and LCI (Linked Color Imaging). The dataset also includes polyps in all Paris classifications <ref type="bibr" target="#b16">[17]</ref> 6 (Ip, Is, IIa, IIb, IIc, III), as well as images without any polyps. The patient's identifying information is removed from each image to ensure anonymity.</p><p>Annotations (including segmentation and classification) are added to each image independently by two experienced endoscopists. Matching annotation labels are accepted into the dataset, while those without full consensus from annotators or declared unknown by at least one annotator are marked with the label "Unknown". The dataset is randomly split into a training set of 5,966 images and a test set of 1,500 images.</p><p>In order to compare NeoUNet with existing approaches, we also create a filtered dataset without any "Unknown" labels. This dataset consists of 5,277 training images and 1,353 test images. We denote the full dataset as NeoPolyp and the filtered version as NeoPolyp-Clean.</p><p>Due to their diverse nature, neoplastic polyps take up a majority of the polyps present in NeoPolyp (see <ref type="figure">Fig. 5</ref>). This data imbalance, combined with the inherent challenges of PSND, creates a difficult benchmark for models to overcome. <ref type="figure">Fig. 5</ref>: Pixel-wise distribution of polyp class labels in the NeoPolyp dataset. Percentages are calculated on polyp pixels only (not including background pixels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment setup</head><p>We perform two experiments to validate the effectiveness of our proposed NeoUNet: <ref type="bibr" target="#b5">6</ref> Labels for this type of classification are not available -A comparison with baseline models: we compare NeoUNet with U-Net <ref type="bibr" target="#b24">[25]</ref>, PraNet <ref type="bibr" target="#b5">[6]</ref>, and HarDNet-MSEG [9] on the PSND task; -A comparison on NeoPolyp and NeoPolyp-Clean: we compare the performance of NeoUNet when trained on NeoPolyp and NeoPolyp-Clean, in order to show the benefits of learning on "Unknown"-labeled data.</p><p>In both experiments,common used metrics are employed for performance measurement:</p><p>-Polyp segmentation performance (without classification), measured using Dice score and IoU score. We denote these metrics as Dice seg and IoU seg ; -Segmentation performance on each class (non-neoplastic and neoplastic), measured using Dice score and IoU score. We denote these metrics as Dice non , IoU non , Dice neo and IoU neo , respectively. -Inference speed for each model, measured as frames per second (FPS). This metric is only of significance in the first experiment.</p><p>Dice and IoU are calculated pixel-wise on the entire test set (micro-averaged, not averaged over each image). Evaluation is done on images resized to 352?352. To measure inference speed for a model, we average the latency of inferring 100 test images with a batch size of 1.</p><p>Additionally, the following settings and techniques are applied to the training process in both experiments:</p><p>-To avoid bias due to class imbalance in the dataset, we oversample images containing non-neoplastic polyps during training such that P non ? P neo , where P non and P neo are the number of pixels containing non-neoplastic and neoplastic polyps, respectively; -NeoUNet is trained using Stochastic Gradient Descent with Nesterov momentum and a learning rate of 0.001. The learning rate lr is adjusted according to a combination of warmup and cosine annealing schedule. Concretely, lr increases linearly up to the target value in the first t w epochs. In the remaining epochs, lr is annealed following the cosine function <ref type="bibr" target="#b10">[11]</ref>. -Each training batch is put through the network in 3 different scales: 448?448, 352 ? 352, and 256 ? 256; -Data augmentation is employed to improve models' generality. Specifically, we use five types of augmentation: rotate, horizontal/vertical flip, motion blur, and color jittering. Augmentation is performed on the fly with a probability of 0.7 (i.e., each image has a 70% chance of being augmented each time it is selected for a training batch).</p><p>NeoUNet and the original U-Net are implemented in Python 3.6 using the PyTorch <ref type="bibr" target="#b22">[23]</ref> framework. We use the official PraNet 7 and HarDNet-MSEG 8 implementations to perform comparison. All training is done on a machine with a 3.7GHz AMD Ryzen 3970X CPU, 128GB RAM, and an NVIDIA GeForce GTX 3090 GPU. We use a Google Colab instance with 2 CPU cores and an NVIDIA Tesla V100 GPU to measure inference speed.</p><p>Comparison with the baseline models This experiment uses the NeoPolyp-Clean dataset, as the baseline models do not handle "Unknown" labels in the input data. Each model is first pretrained for 200 epochs on the polyp segmentation training data from <ref type="bibr" target="#b14">[15]</ref> (including images from the Kvasir-SEG and CVC-ClinicDB dataset). We then train each model until convergence on the NeoPolyp-Clean dataset (by swapping the final output layers with 2-channel convolutional blocks).</p><p>Comparison on NeoPolyp and NeoPolyp-Clean For this experiment, NeoUNet is first trained on the polyp segmentation task using data from <ref type="bibr" target="#b14">[15]</ref>. We then compare the performance when training this model to convergence on the NeoPolyp and NeoPolyp-Clean datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and discussion</head><p>Comparison with baseline models <ref type="table" target="#tab_0">Table 1</ref> shows the performance metrics for NeoUNet and HarDNet-MSEG on the NeoPolyp-Clean dataset. We can see that all four models struggle with the non-neoplastic class. The highest Dice score for this class is only 0.713 from NeoUNet, significantly lower than on the neoplastic class (0.887) and the segmentation task (0.902). This is likely due to the heavy class imbalance in the training dataset. Although oversampling and Focal loss partially mitigates the issue, there is still a noticeable drop in the performance.</p><p>NeoUNet significantly outperforms all baseline models in all Dice and IoU metrics. Against the best baseline, PraNet, our model achieves better scores by ? 2% in each metric. The segmentation task sees the most improvement from NeoUNet, with a 1.6% increase in Dice score and 2.6% increase in IoU. These results show that the combination of HDB and attention gates used in NeoUNet is very effective for the PSND problem. Additionally, the use of the secondary segmentation loss in NeoUNet has helped in maintaining segmentation accuracy. Meanwhile, U-Net performs significantly worse than the other three methods.</p><p>In terms of speed, HarDNet-MSEG is the fastest model by a large margin, achieving 77.1 FPS. This is consistent with the results in <ref type="bibr" target="#b8">[9]</ref>, as speed is a major focus for this model. PraNet is the slowest of the four models, at only 55.6 FPS. NeoUNet and U-Net have similar speeds at 68.3 and 69.6 FPS, respectively. Overall, despite being slower than HarDNet-MSEG and U-Net, NeoUNet provides a good tradeoff between accuracy and speed, while still being faster and more accurate than PraNet. <ref type="table" target="#tab_1">Table 2</ref> shows performance metrics for NeoUNet when trained on NeoPolyp and NeoPolyp-Clean. We notice a slight drop in accuracy for the same model when testing on NeoPolyp compared to NeoPolyp-Clean, since images containing "Unknown" polyps are typically more challenging. Interestingly, the use of "Unknown"-label data shows improvement for all metrics, not just segmentation. While the difference is slight (0.2 ? 0.5%), it shows that making use of data for one task can yield benefits in other tasks. In this case, better segmentation masks invariantly improve classification, as these two tasks are intertwined. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison on NeoPolyp and NeoPolyp-Clean</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper has presented the polyp segmentation and neoplasm detection problem, a challenging combination of fine-grained classification and semantic segmentation. To solve this problem, we propose NeoUNet, a UNet-based architecture incorporating attention gates, an efficient HarDNet backbone, and a hybrid loss function to take advantage of unknown labels. Our experiments show very competitive results when compared to existing models for polyp segmentation. We hope that this work can be a basis for further improvements on this challenging problem. Our future works include improving classification accuracy, especially for non-neoplastic polyps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Expected outputs for polyp segmentation and PSND. Black regions denote background pixels. White regions denote polyp regions. Green, red and yellow regions denote non-neoplastic, neoplastic and unknown polyp regions, respectively. could result in over indications of endoscopic interventions or surgery, while false negatives could result in delaying suitable treatment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of NeoUNet's architecture reuse through skip connections: each layer in a Dense Block receives the concatenated feature map of every preceding layer, essentially having a global state through which high-value features can be shared. While achieving high accuracy, DenseNet also has a large memory footprint, leading to low throughput and increased latency due to memory traffic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Structure of an example Harmonic Dense Block. The value on each layer denotes the number of output channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>This work was funded by Vingroup Innovation Foundation (VINIF) under project code VINIF.2020.DA17. Phan Ngoc Lan was funded by Vingroup Joint Stock Company and supported by the Domestic Master/Ph.D. Scholarship Programme of Vingroup Innovation Foundation (VINIF), Vingroup Big Data Institute (VIN-BIGDATA), code VINIF.2020.ThS.BK.02.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance metrics on the NeoPolyp-Clean test set for U-Net, PraNet, HarDNet-MSEG, and NeoUNet</figDesc><table><row><cell>Method</cell><cell>Diceseg IoUseg Dicenon IoUnon Diceneo IoUneo FPS</cell></row><row><cell>U-Net [25]</cell><cell>0.785 0.646 0.525 0.356 0.773 0.631 69.6</cell></row><row><cell cols="2">HarDNet-MSEG [9] 0.883 0.791 0.659 0.492 0.869 0.769 77.1</cell></row><row><cell>PraNet [6]</cell><cell>0.895 0.811 0.705 0.544 0.873 0.775 55.6</cell></row><row><cell>NeoUNet</cell><cell>0.911 0.837 0.720 0.563 0.889 0.800 68.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance metrics for NeoUNet when training on NeoPolyp and NeoPolyp-Clean, measured on the NeoPolyp test set</figDesc><table><row><cell cols="2">Training dataset Diceseg IoUseg Dicenon IoUnon Diceneo IoUneo</cell></row><row><cell>NeoPolyp-Clean</cell><cell>0.906 0.828 0.725 0.569 0.888 0.799</cell></row><row><cell>NeoPolyp</cell><cell>0.908 0.831 0.729 0.573 0.891 0.804</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/DengPingFan/PraNet 8 https://github.com/james128333/HarDNet-MSEG</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A novel focal tversky loss function with improved attention u-net for lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="683" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visibility map: a new method in evaluation quality of optical colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Armin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Conlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grimpen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Salvado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comparative validation of polyp detection methods in video colonoscopy: Results from the miccai 2015 endoscopic vision challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajkbaksh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Matuszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Angermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rustad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Balasingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pogorelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Debard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brandao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>C?rdova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>S?nchez-Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fern?ndez-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2017.2664042</idno>
		<ptr target="https://doi.org/10.1109/TMI.2017.2664042" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1231" to="1249" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hardnet: A low memory traffic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3552" to="3561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pranet: Parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High-grade dysplasia and invasive carcinoma in colorectal adenomas: a multivariate analysis of the impact of adenoma and patient characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gschwantler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kriwanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>G?ritzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schrutka-K?lbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brownstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feichtinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European journal of gastroenterology &amp; hepatology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="188" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hardnet-mseg: A simple encoder-decoder polyp segmentation neural network that achieves over 0.9 mean dice and 86 fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07172</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sgdr: stochastic gradient descent with restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Colorectal cancer screening: An updated review of the available options</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noureddine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World journal of gastroenterology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page">5086</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic polyp detection in endoscope images using a hessian filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwahori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Woodham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Bhuyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kasugai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MVA. pp</title>
		<imprint>
			<biblScope unit="page" from="21" to="24" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Doubleu-net: A deep convolutional neural network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D. ; A G S</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Gonz?lez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Santosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Temesgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kane</surname></persName>
		</author>
		<idno type="DOI">10.1109/CBMS49503.2020.00111</idno>
		<ptr target="https://doi.org/10.1109/CBMS49503.2020.00111" />
	</analytic>
	<monogr>
		<title level="m">33rd IEEE International Symposium on Computer-Based Medical Systems</title>
		<editor>Soda, P.</editor>
		<meeting><address><addrLine>Rochester, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="558" to="564" />
		</imprint>
	</monogr>
	<note>de Herrera</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Resunet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The paris endoscopic classification of superficial neoplastic lesions: esophagus, stomach, and colon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointest Endosc</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="3" to="43" />
			<date type="published" when="2002-12-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An adequate level of training for technical competence in screening and diagnostic colonoscopy: a prospective multicenter evaluation of the learning curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">K</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hwangbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointestinal endoscopy</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="683" to="689" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Factors influencing the miss rate of polyps in a back-to-back colonoscopy study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leufkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Oijen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vleggaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siersema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endoscopy</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="470" to="475" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Attention u-net: Learning where to look for the pancreas</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alch?-Buc, F., Fox, E., Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exploring deep learning and transfer learning for colonic polyp classification. Computational and mathematical methods in medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>H?fner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tversky loss function for image segmentation using 3d fully convolutional deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on machine learning in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Granado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio, Y., LeCun, Y.</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<title level="m">Cu-net: Coupled u-nets. In: 29th British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unet++: Redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
