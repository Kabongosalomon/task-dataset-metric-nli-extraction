<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SSMTL++: Revisiting Self-Supervised Multi-Task Learning for Video Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Barbalau</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei Street</addrLine>
									<postCode>010014</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei Street</addrLine>
									<postCode>010014</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Romanian Young Academy</orgName>
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>90 Panduri Street</addrLine>
									<postCode>050663</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">SecurifAI</orgName>
								<address>
									<addrLine>21D Mircea Voda</addrLine>
									<postCode>030662</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Georgescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei Street</addrLine>
									<postCode>010014</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">SecurifAI</orgName>
								<address>
									<addrLine>21D Mircea Voda</addrLine>
									<postCode>030662</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Dueholm</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Architecture, Design, and Media Technology</orgName>
								<orgName type="institution">Aalborg University</orgName>
								<address>
									<addrLine>Rendsburggade 14</addrLine>
									<postCode>9000</postCode>
									<settlement>Aalborg</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Milestone Systems</orgName>
								<address>
									<postCode>50C, 2605</postCode>
									<settlement>Banemarksvej, Br?ndby</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharathkumar</forename><surname>Ramachandra</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Geopipe Inc</orgName>
								<address>
									<addrLine>460 W 51st</addrLine>
									<postCode>10019</postCode>
									<region>New York City, NY, US</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nasrollahi</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Architecture, Design, and Media Technology</orgName>
								<orgName type="institution">Aalborg University</orgName>
								<address>
									<addrLine>Rendsburggade 14</addrLine>
									<postCode>9000</postCode>
									<settlement>Aalborg</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Milestone Systems</orgName>
								<address>
									<postCode>50C, 2605</postCode>
									<settlement>Banemarksvej, Br?ndby</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Khan</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">MBZ University of Artificial Intelligence</orgName>
								<address>
									<addrLine>Masdar City</addrLine>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<postCode>581 83</postCode>
									<settlement>Link?ping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Architecture, Design, and Media Technology</orgName>
								<orgName type="institution">Aalborg University</orgName>
								<address>
									<addrLine>Rendsburggade 14</addrLine>
									<postCode>9000</postCode>
									<settlement>Aalborg</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<affiliation key="aff8">
								<orgName type="department" key="dep1">Center for Research in Computer Vision (CRCV)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<postCode>32816</postCode>
									<settlement>Orlando</settlement>
									<region>FL, US</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SSMTL++: Revisiting Self-Supervised Multi-Task Learning for Video Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 Preprint</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A self-supervised multi-task learning (SSMTL) framework for video anomaly detection was recently introduced in literature. Due to its highly accurate results, the method attracted the attention of many researchers. In this work, we revisit the self-supervised multi-task learning framework, proposing several updates to the original method. First, we study various detection methods, e.g. based on detecting high-motion regions using optical flow or background subtraction, since we believe the currently used pre-trained YOLOv3 is suboptimal, e.g. objects in motion or objects from unknown classes are never detected. Second, we modernize the 3D convolutional backbone by introducing multi-head self-attention modules, inspired by the recent success of vision transformers. As such, we alternatively introduce both 2D and 3D convolutional vision transformer (CvT) blocks. Third, in our attempt to further improve the model, we study additional self-supervised learning tasks, such as predicting segmentation maps through knowledge distillation, solving jigsaw puzzles, estimating body pose through knowledge distillation, predicting masked regions (inpainting), and adversarial learning with pseudo-anomalies. We conduct experiments to assess the performance impact of the introduced changes. Upon finding more promising configurations of the framework, dubbed SSMTL++v1 and SSMTL++v2, we extend our preliminary experiments to more data sets, demonstrating that our performance gains are consistent across all data sets. In most cases, our results on Avenue, ShanghaiTech and UBnormal raise the state-of-the-art performance bar to a new level.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Due its applicability in video surveillance, anomaly detection is an actively studied topic in the video domain, with many recent attempts trying to solve the problem by employing various approaches ranging from outlier detection models <ref type="bibr" target="#b1">(Antic and Ommer, 2011;</ref><ref type="bibr" target="#b7">Cheng et al., 2015;</ref><ref type="bibr" target="#b8">Cong et al., 2011;</ref><ref type="bibr" target="#b10">Dong et al., 2020;</ref><ref type="bibr" target="#b14">Dutta and Banerjee, 2015;</ref><ref type="bibr" target="#b20">Hasan et al., 2016;</ref><ref type="bibr" target="#b28">Ionescu et al., 2019b;</ref><ref type="bibr" target="#b31">Kim and Grauman, 2009;</ref><ref type="bibr" target="#b34">Lee et al., 2019;</ref><ref type="bibr" target="#b36">Li et al., 2014;</ref><ref type="bibr" target="#b39">Liu et al., 2018a;</ref><ref type="bibr" target="#b42">Lu et al., 2013;</ref><ref type="bibr" target="#b44">Luo et al., 2017;</ref><ref type="bibr" target="#b46">Mahadevan et al., 2010;</ref><ref type="bibr" target="#b48">Mehran et al., 2009;</ref><ref type="bibr" target="#b52">Park et al., 2020;</ref><ref type="bibr" target="#b54">Ramachandra and Jones, 2020;</ref><ref type="bibr" target="#b55">Ramachandra et al., 2020a</ref><ref type="bibr" target="#b56">Ramachandra et al., , 2021</ref><ref type="bibr" target="#b58">Ravanbakhsh et al., 2018</ref><ref type="bibr" target="#b59">Ravanbakhsh et al., , 2017</ref><ref type="bibr" target="#b61">Ren et al., 2015;</ref><ref type="bibr" target="#b64">Sabokrou et al., 2017;</ref><ref type="bibr" target="#b69">Tang et al., 2020;</ref><ref type="bibr" target="#b75">Wu et al., 2019;</ref><ref type="bibr" target="#b78">Xu et al., 2017;</ref><ref type="bibr" target="#b87">Zhao et al., 2011;</ref><ref type="bibr" target="#b85">Zhang et al., 2020</ref><ref type="bibr" target="#b86">Zhang et al., , 2016</ref> and weakly-supervised learning frameworks <ref type="bibr" target="#b15">(Feng et al., 2021;</ref><ref type="bibr" target="#b53">Purwanto et al., 2021;</ref><ref type="bibr" target="#b67">Sultani et al., 2018;</ref><ref type="bibr" target="#b70">Tian et al., 2021;</ref><ref type="bibr" target="#b83">Zaheer et al., 2020;</ref><ref type="bibr" target="#b88">Zhong et al., 2019)</ref> to supervised open-set meth-  <ref type="figure">Fig. 1</ref>. Overview of the proposed SSMTL++v1 and SSMTL++v2 frameworks. Both SSMTL++v1 and SSMTL++v2 use a hybrid detection method based on YOLOv3 and optical flow, as well as an enhanced backbone formed of a 3D convolutional network followed by a 3D convolutional vision transformer (CvT) . The first promising combination of tasks (termed SSMTL++v1) is formed of tasks T 1 (arrow of time prediction), T 2 (motion irregularity prediction), T 3 (middle bounding box prediction) and T 5 (adversarial reconstruction), while the second promising combination of tasks (termed SSMTL++v2) is formed of tasks T 1 , T 2 , T 3 and T 6 (patch inpaiting). Notice that task T 4 (knowledge distillation), which was used in the original SSMTL method <ref type="bibr" target="#b17">(Georgescu et al., 2021a)</ref>, is now removed from both SSMTL++v1 and SSMTL++v2. Our novel components are illustrated inside red dashed contours. Best viewed in color.</p><formula xml:id="formula_0">Pseudo-anomaly YOLOv3 (pre-trained) i i+1 i+2 i-1 i-2 i i-1 i-2 i+1 i+2 i i+1 i+2 i-1 i-2 i i+3 i+5 i-1 i-4 i+1 i+2 i-1 i-</formula><p>ods <ref type="bibr" target="#b0">(Acsintoae et al., 2022)</ref>. Despite the numerous attempts in solving the problem, video anomaly detection remains a challenging task, especially due to the fact that abnormal events are determined by the context. For example, a pedestrian walking on the sidewalk is a normal event, but a pedestrian who crosses the street far from a crosswalk is an abnormal event (in some countries, pedestrians can even get fines for crossing the street in forbidden areas). Furthermore, since abnormal events are typically rare, it is difficult to collect training data to build fully supervised models. This adds to the difficulty of solving the task. Hence, more research efforts are required towards solving video anomaly detection.</p><p>Perhaps one of the most promising directions in video anomaly detection without anomalies at training time is to address the task by learning a self-supervised framework on multiple proxy tasks, which are correlated to anomaly detection, as proposed by <ref type="bibr" target="#b17">Georgescu et al. (2021a)</ref>. <ref type="bibr">Indeed, Georgescu et al. (2021a)</ref> introduced a self-supervised multi-task learning (SSMTL) method that learns a set of four proxy tasks using a single 3D convolutional backbone with multiple heads (one head per task), obtaining state-of-the-art performance levels.</p><p>Although the SSMTL model attains very good results, we consider that the framework has a very high potential of obtaining even better results, which can be unearthed by studying and evaluating component variations in depth. To this end, we revisit our self-supervised multi-task learning framework <ref type="bibr" target="#b17">(Georgescu et al., 2021a)</ref>, proposing several updates that boost the performance of the method, especially when we combine these updates and generate new frameworks, which we term SSMTL++v1 and SSMTL++v2. We identified three important components worth revisiting, namely the object detection method, the multi-task learning backbone architecture, and the proxy tasks. First, we study additional detection methods, e.g. based on detecting high-motion regions using optical flow or background subtraction, since we conjecture the currently used pre-trained YOLOv3 is suboptimal, e.g. objects in motion or objects from unknown classes are never detected. Next, we modernize the 3D convolutional backbone by introducing multi-head self-attention modules, inspired by the recent success of vision transformers <ref type="bibr" target="#b5">(Bertasius et al., 2021;</ref><ref type="bibr" target="#b13">Dosovitskiy et al., 2021;</ref><ref type="bibr" target="#b74">Wu et al., 2021)</ref>. As such, we alternatively introduce both 2D and 3D convolutional vision transformer (CvT) blocks . Finally, we study additional proxy tasks, such as predicting segmentation maps through knowledge distillation, solving jigsaw puzzles, estimating body pose through knowledge distillation, predicting masked regions (patch inpainting), and adversarial learning with pseudoanomalies. Our most influential updates are illustrated inside red dashed contours in <ref type="figure">Figure 1</ref>.</p><p>We perform preliminary experiments to determine the impact of introducing the novel components into the SSMTL framework. Through the preliminary experiments, we find two novel and promising combinations (SSMTL++v1 and SSMTL++v2). We evaluate our new frameworks on three benchmark data sets: Avenue <ref type="bibr" target="#b42">(Lu et al., 2013)</ref>, ShanghaiTech <ref type="bibr" target="#b44">(Luo et al., 2017)</ref> and UBnormal <ref type="bibr" target="#b0">(Acsintoae et al., 2022)</ref>. We report considerable performance gains with respect to SSMTL <ref type="bibr" target="#b17">(Georgescu et al., 2021a)</ref>, while also attaining superior results compared to other recent state-of-the-art methods <ref type="bibr">(Astrid et al., 2021a,b;</ref><ref type="bibr" target="#b5">Bertasius et al., 2021;</ref><ref type="bibr" target="#b6">Chang et al., 2022;</ref><ref type="bibr" target="#b10">Dong et al., 2020;</ref><ref type="bibr">Doshi and Yilmaz, 2020a,b;</ref><ref type="bibr" target="#b18">Georgescu et al., 2021b;</ref><ref type="bibr" target="#b19">Gong et al., 2019;</ref><ref type="bibr">Ionescu et al., 2019a,b;</ref><ref type="bibr" target="#b29">Ji et al., 2020;</ref><ref type="bibr" target="#b34">Lee et al., 2019;</ref><ref type="bibr" target="#b37">Lin et al., 2022;</ref><ref type="bibr" target="#b41">Liu et al., 2021;</ref><ref type="bibr" target="#b43">Lu et al., 2020;</ref><ref type="bibr" target="#b45">Madan et al., 2021;</ref><ref type="bibr" target="#b49">Nguyen and Meunier, 2019;</ref><ref type="bibr" target="#b51">Park et al., 2022</ref><ref type="bibr" target="#b52">Park et al., , 2020</ref><ref type="bibr" target="#b54">Ramachandra and Jones, 2020;</ref><ref type="bibr" target="#b55">Ramachandra et al., 2020a;</ref><ref type="bibr" target="#b62">Ristea et al., 2022;</ref><ref type="bibr" target="#b67">Sultani et al., 2018;</ref><ref type="bibr" target="#b68">Sun et al., 2020;</ref><ref type="bibr" target="#b72">Vu et al., 2019;</ref><ref type="bibr" target="#b73">Wang et al., 2020;</ref><ref type="bibr" target="#b75">Wu et al., 2019;</ref><ref type="bibr" target="#b79">Yang et al., 2021;</ref><ref type="bibr" target="#b80">Yu et al., 2022</ref><ref type="bibr" target="#b81">Yu et al., , 2020</ref><ref type="bibr" target="#b82">Yu et al., , 2021</ref><ref type="bibr" target="#b84">Zaheer et al., 2022)</ref>.</p><p>In summary, our contribution is threefold:</p><p>? We introduce additional detection methods into SSMTL to increase the number of detected objects, providing empirical evidence to showcase the benefit of each detection method.</p><p>? We introduce convolutional vision transformer blocks into the backbone architecture, reporting performance improvements with our stronger backbone.</p><p>? We study various proxy tasks to be included into the SSMTL framework, finding novel task combinations that produce superior performance levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video anomaly detection. One dimension of taxonomy divides video anomaly detection methods into those that address single-scene and multi-scene problem formulations. Under this classification, the self-supervised multi-task learning framework treats the multi-scene formulation of the problem, where the training set may contain videos from multiple scenes and anomalies are not expected to be location-dependent. Another dimension categorizes methods into distance-based <ref type="bibr" target="#b26">(Ionescu et al., 2019a;</ref><ref type="bibr" target="#b55">Ramachandra et al., 2020a</ref><ref type="bibr" target="#b56">Ramachandra et al., , 2021</ref><ref type="bibr" target="#b58">Ravanbakhsh et al., 2018;</ref><ref type="bibr" target="#b65">Saligrama and Chen, 2012;</ref><ref type="bibr" target="#b66">Smeureanu et al., 2017;</ref><ref type="bibr" target="#b71">Tran and Hogg, 2017;</ref><ref type="bibr" target="#b77">Xu et al., 2015)</ref>, probabilistic <ref type="bibr" target="#b1">(Antic and Ommer, 2011;</ref><ref type="bibr" target="#b7">Cheng et al., 2015;</ref><ref type="bibr" target="#b16">Feng et al., 2017;</ref><ref type="bibr" target="#b25">Hinami et al., 2017;</ref><ref type="bibr" target="#b31">Kim and Grauman, 2009;</ref><ref type="bibr" target="#b48">Mehran et al., 2009;</ref><ref type="bibr" target="#b76">Wu et al., 2010)</ref>, reconstruction-based <ref type="bibr" target="#b19">(Gong et al., 2019;</ref><ref type="bibr" target="#b20">Hasan et al., 2016;</ref><ref type="bibr" target="#b44">Luo et al., 2017;</ref><ref type="bibr" target="#b49">Nguyen and Meunier, 2019;</ref><ref type="bibr" target="#b52">Park et al., 2020;</ref><ref type="bibr" target="#b59">Ravanbakhsh et al., 2017;</ref><ref type="bibr" target="#b62">Ristea et al., 2022;</ref><ref type="bibr" target="#b72">Vu et al., 2019)</ref> and change detection <ref type="bibr" target="#b9">(Del Giorno et al., 2016;</ref><ref type="bibr" target="#b27">Ionescu et al., 2017;</ref><ref type="bibr" target="#b40">Liu et al., 2018b)</ref> approaches. As SSMTL is based on multiple self-supervised tasks, it is not possible to place it into only one of these categories. For example, due to the middle bounding box prediction task, SSMTL can be viewed as a reconstruction-based approach, where the basic premise is to train a model that reconstructs normal data with higher fidelity as compared to abnormal data, and some composite measure of the reconstruction error subsequently acts as the anomaly score. Similar to several other methods in video anomaly detection <ref type="bibr" target="#b7">(Cheng et al., 2015;</ref><ref type="bibr" target="#b16">Feng et al., 2017;</ref><ref type="bibr" target="#b25">Hinami et al., 2017;</ref><ref type="bibr" target="#b26">Ionescu et al., 2019a;</ref><ref type="bibr" target="#b56">Ramachandra et al., 2021;</ref><ref type="bibr" target="#b72">Vu et al., 2019)</ref>, SSMTL operates at the object patch level as opposed to at the frame level, but is one of the few reconstruction-based methods to do so. For an extensive treatment of taxonomy in video anomaly detection, we refer the reader to the survey of <ref type="bibr" target="#b57">Ramachandra et al. (2020b)</ref>.</p><p>Certainly, we consider SSMTL <ref type="bibr" target="#b17">(Georgescu et al., 2021a</ref>) the closest method to the approaches presented in this paper, namely SSMTL++v1 and SSMTL++v2. We underline that the contributions presented in Section 1 also represent the differences with respect to our previous work <ref type="bibr" target="#b17">(Georgescu et al., 2021a)</ref>.</p><p>Multi-task learning. As computing devices get faster and more specialized for deep learning applications, benefiting from more memory and processing units, multi-task learning approaches have started gaining popularity, such as with Mask-RCNN <ref type="bibr" target="#b23">(He et al., 2017)</ref> for object detection and instance segmentation. The basic underlying premise of multi-task approaches is that learning to solve multiple tasks relevant to a primary (target) task is beneficial. When dealing with a problem such as video anomaly detection, where anomalous data is not provided at training time, the primary task cannot be directly supervised; herein lies the motivation for using multi-task learning. Multitask approaches to video anomaly detection have been used sparsely before <ref type="bibr" target="#b52">(Park et al., 2020;</ref><ref type="bibr" target="#b69">Tang et al., 2020)</ref>. However, to the best of our knowledge, SSMTL is the first to propose multi-task learning explicitly and intentionally for generalizing better to the out-of-distribution anomalous patterns in video. Certainly, our current work is based on the same principle.</p><p>Self-supervised learning. Self-supervised learning is garnering traction with recent advances showing that, under the right conditions, self-supervised pre-training can outperform fully supervised pre-training in terms of transfer performance in downstream tasks <ref type="bibr" target="#b21">(He et al., 2022</ref><ref type="bibr" target="#b22">(He et al., , 2020</ref>. Self-supervised learning has been widely used before for video anomaly detection. Most reconstruction-based approaches use some form of self-supervised learning. The major approaches include frame-level reconstruction <ref type="bibr" target="#b20">(Hasan et al., 2016)</ref>, future frame prediction <ref type="bibr" target="#b10">(Dong et al., 2020;</ref><ref type="bibr" target="#b38">Liu et al., 2019)</ref> or middle frame prediction <ref type="bibr" target="#b34">(Lee et al., 2019)</ref>. The SSMTL framework is however the first to use middle patch prediction at the object level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Original framework. In our previous work <ref type="bibr" target="#b17">(Georgescu et al., 2021a)</ref>, we proposed SSMTL, an object-centric framework based on self-supervised and multi-task learning on four proxy tasks. Indeed, the proposed model is trained on three selfsupervised tasks and one knowledge distillation task. The whole architecture is composed of a shared 3D CNN backbone and four independent prediction or reconstruction heads (one per proxy task). The last layer of the shared 3D CNN is global temporal pooling. Therefore, the prediction heads can use 2D convolutions.</p><p>The first step of the SSMTL framework is to obtain the object bounding boxes using the YOLOv3 <ref type="bibr" target="#b60">(Redmon and Farhadi, 2018)</ref> object detector. For each object in the frame i, a socalled object-centric temporal sequence is created by cropping the corresponding bounding box from the frames {i ? t, ..., i ? 1, i, i + 1, ..., i + t}. The object-centric temporal sequence is used as input to the 3D CNN.</p><p>The first proxy task (T 1 ) is predicting the arrow of time, where the model learns to predict if the temporal sequence is moving forward or backward in time. The second proxy task (T 2 ) is predicting motion irregularity, where the model is trained to predict if the object-centric temporal sequence is cropped from consecutive or intermittent frames, i.e. some frames are skipped in the forward direction to create irregular motion. The third self-supervised proxy task (T 3 ) is middle bounding box prediction, where the middle crop (the bounding box cropped from the frame i) is deleted from the temporal sequence and the model is trained to predict the content of the missing bounding box. The fourth proxy task (T 4 ) is model distillation. Here, the model is trained to predict the presoftmax features of a pre-trained ResNet-50 <ref type="bibr" target="#b24">(He et al., 2016)</ref> and the class probabilities predicted by the YOLOv3 <ref type="bibr" target="#b60">(Redmon and Farhadi, 2018)</ref> object detector.</p><p>The model is jointly optimized on all four proxy tasks. During inference, the object detector is applied on each frame. Then, for each detected object, the object-centric temporal sequence is created. The temporal sequence is passed thought the CNN model, obtaining the output of each prediction head. For T 1 , the probability that the temporal sequence is moving backward is interpreted as the anomaly score. Similarly, the probability of the temporal sequence to be intermittent is considered as the anomaly score for T 2 . For T 3 , the anomaly score is computed as the mean absolute difference between the reconstruction of the middle bounding box predicted by the model and the ground-truth middle bounding box. For T 4 , only the absolute difference between the class probabilities predicted by the YOLOv3 object detector and those predicted by the model are taken into consideration, saving the time need to run ResNet-50 during inference. The anomaly score for each object is computed as the average of the anomaly scores given by each prediction head.</p><p>Updates overview. We identified three main components that are promising candidates for receiving updates that could positively impact the performance of SSMTL. The first component is the object detection method, which is currently based on a single pre-trained object detector. Increasing the number of detected objects is likely to increase the number of detected anomalies. Hence, we consider adding more detection methods. The second component is the shared backbone architecture. Here, we propose and evaluate two different ways of integrating transformer blocks, which could strengthen the learning capacity of the framework. The last component worth investigating is the set of proxy tasks. <ref type="bibr" target="#b17">Georgescu et al. (2021a)</ref> showed that the four proxy tasks employed in the original SSMTL framework are useful, but we believe that there are many other proxy tasks that could prove beneficial. We present our updates to these three components in separate sections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Introducing New Detection Methods</head><p>The object detector is an important part of the framework because the anomaly analysis is performed only on the detected objects (regions). Thus, if the object detector fails to detect an object of interest (anomalous), the framework will completely miss the respective anomalous event. In the original SSMTL framework, the YOLOv3 <ref type="bibr" target="#b60">(Redmon and Farhadi, 2018)</ref> object detector was employed. Our first update is to consider the YOLOv5 <ref type="bibr" target="#b30">(Jocher et al., 2022)</ref> detector, which is known to outperform YOLOv3. Even though YOLOv5 is supposed to detect the objects more accurately than its previous versions, e.g. YOLOv3, an object detector can only detect a predefined set of classes. However, the set of object classes that can generate an anomaly should not be limited to a fixed number of classes, otherwise we might encounter a frame with objects which the object detector is unable to detect, e.g. a tree that falls on the street, blocking the traffic. Another limitation of object detectors is the inability of detecting objects affected by severe motion blur. However, fast moving objects, e.g. a person running, are very likely to cause an anomaly. Due to these limitations, the SSMTL framework can miss such anomalous objects. To alleviate these issues, we propose to detect objects using optical flow or background subtraction, in conjunction with a pre-trained object detector.</p><p>On the one hand, we propose to detect new objects belonging to unknown object classes or that are affected by motion blur by applying optical flow. For each frame, we compute the optical flow map with the method proposed by <ref type="bibr" target="#b38">Liu et al. (2019)</ref>. We consider that a pixel from the optical flow map is part of a moving object if its magnitude is larger than a threshold. Additionally, we use the YOLO bounding boxes to blackout regions of already detected objects. The resulting connected components are added to the set of detected objects. To eliminate very small objects created by the noise in the optical flow map, we impose a restriction for the width and height of each new object detected through optical flow.</p><p>On the other hand, we propose to use background subtraction as a faster alternative to optical flow. We employ a fast and intuitive approach that starts by converting the RGB frames to grayscale. We compute an initial background image for each scene by averaging several consecutive frames. We continuously update the background throughout the video sequence. We subtract the background image from each frame to obtain the foreground objects. Then, we apply a threshold to separate the foreground pixels from the background pixels and clean up the result with a morphological closing operation. As for the optical flow maps, we blackout the regions already detected by YOLO and add the connected components with an area higher than a certain threshold as new objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Introducing New Backbones</head><p>We extend the original 3D CNN backbone of the SSMTL architecture and introduce a CvT  module formed of multiple sequential transformer blocks. We study two approaches of applying the transformer before and after the global temporal pooling layer, effectively modeling the input as either 2D or 3D. Hence, we name the two approaches 2D CvT and 3D CvT. For the 2D CvT module, we apply average pooling across time to obtain a 2D input of 8 ? 8 tokens, before introducing the module. For the 3D CvT module, we first apply the module directly on the output of the shared 3D CNN backbone, the input for the CvT module having 7 ? 8 ? 8 tokens. To make it compatible, we replace the 2D depthwise convolutions from CvT with 3D depthwise convolutions. The output of the 3D CvT module is passed through a global temporal pooling layer, producing the final output. The number of transformer blocks n as well as the number of attention heads h are decided according to the number of proxy tasks, based on a set of preliminary experiments discussed in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Introducing New Proxy Tasks</head><p>Aside from the existing four tasks, we propose a new set of five proxy tasks, as described below. We note that the prediction and decoding heads used for the new tasks are identical to those used for the original four tasks. T 5 : Adversarial reconstruction. For the adversarial reconstruction of pseudo-anomalies task (T 5 ), we employ an adversarial training procedure based on gradient ascent through our shared backbone. We attach a decoder to our shared encoder which learns to reproduce images from an out-of-distribution data set formed of flowers <ref type="bibr" target="#b50">(Nilsback and Zisserman, 2006)</ref>, textures <ref type="bibr" target="#b33">(Lazebnik et al., 2005)</ref>, and ImageNet <ref type="bibr" target="#b63">(Russakovsky et al., 2015)</ref> classes that do not appear in urban surveillance videos. We optimize the decoder using gradient descent, as usual. However, the shared backbone is optimized to model pseudo-anomalies poorly, that is, when updating the network, we reverse the sign of the learning rate for pseudo-anomalies. This design incentives the network to model general patterns poorly, directing the model towards overfitting the provided normal data. Notice that, in the context of anomaly detection, we need a model that does not generalize too well on out-ofdistribution data, otherwise it would prevent us from leveraging the reconstruction error as a method for anomaly detection. T 6 : Patch inpainting. For the patch inpainting task, our network is tasked with reconstructing a 64 ? 64 single-frame input, out of which, a random patch is cropped out. At test time, for a given input, this procedure is repeated three times in a row, with the output of the previous step serving as input for the current one. At each step, a different random patch is masked out. Finally, we employ the L 2 distance between the original image and the final reconstruction to estimate the anomaly level. T 7 : Segmentation. We pose the segmentation task as a knowledge distillation task, where the teacher is a pre-trained Mask R-CNN <ref type="bibr" target="#b23">(He et al., 2017)</ref> and the student is our model. For this task, we attach a decoder to predict the segmentation map. We employ the L 2 loss between the predicted map and the groundtruth map to train our student. We use only the middle crop in the temporal sequence as input. T 8 : Jigsaw. For the jigsaw task, we split each input image into 4 ? 4 patches (puzzle pieces) of 16 ? 16 pixels each and apply a random shuffle of the patches, out of 100 predefined shuffles. We then task the network to predict the applied shuffle in a multi-way classification setting, using softmax. Let p i be the probability predicted by the jigsaw head for the correct class. The anomaly score is given by 1 ? p i . T 9 : Pose estimation. As for the segmentation task, we consider pose estimation as a knowledge distillation task and only use the middle crop in the temporal sequence as input. As teacher, we choose the pre-trained UniPose <ref type="bibr" target="#b2">(Artacho and Savakis, 2020)</ref>. The student network is tasked with predicting heatmaps for each body joint, being optimized with the L 2 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Sets</head><p>Avenue. The Avenue <ref type="bibr" target="#b42">(Lu et al., 2013)</ref> data set consists of 16 training videos containing only normal activity, and 21 test videos containing both normal and abnormal actions. The resolution of each frame is 360 ? 640 pixels. The data set is annotated at the frame and pixel levels. ShanghaiTech. The ShanghaiTech Campus <ref type="bibr" target="#b44">(Luo et al., 2017)</ref> data set is one of the largest data sets for video anomaly detection, containing 437 videos. The training set contains 330 videos with normal actions while the test set consists of 107 videos with both normal and abnormal events. The resolution of each frame in the data set is 480 ? 856 pixels. The Shang-haiTech data set is annotated at both frame and pixel levels. UBnormal. The UBnormal <ref type="bibr" target="#b0">(Acsintoae et al., 2022)</ref> data set is a new supervised open-set benchmark containing abnormal actions in the training videos which are disjoint from the abnormal actions from the test videos. The entire data set has a total of 543 videos which are divided into 268 training videos, 64 validation videos and 211 test videos. The resolution of the frames can vary, the minimum side of a frame being 720 pixels. UBnormal is also annotated at both frame and pixel levels.</p><p>In the experiments, we only use the normal videos to train our framework, preserving its self-supervised nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation and Implementation Details</head><p>Evaluation metrics. We employ the widely-used area under the curve (AUC) computed with respect to the ground-truth frame-level annotations to evaluate detection performance. Following <ref type="bibr" target="#b0">(Acsintoae et al., 2022;</ref><ref type="bibr" target="#b18">Georgescu et al., 2021b;</ref><ref type="bibr" target="#b62">Ristea et al., 2022)</ref>, we report both the micro and macro framelevel AUC scores when we compare to other state-of-the-art methods. To evaluate the localization performance of the proposed frameworks, we consider the region-based detection criterion (RBDC) and track-based detection criterion (TBDC) introduced by <ref type="bibr" target="#b54">Ramachandra and Jones (2020)</ref>. To draw our conclusions after each preliminary experiment, we only look at the more popular micro AUC measure, which we consider the most important due to the sheer number of previous works that have used it <ref type="bibr" target="#b57">(Ramachandra et al., 2020b)</ref>. Hyperparameter settings. We start from the official implementation of SSMTL 1 . We keep all the original hyperparameters of the framework, established by <ref type="bibr" target="#b17">Georgescu et al. (2021a)</ref>. For YOLOv3 and YOLOv5, we set the object detection confidence threshold to 0.8. We use a pre-trained SelFlow  model to detect objects in motion, forming the objects out of pixels with a motion magnitude higher than 1. For the SelFlow and background subtraction methods, we eliminate all objects with an area smaller than 1500 pixels. As <ref type="bibr" target="#b17">Georgescu et al. (2021a)</ref>, we use temporal sequences of length 7, resulting in input tensors of 7?64?64?3 components. Since we remove task T 4 in our final framework configurations (SSMTL++v1/v2), we eliminate the hyperparameter ? from the original framework, using equal weights for all the remaining proxy tasks.</p><p>All models are optimized for 20 epochs using Adam <ref type="bibr" target="#b32">(Kingma and Ba, 2015)</ref> with a learning rate of ? = 10 ?3 , keeping the default values for the other hyperparameters of Adam. Depending on the capacity of the backbone architecture, we train the models on mini-batches of 64, 128 or 256 samples. We keep 15% of the training data for validation and choose the model with the lowest validation error on the proxy tasks to be employed on the target task (anomaly detection).</p><p>There are some additional hyperparameters for the new proxy tasks added to the model. For the adversarial training with pseudo-anomalies (T 5 ), we adjust the adversarial learning rate to ?0.2 ? ? following (McHardy et al., 2019), thus maximizing the loss when an adversarial example is given as input. For the inpainting task (T 6 ), we generate mask patches of random sizes between 4 and 32 pixels. The center of each patch is generated using a 2D Gaussian distribution centered in the middle of the input image, having a standard deviation of 20 in each direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Preliminary Experiments</head><p>Experimenting with new detection methods. We first experiment with various detection methods, presenting the results on the Avenue data set in <ref type="table" target="#tab_1">Table 1</ref>. To evaluate each detector, we report the total number of detections in the test set together with the recall calculated with respect to the ground-truth annotations provided by <ref type="bibr" target="#b54">Ramachandra and Jones (2020)</ref>. Since only the anomalies are annotated, there is no way to compute precision. Optical flow adds a few additional detections for a better recall, while background subtraction adds a higher number of detections, leading to the highest recall score. However, the AUC is more important. The original SSMTL framework uses the YOLOv3 detector, which leads to a micro AUC of 83.5%. 1 https://github.com/lilygeorgescu/AED-SSMTL </p><formula xml:id="formula_1">Tasks Backbone AUC T 3 3D CNN 89.6% T 3 3D CNN + 2D CvT 90.1% T 3 3D CNN + 3D CvT 90.6% T 1 +T 2 +T 3 3D CNN 90.7% T 1 +T 2 +T 3 3D CNN + 3D CvT 92.5%</formula><p>YOLOv5 alone seems to output better object detections, providing a performance gain of 3.4% over YOLOv3. When we add optical flow detections, the performance increases by significant margins for both YOLOv3 and YOLOv5. Interestingly, there seems to be a much higher gain by combining YOLOv3 and optical flow detections. When we introduce the detections obtained via background subtraction, we again observe considerable performance gains, but not as high as compared to optical flow. This is caused by some of the added detections being labeled as anomalies by mistake, generating a higher false positive rate. Hence, we conclude that optical flow is a better choice than background subtraction. Since the combination of YOLOv3 and optical flow gives the best micro AUC (89.6%), we continue with this detection approach in the subsequent experiments. Experimenting with new backbones. In our second experiment, we study the effect of changing the backbone architecture from a pure 3D CNN to one based on transformer blocks. For this empirical study, we select only one task (middle bounding box reconstruction) and perform the experiments on the Avenue data set, just as before. We report the corresponding results in the first three rows of Table 2. Since the model learns only one task, we choose the shallow and narrow <ref type="bibr" target="#b17">(Georgescu et al., 2021a)</ref> configuration for the 3D CNN. By introducing 2D CvT blocks after performing average pooling across time, we observe a gain of 0.5% in terms of the micro AUC. We notice a higher gain (1%) upon introducing the 3D transformer module prior to the pooling across time. We keep this backbone in the following experiments.</p><p>Next, we aim to determine if the performance gains brought by the new transformer-based backbone are consistent when in- <ref type="table">Table 3</ref>. Results (in terms of the frame-level micro AUC) on Avenue while varying the number of transformer blocks (from 1 to 3) and the number of attention heads (from 6 to 18). All models are trained on three tasks: T 1 (arrow of time), T 2 (motion irregularity), T 3 (middle bounding box prediction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blocks</head><p>Heads <ref type="formula">6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tasks</head><p>Backbone AUC</p><formula xml:id="formula_2">T 1 +T 2 +T 3 shared 92.5% T 1 +T 2 +T 3 +T 4 shared 92.0% T 1 +T 2 +T 3 +T 5 shared 93.7% T 1 +T 2 +T 3 +T 6 shared 91.6% T 1 +T 2 +T 3 +T 7 shared 90.7% T 1 +T 2 +T 3 +T 8 shared 90.2% T 1 +T 2 +T 3 +T 9 shared 91.1% T 1 +T 2 +T 3 +T 4 +T 5 shared 89.8% T 1 +T 2 +T 3 +T 4 +T 6 shared 89.5% T 1 +T 2 +T 3 +T 5 +T 6 shared 90.1% T 1 +T 2 +T 3 +T 4 +T 5 +T 6 shared 89.6% T 1 +T 2 +T 3 +T 4 +T 5 +T 6 separate 90.5%</formula><p>troducing more tasks. We underline that for the original backbone, <ref type="bibr" target="#b17">Georgescu et al. (2021a)</ref> increased the depth and width of the architecture along with the number of tasks. In a similar manner, we study how the number of transformer blocks (from 1 to 3) and the number of attention heads (from 6 to 18) influences the performance of the framework when we switch from one task (middle bounding box reconstruction) to the following three tasks: T 1 (arrow of time prediction), T 2 (motion irregularity prediction), T 3 (middle bounding box reconstruction). We present the results with various depths and widths on Avenue in <ref type="table">Table 3</ref>. The empirical results indicate that the best configuration is to use 3 blocks with 12 attention heads each. Upon finding the optimal architecture in the context of multitask learning, we now compare the deep+wide 3D CNN with the new backbone based on 3D CNN + 3D CvT with 3 blocks and 12 attention heads. Both models are trained on the first three tasks for a fair comparison. We report the corresponding results in the last two rows of <ref type="table">Table 2</ref>. We observe that introducing more tasks increases the gap between the old and new backbones, by up to 1.8%. We keep the configuration based on 3 blocks and 12 attention heads for the remaining experiments. Experimenting with new tasks. In <ref type="table">Table 4</ref>, we present results with various task combinations on Avenue. First, we underline that the knowledge distillation task (T 4 ) from the original framework is not immediately compatible with the addition of detections based on optical flow or background subtraction, since these detections do not have an assigned object class, unlike the YOLOv3 detections. Moreover, assigning classes to these detections is not trivial as they sometimes include a single object part or multiple objects. To this end, the experiments conducted so far do not include task T 4 . However, we can introduce a new class of objects that comprises all the optical flow detections. As shown in <ref type="table">Table 4</ref>, this solution is suboptimal, leading to a slight performance drop from 92.5% to 92.0%. Next, we experiment with independently adding our new proxy tasks to the first three tasks, to assess the impact of each new proxy task on the performance of the whole framework. Among the new proxy tasks, we find tasks T 5 (adversarial reconstruction of pseudo-anomalies) and T 6 (patch inpainting) as the most promising. We note that task T 5 is not introduced right from the beginning, as the network needs some time to converge on the other tasks before adversarial training is enabled. As confirmed by <ref type="figure" target="#fig_0">Figure 2</ref>, it is worth waiting for a few epochs before enabling task T 5 , the optimal starting point being epoch 5.</p><p>We next attempt to combine 5 or 6 proxy tasks together, considering the most promising options. However, the results indicate significant performance drops when jointly optimizing the framework on 5 or more tasks. Our first assumption for explaining the lower results is that the backbone needs a higher capacity to cope with the larger number of tasks. We tried to increase its capacity, without obtaining any performance gains. We also tried to use a separate backbone for each task, which seems to be somewhat useful, but not enough to surpass our best performing combination of tasks (T 1 , T 2 , T 3 and T 5 ).</p><p>For the final comparison with the existing state-of-the-art methods, we choose two of our most promising models. Our first combination of tasks (SSMTL++v1) is formed of tasks T 1 , T 2 , T 3 and T 5 . Our second combination of tasks (SSMTL++v2) is formed of tasks T 1 , T 2 , T 3 and T 6 . We recall that both SSMTL++v1 and SSMTL++v2 use a hybrid detection method based on YOLOv3 and optical flow, as well as an enhanced backbone (3D CNN + 3D CvT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State of the Art</head><p>Results on Avenue. We present the comparative results of SSMTL++v1 and SSMTL++v2 versus the state-of-the-art methods on the Avenue data set in <ref type="table">Table 5</ref>. Compared to SSMTL, we observe that SSMTL++v1 and SSMTL++v2 attain better micro AUC, macro AUC and TBDC scores, but the new models register drops in terms of RBDC. The high gains <ref type="table">Table 5</ref>. Comparison of the proposed frameworks (SSMTL++v1 and SSMTL++v2) with the original SSMTL <ref type="bibr" target="#b18">(Georgescu et al., 2021b)</ref> as well as other state-of-the-art methods on the Avenue data set. The top three scores for each metric are highlighted in red bold (top method), green (second best) and blue (third best).</p><p>Year Method AUC RBDC TBDC Micro Macro 2019 <ref type="bibr" target="#b19">Gong et al. (2019)</ref> 83.3 --- <ref type="bibr" target="#b26">Ionescu et al. (2019a)</ref> 87.4 90.4 15.8 27.0 <ref type="bibr" target="#b28">Ionescu et al. (2019b)</ref> 88.9 --- <ref type="bibr" target="#b34">Lee et al. (2019)</ref> 90.0 --- <ref type="bibr" target="#b49">Nguyen and Meunier (2019)</ref> 86.9 --- <ref type="bibr" target="#b72">Vu et al. (2019)</ref> 71.5 --- <ref type="bibr" target="#b75">Wu et al. (2019)</ref> 86.6 ---2020 <ref type="bibr" target="#b10">Dong et al. (2020)</ref> 84.9 --- <ref type="bibr">Doshi and Yilmaz (2020a,b)</ref> 86.4 --- <ref type="bibr" target="#b29">Ji et al. (2020)</ref> 78.3 --- <ref type="bibr" target="#b43">Lu et al. (2020)</ref> 85.8 --- <ref type="bibr" target="#b52">Park et al. (2020)</ref> 88.5 --- <ref type="bibr" target="#b54">Ramachandra and Jones (2020)</ref> 72. Results on ShanghaiTech. We present the results of the comparative study conducted on ShanghaiTech in <ref type="table" target="#tab_4">Table 6</ref>. We observe that both SSMTL++v1 and SSMTL++v2 obtain consistent improvements over SSMTL <ref type="bibr" target="#b17">(Georgescu et al., 2021a)</ref> across all metrics. Remarkably, SSMTL++v2 attains the top performance on each metric, surpassing all other approaches.</p><p>At the same time, SSMTL++v1 shares the second and third places (depending on the metric) with two state-of-the-art frameworks <ref type="bibr" target="#b18">(Georgescu et al. (2021b)</ref> and <ref type="bibr" target="#b39">Liu et al. (2018a)</ref>) which were recently enhanced with self-supervised predictive convolutional attentive blocks (SSPCAB) <ref type="bibr" target="#b62">(Ristea et al., 2022)</ref>. Results on UBnormal. As the UBnormal <ref type="bibr" target="#b0">(Acsintoae et al., 2022)</ref> benchmark is very new, the number of existing results is relatively small. Nevertheless, we showcase the results of the comparative study conducted on UBnormal in <ref type="table" target="#tab_6">Table 7</ref>. As for the ShanghaiTech data set, we notice that both SSMTL++v1 and SSMTL++v2 surpass the original SSMTL method. Furthermore, SSMTL++v1 establishes new state-of-the-art levels for three metrics (macro AUC, RBDC and TBDC), while SSMTL++v2 is the second best method for two of the metrics  <ref type="bibr" target="#b18">(Georgescu et al., 2021b)</ref> as well as other state-of-the-art methods on the ShanghaiTech data set. The top three scores for each metric are highlighted in red bold (top method), green (second best) and blue (third best).</p><p>Year Method AUC RBDC TBDC Micro Macro 2019 <ref type="bibr" target="#b19">Gong et al. (2019)</ref> 71.2 --- <ref type="bibr" target="#b26">Ionescu et al. (2019a)</ref> 78.7 84.9 20.7 44.5 <ref type="bibr" target="#b34">Lee et al. (2019)</ref> 76.2 ---2020 <ref type="bibr" target="#b10">Dong et al. (2020)</ref> 73.7 --- <ref type="bibr">Doshi and Yilmaz (2020a,b)</ref> 71.6 --- <ref type="bibr" target="#b43">Lu et al. (2020)</ref> 77.9 --- <ref type="bibr" target="#b52">Park et al. (2020)</ref> 70.5 --- <ref type="bibr" target="#b68">Sun et al. (2020)</ref> 74.7 --- <ref type="bibr" target="#b73">Wang et al. (2020)</ref> 79.3 --- <ref type="bibr" target="#b81">Yu et al. (2020)</ref> 74 <ref type="formula">.</ref> We present a few test cases to assess the quality of the predicted anomaly scores for SSTML and SSMTL++v1/v2 with respect to the ground-truth. Upon analyzing <ref type="figure">Figure 3</ref>, we can observe that the AUC gains brought by SSMTL++v1 over SSMTL are higher than 5% on test video 06 from Avenue. The person labeled as anomalous by SSMTL++v1 is walking in the wrong direction. Similarly, in <ref type="figure">Figure 4</ref>, we can easily notice that the AUC gains brought by SSMTL++v1 over SSMTL are higher than 4% on test video 20 from Avenue. The person labeled as anomalous by SSMTL++v1 is throwing and gathering papers on the ground. Looking at <ref type="figure">Figure 5</ref>, we notice that SSMTL++v2 is outperforming SSMTL by a significant margin (+14%) on test video 07 0049 from ShanghaiTech. SSMTL++v2 labels two humans as anomalous because they are fighting. <ref type="figure">Figure 6</ref> also shows that SSMTL++v2 is outperforming SSMTL by a signifi-cant margin (+11%) on test video 04 0013 from ShanghaiTech. SSMTL seems to activate on the last 50 frames (with indexes higher than 310) in the video, producing a false positive event, while SSMTL++v2 does not trigger an anomaly for the same event. SSMTL++v2 outputs higher anomaly scores only for the person jumping, which is labeled as a true anomaly.</p><p>Final remarks on the experiments. With a few exceptions, SSMTL++v1 and SSMTL++v2 attain very high performance levels, generally surpassing the competing methods across different data sets and metrics. In the majority of cases, our new frameworks reach new state-of-the-art levels. We thus conclude that our updates brought to the SSMTL framework are noteworthy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Running Time</head><p>In <ref type="table">Table 8</ref>, we compare the inference time of SSMTL <ref type="bibr" target="#b17">(Georgescu et al., 2021a)</ref> with SSMTL++v1 and SSMTL++v2. Regarding the running time, our main changes of the original framework (SSMTL) <ref type="bibr" target="#b17">(Georgescu et al., 2021a)</ref> introduce additional processing (i) in the object detection phase due to optical flow, (ii) in the forward pass through the backbone due to the appended transformer blocks, and (iii) in the prediction phase  <ref type="bibr" target="#b18">(Georgescu et al., 2021b)</ref> as well as other state-of-the-art methods on the UBnormal data set. The top three scores for each metric are highlighted in red bold (top method), green (second best) and blue (third best). of SSMTL++v2 due to the additional inpainting task (T 6 ), as it requires 3 passes through the model. We note that the pseudoanomalies task (T 5 ) of SSMTL++v1 is only used during training, thus not having any influence on the inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>YOLOv3 <ref type="bibr" target="#b60">(Redmon and Farhadi, 2018)</ref>, which is used by all methods, takes nearly 0.84 seconds to process a mini-batch of 64 frames, thus running at about 72 frames per second (FPS). The optical flow is obtained using the pre-trained SelFlow , running at 30 FPS on mini-batches of 32 frames.</p><p>The SSMTL++v1 network processes one object-centric temporal sequence in 12 milliseconds (ms), without batching. For an object-centric approach, we can naturally batch the objects detected in each frame, resulting in a processing time of 3 ms per object-centric temporal sequence for a mini-batch size of 7 samples, corresponding to the average number of detections per frame in the Avenue test set. SSMTL++v2 processes an objectcentric temporal sequence in 4 ms with an identical mini-batch size.</p><p>SSMTL can process the video frames from the Avenue test set at about 50 FPS. Due to the introduction of optical flow in the object detection phase and the deeper backbone, <ref type="figure">Fig. 4</ref>. Comparing the anomaly scores (on the vertical axis) of SSMTL <ref type="bibr" target="#b17">(Georgescu et al., 2021a)</ref> and SSMTL++v1 on the frames (on the horizontal axis) of test video 20 from Avenue. Best viewed in color.</p><p>the inference times for SSMTL++v1 and SSMTL++v2 decrease to about 15 FPS, considering a sequential processing pipeline on a single thread. However, running SSMTL++v1 and SSMTL++v2 on two threads in parallel increases the speed to about 20 FPS, while still using a single GPU. Hence, both SSMTL++v1 and SSMTL++v2 are fast enough to process the video in real-time. The reported running times were measured on a GeForce GTX 3090 GPU with 24 GB of VRAM.</p><p>Our main bottleneck is the optical flow framework. We note that the focus of this work has been on precision in terms of anomaly detection capabilities, without much effort being dedicated towards optimizing the running time. Alternatively, faster object detectors can be applied, as well as using the faster background subtraction (55 FPS), at a small cost of precision, if inference time is a higher priority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we revisited the self-supervised multi-task learning framework introduced by <ref type="bibr" target="#b17">Georgescu et al. (2021a)</ref>, proposing a series of updates that boost the performance of the <ref type="figure">Fig. 5</ref>. Comparing the anomaly scores (on the vertical axis) of SSMTL <ref type="bibr" target="#b17">(Georgescu et al., 2021a)</ref> and SSMTL++v2 on the frames (on the horizontal axis) of test video 07 0049 from ShanghaiTech. Best viewed in color. <ref type="figure">Fig. 6</ref>. Comparing the anomaly scores (on the vertical axis) of SSMTL <ref type="bibr" target="#b17">(Georgescu et al., 2021a)</ref> and SSMTL++v2 on the frames (on the horizontal axis) of test video 04 0013 from ShanghaiTech. Best viewed in color.</p><p>original method to new state-of-the-art levels. We provided empirical evidence for several beneficial updates. First, we showed that using optical flow along with YOLOv3 to obtain object detections is very useful in finding more objects. Second, we obtained additional performance gains by integrating 3D convolutional multi-head attention blocks into the backbone architecture. Furthermore, we showed that the adversarial training on pseudo-anomalies and patch inpainting tasks are wellcorrelated to anomaly detection, leading to performance improvements of the multi-task learning pipeline. Interestingly, these new proxy tasks are useful when replacing the original knowledge distillation task (T 4 ), rather than being added as additional proxy tasks.</p><p>Noting that models trained on more than 5 tasks seem to underperform, in future work, we aim to study more ways to learn from as many tasks as possible, which, at least in principle, should lead to even better results. <ref type="table">Table 8</ref>. Running time (in terms of FPS) on Avenue for SSMTL versus SSTML++v1 and SSMTL++v2. All models are trained on four proxy tasks, thus having four prediction heads. The reported running times were measured on a GeForce GTX 3090 GPU with 24 GB of VRAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>FPS SSMTL <ref type="bibr" target="#b17">(Georgescu et al., 2021a)</ref> 50.0 SSMTL++v1 20.2 SSMTL++v2 18.8</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Frame-level AUC scores showing the effect of introducing pseudoanomalies at different epochs while training SSMTL++v1 on Avenue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Results (in terms of recall and frame-level micro AUC) on Avenue while varying the object detection methods of a framework trained on task T 3 (middle bounding box prediction).</figDesc><table><row><cell>Detection Approach</cell><cell cols="2">#Objects Recall AUC</cell></row><row><cell>YOLOv3</cell><cell>111k</cell><cell>88.2% 83.5%</cell></row><row><cell>YOLOv3 + Optical Flow</cell><cell>113k</cell><cell>91.3% 89.6%</cell></row><row><cell>YOLOv3 + Background</cell><cell>118k</cell><cell>94.8% 88.4%</cell></row><row><cell>YOLOv5</cell><cell>105k</cell><cell>87.7% 86.9%</cell></row><row><cell>YOLOv5 + Optical Flow</cell><cell>110k</cell><cell>90.9% 89.0%</cell></row><row><cell>YOLOv5 + Background</cell><cell>120k</cell><cell>96.1% 88.2%</cell></row><row><cell cols="3">Table 2. Results (in terms of the frame-level micro AUC) on Avenue while</cell></row><row><cell cols="3">changing the backbone architecture for a model trained either on one task</cell></row><row><cell cols="3">(T 3 -middle bounding box prediction) or three tasks (T 1 -arrow of time,</cell></row><row><cell cols="3">T 2 -motion irregularity, T 3 -middle bounding box prediction).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Comparison of the proposed frameworks (SSMTL++v1 and SSMTL++v2) with the original SSMTL</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Comparison of the proposed frameworks (SSMTL++v1 and SSMTL++v2) with the original SSMTL</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by a grant of the Romanian Ministry of Education and Research, CNCS -UEFISCDI, project number PN-III-P2-2.1-PED-2021-0195, within PNCDI III. This work has also been funded by the Milestone Research Programme at AAU, and by SecurifAI.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ubnormal: New benchmark for supervised open-set video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acsintoae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Florescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sumedrea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="20143" to="20153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video parsing for abnormality detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2415" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">UniPose: Unified Human Pose Estimation in Single Images and Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Artacho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Savakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7035" to="7044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning memory-guided normality for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Astrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Synthetic Temporal Anomaly Guided End-to-End Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Astrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCVW</title>
		<meeting>ICCVW</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="207" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Is Space-Time Attention All You Need for Video Understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video anomaly detection with spatio-temporal dissociation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page">108213</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video anomaly detection and localization using hierarchical feature representation and Gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2909" to="2917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Discriminative Framework for Anomaly Detection in Large Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dual Discriminator Generative Adversarial Network for Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="88170" to="88176" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Any-Shot Sequential Anomaly Detection in Surveillance Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="934" to="935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Continual Learning for Anomaly Detection in Surveillance Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="254" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online Detection of Abnormal Events Using Incremental Coding Length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3755" to="3761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14009" to="14018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep event models for crowd anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="page" from="548" to="556" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Anomaly Detection in Video via Self-Supervised and Multi-Task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbalau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12742" to="12752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Background-Agnostic Framework with Adversarial Training for Abnormal Event Detection in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Memorizing Normality to Detect Anomaly: Memory-Augmented Deep Autoencoder for Unsupervised Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3639" to="3647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object-Centric Auto-Encoders and Dummy Anomalies for Abnormal Event Detection in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7842" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unmasking the abnormal events in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2895" to="2903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detecting abnormal events in video using Narrowed Normality Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1951" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">TAM-Net: Temporal Enhanced Appearance-to-Motion Generative Network for Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">ultralytics/yolov5: v6.1 -TensorRT, TensorFlow Edge TPU and OpenVINO Export and Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stoken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Borovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.6222936</idno>
		<idno>doi:10.5281/zenodo.6222936</idno>
		<ptr target="https://doi.org/10.5281/zenodo.6222936" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: A space-time MRF for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Sparse Texture Representation Using Local Affine Regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1265" to="1278" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BMAN: Bidirectional Multi-Scale Aggregation Networks for Abnormal Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2395" to="2408" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Decoupled appearance and motion learning for efficient anomaly detection in surveillance video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leroux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simoens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">210</biblScope>
			<biblScope unit="page">103249</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Causal Inference Look at Unsupervised Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1620" to="1629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SelFlow: Self-Supervised Learning of Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4571" to="4580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Future Frame Prediction for Anomaly Detection -A New Baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Classifier Two-Sample Test for Video Anomaly Detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Hybrid Video Anomaly Detection Framework via Memory-Augmented Flow Reconstruction and Flow-Guided Frame Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13588" to="13597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection at 150 FPS in MAT-LAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Few-Shot Scene-Adaptive Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="125" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Temporal Cues From Socially Unacceptable Trajectories for Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farkhondeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nasrollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCVW</title>
		<meeting>ICCVW</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2150" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Anomaly Detection in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adversarial Training for Satire Detection: Controlling for Confounding Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mchardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="660" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Anomaly detection in video sequence with appearance-motion correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A Visual Vocabulary for Flower Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1447" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">FastAno: Fast Anomaly Detection via Spatio-Temporal Patch Transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2249" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning Memory-guided Normality for Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14372" to="14381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dance With Self-Attention: A New Look of Conditional Random Fields on Anomaly Detection in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Purwanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="173" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Street Scene: A new dataset and evaluation protocol for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning a distance function with a Siamese network to localize anomalies in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vatsavai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2598" to="2607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Perceptual metric learning for video anomaly detection. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Vatsavai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1432" to="1769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A Survey of Single-Scene Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Vatsavai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Plugand-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1689" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection in Videos using Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised Behavior-Specific Dictionary Learning for Abnormal Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="28" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Self-supervised predictive convolutional attentive block for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Ristea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nasrollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="13576" to="13586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep-Cascade: Cascading 3D Deep Neural Networks for Fast Anomaly Detection and Localization in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on local statistical aggregates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2112" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep Appearance Features for Abnormal Behavior Detection in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIAP</title>
		<meeting>ICIAP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="779" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Real-World Anomaly Detection in Surveillance Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Scene-Aware Context Reasoning for Unsupervised Abnormal Event Detection in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACMMM</title>
		<meeting>ACMMM</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="184" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Integrating prediction and reconstruction for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="123" to="130" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Weakly-Supervised Video Anomaly Detection With Robust Temporal Feature Magnitude Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Verjans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4975" to="4986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Anomaly Detection using a Convolutional Winner-Take-All Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Robust Anomaly Detection in Videos Using Multilevel Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5216" to="5223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Cluster Attention Contrast for Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACMMM</title>
		<meeting>ACMMM</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2463" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">CvT: Introducing Convolutions to Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A Deep One-Class Neural Network for Anomalous Event Detection in Complex Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2609" to="2622" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Chaotic Invariants of Lagrangian Particle Trajectories for Anomaly Detection in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2054" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Learning Deep Representations of Appearance and Motion for Anomalous Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>8.1-8.12</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Detecting Anomalous Events in Videos by Learning Deep Representations of Appearance and Motion. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="117" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Bidirectional retrospective generation adversarial network for anomaly detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="107842" to="107857" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Deep Anomaly Discovery From Unlabeled Videos via Normality Advantage and Self-Paced Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="13987" to="13998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Cloze Test Helps: Effective Video Anomaly Detection via Learning to Complete Video Events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACMMM</title>
		<meeting>ACMMM</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="583" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Abnormal event detection and localization via adversarial event prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Yow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">CLAWS: Clustering Assisted Weakly Supervised Learning with Normalcy Suppression for Anomalous Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Astrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="358" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Generative Cooperative Learning for Unsupervised Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Segu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="14744" to="14754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Video Anomaly Detection and Localization using Motion-field Shape Description and Homogeneity Testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page">107394</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on locality sensitive hashing filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="302" to="311" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Online Detection of Unusual Events in Videos via Dynamic Sparse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3313" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Graph Convolutional Label Noise Cleaner: Train a Plug-And-Play Action Classifier for Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1237" to="1246" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
