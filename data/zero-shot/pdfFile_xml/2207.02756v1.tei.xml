<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STVGFormer: Spatio-Temporal Video Grounding with Static-Dynamic Cross-Modal Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaolei</forename><surname>Tan</surname></persName>
							<email>tanchlei@mail2.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Fang</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Guangdong Province Key Laboratory of Information Security Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Ye</surname></persName>
							<email>tiancaiye@tencent.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Guangzhou</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
							<email>wszheng@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STVGFormer: Spatio-Temporal Video Grounding with Static-Dynamic Cross-Modal Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>The man in white clothes lifts the tray on the table with both hands and hands it to the person opposite.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">METHOD</head><p>Human-centric spatio-temporal video grounding (HCSTVG) task <ref type="bibr" target="#b4">[5]</ref> aims to localize the target person spatially and temporally according to a language query. We propose a framework named STVG-Former to tackle this problem. As illustrated in <ref type="figure">Figure 1</ref>, our framework mainly consists of a static branch and a dynamic branch to model static and dynamic visual-linguistic dependencies for complete cross-modal understanding. The static branch performs cross-modal understanding for static contextual information, i.e., finding the target person that matches the query text in a still frame according to static visual cues, like appearance which is important for achieving accurate spatial grounding. The dynamic branch performs cross-modal understanding for dynamic contextual information, i.e., finding the temporal moment that best matches the query sentence according to dynamic visual cues, like action which is important for achieving accurate temporal grounding. In order to enable information transition between the two branches, we further develop a novel static-dynamic interaction block to exchange complementary information between the static and dynamic branches. In this way, both the static and dynamic branch can absorb useful and complementary information from the other one, which can help greatly reduce the uncertainty in the ambiguous and hard cases. Finally, we employ a bounding box prediction head on top of the static branch and a temporal moment prediction head on top  of the dynamic branch to predict spatial and temporal grounding result, respectively. Overall, our framework is concise but effective.</p><p>In the following, we will introduce each component in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Static Branch</head><p>The static branch is employed to perform cross-modal static context understanding. Following the implementations in MDETR <ref type="bibr" target="#b4">[5]</ref>, we define our static branch as a stack of cross-modal transformer encoder layers and cross-modal transformer decoder layers, as presented in <ref type="figure">Figure 1</ref> and 2 (a). The inputs of our cross-modal transformer encoder are the concatenation of R ? -sized visual features and R ? -sized language features, where , are the resolution of the visual features extracted from the static image frame, is the number of the text tokens in the input text query. The visual and language features are obtained by a pre-trained image encoder and language encoder <ref type="bibr" target="#b5">[6]</ref>, respectively. Here, we also employ an FC embedding layer behind each feature extractor, so that both extracted visual features and language features are projected to have the same channel dimension. The outputs of the last cross-modal transformer encoder layer form a R ( + )?sized cross-modal memory M , which captures rich interactions between intra-frame static visual cues and linguistic descriptions. Then the cross-modal memory is fed into a stack of transformer decoder layers, together with a learnable object query vector ? R , so that the static information depicted in the images and texts can be encoded in the outputs (query vector) by repeatedly querying memory M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Dynamic Branch</head><p>The dynamic branch performs cross-modal understanding for dynamic contextual information. As illustrated in <ref type="figure">Figure 1</ref>, we first extract clip-level visual features ? R ? ? ? from uniformly sampled video clips by a pretrained 3D-CNN <ref type="bibr" target="#b1">[2]</ref> and employ an FC embedding layer to project the channel dimension from to . Then the features after projection are fed into a Space-Time Cross-Modal Transformer (STCMT) which consists of layers to model the visual-linguistic dependencies from a dynamic perspective. The detailed architecture of each layer in STCMT is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> (b). In each layer, we first perform intra-modality selfattention for the dynamic visual features and linguistic features. For visual features, in order to reduce computation cost, we follow TimesFormer <ref type="bibr" target="#b0">[1]</ref> to split the spatio-temporal attention into separate attentions. Denoting the visual features after self-attention as ? R ? ? ? and the language features after self-attention as ? R ? . We then perform cross-attention between and as:</p><formula xml:id="formula_0">(?, ) = (?, ) , = , = , = , = , = , (?, ) = (?, ) + Attention( (?, )</formula><p>, , ),</p><p>= + Attention( , , ),</p><formula xml:id="formula_2">Attention(Q,K,V) = softmax( ?? ) , where , , , , ,</formula><p>are learnable weighted matrices for computing queries, keys and values for the attention mechanism.</p><p>? R ? is obtained by conducting mean pooling on along the spatial dimensions.</p><p>is the dimension of the queries and keys.</p><p>(?, ) ? R ? indicates the visual feature at spatial position (?, ). , are the output visual and linguistic features after crossattention, respectively. The cross-attention operation is designed in the way that rich interactions between the dynamic visual and linguistic features can be explored, which enables the model to learn a powerful cross-modal representation about the depicted dynamic cues. This is essential for grounding temporal moments. After computing the cross-attention between the visual features and linguistic features, we finally employ Feed-Forward Network (FFN) to process both features. In our dynamic branch, rich context is learned from the two modalities, and the visual dynamic features and the linguistic features are fused well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Static-Dynamic Interaction</head><p>The Static-Dynamic Interaction Block (SDIB) enables information transition between the static branch and dynamic branch. As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>, this block is placed after each decoder layer B =1 in static branch and each layer B =1 in dynamic branch. It consists of a dynamic-to-static interaction block and a static-todynamic interaction block as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>The static-to-dynamic interaction block is designed to guide the dynamic branch to attend to the image region that is highly related to the objects depicted in the query text, by utilizing the cross-attention matrices calculated in decoder layers of the static branch. Concretely, we employ static-to-dynamic interaction in each network layer as follows:</p><formula xml:id="formula_3">= LayerNorm( + ? FC( )),<label>(2)</label></formula><p>where ? R ? ? is the output visual feature of the i-th layer in dynamic branch and ? R ? ? is the cross-attention weights (replicate to have channels at the last dimension) calculated between the object queries =1 and corresponding encoded memories { } =1 in B . ? indicates hadamard product. The intuition behind this design is that the attention weights in the decoder layers of the static branch will attend to the object regions matching the language description, and this can serve as a strong guidance to help the dynamic branch focus more on the dynamic variations around the object-related regions. In this way, the model can better capture the motion of the target object.</p><p>The dynamic-to-static interaction block is depicted in <ref type="figure" target="#fig_1">Figure  3</ref>(c). In each block, the object query in the static branch first queries some dynamic information from [ ] (the output visual feature of the i-th layer in Space-Time Cross-Modal Transformer for frame ). Then, the object queries of different frames are mixed up by a temporal self-attention layer. This block enhances the object query representation with cross-frame dynamic information like object motion and human action etc. With the proposed staticdynamic interaction block, both the static and dynamic branches can learn the complementary information from the other branch in an effective way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Prediction Heads</head><p>In this section, we introduce the prediction heads used in our static branch and dynamic branch. Prediction Head for Static Branch. The prediction head of the static branch is designed to predict the location of the target object. The input to the prediction head is the object query representation ? R outputted by the static branch at -th frame. We implement a 3-layer MLP to regress the bounding box location (represented by center coordinates and size)?? R 4 of the target object. In order to make the static branch be aware of whether the input frame is well matched to the input text query, we further employ an FC layer to predict a score?, which indicates whether frame is inside the target temporal moment. Prediction Head for Dynamic Branch. The prediction head in the dynamic branch is designed to predict the time span of the target temporal moment. Specifically, we first perform mean pooling on the features outputted by the dynamic branch at the spatial dimension to obtain a temporal feature ? R ? , then employ an FC layer to adjust the dimension from to . We follow Aug2DTAN <ref type="bibr" target="#b7">[8]</ref> to implement a 2D-proposal-based prediction head and construct the 2D proposal map ? R ? ? as:</p><formula xml:id="formula_4">= , +1 , ..., ? 0 &gt;<label>(3)</label></formula><p>where indicates the feature representation of temporal moment proposal which consists of clips , +1 , ..., . Following the implementation in <ref type="bibr" target="#b7">[8]</ref>, we employ several convolutional layers to the 2D map to obtain a score map ? R ? ?1 where each element represents the matching score of the temporal proposal . For inference, we take the proposal with the highest score as the prediction of target temporal time span.</p><p>Similar to the static branch, we implement an auxiliary head (3layer MLP) to predict whether each clip is inside the target temporal time span. Formally, = ( ), where ? R and indicate the probability of clip to be inside the target temporal moment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Model Training</head><p>We train our model with loss = + , where and are the losses on the output of the static branch and dynamic branches, respectively. Specifically, they are defined as following:</p><formula xml:id="formula_5">= 1 1 (?, ) + 2 (?, ) + 3 ,<label>(4)</label></formula><formula xml:id="formula_6">= 4 (?, ) + 5 ,<label>(5)</label></formula><p>where 1 and are L1 loss and gIoU loss on the predicted bounding boxes, respectively. is a temporal grounding loss which is defined as a binary cross-entropy loss as done in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>, it is computed on the predicted score map?and the ground truth map in which each element is the between the corresponding proposal and ground truth temporal moment.</p><p>, are temporal attentive losses (implemented following <ref type="bibr" target="#b12">[13]</ref>) on the prediction of the auxiliary prediction head of the static branch and dynamic branch, respectively.</p><p>Specifically, we define the formulation of auxiliary losses as following:</p><formula xml:id="formula_7">= ? =1 log? =1 , = ? =1 log =1 .<label>(6)</label></formula><p>where denotes the temporal mask at time , i.e. takes 1 if ? [ , ] otherwise takes 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">EXPERIMENTS 2.1 Experimental Settings</head><p>Evaluation Metrics. We follow previous works <ref type="bibr">[7-9, 11, 15</ref>] to use mean vIoU as the main evaluation metric. vIoU is defined as</p><formula xml:id="formula_8">1 | | ? (?, ),</formula><p>where and indicate the interaction and union between the time intervals obtained from ground truth annotation and system prediction, respectively.?, are the predicted bounding box and ground truth bounding box for the -th frame, respectively. We average the vIoU score over all the samples to obtain mean vIoU. We also report vIoU@R which indicates the proportion of samples with vIoU higher than . Implementation Details. We use ResNet101 <ref type="bibr" target="#b3">[4]</ref> as our image encoder and Roberta-base <ref type="bibr" target="#b5">[6]</ref> as the text encoder to extract image visual features and language features. And we use the Slowfast <ref type="bibr" target="#b1">[2]</ref> model pre-trained on AVA <ref type="bibr" target="#b2">[3]</ref> as the video encoder. We initialize the weights of our static branch using pre-trained MDETR <ref type="bibr" target="#b4">[5]</ref> as done in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>. We train our model with a batch size of 8 and the loss weights are set as 1 = 5, 2 = 2, 3 = 0.5, 4 = 5, 5 = 1. For details on training strategies, we follow the practice in MDETR <ref type="bibr" target="#b4">[5]</ref>. To reduce GPU memory usage, we uniformly sampled 48 and 96 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experimental Results</head><p>We first compare our method with state-of-the-arts on HCSTVG-v2 validation set <ref type="bibr" target="#b8">[9]</ref>. As shown in Table1, we outperform all previous methods by a considerable margin. Then we conduct ablation study by removing the static-to-dynamic interaction blocks (termed "w/o s-to-d interaction"), the dynamic-to-static interaction blocks (termed "w/o d-to-s interaction"), the whole static-dynamic interaction blocks (temred "w/o interaction"). As shown in <ref type="table" target="#tab_1">Table 1</ref>, both the interaction blocks bring some performance gains, which demonstrates the effectiveness of the proposed static-dynamic interaction blocks for Spatio-Temporal Video Grounding.</p><p>HC-STVG Challenge. We submitted the prediction results of the proposed STVGFormer to the HC-STVG track of the 4th Person in Context Workshop. The naive STVGFormer achieved 36.8% vIoU on the test set and the 5-model-ensemble STVGFormer achieved 39.6% vIoU and won the first place in the HC-STVG track (results can be found on http://picdataset.com:8000/challenge/leaderboard/hcvg2022).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The detailed architectures of the proposed framework. (a): The cross-modal transformer in the static branch. (b): The architecture of each space-time cross-modal transformer layer in the dynamic branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The architectures of the proposed Static-Dynamic Interaction Block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison results on HCSTVG-v2<ref type="bibr" target="#b8">[9]</ref> validation set. During testing, we inference a video multiple times with different sampled frames until all frames are sampled at least once.</figDesc><table><row><cell>Method</cell><cell cols="3">m_vIoU vIoU@0.3 vIoU@0.5</cell></row><row><cell>Yu et al.[12]</cell><cell>30.0</cell><cell>-</cell><cell>-</cell></row><row><cell>MMN[10]</cell><cell>30.3</cell><cell>49.0</cell><cell>25.6</cell></row><row><cell>Aug. 2D-TAN[8]</cell><cell>30.4</cell><cell>50.4</cell><cell>18.8</cell></row><row><cell>TubeDETR[11]</cell><cell>36.4</cell><cell>58.8</cell><cell>30.6</cell></row><row><cell>STVGFormer(Ours)</cell><cell>38.7</cell><cell>65.5</cell><cell>33.8</cell></row><row><cell cols="2">w/o s-to-d interaction 37.1</cell><cell>62.3</cell><cell>30.2</cell></row><row><cell cols="2">w/o d-to-s interaction 37.5</cell><cell>63.6</cell><cell>31.4</cell></row><row><cell>w/o interaction</cell><cell>36.4</cell><cell>61.2</cell><cell>29.5</cell></row><row><cell cols="4">frames for the static branch during training and testing, respec-</cell></row><row><cell>tively.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Bing Shuai for the helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Is Space-Time Attention All You Need for Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ava: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6047" to="6056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ishan Misra, and Nicolas Carion. 2021. MDETR-modulated detection for end-to-end multimodal understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="1780" to="1790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">STVGBert: A Visual-Linguistic Transformer Based Framework for Spatio-Temporal Video Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1533" to="1542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Augmented 2d-tan: A two-stage approach for human-centric spatio-temporal video grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaolei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Fang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10634</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human-centric Spatio-Temporal Video Grounding With Visual Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2021.3085907</idno>
		<ptr target="https://doi.org/10.1109/TCSVT.2021.3085907" />
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/2109.04872</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TubeDETR: Spatio-Temporal Video Grounding with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">2021. 2rd Place Solutions in the HC-STVG track of Person in Context Challenge 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07166</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">To find where you talk: Temporal sentence localization in video with attention based location regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9159" to="9166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning 2d temporal adjacent networks for moment localization with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12870" to="12877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huasheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
