<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Person Absolute 3D Human Pose Estimation with Weak Depth Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M?rton</forename><surname>V?ges</surname></persName>
							<email>vegesm@inf.elte.hu</email>
							<affiliation key="aff0">
								<orgName type="institution">Eotvos Lorand University</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>L?rincz</surname></persName>
							<email>lorincz@inf.elte.hu</email>
							<affiliation key="aff0">
								<orgName type="institution">Eotvos Lorand University</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Person Absolute 3D Human Pose Estimation with Weak Depth Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In 3D human pose estimation one of the biggest problems is the lack of large, diverse datasets. This is especially true for multi-person 3D pose estimation, where, to our knowledge, there are only machine generated annotations available for training. To mitigate this issue, we introduce a network that can be trained with additional RGB-D images in a weakly supervised fashion. Due to the existence of cheap sensors, videos with depth maps are widely available, and our method can exploit a large, unannotated dataset. Our algorithm is a monocular, multiperson, absolute pose estimator. We evaluate the algorithm on several benchmarks, showing a consistent improvement in error rates. Also, our model achieves state-of-the-art results on the MuPoTS-3D dataset by a considerable margin. Our code will be publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While the initial focus in 3D pose estimation was on single pose estimators <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, recently multi-pose methods also started to appear <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b3">4]</ref>. Usually, in single-pose detection the problem is simplified to relative pose prediction, that is, joint coordinates are estimated relative to a root joint. However, in a multi-pose setting this could be insufficient for downstream tasks. For instance, to detect proximity, locations are needed as well. The combined prediction of the relative pose and the location of the person is called absolute pose estimation <ref type="bibr" target="#b30">[31]</ref>. Here, the coordinates of the joints are predicted in a coordinate system relative to the camera, instead of the hip.</p><p>The largest obstacle in 3D pose estimation is the lack of large, diverse datasets. Annotations require special equipment, thus large 3D pose datasets are restricted to a studio setting <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12]</ref>. This is especially true for multi-pose estimation, where, to our knowledge, the only non-synthetic multi-pose 3D database for training is Panoptic <ref type="bibr" target="#b12">[13]</ref>, and it only has approximate 3D joint coordinates.</p><p>To overcome this issue, various methods using different kind of auxiliary datasets were proposed: 2D pose datasets <ref type="bibr" target="#b39">[40]</ref>, synthetic poses <ref type="bibr" target="#b33">[34]</ref> or 3D poses generated from multi-view cameras <ref type="bibr" target="#b13">[14]</ref>. In our work, we focus on RGB-D videos. Videos with depth maps have advantages. First, it is not required to manually annotate additional data, unlike 2D pose datasets. Second, RGB-D videos can be recorded with a single camera, while multi-camera setups require special hardware with synchronization. The accessibility of the Kinect resulted in a multitude of RGB-D datasets taken in diverse environments <ref type="bibr" target="#b6">[7]</ref>. Specifically, we use Panoptic <ref type="bibr" target="#b12">[13]</ref> as a weak supervisory signal.</p><p>However, depth maps do not provide an accurate representation of the 3D body joint location. Keypoints are easily occluded by other objects, or by the person him/herself. To solve the problem of occlusion we propose a pose decoder network that reconstructs the depth map at the joint locations. Then, the output of the network can be compared with the ground truth depth and the errors backpropagated.</p><p>To summarize, we propose a method that exploits RGB-D datasets to improve absolute 3D pose estimation. The RGB-D datasets do not require further annotations, only camera calibration matrices are needed. We tested our method on the Panoptic and MuPo-TS datasets, improving previous results and achieving state-of-the-art results in absolute pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>3D Pose Estimation Although various approaches were used for 3D pose estimation, such as dictionary based methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39]</ref> or conditional random fields <ref type="bibr" target="#b1">[2]</ref>, recently state-of-the-art results are dominated by deep learning based algorithms <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30]</ref>. Methods include regressing 3D joints directly <ref type="bibr" target="#b15">[16]</ref>, using volumetric heatmaps <ref type="bibr" target="#b22">[23]</ref> or including soft-argmax layers <ref type="bibr" target="#b29">[30]</ref>.</p><p>A common approach is to split the 3D estimation into two steps: first estimating 2D joint coordinates with an off-the-shelf pose detector, then lifting the 2D coordinates into 3D <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32]</ref>. In <ref type="bibr" target="#b31">[32]</ref>, an equivariant network is built to decrease the overfitting to cameras in the training set. Li and Lee <ref type="bibr" target="#b14">[15]</ref> predict a distribution for each joint instead of a point estimation to produce uncertainty estimations.</p><p>Weak-and semi-supervised approaches The lack of diverse datasets for 3D pose estimation led to interest in weak-and self-supervised approaches. For instance, depthwise ordinal ranking of joints helps training <ref type="bibr" target="#b21">[22]</ref>. Datasets with 2D annotations can be used as well, via a reprojection loss <ref type="bibr" target="#b39">[40]</ref>. Another approach is to add adversarial losses. Drover et al. <ref type="bibr" target="#b4">[5]</ref> backproject rotated poses into 2D and regularize them with a discriminator trained on real 2D poses.</p><p>Additionally, multi-camera setup can effectively decrease the required amount of training data <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref>. Kocabas et al. <ref type="bibr" target="#b13">[14]</ref> uses epipolar geometry to generate 3D ground truth data from unannotated multi-view images.</p><p>Absolute pose estimation Unlike the above approaches, absolute pose estimation predicts not only the root-relative pose, but the location of the person as well. One approach is to find an optimal translation that minimizes the reprojection error <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37]</ref> to 2D. However, this needs the 2D pose to be estimated along the 3D pose. Additionally, the prediction is done in a separate stage preventing end-to-end training. Moon et al. <ref type="bibr" target="#b20">[21]</ref> predicts the location of the root  joint directly, using a separate network. In contrast, in the method of <ref type="bibr" target="#b30">[31]</ref>, the location prediction and relative pose estimation share the same network.</p><p>Using depth in pose estimation While using depth maps as additional data is largely unexplored in 3D pose estimation, it is popular in hand joint prediction <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">36]</ref>. In <ref type="bibr" target="#b2">[3]</ref> the authors directly estimate the depth map from the predicted 3D coordinates. The estimation error is used as an additional regularizer.</p><p>Our work is closest to <ref type="bibr" target="#b2">[3]</ref>. However, we do not predict a full depth map, rather just depth at individual points. This results in a smaller network and faster training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The problem of absolute pose estimation can be formalized as follows: the algorithm must predict the P 3D j ? R 3 , 1 ? j ? J joint coordinates, where J is the number of joints. The P 3D j points are in a camera-centered coordinate system where depth is perpendicular to the camera plane and the x and y axes align with the image edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Our method is a multi-person, monocular estimator that takes single images as input. We use RGB-D images as additional training examples, providing weak supervision. That is, we have two datasets: D = (I 1 , P 3D 1 ), (I 2 , P 3D 2 ), . . . , images with 3D pose annotations and D * = {(I 1 , D 1 ), (I 2 , D 2 ), . . .}, images with pixelwise depth maps. On training images that have pose annotations, the prediction error can be calculated easily. For images from D * , one option would be to simply reconstruct the skeleton from the depth map and compare our prediction with that. However, due to self-occlusions and occlusion by other objects, the reconstructed skeleton would be noisy (see <ref type="figure" target="#fig_2">Figure 2</ref>).</p><p>To solve this problem, we take a reverse approach: instead of comparing our predicted skeleton to a reconstructed one, we transform the predicted skeleton further, calculating the values of the depth map at the joint locations (see <ref type="figure" target="#fig_1">Fig. 1</ref> for the architecture). We introduce a network called JointDepthNet that takes as input the predicted 3D poseP 3D and outputs the value of the depth map at every joint j, denoted by D P j . It is important to note that D P j is not the z coordinate of the 3D pose in the camera coordinate system, but the depth detected by the depth sensor at the 2D position of joint j. To get the 2D locations of D P j we use the output of the 2D PoseNet (see Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Handling occlusions with robust loss</head><p>While JointDepthNet can resolve self-occlusions, it can not detect occlusion caused by the environment as it receives only the pose as input.</p><p>We use the Geman-McClure robust loss function <ref type="bibr" target="#b7">[8]</ref> to overcome the large errors in the loss due to occlusions. The function is defined as:</p><formula xml:id="formula_0">?(x) = x 2 x 2 + ? ,</formula><p>where ? is a parameter. The function is close to x 2 /? around zero and converges to constant 1 for large numbers.</p><p>Since JointDepthNet gets only the pose as input, it can not predict the depth at an occluded joint, resulting in a large error. If the error x is large, then ? is approximately constant 1 around x, so the gradient is close to zero and such errors are effectively eliminated during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Final model</head><p>Our network is based on our previous work <ref type="bibr" target="#b30">[31]</ref>, the architecture is illustrated in <ref type="figure" target="#fig_1">Fig 1.</ref> We briefly review it here for completeness, please refer to <ref type="bibr" target="#b30">[31]</ref> for details.</p><p>The network is a multi-stage architecture: the 2D PoseNet takes an image as input, and detects and predicts all the P 2D (i) 2D poses on the image, while DepthNet predicts a single pixelwise depth map D. Then, for each pose P 2D (i) , the depth at the joint locations are read out from D and concatenated to the calibration matrix normalized P 2D (i) . The normalization is needed by the model to handle different focal lengths and viewing angles. The concatenated vector is passed to the 3D PoseNet that predicts the 3D poses P 3D (i) . If the input image does not have a pose annotation, then P 3D (i) is given to the JointDepthNet that resolves occlusions and outputs the D P j,</p><formula xml:id="formula_1">(i) , 1 ? j ? J depth values (see Section 3.1).</formula><p>JointDepthNet is run only for unannotated images during training. In inference and for images with pose annotations it is ignored.</p><p>For pose annotated images, the loss is the L1 loss, for unannotated images it is the Geman-McClure loss. The total loss is then:</p><formula xml:id="formula_2">L = I?D L 1 P 3D (i) , P 3D (i) + ? I?D * J j=1 ? D P j,(i) ? D P j,(i) ,</formula><p>where ? is a weight,P 3D (i) are the predicted poses, andD P j,(i) are the predicted depths at each joint j.</p><p>One difference compared to <ref type="bibr" target="#b30">[31]</ref> is that instead of Batch Normalization layers <ref type="bibr" target="#b10">[11]</ref> we used Layer Normalization <ref type="bibr" target="#b0">[1]</ref>. We have found that normalizing over the entire batch while mixing two datasets led to suboptimal results, since the characteristics of the two databases were different. When normalizing each training example separately, the performance of the network increased, see <ref type="table" target="#tab_4">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MuPoTS-3D</head><p>We conducted experiments on the MuPoTS-3D database <ref type="bibr" target="#b19">[20]</ref>. It is a multi-person dataset having both indoor and outdoor videos. In the standard protocol, the training set is the MuCo-3DHP dataset <ref type="bibr" target="#b19">[20]</ref> that contains synthetic images, composited from frames in the MPI-INF-3DHP <ref type="bibr" target="#b18">[19]</ref> database. We have created 150k images, each containing 4 persons. The training and test set are quite different so it is a good measurement of in-the-wild robustness.</p><p>In addition to the raw annotations, the MuCo-3DHP and MuPoTS-3D databases contains normalized skeletons. The normalization process rescales the skeleton from the hip such that the knee to neck distance becomes a fixed value. Prior work uses either one or the other skeleton, we evaluate our method on both annotations for completeness. Note that using unnormalized skeletons for absolute pose estimation is more principled. Normalized poses were proposed for the relative pose estimation task where only the orientation and angles of limbs are relevant. In contrast, in absolute pose estimation the location of the joints is important too. Applying a hip-centered scaling on the skeleton leaves the hip in the correct position, while all the other joints are moved to an incorrect location.</p><p>Panoptic We also performed experiments on the Panoptic dataset <ref type="bibr" target="#b12">[13]</ref>. The dataset consists of multiple RGB-D videos, recorded by Kinect sensors, from multiple viewpoints.</p><p>Since there is no standard training/test set defined, we selected one session as test and another as validation. The test/validation split contained the recordings of all the RGB-D cameras for the selected session. <ref type="table">Table 1</ref>. Relative-3DPCK on the MuPoTS-3D dataset (normalized skeletons). Comparison with previous work that uses normalized coordinates (see text for details). Each column corresponds to a video sequence. Higher values are better. Our results are competitive to the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation metrics</head><p>Method S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg.  Different works use different evaluation metrics for absolute pose estimation. For completeness we evaluate our method on all of them. We shortly review these below:</p><p>A-MPJPE or Absolute Mean per Joint Position Error <ref type="bibr" target="#b30">[31]</ref>. It is the Euclidean distance between the ground-truth and predicted joints, averaged over all poses and joints. The metric has the drawback that it does not take into account undetected poses. We present it in mm in our results.</p><p>R-MPJPE or Relative Mean per Joint Position Error <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref>. It is the same as A-MPJPE but with root joints moved to the ground-truth location. The metric is often referred as MPJPE in other work. Presented in mm everywhere. <ref type="table">Table 2</ref>. Absolute-3DPCK on the MuPoTS-3D dataset (normalized skeletons). Comparison with previous work that uses normalized coordinates (see text for details). Each column corresponds to a video sequence. Higher values are better. We achieve state-of-the-art results both for detected poses and all poses.</p><p>Method S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg. Comparing every pose Moon <ref type="bibr" target="#b20">[21]</ref>  A-3DPCK or Absolute 3D Percentage of Correct Keypoints <ref type="bibr" target="#b20">[21]</ref>. It is the percentage of keypoints where the prediction error is less then 15 cm. If a pose is not detected, then the prediction error is defined as infinite thus it does not contribute to the metric. In contrast to A-MPJPE, the metric is sensitive to undetected poses.</p><p>R-3DPCK or Relative 3D Percentage of Correct Keypoints <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, commonly referred as 3DPCK. Same as A-3DPCK but with root joints moved to the ground-truth position. It is the standard metric for relative pose estimation on the MuPo-TS dataset. Similarly to A-3DPCK, it takes into account undetected poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation details</head><p>Our network architecture is based on <ref type="bibr" target="#b30">[31]</ref>. For the 2D PoseNet we selected the state-of-the-art HR-Net pose estimator <ref type="bibr" target="#b28">[29]</ref> with Mask-RCNN <ref type="bibr" target="#b8">[9]</ref> as the human bounding-box detector; for DepthNet, following <ref type="bibr" target="#b30">[31]</ref>, we used MegaDepth <ref type="bibr" target="#b16">[17]</ref>. The 3D PoseNet consists of two residual blocks, each having two fully connected layers of 1024 neurons. The dense layers are followed by a Layer Normalization, Dropout and ReLU activation layers. The JointDepthNet has the same structure, having two residual blocks. The dropout rate was 0.5. We normalized the poses to have standard deviation of 1 and zero mean before training and split it to relative pose and root joint localization. See <ref type="bibr" target="#b30">[31]</ref> for details. JointDepthNet predicts D P i for only 14 joints that we found to be stable, these are the wrists, elbows, shoulders, hips, knees and elbows.</p><p>The 3D PoseNet and JointDepthNet were trained jointly, using Adam with a learning rate of 0.001. Every four epoch, the learning rate was multiplied by a factor of 0.96. Half of a mini-batch contained images with pose annotations and the other half contained images with depth map only. The network was trained for 100 epochs.</p><p>During training, we applied image augmentation by randomly zooming into the images, while camera intrinsics remained unchanged. This augmentation was performed both for images in D and D * , while the target depth maps and poses were appropriately scaled. With this setup, zooming corresponds to moving the poses closer or further away from the camera. We found that this step is essential, otherwise the network overfits to the y locations in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quantitative results</head><p>We evaluated our model quantitatively on the MuPoTS-3D and Panoptic datasets.</p><p>On MuPoTS-3D, we trained our model on the MuCo and on Panoptic datasets jointly, using only depth maps from the latter (Ours in the results). Previous work either used the raw unnormalized, or height-normalized coordinates (see Section 4.1 for details).</p><p>The absolute pose estimation results on normalized coordinates are shown in <ref type="table">Table 2</ref>. The A-3DPCK metric can be calculated on all poses or only on the detected ones. The latter is useful to asses the pose estimation performance, while the former also takes into account the detection performance. On all poses, our model achieved 37.3%, which is 5.8% larger than the previous state of the art. This corresponds to a 18.4% relative increase in the metric. On detected poses only, we improved A-3DPCK by 7.8% (24.5% relatively). Our model remains competitive with the state-of-the-art in the relative pose prediction metrics (Table 1). <ref type="table" target="#tab_2">Table 3</ref> compares our method to prior work using unnormalized coordinates on MuPoTS-3D. The authors of <ref type="bibr" target="#b30">[31]</ref> evaluated their method using the MPJPE metrics. Our method decreases the A-MPJPE and R-MPJPE error by 37 mm and 12 mm (12.6% and 10% relatively). Moreover, our model's detection rate is also higher (93% vs 91%). Finally, we evaluate our algorithm on the Panoptic dataset. The database contains both depth maps and 3D pose annotations, thus we split the training set in two parts, one part uses only pose annotations, the other only depth maps. The results are shown in <ref type="table" target="#tab_3">Table 4</ref>. The weak supervision improves both A-MPJPE and A-3DPCK, while the relative metrics remain unchanged or change slightly.</p><p>The 3D pose estimator has a depth estimator component. Since we have depth images in the Panoptic dataset, it is natural to investigate, whether fine-tuning the DepthNet on images from D * negates the improvement from JointDepthNet. We performed this experiment (Fine-tuned: Yes in <ref type="table" target="#tab_3">Table 4</ref>). Results show that the weak supervision still improves the performance, in fact with larger margin than without fine-tuning. We present example outputs of our model in <ref type="figure" target="#fig_4">Figure 3</ref>. Each row shows random examples from the best, median and worse deciles. A common failure case of our model are non-standing poses (in he middle of the bottom row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation studies</head><p>We investigate the effectiveness of our JointDepthNet in <ref type="table" target="#tab_4">Table 5</ref>. We trained our network without JointDepthNet, only using full supervision. Our weak supervision improves on the absolute metrics. However, on relative metrics they remain unchanged. This finding is consistent on other databases, see <ref type="table" target="#tab_3">Table 4</ref>. We attribute this to two facts. First, depth images hold information only for visible joints, so the root-relative location of an occluded joint can be guessed only with a high uncertainty. On the other hand, even if large part of the body is occluded, the absolute distance from the camera can still be deduced. Second, the relative error of the Kinect is different in the two tasks. The depth sensor of the Kinect has an error of 1-2 cm <ref type="bibr" target="#b34">[35]</ref>. The z coordinate of a relative pose varies mostly between -50 cm and 50 cm in the MuPoTS-3D dataset, while the absolute depth is between 200 cm and 700 cm. That is, the error of the Kinect in proportion to the target value is 4-14 times larger for relative pose estimation than for absolute pose estimation.</p><p>We also show the effect of Layer Normalization vs Batch Normalization. When using Batch Normalization instead of Layer Normalization, the performance drops considerably in all metrics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>We proposed a multi-person absolute pose estimation algorithm that can utilize unannotated RGB-D datasets. The inclusion of depth images improved absolute pose metrics over two datasets. We also achieved new state-of-the-art results on the MuPoTS-3D dataset in absolute pose estimation, beating previous best results by a large margin. However, the weak supervision did not affect the relative pose estimation results. We attribute this to self-occlusion and measurement errors.</p><p>In future work, larger RGB-D datasets can be explored. In our work we used only the Panoptic dataset but creating a large, unified database of RGB-D images with human poses could bring further improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The network architecture. The input image is fed to the 2D PoseNet and DepthNet. The 2D PoseNet detects humans on the picture and also returns the keypoint coordinates in pixel space. DepthNet estimates the depth for each pixel. Then the 2D pose and predicted depth are combined by reading out the predicted depth at the 2D joint locations. The 3D PoseNet predicts 3D poses from the concatenated 2D pose and depth features. The 3D estimation is performed for all poses separately. If the image does not have 3D pose annotation, the JointDepthNet estimates the depth at each joint on the ground-truth depth map (note that this is different from the depth of the joint because of occlusions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Reconstructing a skeleton from the depth map under heavy selfocclusion. a) Original image. b) Ground-truth 3D pose, from a different angle. c) Pose created by using ground-truth 2D coordinates and the depth from the depth images. Note how the (orange) right leg and arm are incorrect since the person is sideways on the image. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Comparing every pose Rogez [26] 67.7 49.8 53.4 59.1 67.5 22.8 43.7 49.9 31.1 78.1 50.2 51.0 51.6 49.3 56.2 66.5 65.2 62.9 66.1 59.1 53.8 Mehta [20] 81.0 60.9 64.4 63.0 69.1 30.3 65.0 59.6 64.1 83.9 68.0 68.6 62.3 59.2 70.1 80.0 79.6 67.3 66.6 67.2 66.0 Rogez [27] 87.3 61.9 67.9 74.6 78.8 48.9 58.3 59.7 78.1 89.5 69.2 73.8 66.2 56.0 74.1 82.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Qualitative results on MuPoTS-3D. Top row contains random images from the top 10 percentile, medium row contains those from the middle 10 percentile and bottom row from the worst 10 percentile. Green skeletons are the ground truth poses, blue ones are our predictions. Note that not every person in an image has ground truth annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1 78.1 72.6 73.1 61.0 70.6 Moon [21] 94.4 77.5 79.0 81.9 85.3 72.8 81.9 75.7 90.2 90.4 79.2 79.9 75.1 72.7 81.1 89.9 89.6 81.8 81.7 76.2 81.8 Ours 89.5 75.9 85.2 83.9 85.0 73.4 83.6 58.7 65.1 90.4 76.8 81.9 67.0 55.9 80.8 90.6 90.0 81.1 81.1 68.6 78.2 Comparing detected poses only Rogez [26] 69.1 67.3 54.6 61.7 74.5 25.2 48.4 63.3 69.0 78.1 53.8 52.2 60.5 60.9 59.1 70.5 76.0 70.0 77.1 81.4 62.4 Mehta [20] 81.0 65.3 64.6 63.9 75.0 30.3 65.1 61.1 64.1 83.9 72.4 69.9 71.0 72.9 71.3 83.6 79.6 73.5 78.9 90.9 70.8 Rogez [27] 88.0 73.3 67.9 74.6 81.8 50.1 60.6 60.8 78.2 89.5 70.8 74.4 72.8 64.5 74.2 84.9 85.2 78.4 75.8 74.4 74.0 Moon [21] 94.4 78.6 79.0 82.1 86.6 72.8 81.9 75.8 90.2 90.4 79.4 79.9 75.3 81.0 81.0 90.7 89.6 83.1 81.7 77.3 82.5 Ours 89.5 81.6 85.9 84.4 90.5 73.5 85.5 68.9 65.1 90.4 79.1 82.6 72.7 68.1 81.0 94.0 90.4 87.4 90.4 92.6 82.7</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>59.5 44.7 51.4 46.0 52.2 27.4 23.7 26.4 39.1 23.6 18.3 14.9 38.2 26.5 36.8 23.4 14.4 19.7 18.8 25.1 31.5 Ours 50.4 33.4 52.8 27.5 53.7 31.4 22.6 33.5 38.3 56.5 24.4 35.5 45.5 34.9 49.3 23.2 32.0 30.7 26.3 43.8 37.3 Comparing detected poses only Moon [21] 59.5 45.3 51.4 46.2 53.0 27.4 23.7 26.4 39.1 23.6 18.3 14.9 38.2 29.5 36.8 23.6 14.4 20.0 18.8 25.4 31.8 Ours 50.4 35.9 53.3 27.7 57.2 31.4 23.1 39.3 38.3 56.5 25.2 35.8 49.3 42.5 49.4 24.1 32.1 33.1 29.3 59.2 39.6</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on the MuPoTS-3D dataset (unnormalized skeletons). Comparison with previous work that uses unnormalized coordinates (see text for details). Metrics are calculated on detected poses only. MPJPE errors are in mm.</figDesc><table><row><cell></cell><cell cols="5">A-MPJPE ? R-MPJPE ? A-3DPCK ? R-3DPCK ? Det. Rate ?</cell></row><row><cell>LCR-Net [26]</cell><cell>-</cell><cell>146</cell><cell>-</cell><cell>-</cell><cell>86%</cell></row><row><cell>Mehta et al. [20]</cell><cell>-</cell><cell>132</cell><cell>-</cell><cell>-</cell><cell>93%</cell></row><row><cell>Veges et al. [31]</cell><cell>292</cell><cell>120</cell><cell>30.1</cell><cell>-</cell><cell>91%</cell></row><row><cell>Ours</cell><cell>255</cell><cell>108</cell><cell>35.9</cell><cell>78.7</cell><cell>93%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results on the Panoptic dataset. Ours w/o JDN is our network without JointDepthNet, using only frames with pose annotations. Evaluating on detected poses only. MPJPE errors are in mm. Ties are not marked in bold. To our knowledge, no other work predicts absolute poses on Panoptic.</figDesc><table><row><cell></cell><cell cols="5">Fine-tuned A-MPJPE ? A-3DPCK ? R-MPJPE ? R-3DPCK ?</cell></row><row><cell>Ours w/o JDN</cell><cell>No</cell><cell>151</cell><cell>60.6</cell><cell>67.4</cell><cell>95.2</cell></row><row><cell>Ours</cell><cell>No</cell><cell>147</cell><cell>62.5</cell><cell>67.6</cell><cell>95.2</cell></row><row><cell>Ours w/o JDN</cell><cell>Yes</cell><cell>144</cell><cell>64.1</cell><cell>62.2</cell><cell>96.6</cell></row><row><cell>Ours</cell><cell>Yes</cell><cell>134</cell><cell>68.8</cell><cell>62.3</cell><cell>96.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation studies. We turned off features of our network separately. All results are on MuPoTS-3D, using unnormalized coordinates. 3DPCK is calculated on detected poses.A-MPJPE ? A-3DPCK ? R-MPJPE ? R-3DPCK ?</figDesc><table><row><cell>BatchNorm vs LayerNorm</cell><cell>288</cell><cell>28.1</cell><cell>114</cell><cell>75.2</cell></row><row><cell>w/o JDN</cell><cell>264</cell><cell>33.0</cell><cell>108</cell><cell>78.7</cell></row><row><cell>Full model</cell><cell>255</cell><cell>35.9</cell><cell>108</cell><cell>78.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>MV received support from the European Union and co-financed by the European Social Fund (EFOP-3.6.3-16-2017-00002). AL was supported by the National Research, Development and Innovation Fund of Hungary via the Thematic Excellence Programme funding scheme under Project no. ED 18-1-2019-0030 titled Application-specific highly reliable IT solutions</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Weakly-supervised 3d hand pose estimation from monocular rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="678" to="694" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multi-person 3d human pose estimation from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename></persName>
		</author>
		<title level="m">Can 3d pose be learned from 2d projections alone? ECCV Workshops p</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">7894</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rgbd datasets: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical methods for tomographic image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Mcclure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the International Statistical Institut</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5" to="21" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>Mask R-CNN. ICCV</editor>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<title level="m">Exploiting temporal information for 3d pose estimation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generating multiple hypotheses for 3d human pose estimation with mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<editor>Cremers, D., Reid, I., Saito, H., Yang, M.H.</editor>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="332" to="347" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Megadepth: Learning single-view depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2659" to="2668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation in the wild using improved CNN supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3d pose estimation from monocular RGB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">LCR-Net: Localization-classificationregression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1216" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Lcr-net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-09" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Absolute human pose estimation with depth prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>V?ges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?rincz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCNN. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d human pose estimation with siamese equivariant embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>V?ges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?rincz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">339</biblScope>
			<biblScope unit="page" from="194" to="201" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-supervised 3d hand pose estimation through training by fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10853" to="10862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generalizing monocular 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Comparison of Kinect V1 and V2 depth images in terms of accuracy and precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wasenm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV Workshops</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="34" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Rgb-based 3d hand pose estimation via privileged learning with depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3d sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8410" to="8419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3d shape estimation from 2d landmarks: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4447" to="4455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
