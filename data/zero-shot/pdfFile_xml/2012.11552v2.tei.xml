<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OBoW: Online Bag-of-Visual-Words Generation for Self-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Puy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Crete</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne Universit?</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeo</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">OBoW: Online Bag-of-Visual-Words Generation for Self-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning image representations without human supervision is an important and active research field. Several recent approaches have successfully leveraged the idea of making such a representation invariant under different types of perturbations, especially via contrastive-based instance discrimination training. Although effective visual representations should indeed exhibit such invariances, there are other important characteristics, such as encoding contextual reasoning skills, for which alternative reconstruction-based approaches might be better suited.</p><p>With this in mind, we propose a teacher-student scheme to learn representations by training a convolutional net to reconstruct a bag-of-visual-words (BoW) representation of an image, given as input a perturbed version of that same image. Our strategy performs an online training of both the teacher network (whose role is to generate the BoW targets) and the student network (whose role is to learn representations), along with an online update of the visual-words vocabulary (used for the BoW targets). This idea effectively enables fully online BoW-guided unsupervised learning. Extensive experiments demonstrate the interest of our BoWbased strategy, which surpasses previous state-of-the-art methods (including contrastive-based ones) in several applications. For instance, in downstream tasks such Pascal object detection, Pascal classification and Places205 classification, our method improves over all prior unsupervised approaches, thus establishing new state-of-the-art results that are also significantly better even than those of supervised pre-training. We provide the implementation code at https://github.com/valeoai/obow.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning unsupervised image representations based on convolutional neural nets (convnets) has attracted a significant amount of attention recently. Many different types of convnet-based methods have been proposed in this regard, including methods that rely on using annotation-free pretext tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b84">85]</ref>, generative methods that model the image data distribution <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>, as well as clustering-based approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Several recent methods opt to learn representations via instance-discrimination training <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b79">80]</ref>, typically implemented in a contrastive learning framework <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref>. The primary focus here is to learn low-dimensional image / instance embeddings that are invariant to intra-image variations while being discriminative among different images. Although these methods manage to achieve impressive results, they focus less on other important aspects in representation learning, such as contextual reasoning, for which alternative reconstruction-based approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b85">86]</ref> might be better suited. For instance, the task of predicting from an image region the contents of the entire image requires to recognize the visual concepts depicted in the provided region and then to infer from them the structure of the entire scene. So, training for such a task has the potential of squeezing out more information from the training images and of learning richer and more powerful representations.</p><p>However, reconstructing image pixels is an ambiguous and hard-to-optimize task that forces the convnet to spend a lot of capacity on modeling low-level pixel details. It is all the more unnecessary when the final goal deals with highlevel image understanding, such as image classification. To leverage the advantages of the reconstruction principle without focusing on unimportant pixel details, one can focus on reconstruction over high-level visual concepts, referred to as visual words. For instance, BoWNet <ref type="bibr" target="#b25">[26]</ref> derived a teacherstudent learning scheme following this principle. In this teacher-student setting, given an image, the teacher extracts feature maps that are then quantized in a spatially-dense way over a vocabulary of visual words. Then, the resulting visual words-based image description is exploited for training the student on the self-supervised task of reconstructing the distribution of the visual words of an image, i.e., its bag-ofwords (BoW) representation <ref type="bibr" target="#b80">[81]</ref>, given as input a perturbed version of that same image. By solving this reconstruction task the student is forced to learn perturbation-invariant and context-aware representations while "ignoring" pixel details.</p><p>Despite its advantages, the BoWNet approach exhibits some important limitations that do not allow it to fully exploit the potential of the BoW-based reconstruction task. One of them is that it relies on the availability of an already pre-trained teacher network. More importantly, it assumes that this teacher remains static throughout training. However, due to the fact that during the training process the quality of the student's representations will surpass the teacher's ones, a static teacher is prone to offer a suboptimal supervisory signal to the student and to lead to an inefficient usage of the computational budget for training.</p><p>In this paper, we propose a BoW-based self-supervised approach that overcomes the aforementioned limitations. To that end, our main technical contributions are three-fold:</p><p>1. We design a novel fully online teacher-student learning scheme for BoW-based self-supervised training <ref type="figure" target="#fig_0">(Fig. 1</ref>). This is achieved by online training of the teacher and the student networks.</p><p>2. We significantly revisit key elements of the BoWguided reconstruction task. This includes the proposal of a dynamic BoW prediction module used for reconstructing the BoW representation of the student image. This module is carefully combined with adequate online updates of the visual-words vocabulary used for the BoW targets.</p><p>3. We enforce the learning of powerful contextual reasoning skills in our image representations. Revisiting data augmentation with aggressive cropping and spatial image perturbations, and exploiting multi-scale BoW reconstruction targets, we equip our student network with a powerful feature representation.</p><p>Overall, the proposed method leads to a simpler and much more efficient training methodology for the BoW-guided reconstruction task that manages to learn significantly better image representations and therefore achieves or even surpasses the state of the art in several unsupervised learning benchmarks. We call our method OBoW after the online BoW generation mechanism that it uses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Bags of visual words. Bag-of-visual-words representations are powerful image models able to encode image statistics from hundreds of local features. Thanks to that, they were used extensively in the past <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b70">71]</ref> and continue to be a key ingredient of several recent deep learning approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40]</ref>. Among them, BoWNet <ref type="bibr" target="#b25">[26]</ref> was the first work to use BoWs as reconstruction targets for unsupervised representation learning. Inspired by it, we propose a novel BoW-based self-supervised method with a more simple and effective training methodology that includes generating BoW targets in a fully online manner and further enforcing the learning of context-aware representations.</p><p>Self-supervised learning. A prominent paradigm for unsupervised representation learning is to train a convnet on an artificially designed annotation-free pretext task, e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b87">88]</ref>. Many works rely on pretext reconstruction tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b87">88]</ref>, where the reconstruction target is defined at image pixel level. This is in stark contrast with our method, which uses a reconstruction task defined over high-level visual concepts (i.e., visual words) that are learnt with a teacher-student scheme in a fully online manner.</p><p>Instance discrimination and contrastive objectives. Recently, unsupervised methods based on contrastive learning objectives <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b79">80]</ref> have shown great results. Among them, contrastive-based instance discrimination training <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b79">80]</ref> is the most prominent example. In this case, a convnet is trained to learn image representations that are invariant to several perturbations and at the same time discriminative among different images (instances). Our method also learns intra-image invariant representations, since the convnet must predict the same BoW target (computed from the original image) regardless of the applied perturbation. More than that however, our work also places emphasis on learning context-aware representations, which, we believe, is another important characteristic that effective representations should have. In that respect, it is closer to contrastive-based approaches that target to learn context-aware representations by predicting (in a contrastive way) the state of missing image patches <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b71">72]</ref>.</p><p>Teacher-student approaches. This paradigm has a long research history <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b64">65]</ref> and it is frequently used for distilling a single large network or an ensemble, the teacher, into a smaller network, the student <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56]</ref>. This setting has been revisited in the context of semi-supervised learning where the teacher is no longer fixed but evolves during training <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b72">73]</ref>. In self-supervised learning, BowNet <ref type="bibr" target="#b25">[26]</ref> trains a student to match the BoW representations produced by a self-supervised pre-trained teacher. MoCo <ref type="bibr" target="#b34">[35]</ref> relies on a slow-moving momentum-updated teacher to generate up-to-date representations to fill a memory bank of negative images. BYOL <ref type="bibr" target="#b32">[33]</ref>, which is a method concurrent to our work, also uses a momentum-updated teacher and trains the student to predict features generated by the teacher. However, BYOL, similar to contrastive-based instance discrimination methods, uses low-dimensional global image embeddings as targets (produced from the final convnet output) and primarily focuses on making them intra-image invariant. On the contrary, our training targets are produced by converting the intermediate teacher feature maps to high-dimensional BoW vectors that capture multiple local visual concepts, thus constituting a richer target representation. Moreover, they are built over an online-updated vocabulary from randomly sampled local features, expressing the current image as statistics over this vocabulary (see ? 3.1). Therefore, our BoW targets expose fewer learning "shortcuts" (a critical aspect in self-supervised learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b77">78]</ref>), thus preventing to a larger extent teacher-student collapse and overfitting. Relation to SwAV <ref type="bibr" target="#b7">[8]</ref>. OBoW presents some similarity (e.g., using online vocabularies) with SwAV <ref type="bibr" target="#b7">[8]</ref>. However, the prediction tasks fundamentally differ: OBoW exploits a BoW prediction task while SwAV uses an image-cluster prediction task <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. BoW targets are much richer representations than image-cluster assignments: a BoW encodes all the local-feature statistics of an image whereas an imagecluster assignment encodes only one global image feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our approach</head><p>Here we explain our proposed approach for learning image representations by reconstructing bags of visual words. We start with an overview of our method. Overview. The bag-of-words reconstruction task involves a student convnet S(?) that learns image representations, and a teacher convnet T(?) that generates BoW targets used for training the student network. The student S(?) is parameterized by ? S and the teacher T(?) by ? T .</p><p>To generate a BoW representation y T (x) out of an image x, the teacher first extracts the feature map T (x) ? R c ?h ?w , of spatial size h ? w with c channels, from its th layer (in our experiments is either the last L or penultimate L ? 1 convolutional layer of T(?)). It quantizes the c -dimensional feature vectors T (x)[u] at each location u ? {1, ? ? ? , h ? w } of the feature map over a vocabulary V = [v 1 , . . . , v K ] of K visual words of dimension c . This quantization process produces for each location u a K-dimensional code vector q(x)[u] that encodes the assignment of T(x)[u] to its closest (in terms of squared Euclidean distance) visual word(s). Then, the teacher reduces the quantized feature maps q(x) to a Kdimensional BoW,? T (x), by channel-wise max-pooling, i.e., is the assignment value of the code q(x)[u] for the k th word. Finally,? T (x) is converted into a probability distribution over the visual words by L 1 -normalization, i.e.,</p><formula xml:id="formula_0">y T (x)[k] =? T (x)[k] k ?T(x)[k ]</formula><p>. To learn image representations, the student gets as input a perturbed version of the image x, denoted asx, and is trained to reconstruct the BoW representation y T (x), produced by the teacher, of the original unperturbed image x. To that end, it first extracts a global vector representation S(x) ? R c (with c channels) from the entire imagex and then applies a linear-plus-softmax layer to S(x), as follows:</p><formula xml:id="formula_1">y S (x)[k] = exp(w k S(x)) k exp(w k S(x)) ,<label>(1)</label></formula><p>where W = [w 1 , ? ? ? , w K ] are the c-dimensional weight vectors (one per word) of the linear layer. The Kdimensional vector y S (x) is the predicted softmax probability of the target y T (x). Hence, the training loss that is minimized for a single image x is the cross-entropy loss</p><formula xml:id="formula_2">CE y S (x), y T (x) = ? K k=1 y T (x)[k] log y S (x)[k] (2)</formula><p>between the softmax distribution y S (x) predicted by the student from the perturbed imagex, and the BoW distribution y T (x) of the unperturbed image x given by the teacher. Our technical contributions. In the following, we explain (i) in ? 3.1, how to construct a fully online training methodology for the teacher, the student and the visual-words vocabulary, (ii) in ? 3.2, how to implement a dynamic approach for the BoW prediction that can adapt to continuously-changing vocabularies of visual words, and finally (iii) in ? 3.3, how to significantly enhance the learning of contextual reasoning skills by utilizing multi-scale BoW reconstruction targets and by revisiting the image augmentation schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fully online BoW-based learning</head><p>To make the BoW targets encode more high-level features, BoWNet pre-trains the teacher convnet T(?) with another unsupervised method, such as RotNet <ref type="bibr" target="#b27">[28]</ref>, and computes the vocabulary V for quantizing the teacher feature maps offline by applying k-means on a set of teacher feature maps extracted from training images. After the end of the student training, during which the teacher's parameters remain frozen, the student becomes the new teacher T(?) ? S(?), a new vocabulary V is learned off-line from the new teacher, and a new student is trained, starting a new training cycle. In this case however, (a) the final success depends on the quality of the first pre-trained teacher, (b) the teacher and the BoW reconstruction targets y T (x) remain frozen for long periods of time, which, as already explained, results in a suboptimal training signal, and (c) multiple training cycles are required, making the overall training time consuming.</p><p>To address these important limitations, in this work we propose a fully online training methodology that allows the teacher to be continuously updated as the student training progresses, with no need for off-line k-means stages. This requires an online updating scheme for the teacher as well as for the vocabulary of visual words used for generating the BoW targets, both of which are detailed below. Updating the teacher network. Inspired by MoCo <ref type="bibr" target="#b34">[35]</ref>, the parameters ? T of the teacher convnet are an exponential moving average of the student parameters. Specifically, at each training iteration the parameters ? T are updated as</p><formula xml:id="formula_3">? T ? ? ? ? T + (1 ? ?) ? ? S ,<label>(3)</label></formula><p>where ? ? [0, 1] is a momentum coefficient. Note that, as a consequence, the teacher has to share exactly the same architecture as the student. With a proper tuning of ?, e.g., ? = 0.99, this update rule allows slow and continuous updates of the teacher, avoiding rapid changes of its parameters, such as with ? = 0, which would make the training unstable. As in MoCo, for its batch-norm units, the teacher maintains different batch-norm statistics from the student.</p><p>Updating the visual-words vocabulary. Since the teacher is continuously updated, off-line learning of V is not a viable option. Instead, we explore two solutions for computing V, online k-means and a queue-based vocabulary.</p><p>Online k-means. One possible choice for updating the vocabulary is to apply online k-means clustering after each training step. Specifically, as proposed in VQ-VAE <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b61">62]</ref>, we use exponential moving average for vocabulary updates. A critical issue that arises in this case is that, as training progresses, the features distribution changes over time. The visual words computed by online k-means do not adapt to this distribution shift leading to extremely unbalanced cluster assignments and even to assignments that collapse to a single cluster. In order to counter this effect, we investigate different strategies: (a) detection of rarely used visual words over several mini-batches and replacement of these words with a randomly sampled feature vector from the current mini-batch; (b) enforcing uniform assignments to each cluster thanks to the Sinkhorn optimization as in, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. For more details see ?D.</p><p>A queue-based vocabulary. In this case, the vocabulary V of visual words is a K-sized queue of random features. At each step, after computing the assignment codes over the current vocabulary V, we update V by selecting one feature vector per image from the current mini-batch, inserting it to the queue, and removing the oldest item in the queue if its size exceeds K. Hence, the visual words in V are always feature vectors from past mini-batches. We explore three different ways to select these local feature vectors: (a) uniform random sampling of one feature vector in T (x); (b) global average pooling of T (x) (average feature vector of each image); (c) an intermediate approach between (a) and (b) which consists of a local average pooling with a 3 ? 3 kernel (stride 1, padding 0) of the feature map T (x) followed by a uniform random sampling of one of the resulting feature vectors ( <ref type="figure" target="#fig_2">Fig. 2</ref>). Our intuition for option (c) is that, assuming that the local features in a 3 ? 3 neighborhood belong to one common visual concept, then local averaging selects a more representative visual-word feature from this neighborhood than simply sampling at random one local feature (option (a)). Likewise, the global averaging option (b) produces a representative feature from an entire image, which however, might result in overly coarse visual word features.</p><p>The advantage of the queue-based solution over online k-means is that it is simpler to implement and it does not require any extra mechanism for avoiding unbalanced clusters, since at each step the queue is updated with new randomly sampled features. Indeed, in our experiments, the queuebased vocabulary with option (c) provided the best results.</p><p>Generating BoW targets with soft-assignment codes.</p><p>For generating the BoW targets, we use soft-assignments instead of the hard-assignments used in BoWNet. This is preferable from an optimization perspective due to the fact that the vocabulary of visual words is continuously evolving. We thus compute the assignment codes q(x) <ref type="bibr">[u]</ref> as</p><formula xml:id="formula_4">q(x)[u][k] = exp(? 1 ? T (x)[u] ? v k 2 2 ) k exp(? 1 ? T (x)[u] ? v k 2 2 )</formula><p>.</p><p>The parameter ? is a temperature value that controls the softness of the assignment. We use ? = ? base ?? MSD , where ? base &gt; 0 and? MSD is the exponential moving average (with momentum 0.99) of the mean squared distance of the feature vectors in T (x) from their closest visual words. The reason for using an adaptive temperature instead of a constant one is due to the change of magnitude of the feature activations during training, which induces a change of scale of the distances between the feature vectors and the words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dynamic bag-of-visual-word prediction</head><p>To learn effective image representations, the student must predict the BoW distribution over V of an image using as input a perturbed version of that same image. However, in our method the vocabulary is constantly updated and the visual words are changing or being replaced from one step to the next. Therefore, predicting the BoW distribution over a continuously updating vocabulary V with a fixed linear layer would make training unstable, if not impossible. To address this issue we propose to use a dynamic BoW-prediction head that can adapt to the evolving nature of the vocabulary. To that end, instead of using fixed weights as in (1), we employ a generation network G(?) that takes as input the current vocabulary of visual words V = [v 1 , . . . , v K ] and produces prediction weights for them as</p><formula xml:id="formula_6">G(V ) = [G(v 1 ), . . . , G(v K )], where G(?) : R c ? R c is parameterized by ? G and G(v k )</formula><p>represents the prediction weight vector for the k th visual word. Therefore, Equation 1 becomes</p><formula xml:id="formula_7">y S (x)[k] = exp(? ? G(v k ) S(x)) k exp(? ? G(v k ) S(x)) ,<label>(5)</label></formula><p>where ? is a fixed coefficient that equally scales the magnitudes of all the predicted weights</p><formula xml:id="formula_8">G(V ) = [G(v 1 ), . . . , G(v K )]</formula><p>, which by design are L 2 -normalized. We implement G(?) with a 2-layer perceptron whose input and output vectors are L 2 -normalized (see <ref type="figure" target="#fig_3">Fig. 3</ref>). Its hidden layer has size 2 ? c.</p><p>We highlight that dynamic weight-generation modules are extensively used in the context of few-shot learning for producing classification weight vectors of novel classes using as input a limited set of training examples <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b60">61]</ref>. The advantages of using G(?) instead of fixed weights, which BoWNet uses, are the equivariance to permutations of the visual words, the increased stability to the frequent and abrupt updates of the visual words, a number of parameters |? G | independent from the number of visual words K, hence requiring fewer parameters than a fixed-weights linear layer for large vocabularies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Representation learning based on enhanced contextual reasoning</head><p>Data augmentation. The key factor for the success of many recent self-supervised representation learning methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b69">70]</ref> is to leverage several image augmentation/perturbation techniques, such as Gaussian blur <ref type="bibr" target="#b8">[9]</ref>, color jittering and random cropping techniques, as cutmix <ref type="bibr" target="#b81">[82]</ref> that substitutes one random-size patch of an image with that of another. In our method, we want to fully exploit the possibility of building strong image representations by hiding local information. As the teacher is randomly initialised, it is important to hide large regions of the original image from the student so as to prevent the student from relying only on low-level image statistics for reconstructing the distributions y T (x) over the teacher visual words, which capture lowlevel visual cues at the beginning of the training. Therefore, we carefully design our image perturbations scheme to make sure that the student has access to only a very small portion of the original image. Specifically, similar to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b50">51]</ref>, we extract from a training image multiple crops with two different mechanisms: one that outputs 160 ? 160-sized crops that cover less than 60% of the original image and one with 96 ? 96-sized crops that cover less than 14% of the original image (see <ref type="figure" target="#fig_4">Fig. 4</ref>). Given those image crops, the student must reconstruct the full bags of visual words from each of them independently. Therefore, our cropping strategy definitively forces the student network to understand and learn spatial dependency between visual parts.</p><p>Multi-scale BoW reconstruction targets. We also consider reconstructing BoW from multiple network layers that correspond to different scale levels. In particular, we experiment with using both the last scale level L (i.e., layer conv5 in ResNet) and the penultimate scale level L ? 1 (i.e., layer conv4 in ResNet). The reasoning behind this is that the features of level L?1 still encode semantically Given a training image (left), we extract two types of image crops. The first type (middle) is obtained by randomly sampling an image region whose area covers no more than 60% of the entire image, resizing it to 160 ? 160 and then giving it as input to the student as part of the reconstruction task. The second type (right) is obtained by randomly selecting an area that covers between 60% to 100% of the entire image, resizing it to a 256 ? 256 image, dividing it into 3 ? 3 overlapping patches of size 96 ? 96, and randomly choosing 5 out of these 9 patches (indicated with red rectangles) that are given as 5 separate inputs to the student. The student must then reconstruct the original BoW target independently for each patch. The blue rectangle on the left image indicates the central 224 ? 224 crop from which the teacher produces the BoW target. Note that, except from horizontal flipping, no other perturbation is applied on the teacher's inputs.</p><p>important concepts but have a smaller receptive field than those in the last level. As a result, the visual words of level L?1 that belong to image regions hidden to the student are less likely to be influenced by pixels of image regions the student is given as input. Therefore, by using BoW from this extra feature level, the student is further enforced to learn contextual reasoning skills (and in fact, at a level with higher spatial details due to the higher resolution of level L?1), thus learning richer and more powerful representations. When using BoW extracted from two layers, our method includes a separate vocabulary for each layer, denoted by V L and V L?1 for layers L and L?1 respectively, and two different weight generators, denoted by G L (?) and G L?1 (?) for layers L and L?1, respectively. Regardless of what layer the BoW target comes from, the student uses a single global image representation S(x), typically coming from the global average pooling layer after the last convolutional layer (i.e., layer pool5 in ResNet), to perform the reconstruction task. We show empirically in Section 4.1 that the contextual reasoning skills implicitly developed via using the above two schemes are decisive to learn effective image representations with the BoW-reconstruction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and results</head><p>We evaluate our method (OBoW) on the ImageNet <ref type="bibr" target="#b63">[64]</ref>, Places205 <ref type="bibr" target="#b86">[87]</ref> and VOC07 <ref type="bibr" target="#b21">[22]</ref> classification datasets as well as on the V0C07+12 detection dataset. Implementation details. For our models, the vocabulary size is set to K = 8192 words and, as in BoWNet, when computing the BoW targets we ignore the visual words that correspond to the feature vectors on the edge of the teacher </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Analysis</head><p>Here we perform a detailed analysis of our method. Due to the computationally intensive nature of pre-training on ImageNet, we use a smaller but still representative version created by keeping only 20% of its images and we implement our model with the light-weight ResNet18 architecture. For training we use SGD for 80 epochs with cosine learning rate initialized at 0.05, batch size 128 and weight decay 5e?4. We evaluate models trained with two versions of our method, the vanilla version that uses single-scale BoWs and from each training image extracts one 160 ? 160-sized crop (with which it trains the student), and the full version that uses multi-scale BoWs and extracts from each training image two 160 ? 160-sized crops plus five 96 ? 96-sized patches.</p><p>Evaluation protocols. After pre-training, we freeze the learned representations and use two evaluation protocols. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Online vocabulary updates. In Tab. 1, we compare the approaches for online vocabulary updates described ? 3.1. The   <ref type="table">Table 3</ref>: Ablation of dynamic BoW prediction and softquantization. For these results, we used the vanilla version of our method. "Soft": soft assignment instead of hard assignment. "Dyn": dynamic weight generation instead of fixed weights.</p><p>queue-based solutions achieve in general better results than online k-means. Among the queue-based options, random sampling of locally averaged features, opt. (c), provides the best results. Its advantage over option (b) with global averaging is more evident with multi-scale BoWs where an extra feature level with a higher resolution and more localized features is used, in which case global averaging produces visual words that are too coarse. In all remaining experiments, we use a queue-based vocabulary with option (c).</p><p>Momentum for teacher updates. In <ref type="table" target="#tab_2">Table 2</ref>, we study the sensitivity of our method w.r.t. the momentum ? for the teacher updates <ref type="figure" target="#fig_3">(Equation 3</ref>). We notice a strong drop in performance when decreasing ? from 0.9 to 0.5 (a rapidlychanging teacher), and to 0 (the teacher and student have identical parameters), while keeping the initial learning rate fixed (lr = 0.05). However, we noticed that this was not due to any cluster/mode collapse issue. The issue is that the teacher signal is more noisy at low ? because of the rapid change of its parameters. This prevents the student to converge when keeping the learning rate as high as 0.05. We notice in <ref type="table" target="#tab_2">Table 2</ref> that a reduction of the learning rate to adapt to the reduction of ? reduces the performance gap. This indicates that our method is not as sensitive to the choice of the momentum as MoCo and BYOL were shown to be.   Dynamic BoW prediction and soft quantization. In <ref type="table">Table 3</ref>, we study the impact of the dynamic BoW prediction and of using soft assignment for the codes instead of hard assignment. We see that (1), as expected, the network is unable to learn useful features without the proposed dynamic BoW prediction, i.e., when using fixed weights; (2) soft assignment indeed provides a performance boost.</p><p>Enforcing context-aware representations. In <ref type="table" target="#tab_4">Table 4</ref> we study different types of image crops for the BoW reconstruction tasks, as well as the impact of multi-scale BoW targets. We observe that: (1) as we discussed in ? 3.3, smaller crops that hide significant portions of the original image are better suited for our reconstruction task thus leading to dramatic increase in performance (compare entries 1 ? 224 2 with the 1 ? 160 2 and 5 ? 96 2 entries).</p><p>(2) Randomly sampling two 160 ? 160-sized crops (entries 2 ? 160 2 ) and using 96 ? 96-sized patches leads to another significant increase in performance.</p><p>(3) Finally, employing multi-scale BoWs improves the performance even further.</p><p>BoW-like comparison.</p><p>In <ref type="table" target="#tab_5">Table 5</ref>, we compare our method with the reference BoW-like method BoWNet. For a fair comparison, we implemented BoWNet both with its proposed augmentations, i.e., using one 224 ? 224-sized crop with cutmix ("BoWNet" row), and with the image augmentation we propose in the vanilla version of our method, i.e., one 160 ? 160-sized crop ("BoWNet (160 2 crops)").  <ref type="table">Table 6</ref>: Evaluation of ImageNet pre-trained ResNet50 models. The "Epochs" and "Batch" columns provide the number of pre-training epochs and the batch size of each model respectively. The first section includes models pre-trained with a similar number of epochs as our model (second section). We boldfaced the best results among all sections as well as of only the top two. For the linear classification tasks, we provide the top-1 accuracy. For object detection, we fine-tuned Faster R-CNN (R50-C4) on VOC trainval07+12 and report detection AP scores by testing on test07. For semi-supervised learning, we fine-tune the pre-trained models on 1% and 10% of ImageNet and report the top-5 accuracy. Note that, in this case the "Supervised" entry results come from <ref type="bibr" target="#b82">[83]</ref> and are obtained by supervised training using only 1% or 10% of the labelled data. All the classification results are computed with single-crop testing. ? : results computed by us.</p><p>We see that our method, even in its vanilla version, achieves significantly better results, while using at least two times fewer training epochs, which validates the efficiency of our proposed fully-online training methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Self-supervised training on ImageNet</head><p>Here we evaluate our method by pre-training with it convnet-based representations on the full ImageNet dataset. We implement the full solution of our method (as described in ? 4.1) using the ResNet50 (v1) <ref type="bibr" target="#b36">[37]</ref> architecture. We evaluate the learned representations on ImageNet, Places205, and VOC07 classification tasks as well as on VOC07+12 detection task and provide results in <ref type="table">Table 6</ref>. On the Ima-geNet classification we evaluate on two settings: (1) training linear classifiers with 100% of the data, and (2) fine-tuning the model using 1% or 10% of the data, which is referred to as semi-supervised learning.</p><p>Results. Pre-training on the full ImageNet and then transferring to downstream tasks is the most popular benchmark for unsupervised representations and thus many methods have configurations specifically tuned on it. In our case, due to the computational intensive nature of pre-training on ImageNet, no full tuning of OBoW took place. Nevertheless, it achieves very strong empirical results across the board. Its classification performance on ImageNet is 73.8%, which is substantially better than instance discrimination methods MoCo v2 and SimCLR, and even improves over the recently proposed BYOL and SwAV methods when considering a similar amount of pre-training epochs. Moreover, in VOC07 classification and Places205 classification, it achieves a new state of the art despite using significantly fewer pre-training epochs than related methods. On the semi-supervised Ima-geNet ResNet50 setting, it significantly surpasses the state of the art for 1% labels, and is also better for 10% labels using again much fewer epochs. On VOC detection, it outperforms previous state-of-the-art methods while demonstrating strong performance improvements over supervised pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we introduce OBoW, a novel unsupervised teacher-student scheme that learns convnet-based representations with a BoW-guided reconstruction task. By employing an efficient fully-online training strategy and promoting the learning of context-aware representations, it delivers strong results that surpass prior state-of-the-art approaches on most evaluation protocols. For instance, when evaluating the derived unsupervised representations on the Places205 classification, Pascal classification or Pascal object detection tasks, OBoW attains a new state of the art, surpassing prior methods while demonstrating significant improvements over supervised representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparing with MoCo for the same image augmentations</head><p>In the main paper we saw that the image augmentation techniques that we designed for our method have a strong positive impact on the quality of the learned representations. However, we stress that the performance improvement of our method over state-of-the-art instance-discrimination methods is not simply due a better mix of augmentations.</p><p>For example, in <ref type="table" target="#tab_8">Table 7</ref> we compare OBoW with MoCo v2, when the latter is implemented with the same image augmentations as those in the full solution of OBoW. We see that indeed, although the proposed augmentations also improve MoCo v2, our method is still significantly better, even in its vanilla version that employs simpler augmentations (i.e., only a single 160 ? 160-sized crop). Therefore, the ability of OBoW to learn state-of-the-art representations is mainly due to its BoW-guided reconstruction formulation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visualization of the vocabulary features</head><p>In <ref type="figure" target="#fig_6">Figures 5 and 6</ref> we illustrate visual words from the conv5 and conv4 teacher feature maps of a ResNet50 OBoW model trained on ImageNet. Since we use a queuebased vocabulary that is constantly updated, for the visualizations we used the state of the vocabulary at the end of training. In order to visualize a visual word, we retrieve multiple image patches from images in the ImageNet training set and depict the 8 patches with the highest assignment score for that visual word. As it can be noticed, visual words encode high level visual concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details C.1. Image augmentation during pre-training</head><p>In Section 3.3 of the main paper, we described the two types of image crops that we extract from a training image in order to train the student network with them. In addition, beyond image cropping, similar to SimCLR <ref type="bibr" target="#b8">[9]</ref>, we also applied color jittering, color-to-grayscale conversion, Gaussian blurring and horizontal flipping as augmentation techniques. All implementation details are provided in Section G in the form of PyTorch code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Evaluation protocols in Section 4.1</head><p>To evaluate the quality of the learned representations, we use two protocols. (1) The first protocol consists in freezing the convnet and then training on its features 1000-way linear classifiers for the ImageNet classification task. The classifier is applied on top of the 512-dimensional feature vectors produced from the final global pooling layer of ResNet18. It is trained with SGD for 50 epochs using a learning rate of 10 that is divided by a factor of 10 every 15 epochs. The batch size is 256 and the weight decay 2e?6. For fast experimentation, we train the linear classifier with precached features extracted from the 224 ? 224 central crop of the image and its horizontally flipped version. (2) For the second protocol, we use a few-shot episodic setting <ref type="bibr" target="#b74">[75]</ref>. We choose 300 classes from ImageNet and run 200 episodes of 50-way fewshot classification tasks. Essentially, for each episode, we randomly select 50 classes from the 300 ones and, for each of these selected classes, n training examples and m = 1 test example (both randomly sampled from the validation images of ImageNet). For n, we use 1 and 5 examples corresponding to 1-shot and 5-shot classification settings, respectively. The m test examples per class are classified using a cosinedistance Prototypical-Networks <ref type="bibr" target="#b66">[67]</ref> classifier applied on top of the frozen self-supervised representations. We report the mean accuracy over the 200 episodes. The purpose of this metric is to analyze the ability of the representations to be used for learning with few training examples. Furthermore, it has the advantage of not requiring tuning of any hyperparameters, such as the learning rate of a linear classifier, the number of training steps, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Self-supervised training on ImageNet</head><p>Here we provide implementation details for the pretraining of the ResNet50-based OBoW model that we use in Section 4.3 of the main paper. We present the full implementation of our method, which includes multi-scale BoWs from the conv4 and conv5 layers of ResNet50, and extraction of two crops of size 160 ? 160 plus five patches of size 96 ? 96 per training image. To extract BoW targets, we use K = 8192 as vocabulary size and we ignore the local feature vectors on the edge / border of the teacher's feature maps. The momentum coefficient ? for the teacher updates is initialized at 0.99 and is annealed to 1.0 during training with a cosine schedule. Finally, the hyper-parameters ? and ? base are set to 8 and 1/15 respectively. We train the model for 200 training epochs with SGD using 1e?4 weight decay and 256-sized mini-batches. As a learning-rate schedule, we warm up the learning rate from 0 to 0.03 with linear annealing during the first 10 epochs and then, for the remaining 190 epochs, we decrease it from 0.03 to 0.00003 with cosine-based schedule. To train the model we use 4 Tesla V100 GPUs with data-distributed training (i.e., the mini-batch is divided across the 4 GPUs) while keeping the batch-norm statistics synchronized across all GPUs (i.e., use the SyncBatchNorm units of PyTorch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Evaluation protocols in Section 4.3</head><p>Here we describe the evaluation protocols that we use in Section 4.3 of the main paper. ImageNet linear classification. In this case, we evaluate the performance on the 1000-way ImageNet classification task by applying a linear classifier on top of the 2048dimensional frozen features of the pool5 layer of ResNet50. We train the linear classifier using SGD for 100 training epochs with 0.9 momentum, 0 weight decay, 1024-sized mini-batches and cosine learning-rate schedule initialized at 10.0. We use the typical image augmentations used for the fully-supervised training of ResNet50 models on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Places205 linear classification.</head><p>For this protocol, we evaluate the performance on the 205-way Places classification task by applying a linear classifier on top of the 2048dimensional frozen features of the pool5 layer of ResNet50. We follow the guidelines of <ref type="bibr" target="#b31">[32]</ref> and train the linear classifier using SGD for 14 training epochs and a learning rate of 0.01 that is multiplied by 0.1 after 5 and 10 epochs. The batch size is 256 and the weight decay is 1e?4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VOC07 linear classification with SVMs.</head><p>Here we evaluate on the VOC07 classification task by training and testing linear SVMs on top of the 2048-dimensional frozen features of the pool5 layer. To this end, we use the publicly available code for benchmarking self-supervised methods provided in <ref type="bibr" target="#b31">[32]</ref> that trains the SVMs using the VOC07 train+val splits and tests them using the VOC07 test split.</p><p>Semi-supervised learning setting on ImageNet. For this semi-supervised setting, we fine-tune the self-supervised ResNet50 model (pre-trained on all ImageNet unlabelled images) on 1% or 10% of ImageNet labelled images. We use the same 1% and 10% splits as in SimCLR (i.e., we downloaded and use the split files of their official code release). We train using SGD with 256-sized mini-batches, 0 weight decay, and two distinct learning rates for the classification head and the feature extractor trunk network components respectively. Specifically, in the 1% setting, we use 40 epochs and the initial learning rates 0.5 and 0.0002 for the classification head and feature extractor trunk components, respectively, which are then multiplied by a factor of 0.2 after 24 and 32 epochs. In the 10% setting, we use 20 epochs and the initial learning rates 0.5 and 0.0002 for the classification head and feature extractor trunk components, respectively, which are multiplied by a factor of 0.2 after 12 and 16 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VOC object detection.</head><p>Here we evaluate the utility of OBoW on a complex downstream task: object detection. We follow the setup considered in prior works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b50">51]</ref>: we fine-tune the pre-trained OBoW with a Faster R-CNN [63] model using a ResNet50 backbone <ref type="bibr" target="#b35">[36]</ref> (R50-C4 in Detectron2 <ref type="bibr" target="#b78">[79]</ref>). We use the fine-tuning protocol and most hyper-parameters from He et al. <ref type="bibr" target="#b34">[35]</ref>: fine-tune on trainval07+12 and evaluate on test07. In detail, we train with mini-batches of size 16 across 4 GPUs for 24K steps, using SyncBatchNorm to finetune BatchNorm parameters, as well as inserting an additional BatchNorm layer for the RoI head after conv5, i.e., Res5ROIHeadsExtraNorm layer in Detectron2. The initial learning rate 0.01 is warmed-up with a slope of 1e?3 for 100 steps and then reduced by a factor of 10 after 18K and 22K steps. We report results for the final checkpoint averaged over 3 different runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. On-line k-means vocabulary updates</head><p>As explained in the main paper, one of the explored choices for updating the vocabulary is to use online k-means after each training step. Specifically, as proposed in VQ-VAE <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b61">62]</ref>, we use exponential moving average for vocabulary updates. In this case, for each mini-batch, we compute the number n k of feature vectors assigned to each cluster k and m k the element-wise sum of all feature vectors assigned to cluster k and update</p><formula xml:id="formula_9">N k ? ?N k + (1 ? ?)n k ,<label>(6)</label></formula><formula xml:id="formula_10">M k ? ?M k + (1 ? ?)m k ,<label>(7)</label></formula><p>with ? = 0.99. The k th visual word of the vocabulary V satisfies v k = M k /N k . A critical issue that arises in this case is that, as training progresses, the features distribution changes over time. The visual words computed by online k-means do not adapt to this distribution shift leading to extremely unbalanced cluster assignments and even to assignments that collapse to a single cluster. In order to counter this effect, we investigate two different strategies: (a) Detection and replacement of rare visual words. In this case, for each visual word we keep track of the time of its most recent assignment as closest cluster centroid to a feature vector. If more than 1000 training steps have passed since then, then we replace it with a local feature vector randomly sampled with uniform distribution from the current mini-batch.</p><p>(b) Enforcing uniform assignments via Sinkhorn optimization. Let x 1 , . . . , x b be the b images of the current mini-batch and D be the K ? B matrix (B = h ? w ? b) whose i th row D i contains the squared distances between all the local features in the mini-batch (across all images and spatial dimensions) and the i th visual word:</p><formula xml:id="formula_11">D i = T(x 1 )[1] ? v i 2 2 , . . . , T(x b )[h ? w ] ? v i 2 2</formula><p>, . Similarly to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>, we compute the assignment codes by solving the regularised optimal transport problem min Q?Q i,j Q i,j D i,j + ? Q i,j log Q i,j , where ? is a coefficient that controls the softness of the assignments. The set Q permits us to enforce uniform assignments among all the visual words and satisfies Q = {Q ? R K?B</p><formula xml:id="formula_12">+ |Q1 B = 1 K 1 K , Q 1 K = 1 B 1 B },</formula><p>where 1 K and 1 B are vectors of length K and B, respectively, with all entries equal to 1. We compute Q with the Sinkhorn algorithm <ref type="bibr" target="#b13">[14]</ref> and use the resulting assignment codes for the on-line k-means updates and for the computation of the BoW targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Time and memory consumption</head><p>In <ref type="table" target="#tab_10">Table 8</ref>, we provide the time and memory consumption of our method as well as of MoCo v2 and BYOL. We observe that OBoW achieves state-of-the-art results in less time ("Training time" row) than the competing methods. In terms of GPU memory consumption, with 256-sized mini-batches our method requires 15775Mb per GPU in a 4-GPU machine (or 8901Mb per GPU in a 8-GPU machine).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. COCO detection and instance segmentation</head><p>In <ref type="table">Table 9</ref> we evaluate the learned representations on the downstream tasks of object detection and instance segmentation on COCO <ref type="bibr" target="#b49">[50]</ref>. To this end, we fine-tune the pretrained representations with a Mask R-CNN <ref type="bibr" target="#b35">[36]</ref> model with a ResNet50 backbone and feature pyramid networks <ref type="bibr" target="#b48">[49]</ref> (Mask R-CNN R50-FPN) implemented in Detectron2. We train the Mask R-CNN model on train2017 for 12 epochs (1? schedule) and report the box detection AP (AP bb ) and instance segmentation AP (AP mk ) on COCO val2017. Similar to VOC detection experiments, BatchNorm layers are fine-tuned and synchronized. We see that OBoW achieves better or comparable results to prior methods despite the fact that it used only 200 pre-training epochs.  . We also provide the projected time for the full training of a method ("Training time"), which is estimated based on the specified number of training epochs ("Epochs"). For OBoW, we used its full implementation. ? : for BYOL we provide the time and memory consumption w.r.t. 256-sized mini-batches, but BYOL uses 4096-sized mini-batches to achieve the reported ImageNet classification accuracy. So, in reality BYOL has higher total GPU memory requirements.  <ref type="table">Table 9</ref>: Results of object detection and instance segmentation on COCO. We report the box detection AP (AP bb ) and instance segmentation AP (AP mk ) on val2017. All methods are pretrained on ImageNet and then fined-tuned on COCO for 12 epochs (1? schedule). We use the ResNet50-based Mask-RCNN model equipped with feature pyramid networks (Mask R-CNN R50-FPN). The "Epochs" and "Batch" columns provide the number of pre-training epochs and the batch size of each model respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. PyTorch code of Image augmentations</head><p>Here we provide the PyTorch implementation of the image augmentations used in our work. <ref type="bibr" target="#b0">1</ref> import random 2 import torch 3 import torchvision.transforms as T 4 from PIL import ImageFilter self.num_views = num_views <ref type="bibr">38 39</ref> def __call__(self, img): <ref type="bibr" target="#b39">40</ref> return torch.stack([self.transform(img) for _ in range(self.num_views)], dim=0) <ref type="bibr">41 42</ref>  T.RandomHorizontalFlip(), <ref type="bibr" target="#b77">78</ref> T.ToTensor(), <ref type="bibr" target="#b78">79</ref> normalize, <ref type="bibr" target="#b79">80</ref> CropImagePatches(patch_size=96, patch_jitter=24, num_patches=5)])</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Unsupervised learning with Bag-of-Words guidance. Two encoders T and S learn at different tempos by interacting and learning from each other. An image x is passed through the encoder T and its output feature maps T (x) are embedded into a BoW representation yT(x) over a vocabulary V of features from T. The vocabulary V is updated at each step. The encoder S aims to reconstruct yT(x) from data-augmented instancesx. A dynamic BoW-prediction head learns to leverage the continuously updated vocabulary V to compute the BoW representation from the features S(x). T follows slowly the learning trajectory of S via momentum updates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>y T (x)[k] = max u q(x)[u][k] (alternatively, the reduction can be performed with average pooling), where q(x)[u][k]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Vocabulary queue from randomly sampled local features. For each input image x to T, "local" features are pooled from T (x) by averaging over 3 ? 3 sliding windows. One of the resulting vectors is selected randomly and added as visual word to the vocabulary queue, replacing the oldest word in the vocabulary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Dynamic BoW-prediction head. G(?) learns to quickly adapt to the visual words in the continuously refreshed vocabulary V . The outputs G(V ) are in fact weights that are used for mapping the features S(x) to the corresponding BoW vector yS(x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Reconstructing BoWs from small parts of the original image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 1 )</head><label>1</label><figDesc>The first one consists in training 1000-way linear classifiers for the ImageNet classification task.(2) For the second protocol, our goal is to analyze the ability of the representations to learn with few training examples. To that end, we use 300 ImageNet classes and run with them multiple (200) episodes of 50-way classification tasks with 1 or 5 training examples per class and a Prototypical-Networks [67] classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Examples of visual-word members from the conv5 layer of ResNet50. The visualizations are created by using the state of the queue-based visual-words vocabulary at the end of training. For each visual word, we depict the 8 image patches retrieved from ImageNet with the highest assignment score for that word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Examples of visual-word members from the conv4 layer of ResNet50. The visualizations are created by using the state of the queue-based visual-words vocabulary at the end of training. For each visual word, we depict the 8 image patches retrieved from ImageNet with the highest assignment score for that word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>5 6 class CropImagePatches: 7 " 9 self</head><label>579</label><figDesc>""Crops from an image 3 x 3 overlapping patches.""" 8 def __init__(self, patch_size=96, patch_jitter=24, num_patches=5): height -self.patch_size -self.patch_jitter) // (split_per_side-1) 17 offset_x = (width -self.patch_size -self.patch_jitter) // (split_per_side-1) * offset_y + random.randint(0, self.patch_jitter) 23 x_left = j * offset_x + random.randint(0, self.patch_jitter) 24 y_bottom = y_top + self.patch_size 25 x_right = x_left + self.patch_size 26 patches.append(img[:, y_top:y_bottom, x_left:x_right]) 27 28 if self.num_patches &lt; (split_per_side * split_per_side): 29 indices = torch.randperm(len(patches))[:self.num_patches] 30 patches = [patches[i] for i in indices.tolist()]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of online vocabulary-update approaches.The results in the first two sections are with the vanilla version of our method and with the full version in the third section.</figDesc><table><row><cell>Few-shot</cell></row></table><note>feature maps. The momentum coefficient ? for the teacher updates is initialized at 0.99 and is annealed to 1.0 during training with a cosine schedule. The hyper-parameters ? and ? base are set to 5 and 1/10 respectively for the results in ? 4.1, to 8 and 1/15 respectively for the results in ? 4.3. For more implementation details see ? C.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Influence of the momentum coefficient ? used for the teacher updates. For these results, we used the vanilla version. In the "0.99 ? 1" row, ? is initialized to 0.99 and annealed to 1.0 with cosine schedule. The other entries use constant ? values.</figDesc><table><row><cell cols="2">Few-shot</cell><cell></cell></row><row><cell cols="3">Soft Dyn 1-shot 5-shot Linear</cell></row><row><cell>42.11</cell><cell>62.44</cell><cell>45.86</cell></row><row><cell>38.61</cell><cell>59.98</cell><cell>44.64</cell></row><row><cell>2.00</cell><cell>2.00</cell><cell>0.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of image crop augmentations and of multiscale BoWs. See text.</figDesc><table><row><cell>Few-shot</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Comparison with BoW-like methods. "EP": total num- ber of epochs used for pre-training. Note that the BoWNet method consists of 40 epochs for teacher pre-training with the RotNet method followed by two BoWNet training rounds of 80 epochs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison with MoCo v2 for the same image augmentations. "EP": total number of epochs used for pre-training. The results are obtained by training ResNet18-based models on 20% of ImageNet, similar to Sections 4.1 and 4.2 of the main paper. "MoCo v2 (our aug.) " is a MoCo v2 model implemented with the same augmentations that we use in the full version of our work, i.e., two 160 ? 160-sized crops plus five 96 ? 96-sized patches.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Time and memory consumption relative to supervised training. "Sup." is the supervised ImageNet training. To measure the time and memory consumption, for all methods we used ResNet50-based implementations, 256-sized mini-batches and datadistributed training with 4 Tesla V100 GPUs. We measured the time consumption based on a single training epoch ("Time per epoch")</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>class GaussianBlur: sigma = random.uniform(self.sigma[0], self.sigma[1]) 48 return img.filter(ImageFilter.GaussianBlur(radius=sigma)) 49 50 normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) 51 52 # Define the transformations for extracting the central from of the original image. Define the transformations for generating two 160x160-sized image crops. Define the transformations for generating two 160x160-sized image crops.</figDesc><table><row><cell>43</cell><cell>def __init__(self, sigma=[.1, 2.]):</cell></row><row><cell>44</cell><cell>self.sigma = sigma</cell></row><row><cell>45</cell><cell></cell></row><row><cell>46</cell><cell>def __call__(self, img):</cell></row><row><cell cols="2">53 transform_original_image = T.Compose([</cell></row><row><cell>54</cell><cell>T.Resize(256),</cell></row><row><cell>55</cell><cell>T.CenterCrop(224),</cell></row><row><cell>56</cell><cell>T.RandomHorizontalFlip(),</cell></row><row><cell>57</cell><cell>T.ToTensor(),</cell></row><row><cell>58</cell><cell>normalize])</cell></row><row><cell>59</cell><cell></cell></row><row><cell cols="2">60 # 61 transform_two_160x160_image_crops = StackMultipleViews(</cell></row><row><cell>62</cell><cell>transform=T.Compose([</cell></row><row><cell>63</cell><cell>T.RandomResizedCrop(160, scale=[0.08, 0.6]),</cell></row><row><cell>64</cell><cell>T.RandomApply([T.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),</cell></row><row><cell>65</cell><cell>T.RandomGrayscale(p=0.2),</cell></row><row><cell>66</cell><cell>T.RandomApply([GaussianBlur(sigma=[0.1, 2.0])], p=0.5),</cell></row><row><cell>67</cell><cell>T.RandomHorizontalFlip(),</cell></row><row><cell>68</cell><cell>T.ToTensor(),</cell></row><row><cell>69</cell><cell>normalize]),</cell></row><row><cell>70</cell><cell>num_views=2)</cell></row><row><cell>71</cell><cell></cell></row><row><cell cols="2">72 # 73 transform_five_96x96_image_patches = T.Compose([</cell></row><row><cell>74</cell><cell>T.RandomResizedCrop(256, scale=[0.6, 1.0]),</cell></row><row><cell>75</cell><cell>T.RandomApply([T.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),</cell></row><row><cell>76</cell><cell>T.RandomGrayscale(p=0.2),</cell></row><row><cell>77</cell><cell></cell></row></table><note>47</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The visual centrifuge: Model-free layered video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">Markus</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucil?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Model compression. In KDD</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-supervised GANs via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<title level="m">Improved baselines with momentum contrastive learning. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jutta</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The missing data encoder: Cross-channel image completion with hide-and-seek adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A framework for contrastive self-supervised learning and designing a new approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Are all negatives created equal in contrastive instance discrimination?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morcos</surname></persName>
		</author>
		<idno>arXiv, 2020. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Three unfinished works on the optimal storage capacity of networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Derrida</surname></persName>
		</author>
		<idno>1989. 2</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics A: Mathematical and General</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning representations by predicting bags of visual words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ActionVLAD: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evolving modular fast-weight networks for control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scaling and benchmarking self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valko</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>In NeurIPS, 2020. 2, 5</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPSW</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">QuEST: Quantized embedding space for transferring knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A survey on contrastive self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><forename type="middle">Ramesh</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Zaki</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debapriya</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fillia</forename><surname>Makedon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bulent</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noe</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bayesian dark knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Anoop Korattikara Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Distilling model knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Superdepth: Self-supervised, super-resolved monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rare?</forename><surname>Ambru?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Unsupervised learning of dense visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Pedro O Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Benmalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Golemo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fewshot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with VQ-VAE-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Dynamics of on-line gradient descent learning for multilayer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Video google: Efficient visual search of videos. In Toward category-level object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">To aggregate or not to aggregate: Selective match kernels for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Selfie: Self-supervised pretraining for image embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance-level discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Evaluating bag-of-visual-words representations in scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<editor>MIR</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">CutMix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">S 4 L: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">AED: Unsupervised representation learning by autoencoding transformations rather than data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
