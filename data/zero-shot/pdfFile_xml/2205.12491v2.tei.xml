<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-grained Contrastive Learning for Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hogan</surname></persName>
							<email>whogan@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Li</surname></persName>
							<email>j9li@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
							<email>jshang@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-grained Contrastive Learning for Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent relation extraction (RE) works have shown encouraging improvements by conducting contrastive learning on silver labels generated by distant supervision before fine-tuning on gold labels. Existing methods typically assume all these silver labels are accurate and treat them equally; however, distant supervision is inevitably noisy-some silver labels are more reliable than others. In this paper, we propose fine-grained contrastive learning (FineCL) for RE, which leverages fine-grained information about which silver labels are and are not noisy to improve the quality of learned relationship representations for RE. We first assess the quality of silver labels via a simple and automatic approach we call "learning order denoising," where we train a language model to learn these relations and record the order of learned training instances. We show that learning order largely corresponds to label accuracy-early-learned silver labels have, on average, more accurate labels than later-learned silver labels. Then, during pretraining, we increase the weights of accurate labels within a novel contrastive learning objective. Experiments on several RE benchmarks show that FineCL makes consistent and significant performance gains over state-ofthe-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current state-of-the-art RE models leverage a twophase training: a self-supervised pre-training followed by a supervised fine-tuning. Popular pretrained language models (PLM) such as BERT <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref> and RoBERTa  feature a generic pre-training objective, namely masked language modeling (MLM), that allows them to generalize to various downstream tasks. However, recent RE works have shown impressive performance gains by using a pre-training objective designed specifically for relation extraction <ref type="bibr" target="#b19">(Soares et al., 2019;</ref><ref type="bibr" target="#b13">Peng et al., 2020;</ref><ref type="bibr" target="#b15">Qin et al., 2021)</ref>.</p><p>Recently, <ref type="bibr" target="#b13">Peng et al. (2020)</ref> and <ref type="bibr" target="#b15">Qin et al. (2021)</ref> used a contrastive learning loss function to learn relationship representations during pre-training. However, RE-specific pre-training requires large amounts of automatically labeled data obtained trough distant supervision for RE <ref type="bibr" target="#b11">(Mintz et al., 2009)</ref> which is inherently noisy-not all labels from distantly supervised data are correct. <ref type="bibr" target="#b2">Gao et al. (2021)</ref> manually examined distantly supervised relation data and found that a significant ratio, 53%, of the assigned labels were incorrect. Furthermore, distantly supervised labels can go beyond "correct" or "incorrect"-they can have multiple levels of correctness. Consider the following sentences: Accurate Labels <ref type="figure">Figure 1</ref>: The FineCL framework has three stages: Stage 1: we use distantly supervised data (T ) to train a PLM via cross-entropy to collect ordered subsets of learned (A) and not learned (B) instances over k epochs. Stage 2: function f (k) weighs relation instances (r 0 , r 1 ) relative to their learning order in a contrastive learning pre-training objective that uses cosine similarity to align similar relations. Stage 3: we adapt the model to a discriminative task.</p><p>lationship "born in." Sentence (2) is incorrectly labeled, and sentence <ref type="formula" target="#formula_1">(3)</ref> is, arguably, semi-accurate since one may infer that someone was born in the same place they were raised. Conventional contrastive learning for RE does not account for differences in label accuracy-it treats all instances equally. This can be problematic when learning robust and high-quality relationship representations.</p><p>This paper proposes a noise-aware contrastive pre-training, Fine-grained Contrastive Learning (FineCL) for RE, that leverages additional finegrained information about which instances are and are not noisy to produce high-quality relationship representations. <ref type="figure">Figure 1</ref> illustrates the end-to-end data flow for the proposed FineCL method. We first assess the noise level of all distantly supervised training instances and then incorporate such fine-grained information into the contrastive pretraining. Less noisy, or clean, training instances are weighted more relative to noisy training instances. We then fine-tune the model on gold-labeled data.</p><p>As we demonstrate in this work, this approach produces high-quality relationship representations from noisy data and then optimizes performance using limited amounts of human-annotated data.</p><p>There are several choices of methods to assess noise levels. We select a simple yet effective method we call "learning order denoising" that does not require access to human annotated labels. We train an off-the-shelf language model to predict relationships from distantly supervised data and we record the order of relation instances learned during training. We show that the order in which instances are learned corresponds to the label accuracy of an instance: accurately labeled relation instances are learned first, followed by noisy, inaccurately labeled relation instances.</p><p>We leverage learning-order denoising to improve the relationship representations learned during pretraining by linearly projecting the weights of each relation instance corresponding to the order in which the instance was learned. We apply higher weights to relation instances learned earlier in training relative to those learned later in training. We use these weights to inform a contrastive learning loss function that learns to group instances of similar relationships.</p><p>We compare our method to leading RE pre-training methods and observe an increase in performance on various downstream RE tasks, illustrating that FineCL produces more informative relationship representations.</p><p>The contributions of this work are the following:</p><p>? We demonstrate that learning-order denoising is an effective and automatic method for denoising distantly labeled data. ? Applying a denoising strategy to a contrastive learning pre-training objective creates more informative representations, improving performance on downstream tasks. ? We openly provide all code, trained models, experimental settings, and datasets used to substantiate the claims made in this paper. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early RE methods featured pattern-based algorithms <ref type="bibr" target="#b0">(Califf and Mooney, 1997)</ref> followed by advanced statistical-based RE methods <ref type="bibr" target="#b11">(Mintz et al., 2009;</ref><ref type="bibr" target="#b18">Riedel et al., 2010;</ref>    <ref type="bibr" target="#b28">(Zhang and Wang, 2015;</ref><ref type="bibr" target="#b14">Peng et al., 2017;</ref><ref type="bibr" target="#b12">Miwa and Bansal, 2016)</ref>. The transformer <ref type="bibr" target="#b20">(Vaswani et al., 2017)</ref> enabled the development of wildly successful large pre-trained language models <ref type="bibr" target="#b17">(Radford and Narasimhan, 2018;</ref><ref type="bibr" target="#b1">Devlin et al., 2019;</ref>. At the time of writing, all current leading models in RE 2 leverage large pre-trained language models via a twostep training methodology: a self-supervised pretraining followed by a supervised fine-tuning <ref type="bibr" target="#b23">Xiao et al., 2021)</ref>.</p><p>Building on BERT <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>, <ref type="bibr" target="#b19">Soares et al. (2019)</ref> proposed MTB, a model featuring a pre-training objective explicitly designed for the task of relation extraction. MTB uses dot product similarly to align pairs of randomly masked entities during pre-training. Its success inspired the development of subsequent RE-specific pre-training methods <ref type="bibr" target="#b13">(Peng et al., 2020;</ref><ref type="bibr" target="#b15">Qin et al., 2021)</ref>. <ref type="bibr" target="#b13">Peng et al. (2020)</ref> demonstrated the effectiveness of contrastive learning used to develop relationship representations during pre-training. Their model, named "CP," featured a pre-training objective that combined a relation discrimination task with BERT's masked language modeling (MLM) task. Their work inspired ERICA <ref type="bibr" target="#b15">(Qin et al., 2021)</ref>, which expanded the contrastive learning pre-training objective to include entity and relation discrimination, as well as MLM. <ref type="bibr">Wan et al. (2022)</ref> is a recent extension of <ref type="bibr" target="#b13">Peng et al. (2020)</ref> that proposes a weighted contrastive learning (WCL) method for RE. The authors first fine-tune BERT to predict relationships using gold training data and then use the fine-tuned model to predict relationships from distantly labeled data. Next, they use the softmax probability of each prediction as a confidence value which they then apply to a weighted contrastive learning function used for pre-training. Lastly, they fine-tune the WCL model on gold training data.</p><p>Our work is an extension of ERICA. We introduce a more nuanced RE contrastive learning objective that leverages additional, fine-grained data about which instances are high-quality training signals. <ref type="table" target="#tab_1">Table 1</ref> qualitatively compares recent pre-training methods used for RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>FineCL for RE consists of three discrete stages: learning order denoising, contrastive pre-training, and supervised adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Order Denoising</head><p>For learning order denoising, we automatically label large amounts of training data via distant supervision for RE <ref type="bibr" target="#b11">(Mintz et al., 2009</ref>) which we use to train a PLM to predict relation classes using multi-class cross-entropy loss.</p><formula xml:id="formula_0">L CE = ? N i=1 y o,i ? log (p (y o,i ))<label>(1)</label></formula><p>Where the number of classes N is the number of relation classes plus one for no relation, y is a binary indicator that is 1 if and only if i is the correct classification for observation o, and p(y o,i ) is the Softmax probability that observation o is of class i.</p><p>During training, we record the order of training instances learned. We consider an instance "learned" upon the initial correct prediction. Likewise, an instance is "not learned" if the model fails to predict it correctly during training. Training instances are evaluated by batch within each epoch, exposing the model to all training data points the same number of times. We refer to this method as batch-based learning order.</p><p>Thus, the PLM effectively becomes a mapping function that maps all training instances (T ) into two subsets: learned (A) and not learned instances (B) such that A B = T and A B = ?. The set of learned instances A is further divided into non-intersecting subsets of learned instances A 1 through A k where k corresponds to the epoch in which an instance is learned.</p><formula xml:id="formula_1">A 0 A 1 ... A k = A (2) A i A j = ? for all i = j<label>(3)</label></formula><p>We use k = 15 epochs, resulting in k + 1 subsets of instances-k subsets of learned instances plus one subset of not learned instances. <ref type="figure" target="#fig_1">Figure 2</ref> shows the percent of total training instances learned per epoch during this phase on the DocRED <ref type="bibr" target="#b26">(Yao et al., 2019)</ref> distantly labeled training set which contains 100k documents, 1.5M intra-and inter-sentence relation instances, and 96 relation types (not including no relation).</p><p>More challenging relation classes may be underrepresented within the set of learned instances. Such minority classes can be problematic during pretraining since unlearned instances are weighted less than learned ones, presenting a challenge for the model to learn informative representations for minority classes. To account for this, we ensure that at least P % of instances of each relation class is contained within the set of learned instances. During training, we set P = 50 and observed that 2% of relation classes are underrepresented within the set of learned instances. We upsample underrepresented classes by randomly selecting unlearned instances from the corresponding class, placing them into one of the k subsets of learned instances A. See <ref type="figure">Figure 4</ref> in the Appendix for a detailed chart showing the ratio of learned instances by relation class in each epoch.</p><p>Learning order metadata is then inserted into the original training data T , creating a modified training set T used for the contrastive pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contrastive Pre-training</head><p>This section introduces our pre-training method to learn high-quality entity and relation representations. We first construct informative representation for entities and relationships which we use to implement a three-part pre-training objective that features entity discrimination, relation discrimination, and masked language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Entity &amp; Relation Representation</head><p>We construct entity and relationship representations following ERICA <ref type="bibr" target="#b15">(Qin et al., 2021)</ref>. For the document d i , we use a pre-trained language model to encode d i and obtain the hidden states {h 1 , h 2 , . . . , h |d i | }. Then, mean pooling is applied to the consecutive tokens in entity e j to obtain entity representations. Assuming n start and n end are the start index and end index of entity e j in document d i , the entity representation of e j is represented as:</p><formula xml:id="formula_2">m e j = MeanPool(h nstart , . . . , h n end ),<label>(4)</label></formula><p>To form a relation representation, we concatenate the representations of two entities e j1 and e j2 :</p><formula xml:id="formula_3">r j1j2 = [e j1 ; e j2 ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Entity Discrimination</head><p>For entity discrimination, we use the same method described in ERICA. The goal of entity discrimination (E D ) is inferring the tail entity in a document given a head entity and a relation <ref type="bibr" target="#b15">(Qin et al., 2021)</ref>. The model distinguishes the ground-truth tail entity from other entities in the text. Given a sampled instance tuple t i j k = (d i , e ij , r i jk , e ik ), our model is trained to distinguish the tail entity e ik from other entities in the document d i . Specifically, we concatenate the relation name of r i jk , the head entity e ij and a special token [SEP] in front of d i to get d * i . Then, we encode d * i to get the entity representations using the method from Section 3.2.1. The contrastive learning objective for entity discrimination is formulated as:</p><formula xml:id="formula_4">L E D = ? t i jk ?T log exp(cos(e ij ,e ik )/? ) |E i | l=1,l =j exp(cos(e ij ,e il )/? )</formula><p>where cos(?, ?) denotes the cosine similarity between two entity representations and ? is a temperature hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Relation Discrimination</head><p>To effectively learn representation for downstream task relation extraction, we conduct a Relation Discrimination (R D ) task during pre-training. R D aims to distinguish whether two relations are semantically similar <ref type="bibr" target="#b15">(Qin et al., 2021)</ref>. Existing methods <ref type="bibr" target="#b13">(Peng et al., 2020;</ref><ref type="bibr" target="#b15">Qin et al., 2021)</ref> require large amounts of automatically labeled data from distant supervision which is noisy because not all sentences will adequately express a relationship.</p><p>In this case, the learning order can be introduced to make the model aware of the noise level of relation instances. To efficiently incorporate learning order into the training process, we propose fine-grained, noise-aware relation discrimination.</p><p>In this new method, the noise level of all distantly supervised training instances controls the optimization process by re-weighting the contrastive objective. Intuitively, the model should learn more from high-quality, accurately labeled training instances than noisy, inaccurately labeled instances. Hence, we assign higher weights to earlier learned instances from the learning order denoising stage.</p><p>In practice, we sample a tuple pair of relation</p><formula xml:id="formula_5">instance t A = (d A , e A 1 , r A , e A 2 , k A ) and t B = (d B , e B 1 , r B , e B 2 , k B ) from T and r A = r B ,</formula><p>where d is a document; e is a entity in d; r is the relationship between two entities and k is the first learned order introduced in Section 3.1. Using the method mentioned in Section 3.2.1, we obtain the positive relation representations r t A and r t B . To discriminate positive examples from negative ones, the fine-grained R D is defined as follows:</p><formula xml:id="formula_6">L R D = ? t A ,t B ?T f (k A ) log exp (cos (r t A , r t B ) /? ) Z , Z = N t C ?T /{t A } f (k C ) exp (cos (r t A , r t C ) /? )<label>(5)</label></formula><p>where cos(?, ?) denotes the cosine similarity; ? is the temperature; N is a hyper-parameter and t C is a negative instance (r A = r C ) sampled from T . Relation instances t A and t C are re-weighted by function f which is defined as:</p><formula xml:id="formula_7">f (k) = ? kmax?k kmax?k min ,<label>(6)</label></formula><p>where ? (? &gt; 1) is a hyper-parameter of the function f ; max and min are maximum and minimum first-learned order, respectively. We increase the weight of negative t C if it is a high-quality training instance (i.e., k is small). Because all positives and negatives are discriminated from instance t A , we control the overall weight by the learning order k A .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Overall Objective</head><p>We include the MLM task <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref> to avoid catastrophic forgetting of language understanding <ref type="bibr" target="#b10">(McCloskey and Cohen, 1989</ref>) and construct the following overall objective for FineCL:</p><formula xml:id="formula_8">L F ineCL = L E D + L R D + L M LM<label>(7)</label></formula><p>3.3 Supervised Adaptation</p><p>The primary focus of our work is to improve relationship representations learned during pre-training and, in doing so, improve performance on downstream RE tasks. To illustrate the effectiveness of our pre-training method, we use cross-entropy loss, as described in equation 1, to fine-tune our pre-trained FineCL model on document-level and sentence-level RE tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning Order as Noise Level Hypothesis</head><p>We first seek to confirm our hypothesis that the learning order automatically orders distantly supervised data from clean, high-quality instances to noisy, low-quality instances. However, given the large amount of pre-training data, statistically significant confirmation via manual annotation is prohibitively expensive. So, we devise the following experiment to test our hypothesis in lieu of a significant manual annotation effort.</p><p>We begin with the assumption that a model trained on a dataset without noise will perform better than a model trained on a dataset with noise. Suppose learning order denoising successfully orders instances relative to their noise; then, we should observe a boost in performance by training on a subset of early-learned instances compared to a model trained on the complete, noisy dataset.</p><p>As reported by <ref type="bibr" target="#b2">Gao et al. (2021)</ref>, up to 53% of relation instances labeled via distant supervision are incorrect. Using this estimation, we attempt to use learning order denoising to remove the roughly 50% of instances that are noisy instances from the DocRED's distantly supervised training set. To do this, we first obtain the learning order of relation instances using the methodology described in Section 3.1. Without loss of generalization, we choose RoBERTa , specifically  the roberta-base checkpoint 3 , as the base model to develop the order of learned instances.</p><p>We observe that the set of training instances learned via batch-based learning order in the first epoch, A B 0 , consists of 45% of the total training instances. We use A B 0 to construct a trimmed training set T A B 0 . We then compare performance in two settings: (1) RoBERTa trained with the complete distantly supervised training dataset T and (2) RoBERTa trained on the trimmed, denoised training data T A B 0 . Table 2 reports the results of this experiment. Significantly, the denoised training set consisting of only 45% training data outperforms the baseline model.</p><p>We also conduct an informal manual analysis of the learning order. We randomly selected 120 instances from the first six training epochs-60 correctly, and 60 incorrectly predicted instances. We find that 93% of the correct predictions have accurate labels within the first three epochs. However, in epochs 4 through 6, label accuracy drops to 53% among correct predictions. Furthermore, we find a relatively low label accuracy of 50% from the first three epochs of incorrect predictions, illustrating that the model struggles to learn noisy instances compared to clean instances early in training. We use these results and the results presented in Table 2 to argue that learning order successfully orders instances from clean, high-quality to noisy, low-quality instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning Order: Batch-vs. Epoch-based</head><p>We experiment with two methods of collecting learning order data: batch-based and epoch-based (see Appendix A.1 for pseudo-code describing these methods). Batch-based: As previously mentioned, for batchbased learning order we collect learned instances 3 https://huggingface.co/roberta-base per batch across each epoch during training. However, we recognize that this may bias the set of learned instances by the random batch for which they are selected. For example, accurately labeled relation instances selected for the first few batches during training may not be predicted correctly because the model has not learned much. Epoch-based: To reduce potential selection order bias from batch-based learning order, we experiment with epoch-based learning order by evaluating the model on the entire training set at the end of each epoch. We rerun the experiment detailed in Section 4.1 using epoch-based learning order to construct the trimmed dataset T A E 0 and present the results in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Using epoch-based learning order, we observe that the model learns 64.9% of the training instances within the first epoch, an increase compared to the 45.0% of learned instances from batch-based learning order. However, training RoBERTa on the epoch-based training subset, we obtain an F1 score of 46.0, which under-performs relative to the 46.6 F1 score from the batch-based learning order experiment. We hypothesize that, while epoch-based learning order may capture more learned instances, it leads to noisier instances leaking into the sets of learned data because the model is more prone to simply memorizing noisy labels encountered previously in the epoch.</p><p>Note that we do not use DocRED's humanannotated training data in these learning order experiments. Instead, we train on the distantly supervised training data and test on human-annotated data. This is done to assess the quality of the various subsets of distantly labeled data. It is why the performance of these tests is considerably lower than the results from the experiments in Section 4.4 that leverage human-annotated training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pre-training Details</head><p>To ensure a fair comparison and highlight the effectiveness of FineCL, we align our pre-training data and settings to those used by ERICA. The ERICA pre-training dataset is constructed using distant supervision for RE by pairing documents from Wikipedia (English) with the Wikidata knowledge graph. This distantly labeled dataset creation method mirrors the method used to create the distantly labeled training set in DocRED but differs in that it is much larger and more diverse. It con-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Relation Extraction</head><p>Document-level RE: To assess our framework's ability to extract document-level relations, we report performance on DocRED <ref type="bibr" target="#b26">(Yao et al., 2019)</ref>. We compare our model to the following baselines:</p><p>(1) CNN <ref type="bibr" target="#b27">(Zeng et al., 2014)</ref>, <ref type="formula">(2)</ref>     <ref type="bibr" target="#b15">(Qin et al., 2021)</ref>; all other numbers are from our implementations).</p><p>overall F1-micro score and an F1-micro score computed by ignoring fact triples in the test set that overlap with fact triples in the training and development splits. We observe that FineCL outperforms all baselines in all experimental settings, offering evidence that FineCL produces better relationship representations from noisy data.</p><p>Given that learning-order denoising weighs earlier learned instances over later learned instances, FineCL may be biased towards easier, or common relation classes. The increase in F1-micro performance may result from improved predictions on common relation classes at the expense of predictions on rare classes. To better understand the performance gains, we also report F1-macro and F1-macro weighted in <ref type="table" target="#tab_7">Table 4</ref>. The results show that FineCL outperforms the top baselines in both F1-macro metrics indicating that, on average, our method improves performance across all relation classes. However, the low F1-macro scores from all the models highlight an area for improvementfuture pre-trained RE models should focus on improving performance on long-tail relation classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence-level RE:</head><p>To assess our framework's ability to extract sentence-level relations, we report performance on TACRED <ref type="bibr" target="#b29">(Zhang et al., 2017)</ref> and <ref type="bibr">SemEval-2010</ref><ref type="bibr">Task 8 (Hendrickx et al., 2010</ref>. We compare our model to MTB, CP, BERT, RoBERTa, ERICA BERT , ERICA RoBERTa , and WCL (see Appendix A.3.2 for detailed experimental settings). <ref type="table" target="#tab_8">Table 5</ref> reports F1 scores across multiple data reduction settings <ref type="figure">(1%, 10%, 100%)</ref>. Again, we observe that FineCL outperforms all baselines in all settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Studies</head><p>We conduct a suite of ablation experiments to understand how learning order denoising affects the quality of relationship representations learned during pre-training. We note that the FineCL method is identical to ERICA when we remove fine-grained data and treat all instances equally. As such, ER-ICA can be considered an ablation experiment of FineCL without fine-grained data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning Order Epochs</head><p>In our first ablation experiment, we vary the number of training epochs (k) used to obtain learning order data to determine how the different amounts of batch-based learning order data affect pre-training. We test k = {1, 3, 5, 10, 15} as well as a baseline that does not use learning order denoising. To reduce the high computational requirements for pretraining, we use a shortened pre-training for these experiments where we pre-train for 1000 training steps compared to the full 6000 step training used for our main experiments. We then fine-tune the models using the same settings described in Section 4.4. Notably, our pre-trained model trained at 1000 steps achieves an F1 score of 59.0, which is reasonably close to the 59.5 F1 score from the FineCL trained for 6000 steps. <ref type="table" target="#tab_10">Table 6</ref> contains the results from this ablation experiment. We observe that k = 15 epochs of learned instances produce the best performance, indicating that a more extensive set of learned instances produces better relationship representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Different Learning Order Models</head><p>We chose the RoBERTa base model for the first stage of our FineCL framework to reduce the adoption barrier for our methodology. Popular pretrained models such as roberta-base are easy to implement and require fewer resources compared to larger state-of-the-art (SOTA) RE models. How-     . Therefore, we compare sets of learned instances from SSAN (A S ) and RoBERTa (A R ) by epoch (k) using a cumulative Jaccard Similarity Index: <ref type="figure" target="#fig_2">Figure 3</ref> plots the cumulative Jaccard Similarity Index (JSI) between sets of learned instances from RoBERTa and SSAN. The total cumulative JSI between the two models after k = 15 epochs is 0.771, showing high similarity between sets of learned instances. While the sets are not perfectly aligned, we argue that this high similarity justifies using the smaller and more convenient RoBERTa model in determining learning order. We leave a more  thorough examination of the differences in sets of learned instances obtained using various RE models to future work and present our findings as a proof of concept, demonstrating that obtaining learning order from relatively small and convenient language models is sufficient in improving representations learned during pre-training.</p><formula xml:id="formula_9">J(A R , A S ) = k i=0 |A R i ? A S i | |A R i ? A S i |</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance Relative to Class Difficulty</head><p>As mentioned in Section 3.1, it is possible that learning-order denoising biases the model to easier, common relation classes, as easier classes may be over-represented in the set of learned instances.</p><p>To understand the effectiveness of our approach relative to class difficulty, we assess the end-to-end performance of FineCL on a set of difficult relation classes.</p><p>We recognize that there are multiple ways to define a "difficult" relation class. Difficult classes can be classes with few training instances, classes with a significant number of inaccurate or semi-accurate labels, or classes that suffer from low overall accuracy after training completes. For this ablation study, we define the set of difficult relation classes as classes that attain relatively low accuracy from the training in Stage 1 of FineCL. We claim that any class which achieves less than 80% accuracy after Stage 1 training completes is a "difficult" relation class. This subset of the lowest-performing classes from the DocRED dataset makes up 24% of all the classes in the dataset.</p><p>We compare the end-to-end performance of FineCL to baselines that do not leverage fine-grained contrastive learning on the set of difficult relation classes. <ref type="table" target="#tab_13">Table 7</ref> contains the results from this experiment. We observe that FineCL achieves an F1 score of 36.1% on the subset of difficult classes compared to the best-performing baseline which achieves 35.8%. We argue that these results, as well as the results from <ref type="table" target="#tab_7">Table 4</ref>, offer evidence that the FineCL approach is capable of improving performance on both difficult classes as well as easy classes. However, the low overall performance from all models on difficult classes highlights an area for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we expand on contrastive learning for relation extraction by introducing Fine-grained Contrastive Learning for RE-a method that uses additional, fine-grained information about distantly supervised training data to improve relationship representations learned during pre-training. These improved representations lead to increases in performance across a variety of downstream RE tasks. This report shows that learning order denoising effectively and automatically orders distantly supervised training data from clean to noisy instances.</p><p>In future work, we hope to explore the usefulness of this method when applied to manually annotated data where learning order may instead reflect the level of difficulty of training instances. This could be an easy and automatic way to introduce curricula learning within the fine-tuning training phase. We also intend to explore the pairing of other denoising methods with FineCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A. We initialize our model with roberta-base released by Huggingface 5 . The optimizer is AdamW and we set the learning rate to 3 ? 10 ?5 , weight decay to 1 ? 10 ?5 , batch size to 768 and temperature ? to 5 ? 10 ?2 . The hyper-parameter ? that controls the weights of contrastive learning is e (the base of natural logarithm). We randomly sample 64 negatives for each document. We train our model with 3 NVIDIA Tesla V100 GPUs for 6,000 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Downstream Training Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 DocRED</head><p>We fine-tune our model on DocRED using the following settings: batch size=32, epochs=200, max sequence length=512, gradient accumulation steps=1, learning rate=4e-5, weight decay=0, adam epsilon=1e-8, max gradient norm=1.0, hidden size=768, and a seed=42. Results are reported on the official DocRED test set as an average of three runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 SemEval and TACRED</head><p>We fine tune our moodel on SemEval and TA-CRED using the following settings: batch size=64, max sequence length=100, learning rate=5e-5, 5 https://huggingface.co/roberta-base adam epsilon=1e-8, weight decay=1e-5, max gradient norm=1.0, warm up steps=500, and hidden size=768. We ran tests on training proportions 0.01/0.1/1.0 using 80/20/8 epochs and a dropout of 0.2/0.1/0.35, respectively.</p><p>Results are reported as an average of five runs using the following seed values: 42, 43, 44, 45, and 46. Figure 4: Ratios of instances of learned classes per epoch when recording learning order from distantly supervised DocRED training data. Note, this is before randomized upsampling of underrepresented classes (e.g. lyrics by and producer).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Percent of total training instances learned per epoch when recording batch-based learning order on distantly labeled data from DocRED.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Cumulative Jaccard Similarity between sets of learned instances by epoch from RoBERTa and SSAN using distantly labeled training data from Do-cRED.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: A comparison of RE pre-training meth-</cell></row><row><cell>ods highlighting the pre-training objective: Mask</cell></row><row><cell>Language Modeling (MLM), Dot Product Similar-</cell></row><row><cell>ity (DPS), Contrastive Learning (CL), Weighted Con-</cell></row><row><cell>trastive Learning (WCL), and Fine-grained Contrastive</cell></row><row><cell>Learning (FineCL). R D denotes the presence of relation</cell></row><row><cell>discrimination in the loss function, and E D denotes the</cell></row><row><cell>presence of entity discrimination in the loss function.</cell></row></table><note>2017). Advances in deep learning led to neural- based RE methods</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Learning orderTraining set Training set size F1</figDesc><table><row><cell>None</cell><cell>T</cell><cell>100%</cell><cell>45.8</cell></row><row><cell>Batch-based</cell><cell>T A B 0</cell><cell>45.0%</cell><cell>46.6</cell></row><row><cell>Epoch-based</cell><cell>T A E 0</cell><cell>64.9%</cell><cell>46.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results comparing performance on the Do-cRED test set using trimmed sets of distantly supervised training data. The batch-based and epoch-based training sets consist of training instances determined by the instances learned within the first epoch using the respective learning order collection methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: F1-micro scores reported on the DocRED test</cell></row><row><cell>set. IgF1 ignores performance on fact triples in the test</cell></row><row><cell>set overlapping with triples in the train/dev sets. (* de-</cell></row><row><cell>notes performance as reported in (Qin et al., 2021); all</cell></row><row><cell>other numbers are from our implementations).</cell></row><row><cell>tains 1M documents, 7.2M relation instances, and</cell></row><row><cell>1040 relation types compared to DocRED's 100k</cell></row><row><cell>documents, 1.5M relation instances, and 96 rela-</cell></row><row><cell>tion types (not including no relation). Additional</cell></row><row><cell>checks are performed to ensure no fact triples over-</cell></row><row><cell>lap between the training data and the test sets of</cell></row><row><cell>the various downstream RE tasks. Detailed pre-</cell></row><row><cell>training settings can found in Appendix A.2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>F1-macro and F1-macro-weighted scores reported from the DocRED test set.</figDesc><table><row><cell>Dataset</cell><cell>TACRED</cell><cell>SemEval</cell></row><row><cell>Size</cell><cell cols="2">1% 10% 100% 1% 10% 100%</cell></row><row><cell>MTB*</cell><cell cols="2">35.7 58.8 68.2 44.2 79.2 88.2</cell></row><row><cell>CP*</cell><cell cols="2">37.1 60.6 68.1 40.3 80.0 88.5</cell></row><row><cell>BERT</cell><cell cols="2">22.2 53.5 63.7 41.0 76.5 87.8</cell></row><row><cell>RoBERTa</cell><cell cols="2">27.3 61.1 69.3 43.6 77.7 87.5</cell></row><row><cell>ERICA BERT</cell><cell cols="2">34.9 56.0 64.9 46.4 79.8 88.1</cell></row><row><cell cols="3">ERICA RoBERTa 41.1 61.7 69.5 50.3 80.9 88.4</cell></row><row><cell>WCL RoBERTA</cell><cell cols="2">37.6 61.3 69.7 47.0 80.0 88.3</cell></row><row><cell>FineCL</cell><cell cols="2">43.7 62.7 70.3 51.2 81.0 88.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>F1-micro scores reported from the TACRED and SemEval test sets (* denotes performance as reported in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Ablation experiment results on the DocRED</cell></row><row><cell>test set with pre-trained models that use learning or-</cell></row><row><cell>der data obtained with various training durations. Per-</cell></row><row><cell>cent learned refers to the percent of training instances</cell></row><row><cell>learned in the set of learned instances (A). "Baseline"</cell></row><row><cell>is a pre-trained model that does not leverage learning</cell></row><row><cell>order (i.e., all instances are weighted equally during</cell></row><row><cell>pre-training).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>F1-micro scores on a subset of difficult relation classes from the DocRED dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/wphogan/finecl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://paperswithcode.com/task/ relation-extraction</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://paperswithcode.com/sota/ relation-extraction-on-docred</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Thank you to the anonymous reviewers for their thoughtful feedback. Our work is sponsored in part by National Science Foundation Convergence Accelerator under award OIA-2040727 as well as generous gifts from Google, Adobe, and Teradata. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and should not be interpreted as necessarily representing the views, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for government purposes not withstanding any copyright annotation hereon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>The limitations of our method are as follows: 1. Our method requires access to a robust knowledge graph to define the concepts and the relationships for distant supervision. 2. Our method minimizes the need for but still requires human-annotated data, which is both expensive and time-consuming to create. 3. The low F1-macro scores of our model and all other leading RE models highlight the need to improve performance on long-tail relation classes in future works.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Relational learning of pattern-match rules for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Elaine</forename><surname>Califf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Manual evaluation matters: Reviewing test protocols of distantly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyue</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhuo</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FINDINGS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>S?aghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pad?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Abstractified multiinstance learning (AMIL) for biomedical relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Molly</forename><surname>William P Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Katsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho-Cheol</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiki</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Baeza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nan</forename><surname>Bartko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu</surname></persName>
		</author>
		<idno type="DOI">10.24432/C5V30P</idno>
	</analytic>
	<monogr>
		<title level="m">3rd Conference on Automated Knowledge Base Construction</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Core: A context-aware relation extraction method for relation completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">A</forename><surname>Sharaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurianne</forename><surname>Sitbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="836" to="849" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
	</analytic>
	<monogr>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mem2seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Learning and Motivation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">D</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Rion Snow, and Dan Jurafsky</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno>abs/1601.00770</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning from context or names? an empirical study on neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Erica: Improving entity and relation understanding for pre-trained language models via contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In OpenAI</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno>abs/1906.03158</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need. ArXiv, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wikidata: A free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2629489</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Haiyue Song, and Sadao Kurohashi. 2022. Relation extraction with weighted contrastive pre-training on distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyuan</forename><surname>Mao</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2205.08770</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sais: Supervising and augmenting intermediate steps for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zecheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/2109.12093</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Question answering on freebase via relation extraction and textual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Docred: A largescale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1906.06127</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COL-ING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COL-ING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relation classification via recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1508.01006</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
