<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixi</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalin</forename><surname>Stefanov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
							<email>abhinav@iitrpr.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Technology Ropar</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
							<email>munawar.hayat@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Datasets</term>
					<term>Deepfake Localization</term>
					<term>Detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to its high societal impact, deepfake detection is getting active attention in the computer vision community. Most deepfake detection methods rely on identity, facial attribute and adversarial perturbation based spatio-temporal modifications at the whole video or random locations, while keeping the meaning of the content intact. However, a sophisticated deepfake may contain only a small segment of video/audio manipulation, through which the meaning of the content can be, for example, completely inverted from sentiment perspective. To address this gap, we introduce a content driven audio-visual deepfake dataset, termed as Localized Audio Visual DeepFake (LAV-DF), explicitly designed for the task of learning temporal forgery localization. Specifically, the content driven audio-visual manipulations are performed at strategic locations in order to change the sentiment polarity of the whole video. Our baseline method for benchmarking the proposed dataset is a 3DCNN model, termed as Boundary Aware Temporal Forgery Detection (BA-TFD), which is guided via contrastive, boundary matching and frame classification loss functions. Our extensive quantitative analysis demonstrates the strong performance of the proposed method for both tasks of temporal forgery localization and deepfake detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Advances in computer vision and deep learning methods (e.g. Autoencoders <ref type="bibr" target="#b54">[54]</ref> and Generative Adversarial Networks <ref type="bibr" target="#b19">[19]</ref>) have enabled the creation of very realistic fake videos, known as deepfakes <ref type="bibr" target="#b0">1</ref> . There are various ways of creating deepfakes, including voice cloning <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b65">65]</ref>, face reenactment <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b63">63]</ref>, and face swapping <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b49">49]</ref>. Highly realistic deepfakes are a potential tool for spreading harmful misinformation given our increasing online presence. This success in generating high quality deepfakes has raised serious concerns about their role in shaping people's beliefs with some scholars suggesting that deepfakes are a "threat to democracy" <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b62">62]</ref>. As an example of the potentially harmful effect of deepfakes, consider the recent work <ref type="bibr" target="#b61">[61]</ref> that uses video of the former United States President Barack Obama to showcase a novel face reenactment method. In this work, the lip movements of Barack Obama are synchronised with another person's speech, resulting in high quality and realistic video in which the former president appears to say something he never did. Given the recent surge in <ref type="bibr" target="#b0">1</ref> In the text, deepfake and forgery are used interchangeably. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real Real Real Real Fake Fake</head><p>Real Video Fake Video . . . <ref type="figure">Figure 1</ref>: Content driven audio-visual manipulation. On the left is a real video with the subject saying "Vaccinations are safe". On the right is audio-visual deepfake created from the real video based on the change in perceived sentiment where "safe" is changed to "dangerous". Green-edge and rededge images are real and fake frames, respectively. Through a subtle audio-visual manipulation, the complete meaning of the video content has changed.</p><p>synthesized fake video content on the Internet, it has become increasingly important to identify deepfakes with more accurate and reliable methods. This has led to the release of several benchmark datasets <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b53">53]</ref> and methods <ref type="bibr" target="#b43">[43]</ref> for fake content detection. These fake video detection methods aim to correctly classify any given input video as either real or fake. This suggests that the major assumption behind those datasets and methods is that fake content is present in the entirety of the video/audio signal, that is, there is some form of manipulation throughout the content. And current state-of-the-art deepfake detection methods <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b66">66]</ref> achieve impressive results on this problem using the largest benchmark datasets. However, fake content might constitute only small part of an otherwise long real video, as it was initially suggested in <ref type="bibr" target="#b11">[11]</ref>. Such short modified segments have the power to completely alter the meaning and sentiment of the original real content. As an example, consider the manipulation illustrated in <ref type="figure">Figure 1</ref>. The real video might represent a person saying "Vaccinations are safe", while the fake includes only a short modified segment, for example "safe" is replaced with "dangerous". Hence, the meaning and sentiment of the fake video are significantly different from the real. This type of coordinated manipulation, if done precisely, has the potential to sway public opinion (e.g. when employed for media of a popular person as the example with Barack Obama) in a particular direction, for example, based on target sentiment polarity. Given the discussed major assumption behind current datasets and methods, the stateof-the-art deepfake detectors might not perform well on this type of manipulations.</p><p>Inspired by this idea and observation, in this paper we tackle the identified important task of detecting content altering fake segments in videos. The literature review on benchmark datasets for deepfake detection indicates that there is no dataset suitable for this task, that is, dataset that consists of content driven manipulations. Therefore, in this paper, we describe the process of creating such large-scale dataset that will enable further research in this important direction. In addition, we propose a novel multimodal method for precise prediction of the boundaries of fake segments based on visual and audio information. The main contributions of our work are as follows: <ref type="bibr" target="#b0">(1)</ref> We introduce a new large-scale public audio-visual dataset, called Localized Audio Visual DeepFake (LAV-DF), explicitly designed for the task of temporal forgery localization of content driven manipulations.</p><p>(2) We propose a new multimodal method, called Boundary Aware Temporal Forgery Detection (BA-TFD) for audio-visual temporal forgery localization of content driven manipulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Deepfake Datasets. The body of research in deepfake detection is driven by seminal datasets curated with different manipulation methods. A summary of the relevant datasets is presented in Table 1. Korshunov and Marcel <ref type="bibr" target="#b33">[33]</ref> curated one of the first deepfake datasets DF-TIMIT, where face-swapping was performed on Vid-Timit. Down the lane, other important datasets such as UADFV <ref type="bibr" target="#b68">[68]</ref>, FaceForensics++ <ref type="bibr" target="#b53">[53]</ref>, and Google DFD <ref type="bibr" target="#b48">[48]</ref> were introduced. Due to the complexity of face manipulation and limited availability of open-source face manipulation techniques, these datasets are fairly small in size <ref type="bibr" target="#b38">[38]</ref>. Facebook released a large sized dataset DFDC <ref type="bibr" target="#b16">[16]</ref> in 2020 for the task of deepfake classification. Multiple face manipulation methods were used to generate a total of 128,154 videos including real videos of 3000 actors. DFDC <ref type="bibr" target="#b16">[16]</ref> has become a mainstream benchmark dataset for the task of deepfake detection. With the progress in both audio and visual deepfake manipulation, post DFDC, several new datasets as Celeb-DF <ref type="bibr" target="#b38">[38]</ref>, DeeperForensics <ref type="bibr" target="#b28">[28]</ref>, and WildDeepFake <ref type="bibr" target="#b71">[71]</ref> were introduced. It is worth noting that all these datasets are designed for the binary task of deepfake classification and focus primarily on visual manipulation detection <ref type="bibr" target="#b11">[11]</ref>.</p><p>In 2021, OpenForensics <ref type="bibr" target="#b35">[35]</ref> dataset was introduced for spatial detection and segmentation. Bounding boxes and masks are provided for each video along with real or fake labels. The number of faces in each frame is more than one. Recently, FakeAVCeleb <ref type="bibr" target="#b31">[31]</ref> was released focusing on both face-swap and face-reenactment methods with manipulated audio and video. ForgeryNet <ref type="bibr" target="#b23">[23]</ref> is the latest contribution to the growing list of deepfake detection datasets. This large-scale dataset is also centered around video-only identity manipulation and is suitable for the tasks of video/image classification and spatial/temporal forgery localization.</p><p>To the best of our knowledge, all these previous datasets assume that face manipulation occurs in most of the frames of the video <ref type="bibr" target="#b11">[11]</ref>. Only the latest one, ForgeryNet <ref type="bibr" target="#b23">[23]</ref>, provides examples of the important problem of temporal forgery localization since it includes random face swapping applied to parts of some videos. However, the manipulations present in that dataset are only identity modifications that do not necessarily alter the meaning of the content.</p><p>Our content driven manipulation dataset LAV-DF addresses this important gap. Deepfake Detection. Deepfake detection methods draw inspiration from observations of artifacts such as different eye colors, unnatural blink and lip-sync issues in deepfake videos. These binary classification methods are based on both traditional machine learning methods (e.g. EM <ref type="bibr" target="#b21">[21]</ref> and SVM <ref type="bibr" target="#b69">[69]</ref>) and deep learning methods (e.g. 3DCNN <ref type="bibr" target="#b15">[15]</ref>, GRU <ref type="bibr" target="#b45">[45]</ref> and ViT <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b66">66]</ref>). Previous methods <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b36">36]</ref> also aim to detect temporal inconsistencies in deepfake content and recently, several audio-visual deepfake detection methods such as MDS <ref type="bibr" target="#b11">[11]</ref> and M2TR <ref type="bibr" target="#b64">[64]</ref> were proposed. The methods above are classification centric and do not focus on temporal localization. The only exception is the MDS <ref type="bibr" target="#b11">[11]</ref>, which is shown to work for localization tasks, however, the method is designed primarily for classification. The proposed LAV-DF dataset and method are specifically designed for temporal localization of manipulations. Temporal Localization. Given that the task of temporal forgery localization is similar to the task of temporal action localization, previous work in this area is important. Benchmark datasets in this domain include THUMOS <ref type="bibr" target="#b25">[25]</ref> and ActivityNet <ref type="bibr" target="#b7">[7]</ref> and the proposed methods can be grouped into two categories: 2-step structures which first generate segment proposals and then perform multiclass classification to evaluate the proposals <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b70">70</ref>] and 1-step methods which directly generate the final segment predictions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b46">46]</ref>. For the task of temporal forgery localization there are no classification requirements for the foreground segments, that is, the background is always real and the foreground segments are always fake. Therefore, boundary prediction and 1-step methods are more important and related to our task.</p><p>Bagchi et al. <ref type="bibr" target="#b1">[2]</ref> divided the methods for segment proposal estimation in temporal action localization into two main categories: methods based on anchors and methods based on predicting the boundary probabilities. As for the anchor-based, these methods mainly use sliding windows in the video, such as S-CNN <ref type="bibr" target="#b59">[59]</ref>, CDC <ref type="bibr" target="#b58">[58]</ref>, TURN-TAP <ref type="bibr" target="#b18">[18]</ref> and CTAP <ref type="bibr" target="#b17">[17]</ref>. As for the methods predicting the boundary probabilities, Lin et al. <ref type="bibr" target="#b41">[41]</ref> introduced BSN in 2018. The method can utilize the global information to overcome the problem that anchor-based methods cannot generate precise and flexible segment proposals. Based on BSN, BMN <ref type="bibr" target="#b39">[39]</ref> and BSN++ <ref type="bibr" target="#b60">[60]</ref> were introduced for improved performance. It is worth noting that all these methods are unimodal, which is not optimal for the task of temporal forgery detection. The importance of multimodality was demonstrated recently by AVFusion <ref type="bibr" target="#b1">[2]</ref>. Proposed Approach. For the task of temporal forgery detection, both the audio and visual information are important, in addition to the required precise boundary proposals. In this paper, we introduce a multimodal method based on boundary probabilities and compare the performance with BMN <ref type="bibr" target="#b39">[39]</ref>, AGT <ref type="bibr" target="#b46">[46]</ref>, MDS <ref type="bibr" target="#b11">[11]</ref> and AVFusion <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LOCALIZED AUDIO VISUAL DEEPFAKE</head><p>The proposed dataset is a large audio-visual localised deepfake dataset. The main steps in creating the dataset are: a) Sourcing real videos, b) Processing real videos to manipulate the transcripts and c) Audio and video synthesis. The deepfake generation step is based </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Voice Reenactment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Face Reenactment</head><p>Post Processing Normalization ADJ,VERB, NOUN <ref type="figure">Figure 2</ref>: The pipeline for LAV-DF dataset generation. The green-edge audio and video frames are the real data and the rededge audio and video frames are the generated fake data. The real audio based transcript is used to decide the location and content to be replaced based on the largest change in sentiment. The chosen antonyms are used as an input for generating fake audio with voice cloning. The post-processing and normalization are applied to the audio to maintain the consistency of the loudness between the generated audio and real audio in the neighbourhood. The generated audio is used as input for facial reenactment. Three categories of data are generated: &lt;Fake Audio and Fake Video&gt;, &lt;Fake Audio and Real Video&gt; and &lt;Real Audio and Fake Video&gt;. The details are discussed in Section 3.1.</p><p>on the hypothesis that changing relevant word(s) in a statement can lead to change in its perception. The same is also reflected by changing the sentiment value. The manipulation strategy is to replace word/words with its/their antonym(s), which leads to a significant change in the sentiment score of a statement. The overall pipeline of data generation is shown in <ref type="figure">Figure 2</ref>. Data Source and Parsing. We sourced the real videos from the VoxCeleb2 <ref type="bibr" target="#b12">[12]</ref> dataset. VoxCeleb2 is a facial video dataset with over 1 million utterance videos of over 6000 speakers. The videos in VoxCeleb2 are collected from YouTube and the face parts of the video are tracked and cropped by the CNN facial detector in <ref type="bibr" target="#b32">[32]</ref> with 224? 224 resolution. The original dataset contains videos of different duration, spoken language, and voice loudness. English speaking videos are chosen using the confidence score from Google Speech-to-Text service. The same service generates the transcripts, which are used for manipulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Generation</head><p>Transcript Manipulation. After collecting and wrangling the real data from VoxCeleb2, the next step is to analyse a video's transcript denoted by = { 0 , 1 , ? ? ? , , ? ? ? , }, where denotes word tokens and is the number of tokens in the transcript. The aim is to find the tokens to be replaced in such that the sentiment score of the transcript changes the most. This is essentially to create transcript ? = { 0 , 1 , ? ? ? , ? , ? ? ? , }, composed of most of the tokens of with the exception of a few tokens being replaced. These replaced tokens ? are selected from a set?of antonyms of . We used the sentiment analyzer in NLTK <ref type="bibr" target="#b2">[3]</ref> which predicts the sentiment value of a video transcript. Positive sentiment value means the sentiment in the transcript ( ) is positive, negative sentiment value means the sentiment is negative, and zero sentiment value means the sentiment is neutral. For each token in a transcript , we find the replacement as follows,</p><formula xml:id="formula_0">= argmax ? , ? ??| ( ) ? ( ? )|</formula><p>We find all the replacements in a transcript as,</p><formula xml:id="formula_1">= argmax { } =1 | ?? =1 ? ( )|</formula><p>where ? ( ) is the sentiment difference with the replacement and is the maximum number of replacements in the transcript. For videos shorter than 10 seconds, there is up to 1 replacement; otherwise, there are up to 2 replacements. <ref type="figure" target="#fig_1">Figure 3</ref> (a) illustrates the significant change in the sentiment distribution after the manipulations and <ref type="figure" target="#fig_1">Figure 3</ref> (b) presents the histogram of |? |, suggesting that the sentiment of most transcripts was successfully changed.</p><p>Audio Generation. Once the replacement tokens ? are available, the next step is to generate their corresponding audio in the style of the speaker in the video. We evaluated several recent adaptive TTS methods <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b47">47]</ref>, which can generate the speech style of a person who is not in the training dataset. Based on better performance, we chose speaker verification based multi-speaker TTS (SV2TTS) <ref type="bibr" target="#b27">[27]</ref> as the final method for audio generation. The SV2TTS method comprises of three modules: a) An encoder for extracting style embedding of the reference speaker, b) Tacotron 2 <ref type="bibr" target="#b57">[57]</ref> based spectrogram generated using the replacement tokens and the speaker style embedding as input, and c) WaveNet <ref type="bibr" target="#b50">[50]</ref> based vocoder for generating realistic audio using the spectrogram generated from the synthesizer. We used a pre-trained SV2TTS for generating the fake audio segments and later normalized the loudness of the generated audio using the real audio neighbors. Video Generation. Once the fake audio is generated, we used it as an input for generating the corresponding fake video frames. We used Wav2Lip facial reenactment <ref type="bibr" target="#b52">[52]</ref> for this task as it has been shown to have better output quality than previous methods <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b29">29]</ref>, and has better generalization and robustness to unseen scenarios. It is worth noting that newer methods that achieve better video synthesis quality are not suitable for our task. For example, AD-NeRF <ref type="bibr" target="#b22">[22]</ref> is not designed for zero-shot generation of unseen identities and ATVGNet <ref type="bibr" target="#b10">[10]</ref> reenacts the face based on a reference static image, which causes pose inconsistencies on the boundary between fake and real segments. Wav2Lip takes a reference video and target audio as input, and generates an output video in which the person in the reference video is speaking the target audio content with synced lips. We used pre-trained Wav2Lip model and up-scaled the generated fake video segment to a resolution of 224 ? 224. Finally, the generated fake audio and video segments were synchronised and used to replace the original audio and video segments corresponding to the original tokens. Based on the generation of fake audio and video, we have three variations in the final dataset (similar to <ref type="bibr" target="#b30">[30]</ref>): 1. Fake Audio and Fake Video. Both the audio and corresponding video are generated for replacement tokens. 2. Fake Audio and Real Video. Only the audio is generated for replacement tokens and the corresponding real video is lengthnormalized. 3. Real Audio and Fake Video. Only the video is generated for replacement tokens and the length of the fake video is normalized to match the real audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset Statistics</head><p>The dataset contains a total of 136,304 videos of which 36,431 are completely real and 99,873 contain fake segments, with 153 unique identities. We split the dataset into 3 identity independent subsets for training (78703 videos of 91 identities), validation (31501 videos of 31 identities), and testing (26100 videos of 31 identities).</p><p>Summary of the dataset is shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The total number of fake segments is 114,253 with duration in the range [0-1.6] seconds and average length of 0.65 seconds, where 89.26% of the segments are shorter than 1 second. The maximum video length is 20 seconds and 69.61% of the videos are shorter than 10 seconds. As for the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED METHOD</head><p>The proposed method Boundary Aware Temporal Forgery Detection (BA-TFD) is illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>. The first step of the method is to extract features from the input data = { , }, where is the video and is the audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feature Encoders</head><p>Video Encoder. The goal of the video encoder is to learn framelevel spatio-temporal features from the input video using a 3DCNN. For that purpose we designed the video encoder to take the whole video ? R ? ? ? as input, where is the number of frames, is the number of channels, and and are the height and width of the frame. The output of the are the framelevel features ? R ? , where is the features dimension. is composed of 4 blocks, each containing multiple 3D convolutional layers with kernel size 3 ? 3 ? 3 and a final max-pooling layer. Audio Encoder. The goal of the audio encoder is to learn features from the input audio using a 2DCNN. In addition, the learned audio features are temporarily aligned with the learned framelevel video features. The first step is to generate the spectrogram ? ? R ? of the audio signal in log-space, where is the temporal dimension and is the length of mel-frequency cepstrum features. In the second step, we designed the audio encoder to take the spectrogram ? as input. The output of the are the audio frame features ? R ? , where is the features dimension. is composed of multiple 2D convolutional layers with kernel size 3 ? 3 and a final max-pooling layer to reduce the temporal dimension to .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Loss Functions</head><p>Contrastive Loss. Our hypothesis is that content modification in one or more modalities will result in miss-synchronization between the modalities (i.e. video and audio) and contrastive loss has been shown <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b13">13]</ref> to be a powerful objective for similar tasks. In our method, the audio and video features learned from real videos are regarded as positive pairs = 1, and the audio and video features learned from videos with at least one modified modality are regarded as negative pairs = 0. For the positive pairs, the contrastive loss minimizes the difference between the modalities while for negative pairs, the contrastive loss keeps that margin larger than ,</p><formula xml:id="formula_2">= 1 ?? =1 2 + (1 ? ) max( ? , 0) 2 = || ? || 2</formula><p>Auxiliary Frame Classification Loss. Since we have access to the frame-level features and , we can utilize the labels and train the encoders to extract powerful and robust features that capture different deepfake artifacts. For that purpose we designed two frame-level logistic regression classifiers and using and as input. The classifiers consist of 1D convolutional layers and predict the label?as 1 (fake) or 0 (real) for each frame and each modality. The classifiers are trained with cross-entropy loss,</p><formula xml:id="formula_3">= ? 1 2 ?? ? { , } ?? =1 ?? = (?, ) (?, ) = log?+ (1 ? ) log (1 ??) = + (1 ? ) 0</formula><p>where is the number of samples in the dataset, is the number of frames, is the modality (i.e. audio or video ), specifies whether modality is modified, and 0 is the label for real videos.</p><p>Boundary Matching Loss. The ground truth boundary maps are generated following the procedure in <ref type="bibr" target="#b39">[39]</ref>. Given the fusion boundary map?, video boundary map?and audio boundary mapp redicted by the model we use mean squared error as boundary matching loss for?,?and?. The fusion boundary matching loss is,</p><formula xml:id="formula_4">= 1 ?? =1 ?? =1 ?? =1 (?? ) 2</formula><p>where, is the number of samples in the dataset, is the number of all possible proposal durations and is the number of frames. The modality boundary matching loss is similar to the frame classification loss,</p><formula xml:id="formula_5">= 1 2 ?? ? { , } ?? =1 ?? =1 ?? =1 (?? ) 2 = + (1 ? ) 0</formula><p>where, is the modality (i.e. video or audio ), specifies whether modality is modified, and 0 is the ground truth boundary map for real videos. Overall Loss. The overall loss is defined as follows,</p><formula xml:id="formula_6">= + + +</formula><p>where, , , and are weights for different losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multimodal Fusion</head><p>The predictions of and are concatenated with the features and , and used by two boundary matching layers and <ref type="bibr" target="#b39">[39]</ref>. The goal is to predict the boundary maps?? R ? and ? R ? for the video and audio, where is the number of frames and is the maximum duration of the fake segments. The fusion module, illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>, uses the?,?, and as input. For the video modality, the?, and are used to calculate the video weights ? R ? and for the audio modality, the?, and are used to calculate the audio weights ? R ? . In the final step, we perform element-wise weighted average and calculate the fusion boundary map prediction?? R ? ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=?++</head><p>where all operations are element-wise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Inference</head><p>During inference, the model uses the video and audio as input and generates a fusion boundary map?. The boundary map represents the confidence for all proposals in the video and is very dense (i.e. there are many duplicated proposals). Similar to BSN <ref type="bibr" target="#b41">[41]</ref>, we utilize post-processing with Soft Non-Maximum Suppression (S-NMS) <ref type="bibr" target="#b3">[4]</ref> to eliminate the duplicated proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We have performed extensive benchmarking of LAV-DF via several state-of-the-art methods: BMN <ref type="bibr" target="#b39">[39]</ref>, AGT <ref type="bibr" target="#b46">[46]</ref>, AVFusion <ref type="bibr" target="#b1">[2]</ref> and MDS <ref type="bibr" target="#b11">[11]</ref>. Apart from our proposed dataset LAV-DF, we also validate our method BA-TFD for classification on DFDC dataset. Dataset Preparation and Evaluation Protocol. To compare with visual-only methods, we prepare a subset of the test set where the audio-only modified data is removed which is denoted as "subset". The original test set is denoted as "full set" in the experiments. Unlike temporal action localization methods <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b46">46]</ref> that are using only average precision, we follow the protocol proposed in ForgeryNet <ref type="bibr" target="#b23">[23]</ref> and use both average precision (AP) and average recall (AR) as the evaluation metrics for the quantitative comparison. For AP, we follow the protocol of ActivityNet <ref type="bibr" target="#b7">[7]</ref> to set the IoU thresholds to 0.5, 0.75 and 0.95. For AR, as the number of fake segments is small, we set the number of proposals to smaller numbers like 100, 50, 20 and 10 with the IoU thresholds [0.5:0.05:0.95]. Our method can also be used for deepfake detection (i.e. classification) task. We use area under the curve (AUC) for evaluation of the deepfake classification task.</p><p>BA-TFD is a multimodal temporal detection method and if only the visual component is used, the method is very similar to BMN. In this work we focus on the problem of temporal localization instead of classification and only DFDC dataset is used to evaluate the classification performance of the proposed method. Implementation Details. BA-TFD is implemented in PyTorch <ref type="bibr" target="#b51">[51]</ref>. For hyperparameters, we set = 0.1, = 2, = 1, = 1 and = 0.99. For comparison, we trained BMN <ref type="bibr" target="#b39">[39]</ref>, AGT <ref type="bibr" target="#b46">[46]</ref>, AVFusion <ref type="bibr" target="#b1">[2]</ref> and MDS <ref type="bibr" target="#b11">[11]</ref> on temporal forgery localization task.</p><p>In addition, to evaluate the usefulness of the proposed method, we compare with MDS, EfficientViT <ref type="bibr" target="#b14">[14]</ref> and other methods on classification task. We followed the original settings for BMN, AGT, MDS and EfficientViT, and used encoding concatenation fusion for AVFusion. For the methods that require pre-trained features, we trained them end-to-end with trainable encoder. For comparison, we also trained BMN with I3D features <ref type="bibr" target="#b8">[8]</ref> (i.e. fixed encoder). For the models which require S-NMS post-processing, we used the validation set to search for optimal parameters for post-processing. Final evaluation and results are based on the test set.</p><p>For DFDC, we consider the whole fake video as one fake segment. For evaluation, we used 2 methods to generate the classification output for BA-TFD: (1) Using the highest confidence of the predicted segments as the confidence of the video being fake and (2) Training a MLP classifier using the confidences of predicted segments. We chose evaluation method (1) for LAV-DF and method (2) for DFDC based on better performance on the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>Quantitative Analysis. Temporal Localization: We compare our model on the LAV-DF full set with the latest methods for temporal action localization and deepfake detection. From <ref type="table" target="#tab_1">Table 2</ref>, our model achieves the best performance, which is 76.9 for AP@0.5 and 66.9 for AR@100. Unlike temporal action localization datasets such as ActivityNet, there is a single label for the fake segments, so it is reasonable that the AP score is relatively high. The multimodal MDS is not designed for temporal forgery localization tasks and predicts only fixed length segments (i.e. cannot predict the precise boundaries), hence the scores for that model are low. As for AGT and BMN, the scores are low because they are visual-only unimodal methods and cannot detect the fake segments in videos where only the audio is modified. We also evaluate the visual-only unimodal output of our model, which shows worse results than the multimodal BA-TFD and AVFusion. In addition, the results show that when the video encoder is trained with LAV-DF data, BMN performs significantly better than using I3D features.</p><p>We also evaluate those methods on the subset mentioned in Section 5. From <ref type="table" target="#tab_2">Table 3</ref>, the performance of the visual-only models is improved, for BA-TFD, the visual-only score improves from 58.55 to 83.55 (AP@0.5) and the margin between unimodal and multimodal is decreased from 18.35 to 1.65 (AP@0.5). Overall, our method still ranks first, which shows that our method has superior performance for temporal forgery detection. DeepFake Classification: We also compare our method with previous deepfake detection methods on LAV-DF and a subset of DFDC. From <ref type="table" target="#tab_4">Table 5</ref>, our model outperforms MDS and EfficientViT on LAV-DF. As for the subset of DFDC, the performance of our method (see <ref type="table" target="#tab_5">Table 6</ref>) is better than previous methods such as Meso4 <ref type="bibr" target="#b0">[1]</ref> and FWA <ref type="bibr" target="#b37">[37]</ref> and is close to MDS. The reasons of BA-TFD performing better in LAV-DF might be several. Our method BA-TFD is not designed and trained for classification task with classification loss. It is trained for temporal forgery localization and then the segment outputs are summarized as a whole video label prediction.   Therefore, the performance of our method drops as compared to the state-of-the-art classification method MDS. On the other hand, previous deepfake detection methods assume that fake videos are entirely fake, so their performance is reduced on LAV-DF. In summary, our method still performs well on classification task and has potential to reach the state-of-the-art performance. Impact of Loss Functions: To examine the contributions of each loss of BA-TFD, we train four models with different combinations of losses. From <ref type="table" target="#tab_3">Table 4</ref>, all four losses have positive influences on the performance. By observing the difference between the scores, the frame classification loss contributes the most in the model. With the frame-level label supervising the model, the encoders are trained to have a better capacity to extract the features relevant to deepfake artifacts. We visualize the distributions of video features of 10 videos in the dataset in <ref type="figure">Figure 7</ref>. From the plots, with frame classification loss and contrastive loss added, the features are more separable, which contributes to the temporal localization and improves the performance. Qualitative Analysis. We selected a real video sample and three corresponding fake videos each with manipulation in a single or both modalities. For these four videos, we visualize the boundary map outputs?,?and?in <ref type="figure">Figure 6</ref>. It is observed that the video output captures the fake segments when the video modality is manipulated. The same holds for audio. Regardless of whether the audio or video modality is modified, the fusion output captures the correct information from audio and visual outputs and shows the effectiveness of the fusion module. Failure Analysis. We observe that BA-TFD predictions can be noisy in a few cases, which contain only a short duration (? 0.5 ) video manipulations and the corresponding real audio. We argue that for such short video-only manipulations, if the visual transition from real to fake and then to real is smooth, it may lead to a noisy result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION, LIMITATION AND FUTURE WORK</head><p>In this work we investigated the problem of deepfake detection from the perspective of partial video modification. To this end, a new dataset LAV-DF is curated in which the audio and video are modified at specific locations based on the potential change in sentiment of the video content. For this problem, we also proposed a new method BA-TFD. The conducted experiments show that our method achieves better performance than previous relevant state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video-mod Audio-mod Both-mod GT Fusion Output</head><p>Video Output Audio Output <ref type="figure">Figure 6</ref>: Boundary map output visualizations. The first column illustrates the modality-wise boundary map outputs for a real video. The other columns illustrate the modality-wise boundary map ouputs of the corresponding fake videos (i.e. video based modification, audio based modification and audio-visual based modification). mod: modified and GT: ground truth.</p><p>(a) (b) (c) (d) <ref type="figure">Figure 7</ref>: Feature distribution in PCA subspace. Each point is the features of a video frame marked with real or fake.  Potential Risks and Ethical Concerns. Our work especially LAV-DF might include potential negative social impacts. As the subjects in the dataset are celebrities, the content in the dataset may be used for unethical purposes such as making fake rumours. Also, the dataset generation pipeline can be applied to create fake videos. To encounter the negative impacts, we prepared a license for public usage of the dataset and proposed the BA-TFD.</p><p>Limitations. Major limitations are: a) The audio reenactment method used in the dataset does not always generate the reference style and b) The resolution of the dataset is constrained on the basis of source videos. Future work. Major improvement in the future will be increasing LAV-DF with new token insertion, substitution and deletion of existing tokens and converting statements into questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>" 44 "</head><label>44</label><figDesc>Vaccinations are safe!" Sentiment Score: 0.Vaccinations are dangerous!" Sentiment Score: -0.48.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Summary of the LAV-DF dataset: (a) Distribution of sentiment scores, (b) Distribution of sentiment change, (c) Distribution of fake segment lengths, (d) Distribution of video lengths, (e) Proportion of fake segments, and (f) Proportion of modifications.modality modification types, the amount of the 4 types (i.e. videomodified, audio-modified, both-modified, real) are approximately equal. In most videos (62.72%) there is 1 fake segment, and in some videos (10.55%) there are 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Structure of the proposed method Boundary Aware Temporal Forgery Detection (BA-TFD). The video encoder uses raw video as input. The audio encoder uses spectrograms extracted from raw audio. ? denotes concatenation. During inference, post-processing is applied to generate segments from the output of the fusion module. The details are discussed in Section 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The structure of the fusion module. The gray block normalizes the video and audio weights predicted from the 1D convolutional layers and applies element-wise weighted average. ? denotes element-wise addition and ? denotes element-wise multiplication. BM: boundary map. For details refer to Section 4.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison of LAV-DF with previous public deepfake datasets. Cla: Classification, SL: Spatial Localization, TFL: Temporal Forgery Localization, FS: Face Swapping, and RE: ReEnactment.</figDesc><table><row><cell>Dataset</cell><cell>Year</cell><cell>Tasks</cell><cell>Manipulated</cell><cell>Method</cell><cell cols="2">#Subjects #Real</cell><cell>#Fake</cell><cell>#Total</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Modality</cell><cell>Manipulation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DF-TIMIT [33]</cell><cell>2018</cell><cell>Cla</cell><cell>V</cell><cell>FS</cell><cell>43</cell><cell>320</cell><cell>640</cell><cell>960</cell></row><row><cell>UADFV [69]</cell><cell>2019</cell><cell>Cla</cell><cell>V</cell><cell>FS</cell><cell>49</cell><cell>49</cell><cell>49</cell><cell>98</cell></row><row><cell cols="2">FaceForensics++ [53] 2019</cell><cell>Cla</cell><cell>V</cell><cell>FS/RE</cell><cell>-</cell><cell>1,000</cell><cell>4,000</cell><cell>5,000</cell></row><row><cell>Google DFD [48]</cell><cell>2019</cell><cell>Cla</cell><cell>V</cell><cell>FS</cell><cell>-</cell><cell>363</cell><cell>3,068</cell><cell>3,431</cell></row><row><cell>DFDC [16]</cell><cell>2020</cell><cell>Cla</cell><cell>AV</cell><cell>FS</cell><cell>960</cell><cell cols="3">23,654 104,500 128,154</cell></row><row><cell cols="2">DeeperForensics [28] 2020</cell><cell>Cla</cell><cell>V</cell><cell>FS</cell><cell>100</cell><cell>50,000</cell><cell>10,000</cell><cell>60,000</cell></row><row><cell>Celeb-DF [38]</cell><cell>2020</cell><cell>Cla</cell><cell>V</cell><cell>FS</cell><cell>59</cell><cell>590</cell><cell>5,639</cell><cell>6,229</cell></row><row><cell>WildDeepfake [71]</cell><cell>2021</cell><cell>Cla</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3,805</cell><cell>3,509</cell><cell>7,314</cell></row><row><cell>FakeAVCeleb [31]</cell><cell>2021</cell><cell>Cla</cell><cell>AV</cell><cell>RE</cell><cell>600+</cell><cell>570</cell><cell cols="2">25,000+ 25,500+</cell></row><row><cell>ForgeryNet [23]</cell><cell cols="2">2021 SL/TFL/Cla</cell><cell>V</cell><cell>Random FS/RE</cell><cell>5400+</cell><cell cols="3">99,630 121,617 221,247</cell></row><row><cell>LAV-DF (Ours)</cell><cell>2022</cell><cell>TFL/Cla</cell><cell>AV</cell><cell>Content driven RE</cell><cell>153</cell><cell cols="3">36,431 99,873 136,304</cell></row><row><cell>Video</cell><cell cols="4">Sentiment-Driven Text Replacement</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Audio</cell><cell></cell><cell cols="2">Speech-to-Text</cell><cell>Text Parsing</cell><cell></cell><cell cols="3">Antonym Choosing</cell></row><row><cell>Real Data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fake Data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I</cell><cell></cell><cell>II</cell><cell cols="2">III</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Temporal forgery localization results on the full set (please see Section 5) of LAV-DF. The BA-TFD (visual-only) is the output from video boundary matching layer inFigure 4, showing the performance when using only the video modality.</figDesc><table><row><cell>Model</cell><cell cols="7">AP@0.5 AP@0.75 AP@0.95 AR@100 AR@50 AR@20 AR@10</cell></row><row><cell>MDS [11]</cell><cell>12.78</cell><cell>01.62</cell><cell>00.00</cell><cell>37.88</cell><cell>36.71</cell><cell>34.39</cell><cell>32.15</cell></row><row><cell>AGT [46]</cell><cell>17.85</cell><cell>09.42</cell><cell>00.11</cell><cell>43.15</cell><cell>34.23</cell><cell>24.59</cell><cell>16.71</cell></row><row><cell>BMN [39]</cell><cell>24.01</cell><cell>07.61</cell><cell>00.07</cell><cell>53.26</cell><cell>41.24</cell><cell>31.60</cell><cell>26.93</cell></row><row><cell>BMN (I3D)</cell><cell>10.56</cell><cell>01.66</cell><cell>00.00</cell><cell>48.49</cell><cell>44.39</cell><cell>37.13</cell><cell>31.55</cell></row><row><cell>AVFusion [2]</cell><cell>65.38</cell><cell>23.89</cell><cell>00.11</cell><cell>62.98</cell><cell>59.26</cell><cell>54.80</cell><cell>52.11</cell></row><row><cell>BA-TFD (visual-only)</cell><cell>58.55</cell><cell>28.60</cell><cell>00.16</cell><cell>62.49</cell><cell>58.77</cell><cell>53.86</cell><cell>50.29</cell></row><row><cell>BA-TFD</cell><cell>76.90</cell><cell>38.50</cell><cell>00.25</cell><cell>66.90</cell><cell>64.08</cell><cell>60.77</cell><cell>58.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Temporal forgery localization results on the subset (please see Section 5) of LAV-DF. The BA-TFD (visual-only) is the output from video boundary matching layer inFigure 4, showing the performance when using only the video modality.</figDesc><table><row><cell>Model</cell><cell cols="7">AP@0.5 AP@0.75 AP@0.95 AR@100 AR@50 AR@20 AR@10</cell></row><row><cell>MDS [11]</cell><cell>23.43</cell><cell>03.48</cell><cell>00.00</cell><cell>58.53</cell><cell>56.68</cell><cell>53.16</cell><cell>49.67</cell></row><row><cell>AGT [46]</cell><cell>15.69</cell><cell>10.69</cell><cell>00.15</cell><cell>49.11</cell><cell>40.31</cell><cell>31.70</cell><cell>23.13</cell></row><row><cell>BMN [39]</cell><cell>32.32</cell><cell>11.38</cell><cell>00.14</cell><cell>59.69</cell><cell>48.17</cell><cell>39.01</cell><cell>34.17</cell></row><row><cell>BMN (I3D)</cell><cell>28.10</cell><cell>05.47</cell><cell>00.01</cell><cell>55.49</cell><cell>54.44</cell><cell>52.14</cell><cell>47.72</cell></row><row><cell>AVFusion [2]</cell><cell>62.01</cell><cell>22.77</cell><cell>00.11</cell><cell>61.98</cell><cell>58.08</cell><cell>53.31</cell><cell>50.52</cell></row><row><cell>BA-TFD (visual-only)</cell><cell>83.55</cell><cell>41.88</cell><cell>00.24</cell><cell>65.79</cell><cell>62.30</cell><cell>57.95</cell><cell>55.34</cell></row><row><cell>BA-TFD</cell><cell>85.20</cell><cell>47.06</cell><cell>00.29</cell><cell>67.34</cell><cell>64.52</cell><cell>61.19</cell><cell>59.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Temporal forgery localization results of BA-TFD with different losses.</figDesc><table><row><cell cols="9">Loss Function AP@0.5 AP@0.75 AP@0.95 AR@100 AR@50 AR@20 AR@10</cell></row><row><cell></cell><cell></cell><cell>53.16</cell><cell>11.91</cell><cell>00.02</cell><cell>53.99</cell><cell>50.94</cell><cell>47.74</cell><cell>45.55</cell></row><row><cell>,</cell><cell></cell><cell>54.70</cell><cell>15.50</cell><cell>00.04</cell><cell>56.64</cell><cell>53.57</cell><cell>49.46</cell><cell>45.85</cell></row><row><cell>,</cell><cell>,</cell><cell>76.50</cell><cell>39.92</cell><cell>00.18</cell><cell>66.69</cell><cell>63.71</cell><cell>60.07</cell><cell>57.76</cell></row><row><cell>, ,</cell><cell>,</cell><cell>76.90</cell><cell>38.50</cell><cell>00.25</cell><cell>66.90</cell><cell>64.08</cell><cell>60.77</cell><cell>58.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Classification results on the full set of LAV-DF.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell></cell><cell>LAV-DF</cell></row><row><cell>.</cell><cell>Model</cell><cell cols="3">MDS [11] EfficientViT [14] BA-TFD</cell></row><row><cell></cell><cell>AUC</cell><cell>0.828</cell><cell>0.965</cell><cell>0.990</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Classification results on a subset set of DFDC.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell>DFDC</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="5">Meso4[1] FWA[37] Siamese[44] MDS [11] BA-TFD</cell></row><row><cell>AUC</cell><cell>0.753</cell><cell>0.727</cell><cell>0.844</cell><cell>0.916</cell><cell>0.846</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MesoNet: a Compact Facial Video Forgery Detection Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darius</forename><surname>Afchar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Nozick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Echizen</surname></persName>
		</author>
		<idno type="DOI">10.1109/WIFS.2018.8630761</idno>
		<ptr target="https://doi.org/10.1109/WIFS.2018.8630761" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Workshop on Information Forensics and Security (WIFS). 1-7</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2157" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Bagchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jazib</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dolton</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi Kiran</forename><surname>Sarvadevabhatla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14118</idno>
		<idno>arXiv: 2106.14118 version: 3</idno>
		<title level="m">Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization</title>
		<imprint>
			<date type="published" when="2021-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<title level="m">Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. O&apos;Reilly Media, Inc. Google-Books-ID: KGIbfiiP1i4C</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Soft-NMS -Improving Object Detection With One Line of Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">There Are Now 15,000 Deepfake Videos on Social Media. Yes, You Should Worry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Brandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Forbes</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procedings of the British Machine Vision Conference</title>
		<meeting>edings of the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<idno type="DOI">10.5244/c.31.93</idno>
		<ptr target="https://doi.org/10.5244/c.31.93" />
		<title level="m">Publisher: British Machine Vision Association</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edresson</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Shulby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><surname>G?lge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Michael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederico</forename><surname>Santos De Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaldo Candido</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silva</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">Maria</forename><surname>Aluisio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moacir Antonelli</forename><surname>Ponti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05557</idno>
		<idno>arXiv: 2104.05557</idno>
		<title level="m">SC-GlowTTS: an Efficient Zero-Shot Multi-Speaker Text-To-Speech Model</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7832" to="7841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Not made for each other-Audio-Visual Dissonance-based Deepfake Detection and Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Komal</forename><surname>Chugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramanathan</forename><surname>Subramanian</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394171.3413700</idno>
		<ptr target="https://doi.org/10.1145/3394171.3413700" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (MM &apos;20)</title>
		<meeting>the 28th ACM International Conference on Multimedia (MM &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="439" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">VoxCeleb2: Deep Speaker Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Out of Time: Automated Lip Sync in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-54427-4_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-54427-4_19" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2016 Workshops</title>
		<editor>Chu-Song Chen, Jiwen Lu, and Kai-Kuang Ma</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="251" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Combining EfficientNet and Vision Transformers for Video Deepfake Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Coccomini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Gennaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Falchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02612</idno>
		<idno>arXiv: 2107.02612 version: 1</idno>
		<imprint>
			<date type="published" when="2021-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Oscar De Lima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreshtha</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annet</forename><surname>Karwoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14749</idno>
		<idno>arXiv: 2006.14749</idno>
		<title level="m">Deepfake Detection using Spatiotemporal Convolutional Networks</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Dolhansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Pflaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jikuo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07397</idno>
		<idno>arXiv: 2006.07397</idno>
		<title level="m">The DeepFake Detection Challenge (DFDC) Dataset</title>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CTAP: Complementary Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3628" to="3636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1145/3422622</idno>
		<ptr target="https://doi.org/10.1145/3422622" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Spatiotemporal Inconsistency Learning for DeepFake Video Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiping</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouhong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">https:/doi-org.ezproxy.lib.monash.edu.au/10.1145/3474085.3475508</idno>
		<ptr target="https://doi-org.ezproxy.lib.monash.edu.au/10.1145/3474085.3475508" />
		<imprint>
			<date type="published" when="2021" />
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="3473" to="3481" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DeepFake Detection by Analyzing Convolutional Traces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Guarnera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Giudice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastiano</forename><surname>Battiato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="666" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Jin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5784" to="5794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luchuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4360" to="4369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deepfake Detection Scheme Based on Vision Transformer and Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Jin</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Ju</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Woon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byung-Gyu</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01353</idno>
		<idno>arXiv: 2104.01353</idno>
		<imprint>
			<date type="published" when="2021-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The THUMOS Challenge on Action Recognition for Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2016.10.018</idno>
		<idno type="arXiv">arXiv:1604.06182</idno>
		<ptr target="https://doi.org/10.1016/j.cviu.2016.10.018" />
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">You Said That?: Synthesising Talking Faces from Audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-019-01150-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-019-01150-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="1767" to="1779" />
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transfer learning from speaker verification to multispeaker text-tospeech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPS&apos;18)</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems (NIPS&apos;18)<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4485" to="4495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2889" to="2898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards Automatic Face-to-Face Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K R</forename><surname>Prajwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrabha</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerin</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C V</forename><surname>Jawahar</surname></persName>
		</author>
		<idno type="DOI">10.1145/3343031.3351066</idno>
		<ptr target="https://doi.org/10.1145/3343031.3351066" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia (MM &apos;19)</title>
		<meeting>the 27th ACM International Conference on Multimedia (MM &apos;19)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1428" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasam</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minha</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahroz</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Woo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3476099.3484315</idno>
		<idno type="arXiv">arXiv:2109.02993</idno>
		<ptr target="https://doi.org/10.1145/3476099.3484315" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Synthetic Multimedia -Audiovisual Deepfake Generation and Detection</title>
		<meeting>the 1st Workshop on Synthetic Multimedia -Audiovisual Deepfake Generation and Detection</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="7" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasam</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahroz</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Woo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.05080</idno>
		<idno>arXiv: 2108.05080</idno>
		<title level="m">FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset</title>
		<imprint>
			<date type="published" when="2021-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dlib-ml: A Machine Learning Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DeepFakes: a New Threat to Face Recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Korshunov</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Marcel</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1812.08685</idno>
		<idno>arXiv: 1812.08685</idno>
	</analytic>
	<monogr>
		<title level="j">Assessment and Detection</title>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast Face-Swap Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Korshunova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni</forename><surname>Dambre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3677" to="3685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">OpenForensics: Large-Scale Challenging Dataset for Multi-Face Forgery Detection and Segmentation In-the-Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung-Nghia</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Echizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10117" to="10127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepfake Video Detection Based on Spatial, Spectral, and Temporal Inconsistencies Using Multimodal Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">K</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Imad Eddine Toubal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sandesera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zigfried</forename><surname>Lomnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calyam</forename><surname>Hampel-Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kannappan</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palaniappan</surname></persName>
		</author>
		<idno type="DOI">10.1109/AIPR50011.2020.9425167</idno>
		<ptr target="https://doi.org/10.1109/AIPR50011.2020.9425167" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Applied Imagery Pattern Recognition Workshop (AIPR). 1-9</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2332" to="5615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exposing DeepFake Videos By Detecting Face Warping Artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuezun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Celeb-DF: A Large-Scale Challenging Dataset for DeepFake Forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuezun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3207" to="3216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">BMN: Boundary-Matching Network for Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Single Shot Temporal Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3123266.3123343</idno>
		<ptr target="https://doi.org/10.1145/3123266.3123343" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia (MM &apos;17)</title>
		<meeting>the 25th ACM international conference on Multimedia (MM &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">BSN: Boundary Sensitive Network for Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-Shot Temporal Event Localization: A Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12596" to="12606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The Creation and Detection of Deepfakes: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisroel</forename><surname>Mirsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenke</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/3425780</idno>
		<idno>7:1-7:41</idno>
		<ptr target="https://doi.org/10.1145/3425780" />
	</analytic>
	<monogr>
		<title level="j">Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Emotions Don&apos;t Lie: An Audio-Visual Deepfake Detection Method using Affective Cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trisha</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394171.3413570</idno>
		<ptr target="https://doi.org/10.1145/3394171.3413570" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (MM &apos;20)</title>
		<meeting>the 28th ACM International Conference on Multimedia (MM &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2823" to="2832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deepfakes Detection With Automatic Face Weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiang</forename><surname>Daniel Mas Montserrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Yarlagadda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiting</forename><surname>Baireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janos</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Bartusiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengqing</forename><surname>Guera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Delp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="668" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Activity Graph Transformer for Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megha</forename><surname>Nawhal</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:2101.08540</idno>
		<idno>arXiv: 2101.08540</idno>
		<imprint>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paarth</forename><surname>Neekhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shehzeen</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Dubnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farinaz</forename><surname>Koushanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00151</idno>
		<idno>arXiv: 2102.00151</idno>
		<title level="m">Expressive Neural Voice Cloning</title>
		<imprint>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Contributing Data to Deepfake Detection Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dufou</forename><surname>Nick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jigsaw</forename><surname>Andrew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">FSGAN: Subject Agnostic Face Swapping and Reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosi</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7184" to="7193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<idno>arXiv: 1609.03499</idno>
		<title level="m">WaveNet: A Generative Model for Raw Audio</title>
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>Junjie Bai, and Soumith Chintala</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrabha</forename><surname>K R Prajwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394171.3413532</idno>
		<ptr target="https://doi.org/10.1145/3394171.3413532" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (MM &apos;20)</title>
		<meeting>the 28th ACM International Conference on Multimedia (MM &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="484" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">FaceForensics++: Learning to Detect Manipulated Facial Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rossler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Learning Internal Representations by Error Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; California Univ San Diego La Jolla Inst For Cognitive</forename><surname>Science</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<pubPlace>Section</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Reports</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">What are deepfakes -and how can you spot them? The Guardian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Sample</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">You thought fake news was bad? Deep fakes are where truth goes to die. The Guardian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Schwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rj</forename><surname>Skerrv-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Agiomvrgiannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2018.8461368</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2018.8461368" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2379" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5734" to="5743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Temporal Action Localization in Untrimmed Videos via Multi-Stage CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">BSN++: Complementary Boundary Regressor with Scale-Balanced Relation Modeling for Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07641</idno>
		<idno>arXiv: 2009.07641</idno>
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Neural Voice Puppetry: Audio-Driven Facial Reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58517-4_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58517-4_42" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Andrea Vedaldi, Horst Bischof, Thomas Brox; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="716" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Deepfakes: A threat to democracy or just a bit of fun? BBC News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">MoCoGAN: Decomposing Motion and Content for Video Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1526" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09770</idno>
		<idno>arXiv: 2104.09770</idno>
		<title level="m">Multimodal Multi-scale Transformers for Deepfake Detection</title>
		<imprint>
			<date type="published" when="2021-04" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10135</idno>
		<idno>arXiv: 1703.10135</idno>
		<title level="m">Tacotron: Towards End-to-End Speech Synthesis</title>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Deepfake Video Detection Using Convolutional Vision Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deressa</forename><surname>Wodajo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename><surname>Atnafu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11126</idno>
		<idno>arXiv: 2102.11126</idno>
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">G-TAD: Sub-Graph Localization for Temporal Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10156" to="10165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Exploring Temporal Preservation Networks for Precise Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohe</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Exposing Deep Fakes Using Inconsistent Head Poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuezun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2019.8683164</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2019.8683164" />
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2379" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Graph Convolutional Networks for Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bojia</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394171.3413769</idno>
		<ptr target="https://doi.org/10.1145/3394171.3413769" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (MM &apos;20)</title>
		<meeting>the 28th ACM International Conference on Multimedia (MM &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2382" to="2390" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
