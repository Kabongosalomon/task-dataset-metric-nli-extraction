<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Image-Text Embeddings with Instance Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Learning Image-Text Embeddings with Instance Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image-Sentence Retrieval</term>
					<term>Cross-Modal Retrieval</term>
					<term>Language-based Person Search</term>
					<term>Convolutional Neural Networks !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Matching images and sentences demands a fine understanding of both modalities. In this paper, we propose a new system to discriminatively embed the image and text to a shared visual-textual space. In this field, most existing works apply the ranking loss to pull the positive image / text pairs close and push the negative pairs apart from each other. However, directly deploying the ranking loss is hard for network learning, since it starts from the two heterogeneous features to build inter-modal relationship. To address this problem, we propose the instance loss which explicitly considers the intra-modal data distribution. It is based on an unsupervised assumption that each image / text group can be viewed as a class. So the network can learn the fine granularity from every image/text group. The experiment shows that the instance loss offers better weight initialization for the ranking loss, so that more discriminative embeddings can be learned. Besides, existing works usually apply the off-the-shelf features, i.e., word2vec and fixed visual feature. So in a minor contribution, this paper constructs an end-to-end dual-path convolutional network to learn the image and text representations. End-to-end learning allows the system to directly learn from the data and fully utilize the supervision. On two generic retrieval datasets (Flickr30k and MSCOCO), experiments demonstrate that our method yields competitive accuracy compared to state-of-the-art methods. Moreover, in language based person retrieval, we improve the state of the art by a large margin. The code has been made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I MAGE and text both contain very rich semantics but reside in heterogeneous modalities. Comparing to information retrieval within the same modality, matching image-text poses extra critical challenges, i.e., mapping images and text onto one shared feature space. For example, a model needs to distinguish between the "black dog", "gray dog" and "two dogs" in the text, and understand the visual differences in images depicting "black dog", "gray dog" and "two dogs". In this paper, given an unseen image (text) query, we aim to measure its semantic similarity with the text (image) instances in the database and retrieve the true matched texts (images) to the query. Considering the testing procedure, this task requires connecting the two modalities with robust representations. In the early times, some relatively small datasets were used, e.g., Wikipedia <ref type="bibr" target="#b0">[1]</ref> and Pascal Sentence <ref type="bibr" target="#b1">[2]</ref>, which contain around 3,000 and 5,000 image-text pairs, respectively. In recent years, several large-scale datasets with more than 30,000 images, including MSCOCO <ref type="bibr" target="#b2">[3]</ref> and Flickr30k <ref type="bibr" target="#b3">[4]</ref>, have been introduced. Each image in these datasets is annotated with around five sentences. These large datasets allow deep architectures to learn robust representations and provide challenging evaluation scenarios.</p><p>During the past few years, ranking loss is commonly used as the objective function <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b9">[10]</ref> for image-text representation learning. The ranking loss aims to make the distance between positive pairs smaller than that between negative pairs by a predefined margin. In image-text matching, every training pair contains <ref type="figure">Fig. 1</ref>: Motivation. We define an image / text group as an image with its associated sentences. We observe that an image / text group is more or less different from each other. Therefore, we view every image / text group as a distinct class during training, yielding the instance loss. a visual feature and a textual feature. The ranking loss focuses on the distance between the two modalities. Its potential drawback is that it does not explicitly consider the feature distribution in a single modality. For example, when using ranking loss during training which does not distinguish between the slight differences in images, then given two testing images with slightly different semantics, the model may output similar descriptors for the two images. This is clearly undesirable for image / text matching considering the extremely fine granularity of this task. In our experiment, we observe that using the ranking loss alone in endto-end training may cause the network to be stuck in a local minimum.</p><p>What motivates us is the effectiveness of class labels in earlier years of cross-media retrieval <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>. In these works, the class labels are annotated manually and during testing, the aim is to retrieve image / text belonging to the same class to arXiv:1711.05535v4 [cs.CV] 27 Jul 2021 the query. In light of this early practice, this paper explores the feasibility of "class labels" in image / text matching, which is an instance retrieval problem. Two differences exist between crossmedia retrieval on the category level <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref> and on the instance level (considered in this paper). First, the true matches are those with the same category, and those with the exact same content with the query, respectively. That is to say, instance-level retrieval has a more strict matching criteria than category-level retrieval. Second, instance-level retrieval does not assume the existence of class labels. In this field of research, only image / text pairs are utilized during training. Given the intrinsic differences between the two tasks, it is non-trivial to directly transfer the experience from using class labels in category-level retrieval to instance-level retrieval.</p><p>Without annotated class labels, how can we initiate the investigation of the underlying data structures in the image / text embedding space? In this paper, we name an image and its associated sentences an "image / text group". Our key assumption is that each "image / text" group is different from the others, and can be viewed as a distinct class (see <ref type="figure">Fig. 1</ref>). So we propose a classification loss called instance loss to classify the image / text groups. Using this unsupervised class labels as supervision, we aim to enforce the model to discriminate each two images and two sentences (from different groups). It helps to investigate the finegrained difference in single modality (intra-modal) and provides a good initialization for ranking loss which is a driving force for end-to-end retrieval representation learning. In more details, using such an unsupervised assumption, we train the network to classify every image / text group with the softmax loss. In the experiment, we show that the instance loss which classifies a large number of classes, i.e., 113,287 image / text groups on MSCOCO <ref type="bibr" target="#b2">[3]</ref>, is able to converge without any hyper-parameter tuning. Improved retrieval accuracy can be observed as a result of instance loss.</p><p>In addition, we notice in the field of image-text matching that most recent works employ off-the-shelf deep models for image feature extraction <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b21">[22]</ref>. The fine-tuning strategy commonly seen in other computer vision tasks <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref> is rarely adopted. A drawback of using off-the-shelf models is that these models are usually trained to classify objects into semantic categories <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref>. The classification models are likely to miss image details such as color, number, and environment, which may convey critical visual cues for matching images and texts. For example, a model trained on ImageNet <ref type="bibr" target="#b28">[29]</ref> can correctly classify the three images as "dog"; but it may not tell the difference between black dog and gray dog, or between one dog and two dogs. The ability to convey critical visual cues is a necessary component in instance-level image-text matching. Similar observations have been reported with regards to image captioning <ref type="bibr" target="#b29">[30]</ref>. Moreover, for the text feature, word2vec <ref type="bibr" target="#b30">[31]</ref> is a popular choice in image-text matching <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Aiming to model the context information, the word2vec model is learned through a shallow network to predict neighboring words. However, the word2vec model is initially trained on GoogleNews, which differs substantially from the text in the target dataset. As such, instead of using the off-the-shelf model, we explore the possibility of finetuning the word2vec model using image-text matching datasets.</p><p>Briefly, inspired by the effectiveness of class labels in earlytime cross-media retrieval, we propose a similar practice in imagetext matching called "instance loss". Instance loss works by providing better weight initialization for the ranking loss, thus producing more discriminative and robust image / text descrip-tions. Next, we also note that the pretrained CNN models may not meet the fine-grained requirement in image / text matching. So we construct a dual path CNN to extract image and text features directly from data rather. The network is end-to-end trainable and yields superior results to using features extracted from offthe-shelf models as input. Our contributions are summarized as follows:</p><p>? To provide better weight initialization and regularize the dual-path CNN model, we propose a large-number classification loss called instance loss. The robustness and effectiveness of instance loss are demonstrated by classifying each image / text group into one of the 113,287 classes on MSCOCO <ref type="bibr" target="#b2">[3]</ref>. <ref type="bibr">?</ref> We propose a dual-path CNN model for visual-textual embedding learning (see <ref type="figure" target="#fig_0">Fig. 2</ref>). In contrast to the commonly used RNN+CNN model using fixed CNN features, the proposed CNN+CNN structure conducts efficient and effective end-to-end fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We obtain competitive accuracy compared with the stateof-the-art image-text matching methods on three largescale datasets i.e., Flickr30k <ref type="bibr" target="#b3">[4]</ref>, MSCOCO <ref type="bibr" target="#b2">[3]</ref> and CUHK-PEDES <ref type="bibr" target="#b32">[33]</ref>.</p><p>We note that Ma et al. also apply the CNN structure for text feature learning <ref type="bibr" target="#b6">[7]</ref>. The main difference between our method and <ref type="bibr" target="#b6">[7]</ref> is two-fold. First, Ma et al. <ref type="bibr" target="#b6">[7]</ref> use the ranking loss alone. In our method, we show that the proposed instance loss can further improve the result of ranking loss. Second, in <ref type="bibr" target="#b6">[7]</ref>, four text CNN models are used to capture different semantic levels i.e., word, short phrase, long phrase and sentence. In this paper, only one text CNN model is used and the word-level input is considered. Our model uses the residual block shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, which combines low level information i.e., word, as well as high level inference to produce the final feature. In experiment ( <ref type="table" target="#tab_12">Table 1 and Table 8</ref>), we show that using on the same image CNN (VGG-19), our method (with one text CNN) is superior to <ref type="bibr" target="#b6">[7]</ref> with text model ensembles by a large margin.</p><p>The rest of this paper is organized as follows. Section 2 reviews and discusses the related works. Section 3 describes the proposed Image-Text CNN Structure in detail, followed by the objective function in Section 4. Training policy is described in Section 5. Experimental results and comparisons are discussed in Section 6 and conclusions are in Section 7. Furthermore, some qualitative results are included in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>The image-text bidirectional retrieval requires both understanding images and sentences in detail. In this section, we discuss some related works.</p><p>Deep models for image recognition. Deep models have achieved success in computer vision. The convolutional neural network (CNN) won the ILSVRC12 competition <ref type="bibr" target="#b28">[29]</ref> by a large margin <ref type="bibr" target="#b25">[26]</ref>. Later, VGGNet <ref type="bibr" target="#b26">[27]</ref> and ResNet <ref type="bibr" target="#b27">[28]</ref> further deepened the CNN and provide more insights into the network structure. In the field of image-text matching, most recent methods directly use fixed CNN features <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b21">[22]</ref> as input which are extracted from the models pre-trained on ImageNet. While it is efficient to fix the CNN features and learn a visual-textual common space, it may lose the fine-grained differences between the images. This motivates us to fine-tune the image CNN branch  <ref type="bibr" target="#b27">[28]</ref> pre-trained on ImageNet. The deep text CNN is similar to the image CNN but with different basic blocks (see <ref type="figure" target="#fig_1">Fig. 3</ref>). After the average pooling, we add one fully connected layer (input dim: 2, 048, output dim: 2, 048), one batchnorm layer, relu and one fully connected layer (input dim: 2, 048, output dim: 2, 048) in both image CNN and text CNN (We denote as fc and fc * in the figure, and the weights are not shared). Then we add a shared-weight W share classification layer (input dim: 2, 048, output dim: 29, 783). The objectives are the ranking loss and the proposed instance loss. On Flickr30k, for example, the model needs to classify 29,783 classes using instance loss.</p><p>in the image-text matching to provide for more discriminative embedding learning.</p><p>Deep models for natural language understanding. For natural language representation, word2vec <ref type="bibr" target="#b30">[31]</ref> is commonly used <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b31">[32]</ref>. This model contains two hidden layers, which learns from the context information. In the application of image-text matching, Klein et al. <ref type="bibr" target="#b16">[17]</ref> and Wang et al. <ref type="bibr" target="#b7">[8]</ref> pool word vectors extracted from the fixed word2vec model to form a sentence descriptor using Fisher vector encoding. Karpathy et al. <ref type="bibr" target="#b31">[32]</ref> also utilize fixed word vectors as word-level input. With respect to this routine, this paper proposes an equivalent scheme to fine-tuning the word2vec model, allowing the learned text representations to be adaptable to a specific task, which is, in our case, image-text matching.</p><p>Recurrent Neural Networks (RNN) are another common choice in natural language processing <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Mao et al. <ref type="bibr" target="#b15">[16]</ref> employ a RNN to generate image captions. Similarly, Nam et al. <ref type="bibr" target="#b8">[9]</ref> utilize directional LSTM <ref type="bibr" target="#b36">[37]</ref> for text encoding, yielding state-of-the-art multi-modal retrieval accuracy. Conversely, our approach is inspired by recent CNN breakthroughs on natural language understanding. For example, Gehring et al. apply CNNs to conduct machine translation, yielding competitive results and more than 9.3x speedup on the GPU <ref type="bibr" target="#b37">[38]</ref>. There are also researchers who apply layer-by-layer CNNs for efficient text analysis <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b41">[42]</ref>, obtaining competitive results in title recognition, event detection and text content matching. In this paper, in place of RNNs which are more commonly seen in image-text matching, we explore the usage of CNNs for text representation learning.</p><p>Multi-modal learning. There is a growing body of works on the interaction between multiple modalities. One line of works focus on category-level retrieval and leverage the category labels in the training set. Sharma et al. <ref type="bibr" target="#b10">[11]</ref> extend the Canonical Correlation Analysis <ref type="bibr" target="#b44">[45]</ref> (CCA) to learning class labels, and Wang et al. <ref type="bibr" target="#b11">[12]</ref> learn the shared image-text space based on coupled input with class regression. Wu et al. <ref type="bibr" target="#b12">[13]</ref> propose a bi-directional learning to rank for representation learning. In <ref type="bibr" target="#b14">[15]</ref>, Wei et al. perform CNN fine-tuning by classifying categories on the training set and report an improved performance on imagetext retrieval. Castrejon et al. deploy the multiple labels to learn the shared semantic space <ref type="bibr" target="#b45">[46]</ref>. The second line of works consider instance-level retrieval and, except for matched image-text pairs, do not provide any category label. Given a query, the retrieval objective is a specific image or related sentences. Some works apply the auto-encoder to project high-dimensional features from different modalities onto a common low-dimensional latent space <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>. In <ref type="bibr" target="#b50">[51]</ref>, Zhang et al. consider the verification loss, using a binary classifier to classify the true matches and false matches. Another line of works widely apply the ranking loss for instance-level retrieval <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b9">[10]</ref>. Karpathy et al. propose a part-topart matching approach using a global ranking objective <ref type="bibr" target="#b5">[6]</ref>. The "SPE" proposed in <ref type="bibr" target="#b7">[8]</ref> extends the ranking loss with structurepreserving constraints. SPE is similar to our work in that both works consider the intra-modal distance. Nevertheless, our work differs significantly from SPE. SPE enforces the model to rank the texts, i.e., considering the feature separability within the text modality only. In comparison, with the proposed instance loss, our method jointly discriminates the two modalities, i.e., images and their associated texts.</p><p>Briefly, we focus on instance-level retrieval and propose the instance loss, a novel contribution to the cross-modality community. It views each training image / text group as a distinct class and uses the softmax loss for model training. The assumption is unsupervised. We show that this method converges well and yields consistent improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED CNN STRUCTURE</head><p>In this paper, we propose a dual path CNN to simultaneously learn visual and textual representations in an end-to-end fashion, consisting of a deep image CNN for image input and one deep text CNN for sentence input. The entire network only contains four components, i.e., convolution, pooling, ReLU and batch normalisation. Compared to many previous methods which use off-the-shelf image CNNs <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b21">[22]</ref>, end-to-end finetuning is superior in learning representations that encode image details (see <ref type="figure" target="#fig_0">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deep Image CNN</head><p>We use ResNet-50 <ref type="bibr" target="#b27">[28]</ref> pre-trained on ImageNet <ref type="bibr" target="#b25">[26]</ref> as a basic model (the final 1000-classification layer is removed) before conducting fine-tuning for visual feature learning. Given an input image of size 224 ? 224, a forward pass of the network produces a 2, 048-dimension feature vector. Followed by this feature, we add one fully-connected layer (input dim: 2, 048, output dim: 2, 048), one batch normalization, relu and one fully-connected layer (input dim: 2, 048, output dim: 2, 048). We denote the final 2, 048-dim vector f img as the visual descriptor of the input I. The forward pass process of the CNN, which is a non-linear function, is represented by function F img (?) defined as:</p><formula xml:id="formula_0">f img = F img (I).</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep Text CNN</head><p>Text processing. Next, we describe our text processing method and the text CNN structure. Given a sentence, we first convert it into code T of size n ? d, where n is the length of the sentence, and d denotes the size of the dictionary. T is used as the input for the text CNN. We use word2vec <ref type="bibr" target="#b30">[31]</ref> as a general dictionary to filter out rare words; if a word does not appear in the word2vec dictionary (3,000,000 words), it is discarded. For Flickr30k, we eventually use d = 20, 074 words as the dictionary. Every word in Flickr30k thus can find an index l ? <ref type="bibr">[1, d]</ref> in the dictionary; for instance, a sentence of 18 words can be converted to 18 ? d matrix. The text input T can thus be formulated as:</p><formula xml:id="formula_1">T (i, j) = 1 if j = l i 0 otherwise ,<label>(2)</label></formula><p>where i ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>, j ? <ref type="bibr">[1, d]</ref>. The text CNN needs a fixed-length input. We set a fixed length 32 in this paper because about 98% sentences contain less than 32 words. If the length of the sentence is shorter than 32, we pad with zeros to the columns of T . If the length of the sentence is longer than 32, we clip the final several words. Now we obtain the 32 ? d sentence code T . We further reshape T into the 1 ? 32 ? d format, which can be considered as height, width and channel known in the image CNNs <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Position shift. We are motivated by the jittering operation in the image CNN training. For text CNN, we apply a data augmentation policy called position shift. In a baseline approach, if the sentence length n is shorter than the standard input length 32, a straightforward idea is to pad zeros at the end of the sentence, called left alignment. In the proposed position shift approach, we pad a random number of zeros at the beginning and the end of a sentence. In this manner, shift variations are contained in the text representation, so that the learned embeddings are more robust. In the experiment, we observe that position shift is of importance to the performance.</p><p>Deep text CNN. In the text CNN, filter size of the first convolution layer is 1 ? 1 ? d ? 300, which can be viewed as a lookup table. Using the first convolutional layer, a sentence is converted to the word vector as follows. Given input T of Similar with the local pattern of the images, the neighbor words in the sentence may contains important clues. The filter size in the image CNN is 3 ? 3 with height and width padding; the filter size in the text CNN is 1 ? 2 with length padding. Besides, we also use a shortcut connection, which helps to train a deep convolutional network <ref type="bibr" target="#b27">[28]</ref>. The output F(x) + x has the same size with the input x.</p><p>1 ? 32 ? d, the first convolution layer results in a tensor of size 1 ? 32 ? 300. There are two methods to initialize the first convolutional layer: 1) random initialization <ref type="bibr" target="#b51">[52]</ref>, and 2) using the d ? 300-dim matrix from word2vec for initialization. In the experiment, we observe that word2vec initialization is superior to the random initialization.</p><p>For the rest of the text CNN, similar residual blocks are used as per the image CNN (see <ref type="figure" target="#fig_1">Fig. 3</ref>). Similar to the local pattern in the image CNN, every two neighbor components may form a phrase containing content information. We set the filter size of convolution layers in basic text block to 1 ? 2. Additionally, we add the shortcut connection in the basic block, which has been demonstrated to help training deep neural networks <ref type="bibr" target="#b27">[28]</ref>. We apply basic blocks with a short connection to form the deep textual network (see <ref type="figure" target="#fig_0">Fig. 2</ref>). The number of blocks is consistent with the ResNet-50 model in the visual branch. Given a sentence matrix T , its text descriptor f text can be extract in an end-to-end manner from the text CNN F text (?):</p><formula xml:id="formula_2">f text = F text (T ).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED INSTANCE LOSS</head><p>In this paper, two types of losses are used, i.e., the standard ranking loss and the proposed instance loss. In Section 4.1, we briefly review the formulation of the ranking loss and discuss the limitation of the ranking loss. Section 4.2 describes the motivation and the formulation of the instance loss followed by a discussion. The differences between instance loss and ranking loss are discussed, and some primary experiments show the feasibility of instance loss. In Section 4.3, training convergence of the instance loss is discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ranking Loss Review</head><p>Ranking loss is a widely used objective function for retrieval problems. We use the cosine distance</p><formula xml:id="formula_3">D(f xi , f xj ) = fx i ||fx i ||2 ? fx j ||fx j ||2</formula><p>to measure the similarity between two samples, where f is the feature of a sample, and || ? || 2 denotes the L2-norm. The distance value D(f xi , f xj ) ? [?1, 1].</p><p>To effectively account for two modalities, we follow the ranking loss formulation as in some previous works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Here, I denotes the visual input, and T denotes the text input. Given a quadric input (I a , T a , I n , T n ), where I a , T a describe the same image / text group, I n , T n are negative samples, ranking loss can be written as,</p><formula xml:id="formula_4">L rank = image anchor max[0, ? ? (D(f Ia , f Ta ) ? D(f Ia , f Tn ))] + max[0, ? ? (D(f Ta , f Ia ) ? D(f Ta , f In ))] text anchor ,<label>(4)</label></formula><p>where D(?, ?) is the cosine similarity, and ? is a margin. Given an image query I a , the similarity score of the correct text matching should be higher. Similarly, if we use sentence query T a , we expect the correct image content should be ranked higher. Ranking loss explicitly builds the relationship between the image and text.</p><p>Limitations of ranking loss. Although widely used, ranking loss has a potential drawback for the application of image-text matching. According to Eq. 4, every pair contains a visual feature and a textual feature. The ranking loss focuses on the distance between the two modalities. So the potential drawback is that the ranking loss does not explicitly consider the feature distribution in a single modality. For instance, given two testing images with slightly different semantics, the model may output similar features. It is clearly undesirable for the extremely fine granularity of this task. In the experiment, using ranking loss alone is prone to get stuck in a local minimum (as to be shown in <ref type="figure" target="#fig_3">Fig. 5</ref> and <ref type="table" target="#tab_3">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Instance Loss</head><p>Motivation. Some early works use coarse-grain category i.e., art, biology, and sport, as the training supervision <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The multi-class classification loss has shown a good performance. But for instance-level retrieval, the classification loss has not been used. There may be two reasons. First, the category-level annotations are missing for most large-scale datasets. Second, if we use the category to train the model, it forces different instances, i.e., black dog, and white dogs, to the same class. It may compromise the CNN to learn the fine-grained difference.</p><p>In this paper, we propose the instance loss for instance-level image-text matching. We define an image and its related text descriptions as an image / text group. In specific applications such as language-based person retrieval <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b52">[53]</ref>, an image / text group is defined as images and their descriptions which depict the same person (see <ref type="figure">Fig. 7</ref>). Based on image / text groups, our assumption is that each image / text group is distinct (duplicates have been removed in the datasets). Under such assumption, we view each image / text group as a class. So in essence, instance loss is a softmax loss which classifies an image / text group into one of a large number of classes. We want the trained model can tell the difference between every two images as well as every two sentences (from different groups). Formally, we define instance loss below. Formulation. For two modalities, we formulate two classification objectives as follows,</p><formula xml:id="formula_5">P visual = sof tmax(W T share f img ), (5) L visual = ? log(P visual (c)),<label>(6)</label></formula><formula xml:id="formula_6">P textual = sof tmax(W T share f text ), (7) L textual = ? log(P text (c)),<label>(8)</label></formula><p>where f img and f text are image and text features defined in Eq. 1 and Eq. 3, respectively. W share is the parameter of the final fully connected layer <ref type="figure" target="#fig_0">(Fig. 2)</ref>. It can be viewed as concatenated weights W share = [W 1 , W 2 , ..., W 29783 ]. Every weight W i is a 2048-dim vector. L denotes the loss and P denotes the probability over all classes. P (c) is the predicted possibility of the right class c. Here we enforce shared weight W share in the final fully connected layer for the two modalities, because otherwise the learned image and text features may exist in totally different subspaces.</p><p>As to be described in Section 5, in the first training stage, the ranking loss is not used. We only use the instance loss; in the second training stage, both losses are used. The final loss function is a combination of the ranking loss and the instance loss, defined as,</p><formula xml:id="formula_7">L = ? 1 L rank + ? 2 L visual + ? 3 L textual ,<label>(9)</label></formula><p>where ? 1 , ? 2 , ? 3 are predefined weights for different losses.</p><p>Discussion. First, we show that instance loss provides better weight initialization than the ImageNet pretrained model. To prove this, we compare the image features from the off-the-self model pre-trained on ImageNet and the model trained with instance loss. Since the proposed instance loss explicitly considers the intramodal distance, we observe that the feature correlation between two images is smaller after training with the instance loss (see <ref type="figure" target="#fig_2">Fig. 4(b)</ref>). In fact, the instance loss encourages the model to find the fine-grained image details such as ball, stick, and frisbee to discriminate between image / text groups with similar semantics. We visualize the dog retrieval results in <ref type="figure">Fig. 10</ref>. Our model can be well generalized to the test set and still sensitive to the subtle differences.</p><p>Second, we provide an example of two classes to describe the working mechanism of instance loss <ref type="figure" target="#fig_4">(Fig. 6</ref>). W share =  [W 1 , W 2 ]. Given image x 1 which belongs to the first class, the softmax loss function informs the constraint of W T 1 f x1 &gt; W T 2 f x1 . Similarly, if y 1 is an input sentence belonging to the first class, the softmax loss will lead to the constraint of W T 1 f y1 &gt; W T 2 f y1 . The decision boundary indicates equal probability to be classified into the first class and the second class. Since the image and text embedding networks share the same final weight W share , the features of the same image / text group will be close to each other in the embedding space; the data points from different image / text groups will be pushed away from each other. Therefore, after training with the instance loss, the data points will usually locate on the either side of the decision boundary. In this manner, the image / text groups can be separated in the feature space despite of the fine-grained differences among them. This property, as shown in the <ref type="figure" target="#fig_4">Fig. 6 (right)</ref>, will provide better weight initialization for the subsequent training with both the ranking loss and instance loss.</p><p>Third, we demonstrate that using the instance loss alone can lead to a decent initialization. To validate this point, we plot the distribution P of the intra-modal intra-class similarity D p = D(f xi , f yi ) and the distribution Q of the intra-modal inter-class similarity D n = D(f xi , f yj )(j = i) on Flickr30k validation set ( <ref type="figure" target="#fig_3">Fig. 5(b)</ref>). We observe that, using instance loss alone, in most cases, leads to D p &gt; D n by a margin. The mean of D p equals to 0.2405 while the mean of D n is 0.0237.</p><p>Fourth, using the ranking loss alone achieves a relatively large margin between the positive pairs and negative pairs but there also exist many "hard" negative pairs ( <ref type="figure" target="#fig_3">Fig. 5(a)</ref>). These "hard" negative pairs usually have a high similarity which compromises the matching performance of the true matches. Due to the potential drawback of the ranking loss mentioned in Section 4.1, the image / text with slightly difference may have the similar feature, which result in the "hard" negative samples. To quantitatively compare the three models, we propose a simple indicator function, To the other extreme, S = 0 indicates that the positive pairs and negative pairs are perfectly separable: all the similarity scores of the positive pairs are larger than the similarity scores of the negative pairs. In this best case, the retrieval precision and recall are both 100%. Therefore, a lower indicator score S indicates a better retrieval system. In our experiment ( <ref type="figure" target="#fig_3">Fig. 5</ref>), the indicator scores of the three models are S rank = 0.2563, S instance = 0.1633 and S f ull = 0.0914, respectively. It clearly demonstrates that in terms of the extent of feature separability: "Full Model" &gt; "Using Instance Loss Alone" &gt; "Using Ranking loss Alone". With the indicator function, we quantitatively show that using ranking loss alone produces more hard negative pairs than the proposed two competing methods, which compromises the matching performance of the ranking loss. In comparison, using instance loss alone produces a smaller S value, suggesting a better feature separability of the trained model. Importantly, when the two losses, i.e., ranking loss and instance loss, are combined, our full model has the smallest S value, indicating the fewest hard negative samples and the best retrieval accuracy among the three methods.</p><formula xml:id="formula_8">S = 1 ?1 min(P (x), Q(x))dx,<label>(10)</label></formula><p>For the retrieval performance, using the instance loss alone can lead to a competitive accuracy in the experiment ( <ref type="table" target="#tab_3">Table 2</ref>). The effect of the instance loss is two-fold. In the first training stage, when used alone, it pre-trains the text CNN and fine-tunes the two fully-connected layers (and one batchnorm layer) of image CNN so that ranking loss can arrive at a better optimization for both modalities in the second stage <ref type="figure" target="#fig_4">(Fig. 6</ref>). In the second training stage, when used together with ranking loss, it exhibits a regularization effect on the ranking loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Convergence of Instance Loss</head><p>The instance loss views every image / text group as a class, so the number of training classes is usually large. For instance, we have 29,783 classes when training on Flickr30k. In <ref type="figure" target="#fig_6">Fig. 8</ref>, we show the training error curves of the image CNN and text CNN during training. We observe that the image CNN converges faster ( <ref type="figure" target="#fig_6">Fig.  8(a)</ref>) because the image CNN is pretrained on ImageNet. Text CNN converges more slowly because most part of it is trained from scratch, but it still begins to learn something after 20 epochs, and finally converges after 240 epochs.</p><p>On the other hand, the convergence property is evidenced by some previous works. To our knowledge, some practices also suffer from limited data per class, because manually annotating data is usually expensive. For example, in person re-ID, CUHK03 dataset [54] has 9.6 training samples per class; VIPeR dataset <ref type="bibr" target="#b54">[55]</ref> has 2 training samples per class. The previous works <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref> on CUHK03 and VIPeR show that the CNN classification model can be well trained as long as each class has more than a couple of training samples. In our case, there are usually 6 positive training samples per class (1 image and 5 sentences). In the experiment, despite of the limited training data, the learned model has a good generalization ability on the validation set and test set, which accords with existing experience <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A TWO-STAGE TRAINING PROCEDURE</head><p>We describe the training policy in this section. We split the training procedure into two stages. In the experiment, we show this policy helps the training.</p><p>Stage I: In this stage, we fix the pre-trained weights in the image CNN and use the proposed instance loss to tune the remaining part. The main reason is that most weights of the text CNN are learned from scratch. If we train the image and text CNNs simultaneously, the text CNN may compromise the pretrained image CNN. We only use the proposed instance loss in this stage (? 1 = 0, ? 2 = 1, ? 3 = 1). It can provide a good initialization for the ranking loss. We note that even after Stage I, our network can achieve competitive results compared to previous works using off-the-shelf CNNs.</p><p>Stage II: After Stage I converges, we start Stage II for endto-end fine-tuning of the entire network. Note that the weights of the image CNN are also fine-tuned. In this stage, we combine the instance loss with the ranking loss (? 1 = 1, ? 2 = 1, ? 3 = 1), so that both classification and ranking errors are considered. In Section 6.4, we study the mechanism of the two losses. It can be observed that in Stage II, instance loss and ranking loss are complementary, thus further improving the retrieval result. Instance loss still regularizes the model and provides more attentions to discriminate the images and sentences. After Stage II (end-to-end fine-tuning), another round of performance improvement can be observed, and we achieve even more competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENT</head><p>We first introduce the three large-scale image-text retrieval datasets, i.e., Flickr30k, MSCOCO and CUHK-PEDES, followed by the evaluation metric in Section 6.1. Then Section 6.2 describes the implementation details and the reproducibility. We discuss the comparison with state of the art and mechanism study in Section 6.3 and Section 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>Flickr30k <ref type="bibr" target="#b3">[4]</ref> is one of the large-scale image captioning datasets. It contains 31,783 images collected from Flickr, in which every image is annotated with five text descriptions. The average sentence length is 10.5 words after removing rare words. We follow the protocol in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b61">[62]</ref> to split the dataset into 1,000 test images, 1,000 validation images, and 29,783 training images.</p><p>MSCOCO <ref type="bibr" target="#b2">[3]</ref> contains 123,287 images and 616,767 descriptions. Every images contains roughly 5 text descriptions on average. The average length of captions is 8.7 after rare word removal. Following the protocol in <ref type="bibr" target="#b31">[32]</ref>, we randomly select 5,000 images as test data and 5,000 images as validation data. The remaining 113,287 images are used as training data. The evaluation is reported on 1K test images (5 fold) and 5K test images.</p><p>CUHK-PEDES <ref type="bibr" target="#b32">[33]</ref> collects images from many different person re-identification datasets. It contains 40,206 images from 13,003 different pedestrians and 80,440 descriptions. On average, each person has 3.1 images, and each image has 2 sentences. The average sentence length is 19.6 words after we remove rare words. We follow the protocol in <ref type="bibr" target="#b32">[33]</ref>, selecting the last 1,000 persons for evaluation. There are 3,074 test images with 6,156 captions, 3,078 validation images with 6,158 captions, and 34,054 training images with 68,126 captions.</p><p>Evaluation Metric We use two evaluation metrics i.e., Re-call@K and Median Rank. Recall@K is the possibility that the true match appears in the top K of the rank list, where a higher score is better. Median Rank is the median rank of the closest ground truth result in the rank list, with a lower index being better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Implementation Details</head><p>The model is trained by stochastic gradient descent (SGD) with momentum fixed to 0.9 for weight update. While training, the images are resized to 224 ? 224 pixels which are randomly cropped from images whose shorter size is 256. We also perform simple data augmentation such as horizontal flipping. For training text input, we conduct position shift (Section 3.2) as data augmentation. Dropout is applied to both CNNs, and the dropout rate is 0.75. For Flickr30k and MSCOCO, we set the max text length to 32; for CUHK-PEDES, we set the max text length to 56, since most sentences are longer.</p><p>In the first training stage, we fixed the pre-trained image CNN, and train the text CNN only. The learning rate is 0.001. We stop training when instance loss converges. In the second stage, we combine the ranking loss as Eq. 9 (the margin ? = 1) and finetune the entire network.</p><p>When testing, we can use the trained image CNN and trained text CNN separately. We extract the image feature f img by image CNN and the text feature f text by text CNN. We use the cosine distance to evaluate the similarity between the query and candidate images/sentences. It is consistent with the similarity used in the ranking loss objective. The final retrieval result is based on the similarity ranking. We also conduct the horizontal flipping when testing and use the average features (no flip and flip) as the image feature.</p><p>Reproducibility. Our source code is available online 1 . The implementation is based on the Matconvnet package <ref type="bibr" target="#b62">[63]</ref>. Since the entire network only uses four components i.e., convolution,  1: Method comparisons on Flickr30k. "Image Query" denotes using an image as query to search for the relavant sentences, and 'Text Query' denotes using a sentence to find the relevant image. R@K is Recall@K (higher is better). Med r is the median rank (lower is better). "ft" means fine-tuning. ? : Text CNN structure is similar to the image CNN, illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. pooling, ReLU and batch normalization, it can be easily modified to other deep learning packages. Training Time The image CNN (ResNet-50) in our method uses ?119 ms per image batch (batch size = 32) on an Nvidia 1080Ti GPU. The text CNN (similar ResNet-50) also uses ?117 ms per sentence batch (batch size = 32). Therefore, the image feature and text feature can be simultaneously calculated. Although our implementation is sequential, the model can run in a parallel style efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison with State of the Art</head><p>We first compare our method with the state-of-the-art methods on the three datasets, i.e., Flickr30k, MSCOCO, and CUHK-PEDES. The compared methods include recent models on the bidirectional image and sentence retrieval. For a fair comparison, we present the results based on different image CNN structures, i.e., VGGNet <ref type="bibr" target="#b26">[27]</ref> and ResNet <ref type="bibr" target="#b27">[28]</ref>. We also summarise the visual and textual embeddings used in these works in <ref type="table" target="#tab_12">Table 1 and Table 8</ref>. Extensive results are shown in <ref type="table">Table 1</ref>, <ref type="table" target="#tab_12">Table 8</ref>, and   <ref type="bibr" target="#b63">[64]</ref>, our result is also competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Mechanism Study</head><p>The effect of Stage 1 training. We replace the instance loss with the ranking loss at the first stage when fixing the image CNN. As shown in <ref type="table" target="#tab_3">Table 2</ref>, the performance is limited. As discussed in Section 4.2, ranking loss focuses on inter-modal distance. It may be hard to tune the visual and textual features simultaneously at the beginning. As we expected, instance loss performs better, which focuses more on learning intra-modal discriminative descriptors.</p><p>Two losses can works together. In Stage II, the experiment on the validation set verifies that two losses can work together to improve the final retrieval result (see <ref type="table" target="#tab_3">Table 2</ref>). Compared with models using only ranking loss or instance loss, the model with two losses provides for higher performance. In the second stage, instance loss does help to regularize the model.</p><p>End-to-end fine-tuning helps. In Stage II, we fine-tune the entire network. For the two general object datasets Flickr30k and MSCOCO, fine-tuning the whole network can improve the rank-1 accuracy by approximately 10% (see <ref type="table">Table.</ref>    Do we really need so many classes? For instance loss, the number of classes is usually large. Is it possible to use fewer classes? We implement the pseudo-category method by k-means clustering on MSCOCO, since MSCOCO has most images (classes). We use pool5 feature of ResNet50 pretrained on ImageNet to cluster 3, 000 and 10, 000 categories by Kmeans. The clustering results are used as the pseudo label for the images to conduct classification. Although clustering can decrease the number of training classes and add the samples per classes, different instances are forced to be of the same class and details may be lost (black / gray dog, two dogs), which compromises the accuracy. The retrieval result with k-classes on MSCOCO is shown in <ref type="table" target="#tab_6">Table 4</ref>. It shows that the strategy is inferior to the instance loss.</p><p>Deeper Text CNN does not improve the performance Several previous works report the Text CNN may not improve the result when the network is very deep <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>. It is different with the observation in the image recognition <ref type="bibr" target="#b27">[28]</ref>. In our experiment, we also observe a similar result when deepening the Text CNN on Flickr30k and MSCOCO. Deeper Text CNN does not significantly improve the result (see <ref type="table" target="#tab_5">Table 3</ref>).</p><p>Word2vec initialization helps. We compare the result using the word2vec initialization or random initialization <ref type="bibr" target="#b51">[52]</ref> for the first convolution layer of text CNN. Note that we remove the words, which have not appeared in the training set, in the training     data as well as dictionary. So the weight of first convolution layer is d ? 300 instead of 3, 000, 000 ? 300. d is the dictionary size. When testing, the missing words in the dictionary will also be removed in advance. As shown in <ref type="table">Table.</ref> 5, it can be observed that using word2vec initialization outperforms by 1% to 2% compared to the random initialization. Although word2vec is not trained on the target dataset, it still serves as a proper initialization for text CNN.</p><p>Position shift vs. Left alignment: Text CNN has a fixedlength input. As discussed in Section 3.2, left alignment is to pad zeros at the end of text input (like aligning the whole sentence left), if the length of the sentence is shorter than 32. Position shift is to add zeros at the end of text input as well as the begining of the input. We conduct the position shift online when reading data from the disk. We do the experiment on Flickr30k validation set. As shown in <ref type="table" target="#tab_9">Table 6</ref>, the model using position shift outperforms the one using left alignment ? 5%. Position shift serves as a significant data augmentation method for text feature learning.</p><p>In <ref type="figure" target="#fig_7">Fig. 9</ref> and <ref type="figure">Fig. 10</ref>, we present some visual retrieval results on CUHK-PEDES and Flickr30k, respectively. Our method returns reasonable rank lists. (More qualitative results can be found in Appendix.)</p><p>Does Text CNN learn discriminative words? The text CNN is supposed to convey the necessary textual information for image-text matching. To examine whether the text CNN discovers discriminative words, we fix the visual feature. For text input, we remove one word from the sentence each time. If we remove a discriminative word, the matching confidence will drop. In this way, we can determine the learned importance of different words.</p><p>The proposed model learns discriminative words. As show in <ref type="figure">Fig. 11</ref>, we observe that the words which convey the objective/colour information, i.e., basketball, swing, purple, are usually discriminative. If we remove these words, the matching confidence drops. Conversely, the conjunctions, i.e., with, on, at, in, after being removed, have a small impact on the matching confidence.  is Recall@K (high is good). Med r is the median rank (low is good). 1K test images denotes using five non-overlap splits of 5K images to conduct retrieval evaluation and report the average result. 5K test images means using all images and texts to perform retrieval. ft means fine-tuning. ? : Text CNN structure is similar to the image CNN, illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we propose the instance loss for image-text retrieval. It is based on an unsupervised assumption that every image/test group can be viewed as one class. The experiment shows instance loss can provide a proper initialization for ranking loss and further regularize the training. As a minor contribution, we propose a dual-path CNN to conduct end-to-end training on both image and text branches. The proposed method achieves competitive results on two generic retrieval datasets Flickr30k and MSCOCO. Furthermore, we arrive a +18% improvement on the person retrieval dataset CUHK-PEDES. Our code has been made publicly available. Additional examples can be found in Appendix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>Additional examples of image-text bidirectional retrieval can be found in <ref type="figure" target="#fig_0">Fig. 12, Fig. 13, Fig. 14 and Fig.15</ref>. The true matches are in green.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>We learn the image and text representations by two convolutional neural networks, i.e., deep image CNN (top) and deep text CNN (bottom). The deep image CNN is a ResNet-50 model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The basic block of deep image CNN and deep text CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>We extract image features (2,048-dim) from a randomly selected 100 images in the Flickr30k validation set, using the ImageNet pre-trained ResNet-50 model and our model (after Stage I), respectively. We visualize the 100 ? 100 Pearson's correlation. Lower Pearson's correlation between two features indicates higher orthogonality. The instance loss encourages the model to learn the difference between images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>The similarity (cosine distance) distribution of the positive pairs P and negative pairs Q on Flickr30k validation dataset. We show the result obtained by (a) using ranking loss alone, (b) using instance loss alone and (c) full model (instance loss + ranking loss), respectively. Indicator S is calculated as the overlapping area between P and Q (defined in Section 4.2, lower is better). Through comparing their S values, the performance of the three methods is: "Full Model" &gt; "Using Instance Loss Alone" &gt; "Using Ranking Loss Alone".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Geometric Interpretation. The proposed instance loss leads to a decent weight initialization for ranking loss + instance loss in Stage II.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>which encodes the overlapping area of P and Q over the range of cosine similarity [?1, 1]. Indicator S ? [0, 1]. The smaller S is, the better the positive pairs and negative pairs are separated, and thus the better retrieval performance. S = 1 indicates the case where the two distributions, P and Q are completely overlapping. Under this worst case, the positive pairs cannot be distinguished from the negative ones, and the retrieval performance is random.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Classification error curves when training on Flickr30k. The image CNN (a) and text CNN (b) converge well with 29,783 training classes (image / text groups).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Qualitative image search results using text query. The results are sorted from left to right according to their confidence. The images in green boxes are the true matches, and the images in red boxes are the false matches. In the last row, the rank-1 woman also wears a blue shirt, a pair of blue jeans and a pair of white shoes. The model outputs reasonable false matches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :Fig. 11 :</head><label>1011</label><figDesc>Qualitative description search results using image query on Flickr30k. Below each image we show the top five retrieval sentences (there are 5,000 candidate sentences in the gallery) in descending confidence. Here we select four black and white dogs as our query. Except for the main object (dog), we show the model can correctly recognize environment and small object. The sentences in green are the true matches, and the descriptions in red are the false matches. Note that some general descriptions are also reasonable. (Best viewed when zoomed in.) Finding discriminative words on Flickr30k test set. Top-3 discriminative words are in red. Some words, which are not in word2vec vocabulary, are removed in advance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 :Fig. 13 :</head><label>1213</label><figDesc>Additional examples of image search (using text queries) on Flickr30k. Top-5 results are sorted from left to right according to their confidence. True matches are in green. Additional examples of image search (using text queries) on MSCOCO. Top-5 results are sorted from left to right according to their confidence. True matches are in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 :</head><label>14</label><figDesc>Additional examples of text search (using image queries) on Flickr30k. Under each query image, we show the top five retrieved sentences in descending confidence. The descriptions in green are true matches, and the sentences in red are false matches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 :</head><label>15</label><figDesc>Additional examples of text search (using image queries) on MSCOCO. Under each image, we show the top five retrieval sentences in descending confidence. The descriptions in green are true matches, and the sentences in red are false matches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Embedding Fig. 7: Sample images in the three datasets. For the MSCOCO and Flickr30k datasets, we view every image and its captions as an image / text group. For CUHK-PEDES, we view every identity (with several images and captions) as a class.</figDesc><table><row><cell>Method</cell><cell>Visual</cell><cell>Textual</cell><cell cols="8">Image Query R@1 R@5 R@10 Med R@1 R@5 R@10 Med r Text Query</cell></row><row><cell>DeVise [5]</cell><cell>ft AlexNet</cell><cell>ft skip-gram</cell><cell cols="3">4.5 18.1 29.2</cell><cell>26</cell><cell cols="3">6.7 21.9 32.7</cell><cell>25</cell></row><row><cell>Deep Fragment [6]</cell><cell>ft RCNN</cell><cell cols="4">fixed word vector from [58] 16.4 40.2 54.7</cell><cell>8</cell><cell cols="3">10.3 31.4 44.5</cell><cell>13</cell></row><row><cell>DCCA [59]</cell><cell>ft AlexNet</cell><cell>TF-IDF</cell><cell cols="3">16.7 39.3 52.9</cell><cell>8</cell><cell cols="3">12.6 31.0 43.0</cell><cell>15</cell></row><row><cell>DVSA [32]</cell><cell>ft RCNN (init. on Detection)</cell><cell>w2v + ft RNN</cell><cell cols="3">22.2 48.2 61.4</cell><cell cols="4">4.8 15.2 37.7 50.5</cell><cell>9.2</cell></row><row><cell>LRCN [60]</cell><cell>ft VGG-16</cell><cell>ft RNN</cell><cell cols="3">23.6 46.6 58.3</cell><cell>7</cell><cell cols="3">17.5 40.3 50.8</cell><cell>9</cell></row><row><cell>m-CNN [7]</cell><cell>ft VGG-19</cell><cell>4 ? ft CNN</cell><cell cols="3">33.6 64.1 74.9</cell><cell>3</cell><cell cols="3">26.2 56.3 69.6</cell><cell>4</cell></row><row><cell>VQA-A [18]</cell><cell>fixed VGG-19</cell><cell>ft RNN</cell><cell cols="3">33.9 62.5 74.5</cell><cell>-</cell><cell cols="3">24.9 52.6 64.8</cell><cell>-</cell></row><row><cell>GMM-FV [17]</cell><cell>fixed VGG-16</cell><cell>w2v + GMM + HGLMM</cell><cell cols="3">35.0 62.0 73.8</cell><cell>3</cell><cell cols="3">25.0 52.7 66.0</cell><cell>5</cell></row><row><cell>m-RNN [16]</cell><cell>fixed VGG-16</cell><cell>ft RNN</cell><cell cols="3">35.4 63.8 73.7</cell><cell>3</cell><cell cols="3">22.8 50.7 63.1</cell><cell>5</cell></row><row><cell>RNN-FV [19]</cell><cell>fixed VGG-19</cell><cell>feature from [17]</cell><cell cols="3">35.6 62.5 74.2</cell><cell>3</cell><cell cols="3">27.4 55.9 70.0</cell><cell>4</cell></row><row><cell>HM-LSTM [21]</cell><cell>fixed RCNN from [32]</cell><cell>w2v + ft RNN</cell><cell>38.1</cell><cell>-</cell><cell>76.5</cell><cell>3</cell><cell>27.7</cell><cell>-</cell><cell>68.8</cell><cell>4</cell></row><row><cell>SPE [8]</cell><cell>fixed VGG-19</cell><cell>w2v + HGLMM</cell><cell cols="3">40.3 68.9 79.9</cell><cell>-</cell><cell cols="3">29.7 60.1 72.1</cell><cell>-</cell></row><row><cell>sm-LSTM [20]</cell><cell>fixed VGG-19</cell><cell>ft RNN</cell><cell cols="3">42.5 71.9 81.5</cell><cell>2</cell><cell cols="3">30.2 60.4 72.3</cell><cell>3</cell></row><row><cell>RRF-Net [61]</cell><cell>fixed ResNet-152</cell><cell>w2v + HGLMM</cell><cell cols="3">47.6 77.4 87.1</cell><cell>-</cell><cell cols="3">35.4 68.3 79.9</cell><cell>-</cell></row><row><cell>2WayNet [49]</cell><cell>fixed VGG-16</cell><cell>feature from [17]</cell><cell cols="2">49.8 67.5</cell><cell>-</cell><cell>-</cell><cell cols="2">36.0 55.6</cell><cell>-</cell><cell>-</cell></row><row><cell>DAN (VGG-19) [9]</cell><cell>fixed VGG-19</cell><cell>ft RNN</cell><cell cols="3">41.4 73.5 82.5</cell><cell>2</cell><cell cols="3">31.8 61.7 72.5</cell><cell>3</cell></row><row><cell>DAN (ResNet-152) [9]</cell><cell>fixed ResNet-152</cell><cell>ft RNN</cell><cell cols="3">55.0 81.8 89.0</cell><cell>1</cell><cell cols="3">39.4 69.2 79.1</cell><cell>2</cell></row><row><cell>Ours (VGG-19) Stage I</cell><cell>fixed VGG-19</cell><cell>ft ResNet-50  ? (w2v init.)</cell><cell cols="3">37.5 66.0 75.6</cell><cell>3</cell><cell cols="3">27.2 55.4 67.6</cell><cell>4</cell></row><row><cell>Ours (VGG-19) Stage II</cell><cell>ft VGG-19</cell><cell>ft ResNet-50  ? (w2v init.)</cell><cell cols="3">47.6 77.3 87.1</cell><cell>2</cell><cell cols="3">35.3 66.6 78.2</cell><cell>3</cell></row><row><cell>Ours (ResNet-50) Stage I</cell><cell>fixed ResNet-50</cell><cell>ft ResNet-50  ? (w2v init.)</cell><cell cols="3">41.2 69.7 78.9</cell><cell>2</cell><cell cols="3">28.6 56.2 67.8</cell><cell>4</cell></row><row><cell>Ours (ResNet-50) Stage II</cell><cell>ft ResNet-50</cell><cell>ft ResNet-50  ? (w2v init.)</cell><cell cols="3">53.9 80.9 89.9</cell><cell>1</cell><cell cols="3">39.2 69.8 80.8</cell><cell>2</cell></row><row><cell>Ours (ResNet-152) Stage I</cell><cell>fixed ResNet-152</cell><cell cols="4">ft ResNet-152  ? (w2v init.) 44.2 70.2 79.7</cell><cell>2</cell><cell cols="3">30.7 59.2 70.8</cell><cell>4</cell></row><row><cell>Ours (ResNet-152) Stage II</cell><cell>ft ResNet-152</cell><cell cols="4">ft ResNet-152  ? (w2v init.) 55.6 81.9 89.5</cell><cell>1</cell><cell cols="3">39.1 69.2 80.9</cell><cell>2</cell></row></table><note>1. https://github.com/layumi/Image-Text-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 7 ,</head><label>7</label><figDesc>respectively. On Flickr30k, we achieve competitive results with state-of-theart DAN<ref type="bibr" target="#b8">[9]</ref>: Recall@1 = 55.6%, Med r = 1 using image queries, and Recall@1 = 39.1%, Med r = 2 using text queries. While both</figDesc><table><row><cell>Method</cell><cell>Stage</cell><cell cols="4">Image Query R@1 R@10 R@1 R@10 Text Query</cell></row><row><cell>Only Ranking Loss</cell><cell>I</cell><cell>6.1</cell><cell>27.3</cell><cell>4.9</cell><cell>27.8</cell></row><row><cell>Only Instance Loss</cell><cell>I</cell><cell>39.9</cell><cell>79.1</cell><cell>28.2</cell><cell>67.9</cell></row><row><cell>Only Instance Loss</cell><cell>II</cell><cell>50.5</cell><cell>86.0</cell><cell>34.9</cell><cell>75.7</cell></row><row><cell>Only Ranking Loss</cell><cell>II</cell><cell>47.5</cell><cell>85.4</cell><cell>29.0</cell><cell>68.7</cell></row><row><cell>Full model</cell><cell>II</cell><cell>55.4</cell><cell>89.3</cell><cell>39.7</cell><cell>80.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 :</head><label>2</label><figDesc>Ranking loss and instance loss retrieval results on Flickr30k validation set. Except for the different losses, we apply the entirely same network (ResNet-50). For a clear comparison, we also fixed the image CNN in Stage I and tune the entire network in Stage II to observe the overfitting.</figDesc><table><row><cell>based on VGG-19, our method exceeds DAN 6.2% and 3.5%</cell></row><row><cell>Recall@1 using image and text query respectively. On MSCOCO</cell></row><row><cell>1K-test-image setting, we arrive at Recall@1 = 65.6%, Med r =</cell></row><row><cell>1 using image queries, and Recall@1 = 47.1%, Med r = 2 using</cell></row><row><cell>text queries. On 5K-test-image setting, we arrive at Recall@1 =</cell></row><row><cell>41.2%, Med r = 2 using image queries, and Recall@1 = 25.3%,</cell></row><row><cell>Med r = 5 using text queries. CUHK-PEDES is a specific dataset</cell></row><row><cell>for retrieving pedestrian images using the textual description.</cell></row><row><cell>On CUHK-PEDES, we arrive at Recall@1 = 32.15%, Med r</cell></row><row><cell>= 4. While both are based on a VGG-16 network, our model</cell></row><row><cell>has 6.21% higher recall rate. Moreover, our model based on</cell></row><row><cell>ResNet-50 achieves new state-of-the-art performance: Recall@1</cell></row><row><cell>= 44.4%, Med r = 2 using language description to search relevant</cell></row><row><cell>pedestrians. Our method exceeds the second best method [53] by</cell></row><row><cell>18.46% in Recall@1 accuracy.</cell></row><row><cell>Note that m-CNN [7] also fine-tunes the CNN model to extract</cell></row><row><cell>visual and textual features. m-CNN encompasses four different</cell></row><row><cell>levels of text matching CNN while we only use one deep textual</cell></row><row><cell>model with residual blocks. While both are based on VGG-19,</cell></row><row><cell>our model has higher performance than m-CNN. Compared with</cell></row><row><cell>a recent arXiv work, VSE++</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1 and Table. 8). Imagenet collects images from the Internet, while the pedestrian dataset CUHK-PEDES collects images from surveillance cameras. The fine-tuning result is more obvious on the CUHK-PEDES due to the different data distribution. The fine-tuned network (based on ResNet-50) improves the Recall@1 by 29.37%. The experiments indicate the end-to-end training is critical to imagesentence retrieval, especially person search.</figDesc><table><row><cell>Methods</cell><cell cols="3">Dataset Image-Query R@1 Text-Query R@1</cell></row><row><cell>Res152 + Res50  ? Res152 + Res152  ?</cell><cell>Flickr30k</cell><cell>44.4 44.2</cell><cell>29.6 30.7</cell></row><row><cell>Res152 + Res50  ? Res152 + Res152  ?</cell><cell>MSCOCO</cell><cell>52.0 52.8</cell><cell>38.0 37.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 :</head><label>3</label><figDesc>Deeper Text CNN on Flickr30k and MSCOCO. We use fixed Image CNN (StageI). ? : Text CNN structure.</figDesc><table><row><cell>Methods</cell><cell>Image-Query R@1</cell><cell>Text-Query R@1</cell></row><row><cell>3000 categories (StageI)</cell><cell>38.0</cell><cell>26.1</cell></row><row><cell>10000 categories (StageI)</cell><cell>44.7</cell><cell>31.3</cell></row><row><cell>Our (StageI)</cell><cell>52.2</cell><cell>37.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 :</head><label>4</label><figDesc>K-class Loss vs. Instance Loss on MSCOCO. We use the K-means clustering result as pseudo categories. The experiment is based on Res50 + Res50 ? as the model structure.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5 :</head><label>5</label><figDesc>Ablation study. With/without word2vec initialization on Flickr30k validation. The result suggests word2vec serves as a proper initialization for text CNN.</figDesc><table><row><cell>Method</cell><cell cols="2">Image Query R@1 R@10</cell><cell cols="2">Text Query R@1 R@10</cell></row><row><cell>Left alignment</cell><cell>34.1</cell><cell>73.1</cell><cell>23.6</cell><cell>61.4</cell></row><row><cell>Position shift</cell><cell>39.9</cell><cell>79.1</cell><cell>28.2</cell><cell>67.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 :</head><label>6</label><figDesc>Ablation study. Position shift vs. Left alignment on Flickr30k validation. It shows that position shift can serve as a significant data augmentation method for the text CNN.</figDesc><table><row><cell>Method</cell><cell>Visual</cell><cell cols="4">Text Query R@1 R@5 R@10 Med r</cell></row><row><cell cols="2">CNN-RNN (VGG-16  ? ) [10] fixed</cell><cell>8.07</cell><cell>-</cell><cell>32.47</cell><cell>-</cell></row><row><cell cols="3">Neural Talk (VGG-16  ? ) [67] fixed 13.66</cell><cell>-</cell><cell>41.72</cell><cell>-</cell></row><row><cell cols="3">GNA-RNN (VGG-16  ? ) [33] fixed 19.05</cell><cell>-</cell><cell>53.64</cell><cell>-</cell></row><row><cell>IATV (VGG-16) [53]</cell><cell>ft</cell><cell>25.94</cell><cell>-</cell><cell>60.48</cell><cell>-</cell></row><row><cell>Ours (VGG-16) Stage I</cell><cell cols="4">fixed 14.26 33.07 43.47</cell><cell>16</cell></row><row><cell>Ours (VGG-16) Stage II</cell><cell>ft</cell><cell cols="3">32.15 54.42 64.30</cell><cell>4</cell></row><row><cell>Ours (ResNet-50) Stage I</cell><cell cols="4">fixed 15.03 31.66 41.62</cell><cell>18</cell></row><row><cell>Ours (ResNet-50) Stage II</cell><cell>ft</cell><cell cols="3">44.40 66.26 75.07</cell><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7 :</head><label>7</label><figDesc>Method comparisons on CUHK-PEDES. R@K (%) is Recall@K (high is good). Med r is the median rank (low is good). ft means fine-tuning.</figDesc><table /><note>? : pre-trained on person identification.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 8 :</head><label>8</label><figDesc>Method comparisons on MSCOCO. R@K (%)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Yi Yang received the Ph.D. degree in computer science from Zhejiang University, Hangzhou, China, in 2010. He is currently an associate professor with University of Technology Sydney, Australia. He was a Post-Doctoral Research with the School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA. His current research interest includes machine learning and its applications to multimedia content analysis and computer vision, such as multimedia indexing and retrieval, surveillance video analysis and video semantics understanding. Xu is a professor in the School of Information Engineering of Zhengzhou University, China. He received his Ph.D. degree in computer science and technology from the State Key Lab of CAD&amp;CG at Zhejiang University, Hangzhou, China, and the B.S. and M.S. degrees from the Computer Science Department, Zhengzhou University, Zhengzhou, China, respectively. Yi-dong Shen is a professor of computer science in the State Key Laboratory of Computer Science at the Institute of Software, the Chinese Academy of Sciences, China. Prior to joining this laboratory, he was a professor at Chongqing University, China. His main research interests include Artificial Intelligence and Data Mining.</figDesc><table><row><cell>Mingliang</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A new approach to cross-modal multimedia retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Costa</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collecting image annotations using amazon&apos;s mechanical turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="139" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context,&quot; in ECCV</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal convolutional neural networks for matching image and sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generalized multiview analysis: A discriminative latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning coupled feature spaces for cross-modal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross-media semantic representation via bi-directional learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal discriminative binary embedding for large-scale cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval with cnn visual features: A new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="449" to="460" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Deep captioning with multimodal recurrent neural networks (m-rnn),&quot; in ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leveraging visual question answering for imagecaption ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rnn fisher vectors for action recognition and image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Instance-aware image and sentence matching with selective multimodal lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical multimodal lstm for dense visual-semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning two-branch neural networks for image-text matching tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03470</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show and tell: Lessons learned from the 2015 mscoco image captioning challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="652" to="663" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Twitter100k: A real-world dataset for weakly supervised cross-media retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">TMM</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafi?t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernock?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Large-scale cross-modality search via collective matrix factorization hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Flexible multi-view dimensionality co-reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning aligned cross-modal representations from weakly aligned data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A new evaluation protocol and benchmarking results for extendable cross-media retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03567</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Effective multi-modal retrieval based on stacked auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="649" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Linking image and text with 2-way nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cross-modal subspace learning via pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Discriminative bimodal networks for visual localization and detection with natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>AISTAT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Identity-aware textualvisual matching with latent co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</title>
		<meeting>IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">A discriminatively learned cnn embedding for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM TOMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-scale deep learning architectures for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning a recurrent residual fusion network for multimodal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Vse++: Improved visualsemantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Do convolutional networks need to be deep for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cerisara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Denis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04108</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
