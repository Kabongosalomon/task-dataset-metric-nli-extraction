<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FUSIONNET: FUSING VIA FULLY-AWARE ATTENTION WITH APPLICATION TO MACHINE COMPREHENSION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Yuan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Business AI and Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Business AI and Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Business AI and Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
							<email>wzchen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Business AI and Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FUSIONNET: FUSING VIA FULLY-AWARE ATTENTION WITH APPLICATION TO MACHINE COMPREHENSION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a new neural structure called FusionNet, which extends existing attention approaches from three perspectives. First, it puts forward a novel concept of "history of word" to characterize attention information from the lowest word-level embedding up to the highest semantic-level representation. Second, it identifies an attention scoring function that better utilizes the "history of word" concept. Third, it proposes a fully-aware multi-level attention mechanism to capture the complete information in one text (such as a question) and exploit it in its counterpart (such as context or passage) layer by layer. We apply FusionNet to the Stanford Question Answering Dataset (SQuAD) and it achieves the first position for both single and ensemble model on the official SQuAD leaderboard at the time of writing (Oct. 4th, 2017). Meanwhile, we verify the generalization of Fusion-Net with two adversarial SQuAD datasets and it sets up the new state-of-the-art on both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Context: The Alpine Rhine is part of the Rhine, a famous European river. The Alpine Rhine begins in the most western part of the Swiss canton of Graub?nden, and later forms the border between Switzerland to the West and Liechtenstein and later Austria to the East. On the other hand, the Danube separates Romania and Bulgaria.</p><p>Question: What is the other country the Rhine separates Switzerland to?</p><p>Answer: Liechtenstein <ref type="figure">Figure 1</ref>: Question-answer pair for a passage discussing Alpine Rhine.</p><p>Teaching machines to read, process and comprehend text and then answer questions is one of key problems in artificial intelligence. <ref type="figure">Figure 1</ref> gives an example of the machine reading comprehension task. It feeds a machine with a piece of context and a question and teaches it to find a correct answer to the question. This requires the machine to possess high capabilities in comprehension, inference and reasoning. This is considered a challenging task in artificial intelligence and has already attracted numerous research efforts from the neural network and natural language processing communities. Many neural network models have been proposed for this challenge and they generally frame this problem as a machine reading comprehension (MRC) task <ref type="bibr" target="#b7">(Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b22">Wang et al., 2017;</ref><ref type="bibr" target="#b17">Seo et al., 2017;</ref><ref type="bibr" target="#b18">Shen et al., 2017;</ref><ref type="bibr" target="#b13">Xiong et al., 2017;</ref><ref type="bibr">Weissenborn et al., 2017;</ref><ref type="bibr" target="#b3">Chen et al., 2017a)</ref>.</p><p>The key innovation in recent models lies in how to ingest information in the question and characterize it in the context, in order to provide an accurate answer to the question. This is often modeled as attention in the neural network community, which is a mechanism to attend the question into the context so as to find the answer related to the question. Some <ref type="bibr" target="#b3">(Chen et al., 2017a;</ref><ref type="bibr">Weissenborn et al., 2017)</ref> attend the word-level embedding from the question to context, while some <ref type="bibr" target="#b22">(Wang et al., 2017)</ref> attend the high-level representation in the question to augment the context. However we observed that none of the existing approaches has captured the full information in the context or the question, which could be vital for complete information comprehension. Taking image recognition as an example, information in various levels of representations can capture different aspects of details in an image: pixel, stroke and shape. We argue that this hypothesis also holds in language understanding and MRC. In other words, an approach that utilizes all the information from the word embedding level up to the highest level representation would be substantially beneficial for understanding both the question and the context, hence yielding more accurate answers.</p><p>However, the ability to consider all layers of representation is often limited by the difficulty to make the neural model learn well, as model complexity will surge beyond capacity. We conjectured this is why previous literature tailored their models to only consider partial information. To alleviate this challenge, we identify an attention scoring function utilizing all layers of representation with less training burden. This leads to an attention that thoroughly captures the complete information between the question and the context. With this fully-aware attention, we put forward a multi-level attention mechanism to understand the information in the question, and exploit it layer by layer on the context side. All of these innovations are integrated into a new end-to-end structure called FusionNet in <ref type="figure" target="#fig_2">Figure 4</ref>, with details described in Section 3.</p><p>We submitted FusionNet to SQuAD <ref type="bibr" target="#b16">(Rajpurkar et al., 2016)</ref>, a machine reading comprehension dataset. At the time of writing <ref type="bibr">(Oct. 4th, 2017)</ref>, our model ranked in the first place in both single model and ensemble model categories. The ensemble model achieves an exact match (EM) score of 78.8% and F1 score of 85.9%. Furthermore, we have tested FusionNet against adversarial SQuAD datasets <ref type="bibr" target="#b9">(Jia &amp; Liang, 2017)</ref>. Results show that FusionNet outperforms existing state-of-the-art architectures in both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%. In Appendix D, we also applied to natural language inference task and shown decent improvement. This demonstrated the exceptional performance of FusionNet. An open-source implementation of FusionNet can be found at https://github.com/momohuang/FusionNet-NLI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MACHINE COMPREHENSION &amp; FULLY-AWARE ATTENTION</head><p>In this section, we briefly introduce the task of machine comprehension as well as a conceptual architecture that summarizes recent advances in machine reading comprehension. Then, we introduce a novel concept called history-of-word. History-of-word can capture different levels of contextual information to fully understand the text. Finally, a light-weight implementation for history-of-word, Fully-Aware Attention, is proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TASK DESCRIPTION</head><p>In machine comprehension, given a context and a question, the machine needs to read and understand the context, and then find the answer to the question. The context is described as a sequence of word tokens: C = {w C 1 , . . . , w C m }, and the question as:</p><formula xml:id="formula_0">Q = {w Q 1 , . . . , w Q n },</formula><p>where m is the number of words in the context, and n is the number of words in the question. In general, m</p><p>n. The answer Ans can have different forms depending on the task. In the SQuAD dataset <ref type="bibr" target="#b16">(Rajpurkar et al., 2016)</ref>, the answer Ans is guaranteed to be a contiguous span in the context C, e.g., Ans = {w C i , . . . , w C i+k }, where k is the number of words in the answer and k ? m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CONCEPTUAL ARCHITECTURE FOR MACHINE READING COMPREHENSION</head><p>In all state-of-the-art architectures for machine reading comprehension, a recurring pattern is the following process. Given two sets of vectors, A and B, we enhance or modify every single vector in set A with the information from set B. We call this a fusion process, where set B is fused into set A. Fusion processes are commonly based on attention <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>, but some are not. Major improvements in recent MRC work lie in how the fusion process is designed.</p><p>A conceptual architecture illustrating state-of-the-art architectures is shown in <ref type="figure" target="#fig_0">Figure 2</ref>, which consists of three components.</p><p>? Input vectors: Embedding vectors for each word in the context and the question. <ref type="table">Table 1</ref>: A summarized view on the fusion processes used in several state-of-the-art architectures. ? Integration components: The rectangular box. It is usually implemented using an RNN such as an LSTM <ref type="bibr" target="#b7">(Hochreiter &amp; Schmidhuber, 1997)</ref> or a GRU <ref type="bibr" target="#b5">(Cho et al., 2014)</ref>. ? Fusion processes: The numbered arrows (1), (2), (2'), (3), (3'). The set pointing outward is fused into the set being pointed to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D D</head><formula xml:id="formula_1">? ? (1) (2) (2') (3') (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Question</head><p>There are three main types of fusion processes in recent advanced architectures. <ref type="table">Table 1</ref> shows what fusion processes are used in different state-of-the-art architectures. We now discuss them in detail.</p><p>(1) Word-level fusion. By providing the direct word information in question to the context, we can quickly zoom in to more related regions in the context. However, it may not be helpful if a word has different semantic meaning based on the context. Many word-level fusions are not based on attention, e.g., <ref type="bibr" target="#b3">Chen et al., 2017a)</ref> appends binary features to context words, indicating whether each context word appears in the question.</p><p>(2) High-level fusion. Informing the context about the semantic information in the question could help us find the correct answer. But high-level information is more imprecise than word information, which may cause models to be less aware of details.</p><p>(2') High-level fusion (Alternative). Similarly, we could also fuse high-level concept of Q into the word-level of C.</p><p>(3) Self-boosted fusion. Since the context can be long and distant parts of text may rely on each other to fully understand the content, recent advances have proposed to fuse the context into itself.</p><p>As the context contains excessive information, one common choice is to perform self-boosted fusion after fusing the question Q. This allows us to be more aware of the regions related to the question.</p><p>(3') Self-boosted fusion (Alternative). Another choice is to directly condition the self-boosted fusion process on the question Q, such as the coattention mechanism proposed in <ref type="bibr" target="#b13">(Xiong et al., 2017)</ref>. Then we can perform self-boosted fusion before fusing question information.</p><p>A common trait of existing fusion mechanisms is that none of them employs all levels of representation jointly. In the following, we claim that employing all levels of representation is crucial to achieving better text understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">FULLY-AWARE ATTENTION ON HISTORY OF WORD</head><p>Consider the illustration shown in <ref type="figure" target="#fig_1">Figure 3</ref>. As we read through the context, each input word will gradually transform into a more abstract representation, e.g., from low-level to high-level concepts. Altogether, they form the history of each word in our mental flow. For a human, we utilize the history-of-word so frequently but we often neglect its importance. For example, to answer the question in <ref type="figure" target="#fig_1">Figure 3</ref> correctly, we need to focus on both the high-level concept of forms the border and the word-level information of Alpine Rhine. If we focus only on the high-level concepts, we will ``History of Word" Concept  confuse Alpine Rhine with Danube since both are European rivers that separate countries. Therefore we hypothesize that the entire history-of-word is important to fully understand the text.</p><p>In neural architectures, we define the history of the i-th word, HoW i , to be the concatenation of all the representations generated for this word. This may include word embedding, multiple intermediate and output hidden vectors in RNN, and corresponding representation vectors in any further layers. To incorporate history-of-word into a wide range of neural models, we present a lightweight implementation we call Fully-Aware Attention.</p><p>Attention can be applied to different scenarios. To be more conclusive, we focus on attention applied to fusing information from one body to another. Consider two sets of hidden vectors for words in text bodies A and B: </p><formula xml:id="formula_2">{h A 1 , . . . , h A m }, {h B 1 , . . . , h B n } ? R d . Their associated history-of-word are, {HoW A 1 , . . . , HoW A m }, {HoW B 1 , . . . , HoW B n } ? R d h , where d h d.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Concatenate h</head><formula xml:id="formula_3">A i with the summarized information,? A i = j ? ij h B j .</formula><p>In fully-aware attention, we replace attention score computation with the history-of-word.</p><formula xml:id="formula_4">S(h A i , h B j ) =? S(HoW A i , HoW B j )</formula><p>. This allows us to be fully aware of the complete understanding of each word. The ablation study in Section 4.4 demonstrates that this lightweight enhancement offers a decent improvement in performance.</p><p>To fully utilize history-of-word in attention, we need a suitable attention scoring function S(x, y). A commonly used function is multiplicative attention <ref type="bibr" target="#b2">(Britz et al., 2017</ref>):</p><formula xml:id="formula_5">x T U T V y, leading to S ij = (HoW A i ) T U T V (HoW B j ), where U, V ? R k?d h ,</formula><p>and k is the attention hidden size. However, we suspect that two large matrices interacting directly will make the neural model harder to train. Therefore we propose to constrain the matrix U T V to be symmetric. A symmetric matrix can always be decomposed into</p><formula xml:id="formula_6">U T DU , thus S ij = (HoW A i ) T U T DU (HoW B j ), where U ? R k?d h , D ? R k?k and D is a diagonal matrix.</formula><p>The symmetric form retains the ability to give high attention score between dissimilar HoW A i , HoW B j . Additionally, we marry nonlinearity with the symmetric form to provide richer interaction among different parts of the history-of-word. The final formulation for attention score is</p><formula xml:id="formula_7">S ij = f (U (HoW A i )) T D f (U (HoW B j )), where f (x)</formula><p>is an activation function applied element-wise. In the following context, we employ f (x) = max(0, x). A detailed ablation study in Section 4 demonstrates its advantage over many alternatives. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FULLY-AWARE FUSION NETWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">END-TO-END ARCHITECTURE</head><p>Based on fully-aware attention, we propose an end-to-end architecture: the fully-aware fusion network (FusionNet). Given text A and B, FusionNet fuses information from text B to text A and generates two set of vectors</p><formula xml:id="formula_8">U A = {u A 1 , . . . , u A m }, U B = {u B 1 , . . . , u B n }.</formula><p>In the following, we consider the special case where text A is context C and text B is question Q. An illustration for FusionNet is shown in <ref type="figure" target="#fig_2">Figure 4</ref>. It consists of the following components.</p><p>Input Vectors. First, each word in C and Q is transformed into an input vector w. We utilize the 300-dim GloVe embedding <ref type="bibr" target="#b15">(Pennington et al., 2014)</ref> and 600-dim contextualized vector <ref type="bibr" target="#b13">(McCann et al., 2017</ref>). In the SQuAD task, we also include 12-dim POS embedding, 8-dim NER embedding and a normalized term frequency for context C as suggested in <ref type="bibr" target="#b3">(Chen et al., 2017a)</ref>.</p><formula xml:id="formula_9">Together {w C 1 , . . . , w C m } ? R 900+20+1 , and {w Q 1 , . . . , w Q n } ? R 900 . Fully-Aware Multi-level Fusion: Word-level.</formula><p>In multi-level fusion, we separately consider fusing word-level and higher-level. Word-level fusion informs C about what kind of words are in Q. It is illustrated as arrow (1) in <ref type="figure" target="#fig_0">Figure 2</ref>. For this component, we follow the approach in <ref type="bibr" target="#b3">(Chen et al., 2017a)</ref> First, a feature vector em i is created for each word in C to indicate whether the word occurs in the question Q. Second, attention-based fusion on GloVe embedding g i is used</p><formula xml:id="formula_10">g C i = j ? ij g Q j , ? ij ? exp(S(g C i , g Q j )), S(x, y) = ReLU(W x) T ReLU(W y),</formula><p>where W ? R 300?300 . Since history-of-word is the input vector itself, fully-aware attention is not employed here. The enhanced input vector for context isw C i = [w C i ; em i ;? C i ]. Reading. In the reading component, we use a separate bidirectional LSTM (BiLSTM) to form low-level and high-level concepts for C and Q.</p><formula xml:id="formula_11">h Cl 1 , . . . , h Cl m = BiLSTM(w C 1 , . . . ,w C m ), h Ql 1 , . . . , h Ql n = BiLSTM(w Q 1 , . . . , w Q n ), h Ch 1 , . . . , h Ch m = BiLSTM(h Cl 1 , . . . , h Cl m ), h Qh 1 , . . . , h Qh n = BiLSTM(h Ql 1 , . . . , h Ql n )</formula><p>. Hence low-level and high-level concepts h l , h h ? R 250 are created for each word.</p><p>Question Understanding. In the Question Understanding component, we apply a new BiLSTM taking in both h Ql , h Qh to obtain the final question representation U Q :</p><formula xml:id="formula_12">U Q = {u Q 1 , . . . , u Q n } = BiLSTM([h Ql 1 ; h Qh 1 ], . . . , [h Ql n ; h Qh n ]). where {u Q i ? R 250 } n i=1</formula><p>are the understanding vectors for Q. Fully-Aware Multi-level Fusion: Higher-level. This component fuses all higher-level information in the question Q to the context C through fully-aware attention on history-of-word. Since the proposed attention scoring function for fully-aware attention is constrained to be symmetric, we need to identify the common history-of-word for both C, Q. This yields</p><formula xml:id="formula_13">HoW C i = [g C i ; c C i ; h Cl i ; h Ch i ], HoW Q i = [g Q i ; c Q i ; h Ql i ; h Qh i ] ? R 1400 ,</formula><p>where g i is the GloVe embedding and c i is the CoVe embedding. Then we fuse low, high, and understanding-level information from Q to C via fully-aware attention. Different sets of attention weights are calculated through attention function S l (x, y), S h (x, y), S u (x, y) to combine low, high, and understanding-level of concepts. All three functions are the proposed symmetric form with nonlinearity in Section 2.3, but are parametrized by independent parameters to attend to different regions for different level. Attention hidden size is set to be k = 250.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Low-level fusion:?</head><formula xml:id="formula_14">Cl i = j ? l ij h Ql j , ? l ij ? exp(S l (HoW C i , HoW Q j )).<label>(C1)</label></formula><p>2. High-level fusion:?</p><formula xml:id="formula_15">Ch i = j ? h ij h Qh j , ? h ij ? exp(S h (HoW C i , HoW Q j )).<label>(C1)</label></formula><p>3. Understanding fusion:</p><formula xml:id="formula_16">? C i = j ? u ij u Q j , ? u ij ? exp(S u (HoW C i , HoW Q j )).<label>(C1)</label></formula><p>This multi-level attention mechanism captures different levels of information independently, while taking all levels of information into account. A new BiLSTM is applied to obtain the representation for C fully fused with information in the question Q:</p><formula xml:id="formula_17">{v C 1 , . . . , v C m } = BiLSTM([h Cl 1 ; h Ch 1 ;? Cl 1 ;? Ch 1 ;? C 1 ], . . . , [h Cl m ; h Ch m ;? Cl m ;? Ch m ;? C m ]). (C2)</formula><p>Fully-Aware Self-Boosted Fusion. We now use self-boosted fusion to consider distant parts in the context, as illustrated by arrow (3) in <ref type="figure" target="#fig_0">Figure 2</ref>. Again, we achieve this via fully-aware attention on history-of-word. We identify the history-of-word to be</p><formula xml:id="formula_18">HoW C i = [g C i ; c C i ; h Cl i ; h Ch i ;? Cl i ;? Ch i ;? C i ; v C i ] ? R 2400 .</formula><p>We then perform fully-aware attention,v</p><formula xml:id="formula_19">C i = j ? s ij v C j , ? s ij ? exp(S s (HoW C i , HoW C j )). (C3)</formula><p>The final context representation is obtained by</p><formula xml:id="formula_20">U C = {u C 1 , . . . , u C m } = BiLSTM([v C 1 ;v C 1 ], . . . , [v C m ;v C m ]).<label>(C4)</label></formula><p>where</p><formula xml:id="formula_21">{u C i ? R 250 } m i=1</formula><p>are the understanding vectors for C. After these components in FusionNet, we have created the understanding vectors, U C , for the context C, which are fully fused with the question Q. We also have the understanding vectors, U Q , for the question Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">APPLICATION IN MACHINE COMPREHENSION</head><p>We focus particularly on the output format in SQuAD <ref type="bibr" target="#b16">(Rajpurkar et al., 2016</ref>) where the answer is always a span in the context. The output of FusionNet are the understanding vectors for both C and Q,</p><formula xml:id="formula_22">U C = {u C 1 , . . . , u C m }, U Q = {u Q 1 , . . . , u Q n }.</formula><p>We then use them to find the answer span in the context. Firstly, a single summarized question understanding vector is obtained through</p><formula xml:id="formula_23">u Q = i ? i u Q i , where ? i ? exp(w T u Q i )</formula><p>and w is a trainable vector. Then we attend for the span start using the summarized question understanding vector u Q ,</p><formula xml:id="formula_24">P S i ? exp((u Q ) T W S u C i ), where W S ? R d?d is a trainable matrix.</formula><p>To use the information of the span start when we attend for the span end, we combine the context understanding vector for the span start with u Q through a GRU <ref type="bibr" target="#b5">(Cho et al., 2014)</ref>,</p><formula xml:id="formula_25">v Q = GRU(u Q , i P S i u C i ),</formula><p>where u Q is taken as the memory and i P S i u C i as the input in GRU. Finally we attend for the end of the span using v Q ,</p><formula xml:id="formula_26">P E i ? exp((v Q ) T W E u C i ), where W E ? R d?d is another trainable matrix.</formula><p>Training. During training, we maximize the log probabilities of the ground truth span start and end,</p><formula xml:id="formula_27">k (log(P S i s k ) + log(P E i e k )), where i s k , i e</formula><p>k are the answer span for the k-th instance.</p><p>Prediction.</p><p>We predict the answer span to be i s , i e with the maximum P S i s P E i e under the constraint 0 ? i e ? i s ? 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we first present the datasets used for evaluation. Then we compare our end-toend FusionNet model with existing machine reading models. Finally, we conduct experiments to validate the effectiveness of our proposed components. Additional ablation study on input vectors can be found in Appendix C. Detailed experimental settings can be found in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS</head><p>We focus on the SQuAD dataset <ref type="bibr" target="#b16">(Rajpurkar et al., 2016)</ref> to train and evaluate our model. SQuAD is a popular machine comprehension dataset consisting of 100,000+ questions created by crowd workers on 536 Wikipedia articles. Each context is a paragraph from an article and the answer to each question is guaranteed to be a span in the context.</p><p>While rapid progress has been made on SQuAD, whether these systems truly understand language remains unclear. In a recent paper, <ref type="bibr" target="#b9">Jia &amp; Liang (2017)</ref> proposed several adversarial schemes to test the understanding of the systems. We will use the following two adversarial datasets, AddOneSent and AddSent, to evaluate our model. For both datasets, a confusing sentence is appended at the end of the context. The appended sentence is model-independent for AddOneSent, while AddSent requires querying the model a few times to choose the most confusing sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MAIN RESULTS</head><p>We submitted our model to SQuAD for evaluation on the hidden test set. We also tested the model on the adversarial SQuAD datasets. Two official evaluation criteria are used: Exact Match (EM) and F1 score. EM measures how many predicted answers exactly match the correct answer, while F1 score measures the weighted average of the precision and recall at token level. The evaluation results for our model and other competing approaches are shown in <ref type="table" target="#tab_3">Table 2</ref>. 1 Additional comparisons with state-of-the-art models in the literature can be found in Appendix A.</p><p>For the two adversarial datasets, AddOneSent and AddSent, the evaluation criteria is the same as SQuAD. However, all models are trained only on the original SQuAD, so the model never sees the Test Set Single Model EM / F1 LR Baseline <ref type="bibr" target="#b16">(Rajpurkar et al., 2016)</ref> 40.4 / 51.0 Match-LSTM <ref type="bibr" target="#b21">(Wang &amp; Jiang, 2016)</ref> 64.7 / 73.7 BiDAF <ref type="bibr" target="#b17">(Seo et al., 2017)</ref> 68.0 / 77.3 SEDT <ref type="bibr" target="#b12">(Liu et al., 2017)</ref> 68.2 / 77.5 RaSoR <ref type="bibr" target="#b11">(Lee et al., 2016)</ref> 70.8 / 78.7 DrQA <ref type="bibr" target="#b3">(Chen et al., 2017a)</ref> 70.7 / 79.4 ReasoNet <ref type="bibr" target="#b18">(Shen et al., 2017)</ref> 70.6 / 79.4 R. Mnemonic Reader       <ref type="table" target="#tab_4">Table 3</ref> and <ref type="table" target="#tab_5">Table 4</ref>, respectively. 2</p><p>From the results, we can see that our models not only perform well on the original SQuAD dataset, but also outperform all previous models by more than 5% in EM score on the adversarial datasets. This shows that FusionNet is better at language understanding of both the context and question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">COMPARISON ON ATTENTION FUNCTION</head><p>In this experiment, we compare the performance of different attention scoring functions S(x, y) for fully-aware attention. We utilize the end-to-end architecture presented in Section 3.1. Fully-aware attention is used in two places, fully-aware multi-level fusion: higher level and fully-aware selfboosted fusion. Word-level fusion remains unchanged. Based on the discussion in Section 2.3, we consider the following formulations for comparison:</p><p>1. Additive attention (MLP) <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>: s T tanh(W 1 x + W 2 y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Multiplicative attention</head><formula xml:id="formula_28">: x T U T V y.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Scaled multiplicative attention:</head><formula xml:id="formula_29">1 ? k x T U T V y,</formula><p>where k is the attention hidden size. It is proposed in <ref type="bibr" target="#b20">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Scaled multiplicative with nonlinearity:</head><formula xml:id="formula_30">1 ? k f (U x) T f (V y). 5. Our proposed symmetric form: x T U T DU y, where D is diagonal. 6. Proposed symmetric form with nonlinearity: f (U x) T D f (U y).</formula><p>We consider the activation function f (x) to be max(0, x). The results of various attention functions on SQuAD development set are shown in <ref type="table" target="#tab_7">Table 5</ref>. It is clear that the symmetric form consistently outperforms all alternatives. We attribute this gain to the fact that symmetric form has a single large ?: This is a unpublished version of R-net. The published version of R-net <ref type="bibr" target="#b22">(Wang et al., 2017)</ref>   matrix U . All other alternatives have two large parametric matrices. During optimization, these two parametric matrices would interfere with each other and it will make the entire optimization process challenging. Besides, by constraining U T V to be a symmetric matrix U T DU , we retain the ability for x to attend to dissimilar y. Furthermore, its marriage with the nonlinearity continues to significantly boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">EFFECTIVENESS OF HISTORY-OF-WORD</head><p>In FusionNet, we apply the history-of-word and fully-aware attention in two major places to achieve good performance: multi-level fusion and self-boosted fusion. In this section, we present experiments to demonstrate the effectiveness of our application. In the experiments, we fix the attention function to be our proposed symmetric form with nonlinearity due to its good performance shown in Section 4.3. The results are shown in <ref type="table">Table 6</ref>, and the details for each configuration can be found in Appendix B.</p><p>High-Level is a vanilla model where only the high-level information is fused from Q to C via standard attention. When placed in the conceptual architecture <ref type="figure" target="#fig_0">(Figure 2)</ref>, it only contains arrow (2) without any other fusion processes.</p><p>FA High-Level is the High-Level model with standard attention replaced by fully-aware attention.</p><p>FA All-Level is a naive extension of FA High-Level, where all levels of information are concatenated and is fused into the context using the same attention weight.</p><p>FA Multi-Level is our proposed Fully-aware Multi-level fusion, where different levels of information are attended under separate attention weight.</p><p>Self C = None means we do not make use of self-boosted fusion.</p><p>Self C = Normal means we employ a standard attention-based self-boosted fusion after fusing question to context. This is illustrated as arrow (3) in the conceptual architecture ( <ref type="figure" target="#fig_0">Figure 2</ref>).</p><p>Self C = FA means we enhance the self-boosted fusion with fully-aware attention.</p><p>High-Level vs. FA High-Level. From <ref type="table">Table 6</ref>, we can see that High-Level performs poorly as expected. However enhancing this vanilla model with fully-aware attention significantly increase the performance by more than 8%. The performance of FA High-Level already outperforms many state-of-the-art MRC models. This clearly demonstrates the power of fully-aware attention.</p><p>FA All-Level vs. FA Multi-Level. Next, we consider models that fuse all levels of information from question Q to context C. FA All-Level is a naive extension of FA High-Level, but its performance is actually worse than FA High-Level. However, by fusing different parts of history-of-word in Q independently as in FA Multi-Level, we are able to further improve the performance.</p><p>Self C options. We have achieved decent performance without self-boosted fusion. Now, we compare adding normal and fully-aware self-boosted fusion into the architecture. Comparing None and Normal in <ref type="table">Table 6</ref>, we can see that the use of normal self-boosted fusion is not very effective under our improved C, Q Fusion. Then by comparing with FA, it is clear that through the enhancement of fully-aware attention, the enhanced self-boosted fusion can provide considerable improvement.</p><p>Together, these experiments demonstrate that the ability to take all levels of understanding as a whole is crucial for machines to better understand the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we describe a new deep learning model called FusionNet with its application to machine comprehension. FusionNet proposes a novel attention mechanism with following three contributions: 1. the concept of history-of-word to build the attention using complete information from the lowest word-level embedding up to the highest semantic-level representation; 2. an attention scoring function to effectively and efficiently utilize history-of-word; 3. a fully-aware multi-level fusion to exploit information layer by layer discriminatingly. We applied FusionNet to MRC task and experimental results show that FusionNet outperforms existing machine reading models on both the SQuAD dataset and the adversarial SQuAD dataset. We believe FusionNet is a general and improved attention mechanism and can be applied to many tasks. Our future work is to study its capability in other NLP problems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A COMPARISON WITH PUBLISHED MODELS</head><p>In this appendix, we compare with published state-of-the-art architectures on the SQuAD dev set. The comparison is shown in <ref type="figure" target="#fig_4">Figure 5</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DETAILED CONFIGURATIONS IN THE ABLATION STUDY</head><p>In this appendix, we present details for the configurations used in the ablation study in Section 4.4. For all configurations, the understanding vectors for both the context C and the question Q will be generated, then we follow the same output architecture in Section 3.2 to apply them to machine reading comprehension problem. Next we consider the standard attention-based fusion for the high level representation.</p><formula xml:id="formula_31">h Ch i = j ? ij h Qh j , ? ij = exp(S ij ) k exp(S ik ) , S ij = S(h Ch i , h Qh j ).</formula><p>Then we concatenate the attended vector? FA High-Level. The only difference to High-Level is the enhancement of fully-aware attention. This is as simple as changing</p><formula xml:id="formula_32">S ij = S(h Ch i , h Qh j ) =? S ij = S([g C i ; c C i ; h Cl i ; h Ch i ], [g Q j ; c Q j ; h Ql j ; h Qh j ]), where [g i ; c i ; h l i ; h h i ]</formula><p>is the common history-of-word for both context and question. All other places remains the same as High-Level. This simple change results in significant improvement. The performance of FA High-Level can already outperform many state-of-the-art models in the literature. Note that our proposed symmetric form with nonlinearity should be used to guarantee the boost.</p><p>FA All-Level. First, we use the same procedure as High-Level to obtain Next we make use of the fully-aware attention similar to FA High-Level, but take back the entire history-of-word.</p><formula xml:id="formula_33">? ij = exp(S ij ) k exp(S ik ) , S ij = S([g C i ; c C i ; h Cl i ; h Ch i ], [g Q j ; c Q j ; h Ql j ; h Qh j ]), HoW C i = j ? ij [g Q j ; c Q j ; h Ql j ; h Qh j ].</formula><p>Then we concatenate the attended history-of-word?oW</p><formula xml:id="formula_34">C i with the original history-of-word [g C i ; c C i ; h Cl i ; h Ch i ]</formula><p>and pass through two layers of BiLSTM to fully mix the two information. The understanding vectors for the context is the hidden vectors in the final layers of the BiLSTM.</p><formula xml:id="formula_35">u C 1 , . . . , u C m = BiLSTM([g C 1 ; c C 1 ; h Cl 1 ; h Ch 1 ;?oW C 1 ], . . . , [g C m ; c C m ; h Cl m ; h Ch m ;?oW C m ])</formula><p>The understanding vectors for the question is similar to the Understanding component in Section 3.1,</p><formula xml:id="formula_36">u Q 1 , . . . , u Q n = BiLSTM([g Q 1 ; c Q 1 ; h Ql 1 ; h Qh 1 ], . . . , [g Q m ; c Q m ; h Ql m ; h Qh m ])</formula><p>. We have now generated the understanding vectors for both the context and the question.</p><p>FA Multi-Level. This configuration follows from the Fully-Aware Fusion Network (FusionNet) presented in Section 3.1. The major difference compared to FA All-Level is that different layers in the history-of-word uses a different attention weight ? while being fully aware of the entire historyof-word. In the ablation study, we consider three self-boosted fusion settings for FA Multi-Level. The Fully-Aware setting is the one presented in Section 3.1. Here we discuss all three of them in detail.</p><p>? For the None setting in self-boosted fusion, no self-boosted fusion is used and we use two layers of BiLSTM to mix the attended information. The understanding vectors for the context C is the hidden vectors in the final layers of the BiLSTM, </p><formula xml:id="formula_37">S ij = S(v C i , v C j ), ? ij = exp(S ij ) k exp(S ik ) ,v C i = j ? ij v C j .</formula><p>The final understanding vectors for the context C is the output hidden vectors after passing the concatenated vectors into a BiLSTM,</p><formula xml:id="formula_38">u C 1 , . . . , u C m = BiLSTM([v C 1 ;v C 1 ], . . . , [v C m ;v C m ]).</formula><p>? For the Fully-Aware setting, we change S ij = S(v C i , v C j ) in the Normal setting to the fully-aware attention</p><formula xml:id="formula_39">S ij = S([w C i ; h Cl i ; h Ch i ;? Cl u ;? Ch i ;? C i ; v C i ], [w C j ; h Cl j ; h Ch j ;? Cl j ;? Ch j ;? C j ; v C j ])</formula><p>. All other places remains the same. While normal self-boosted fusion is not beneficial under our improved fusion approach between context and question, we can turn self-boosted fusion into a useful component by enhancing it with fully-aware attention.      We have conducted experiments on input vectors (GloVe and CoVe) for the original SQuAD as shown in <ref type="table" target="#tab_13">Table 7</ref>. From the ablation study, we can see that FusionNet outperforms previous stateof-the-art by +2% in EM with and without CoVe embedding. We can also see that fine-tuning top-1000 GloVe embeddings is slightly helpful in the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL ABLATION STUDY ON INPUT VECTORS</head><p>Next, we show the ablation study on two adversarial datasets, AddSent and AddOneSent. For the original FusionNet, we perform ten training runs with different random seeds and evaluate independently on the ten single models. The performance distribution of the ten training runs can be seen in <ref type="figure" target="#fig_7">Figure 7</ref>. Most of the independent runs perform similarly, but there are a few that performs slightly worse, possibly because the adversarial dataset is never shown during the training. For FusionNet (without CoVe), we directly evaluate on the model trained in <ref type="table" target="#tab_13">Table 7</ref>. From <ref type="table" target="#tab_15">Table 8</ref> and 9, we can see that FusionNet, single or ensemble, with or without CoVe, are all better than previous best performance by a significant margin. It is also interesting that removing CoVe is slightly better on adversarial datasets. We assert that it is because AddSent and AddOneSent target the over-stability of machine comprehension models <ref type="bibr" target="#b9">(Jia &amp; Liang, 2017)</ref>. Since CoVe is the output vector of two-layer BiLSTM, CoVe may slightly worsen this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D APPLICATION TO NATURAL LANGUAGE INFERENCE</head><p>FusionNet is an improved attention mechanism that can be easily added to any attention-based neural architecture. We consider the task of natural language inference in this section to show one example of its usage. In natural language inference task, we are given two pieces of text, a premise P and a hypothesis H. The task is to identify one of the following scenarios:</p><p>1. Entailment -the hypothesis H can be derived from the premise P . 2. Contradiction -the hypothesis H contradicts the premise P . 3. Neutral -none of the above.</p><p>We focus on Multi-Genre Natural Language Inference (MultiNLI) corpus <ref type="bibr">(Williams et al., 2017)</ref> recently developed by the creator of Stanford Natural Language Inference (SNLI) dataset <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref>. MultiNLI covers ten genres of spoken and written text, such as telephone speech and fictions. However the training set only contains five genres. Thus there are in-domain and crossdomain accuracy during evaluation. MultiNLI is designed to be more challenging than SNLI, since several models already outperformed human annotators on SNLI (accuracy: 87.7%) 3 .</p><p>A state-of-the-art model for natural language inference is Enhanced Sequential Inference Model (ESIM) by <ref type="bibr" target="#b4">Chen et al. (2017b)</ref>, which achieves an accuray of 88.0% on SNLI and obtained 72.3% (in-domain), 72.1% (cross-domain) on MultiNLI <ref type="bibr">(Williams et al., 2017)</ref>. We implemented a version of ESIM in PyTorch. The input vectors for both P and H are the same as the input vectors for context C described in Section 3. Therefore,</p><formula xml:id="formula_40">w P i , w H j ? R 900+20+1</formula><p>. Then, two-layer BiLSTM with shortcut connection is used to encode the input words for both premise P and hypothesis H, i.e.,</p><formula xml:id="formula_41">{h P l i } = BiLSTM(w P i ), {h Hl j } = BiLSTM(w H j ), {h P h i } = BiLSTM([w P i ; h P l i ]), {h Hh j } = BiLSTM([w H j ; h Hl j ])</formula><p>. The hidden size of each LSTM is 150, so h P l i , h P h i , h Hl j , h Hh j ? R 300 . Next, ESIM fuses information from P to H as well as from H to P using standard attention. We consider the following,</p><formula xml:id="formula_42">g P i = [h P h i ;? P h i ],? P h i = j ? P ij h Hh j , ? P ij = exp(S P ij ) k exp(S P ik ) , S P ij = S P (h P h i , h Hh j ), g H j = [h Hh j ;? Hh j ],? Hh j = i ? H ij h P h i , ? H ij = exp(S H ij ) k exp(S H kj ) , S H ij = S H (h P h i , h Hh j ).</formula><p>We set the attention hidden size to be the same as the dimension of hidden vectors h. Next, ESIM feed g P i , g H j into separate BiLSTMs to perform inference. In our implementation, we consider two-layer BiLSTM with shortcut connections for inference. The hidden vectors for the two-layer  </p><formula xml:id="formula_43">h P,H = 1 n i u P i ; max(u P 1 , . . . , u P n ); 1 m j u H j ; max(u H 1 , . . . , u H m ) .</formula><p>The final hidden vector h P,H is then passed into a multi-layer perceptron (MLP) classifier. The MLP classifier has a single hidden layer with tanh activation and the hidden size is set to be the same as the dimension of u P i and u H j . Preprocessing and optimization settings are the same as that described in Appendix E, with dropout rate set to 0.3. Now, we consider improving ESIM with our proposed attention mechanism. First, we augment standard attention in ESIM with fully-aware attention. This is as simple as replacing</p><formula xml:id="formula_44">S(h P h i , h Hh j ) =? S(HoW P i , HoW H j ),</formula><p>where HoW i is the history-of-word,</p><formula xml:id="formula_45">[w i , h l i , h h i ].</formula><p>All other settings remain unchanged. To incorporate fully-aware multi-level fusion into ESIM, we change the input for inference BiLSTM from</p><formula xml:id="formula_46">[h h ;? h ] ? R 2d =? [h l ; h h ;? l ;? h ] ? R 4d ,</formula><p>where? l i ,? h i are computed through independent fully-aware attention weights and d is the dimension of hidden vectors h. Word level fusion discussed in Section 3.1 is also included. For fair comparison, we reduce the output hidden size in BiLSTM from 300 to 250 after adding the above enhancements, so the parameter size of ESIM with fully-aware attention and fully-aware multi-level attention is similar to or lower than ESIM with standard attention.</p><p>The results of ESIM under different attention mechanism is shown in <ref type="table" target="#tab_18">Table 10</ref>. Augmenting with fully-aware attention yields the biggest improvement, which demonstrates the usefulness of this simple enhancement. Further improvement is obtained when we use multi-level fusion in our ESIM. Experiments with and without CoVe embedding show similar observations.</p><p>Together, experiments on natural language inference conform with the observations in Section 4 on machine comprehension task that the ability to take all levels of understanding as a whole is crucial for machines to better understand the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E MODEL DETAILS</head><p>We make use of spaCy for tokenization, POS tagging and NER. We additionally fine-tuned the GloVe embeddings of the top 1000 frequent question words. During training, we use a dropout rate of 0.4 <ref type="bibr" target="#b19">(Srivastava et al., 2014)</ref> after the embedding layer (GloVe and CoVe) and before applying any linear transformation. In particular, we share the dropout mask when the model parameter is shared <ref type="bibr" target="#b6">(Gal &amp; Ghahramani, 2016)</ref>.</p><p>The batch size is set to 32, and the optimizer is Adamax <ref type="bibr" target="#b10">(Kingma &amp; Ba, 2014)</ref> with a learning rate ? = 0.002, ? = (0.9, 0.999) and = 10 ?8 . A fixed random seed is used across all experiments. All models are implemented in PyTorch (http://pytorch.org/). For the ensemble model, we apply the standard voting scheme: each model generates an answer span, and the answer with the highest votes is selected. We break ties randomly. There are 31 models in the ensemble. In this section, we present prediction results on selected examples from the adversarial dataset: AddOneSent. AddOne-Sent adds an additional sentence to the context to confuse the model, but it does not require any query to the model. The prediction results are compared with a state-of-the-art architecture in the literature, BiDAF <ref type="bibr" target="#b17">(Seo et al., 2017)</ref>.</p><p>First, we compare the percentage of questions answered correctly (exact match) for our model FusionNet and the state-ofthe-art model BiDAF. The comparison is shown in <ref type="figure" target="#fig_8">Figure 8</ref>. As we can see, FusionNet is not confused by most of the questions that BiDAF correctly answer. Among the 3.3% answered correctly by BiDAF but not FusionNet, ? 1.6% are being confused by the added sentence; ? 1.2% are correct but differs slightly from the ground truth answer; and the remaining ? 0.5% are completely incorrect in the first place. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID: 57273cca708984140094db35-high-conf-turk1</head><p>Context: Large-scale construction requires collaboration across multiple disciplines. An architect normally manages the job, and a construction manager, design engineer, construction engineer or project manager supervises it. For the successful execution of a project, effective planning is essential. Those involved with the design and execution of the infrastructure in question must consider zoning requirements, the environmental impact of the job, the successful scheduling, budgeting, construction-site safety, availability and transportation of building materials, logistics, inconvenience to the public caused by construction delays and bidding, etc. The largest construction projects are referred to as megaprojects. Confusion is essential for the unsuccessful execution of a project.</p><p>Question: What is essential for the successful execution of a project? Answer: effective planning FusionNet Prediction: effective planning BiDAF Prediction: Confusion ID: 5727e8424b864d1900163fc1-high-conf-turk1 Context: According to PolitiFact the top 400 richest Americans "have more wealth than half of all Americans combined." According to the New York Times on July 22, 2014, the "richest 1 percent in the United States now own more wealth than the bottom 90 percent". Inherited wealth may help explain why many Americans who have become rich may have had a "substantial head start". In September 2012, according to the Institute for Policy Studies, "over 60 percent" of the Forbes richest 400 Americans "grew up in substantial privilege". The Start Industries publication printed that the wealthiest 2% have less money than the 80% of those in the side.</p><p>Question: What publication printed that the wealthiest 1% have more money than those in the bottom 90%? Answer: New York Times FusionNet Prediction: 587,000 BiDAF Prediction: 187000 alism was a basic component to the conquests of Genghis Khan during the Mongol Empire, and of other war-lords. Historically recognized Muslim empires number in the dozens. Sub-Saharan Africa has also featured dozens of empires that predate the European colonial era, for example the Ethiopian Empire, Oyo Empire, Asante Union, Luba Empire, Lunda Empire, and Mutapa Empire. The Americas during the pre-Columbian era also had large empires such as the Aztec Empire and the Incan Empire. The British Empire is older than the Eritrean Conquest.</p><p>Question: Which is older the British Empire or the Ethiopian Empire? Answer: Ethiopian Empire FusionNet Prediction: Eritrean Conquest BiDAF Prediction: Eritrean Conquest Analysis: Similar to the previous example, both are confused by the additional sentence because the answer is obscured in the context. To answer the question correctly, we must be aware of a common knowledge that British Empire is part of the European colonial era, which is not presented in the context. Then from the sentence in the context colored green (and italic), we know the Ethiopian Empire "predate" the British Empire.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID: 57111713a58dae1900cd6c02-high-conf-turk2</head><p>Context: In February 2010, in response to controversies regarding claims in the Fourth Assessment Report, five climate scientists all contributing or lead IPCC report authors wrote in the journal Nature calling for changes to the IPCC. They suggested a range of new organizational options, from tightening the selection of lead authors and contributors, to dumping it in favor of a small permanent body, or even turning the whole climate science assessment process into a moderated "living" Wikipedia-IPCC. Other recommendations included that the panel employ a full-time staff and remove government oversight from its processes to avoid political interference. It was suggested that the panel learn to avoid nonpolitical problems.</p><p>Question: How was it suggested that the IPCC avoid political problems? Answer: remove government oversight from its processes FusionNet Prediction: the panel employ a full-time staff and remove government oversight from its processes BiDAF Prediction: the panel employ a full-time staff and remove government oversight from its processes Analysis: In this example, both BiDAF and FusionNet are not confused by the added sentence. However, the prediction by both model are not precise enough. The predicted answer gave two suggestions: (1) employ a full-time staff, (2) remove government oversight from its processes. Only the second one is suggested to avoid political problems. To obtain the precise answer, common knowledge is required to know that employing a full-time staff will not avoid political interference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID: 57111713a58dae1900cd6c02-high-conf-turk2</head><p>Context: Most of the Huguenot congregations (or individuals) in North America eventually affiliated with other Protestant denominations with more numerous members. The Huguenots adapted quickly and often married outside their immediate French communities, which led to their assimilation. Their descendants in many families continued to use French first names and surnames for their children well into the nineteenth century. Assimilated, the French made numerous contributions to United States economic life, especially as merchants and artisans in the late Colonial and early Federal periods. For example, E.I. du Pont, a former student of Lavoisier, established the Eleutherian gunpowder mills. Westinghouse was one prominent Neptune arms manufacturer.</p><p>Question: Who was one prominent Huguenot-descended arms manufacturer? Answer: E.I. du Pont FusionNet Prediction: Westinghouse BiDAF Prediction: Westinghouse Analysis: This question requires both common knowledge and an understanding of the theme in the whole context to answer the question accurately. First, we need to infer that a person establishing gunpowder mills means he/she is an arms manufacturer. Furthermore, in order to relate E.I. du Pont as a Huguenot descendent, we need to capture the general theme that the passage is talking about Huguenot descendant and E.I. du Pont serves as an example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G MULTI-LEVEL ATTENTION VISUALIZATION</head><p>In this section, we present the attention weight visualization between the context C and the question Q over different levels. From <ref type="figure" target="#fig_11">Figure 9</ref> and 10, we can see clear variation between low-level attention and high-level attention weights. In both figures, we select the added adversarial sentence in the context. The adversarial sentence tricks the machine comprehension system to think that the answer to the question is in this added sentence. If only the high-level attention is considered (which is common in most previous architectures), we can see from the high-level attention map in the right hand side of <ref type="figure" target="#fig_11">Figure 9</ref> that the added sentence "The proclamation of the Central Park abolished protestantism in Belgium" matches well with the question "What proclamation abolished protestantism in France?" This is because "Belgium" and "France" are similar European countries. Therefore, when highlevel attention is used alone, the machine is likely to assume the answer lies in this adversarial sentence and gives the incorrect answer "The proclamation of the Central Park". However, when low-level attention is used (the attention map in the left hand side of <ref type="figure" target="#fig_11">Figure 9</ref>), we can see that "in Belgium" no longer matches with "in France". Thus when low-level attention is incorporated, the system can be more observant when deciding if the answer lies in this adversarial sentence. Similar observation is also evident in <ref type="figure">Figure 10</ref>. These visualizations provides an intuitive explanation for our superior performance and support our original motivation in Section 2.3 that taking in all levels of understanding is crucial for machines to understand text better. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>A conceptual architecture illustrating recent advances in MRC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustrations of the history-of-word for the example shown inFigure 1. Utilizing the entire history-of-word is crucial for the full understanding of the context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>An illustration of FusionNet architecture. Each upward arrow represents one layer of BiL-STM. Each circle to the right is a detailed illustration of the corresponding component in FusionNet. Circle 1: Fully-aware attention between C and Q. Illustration of Equation (C1) in Section 3.1. Circle 2: Concatenate all concepts in C with multi-level Q information, then pass through BiLSTM. Illustration of Equation (C2) in Section 3.1. Circle 3: Fully-aware attention on the context C itself. Illustration of Equation (C3) in Section 3.1. Circle 4: Concatenate the understanding vector of C with self-attention information, then pass through BiLSTM. Illustration of Equation (C4) in Section 3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>and 6 for EM and F1 score respectively. The performance of FusionNet is shown under different training epochs. Each epoch loops through all the examples in the training set once. On a single NVIDIA GeForce GTX Titan X GPU, each epoch took roughly 20 minutes when batch size 32 is used. The state-of-the-art models compared in this experiment include: 1. Published version of R-net in their technical report (Wang et al., 2017), 2. Reinforced Mnemonic Reader (Hu et al., 2017), 3. MEMEN (Pan et al., 2017), 4. ReasoNet (Shen et al., 2017), 5. Document reader (DrQA) (Chen et al., 2017a), 6. DCN (Xiong et al., 2017), 7. DCN + character embedding (Char) + CoVe (McCann et al., 2017), 8. BiDAF<ref type="bibr" target="#b17">(Seo et al., 2017)</ref>, 9. the best-performing variant of Match-LSTM<ref type="bibr" target="#b21">(Wang &amp; Jiang, 2016)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>EM score on the SQuAD dev set under different training epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>F1 score on the SQuAD dev set under different training epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>layers of BiLSTM to fully mix the two information. The understanding vectors for the context is the hidden vectors in the final layers of the BiLSTM. The understanding vectors for the question is the high level representation itself, u Q 1 , . . . , u Q n = h Qh 1 , . . . , h Qh n . Now we have obtained the understanding vectors for both the context and the question. The answer can thus be found. Neither word-level fusion (1) nor self-boosted fusion (3, 3') inFigure 2are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Single model performance (EM) on AddSent over 10 training runs. (dashed vertical line indicates previous best performance)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FFigure 8 :</head><label>8</label><figDesc>Questions answered correctly on AddOneSent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Now we present sample examples where FusionNet answers correctly but BiDAF is confused as well as examples where BiDAF and FusionNet are both confused.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>FusionNet</head><label></label><figDesc>Prediction: New York Times BiDAF Prediction: The Start Industries Question: In the year 2000 how many square kilometres of the Amazon forest had been lost? Answer: 587,000</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Multi-level Attention visualization between the added adversarial sentence and the question Q on an article about Protestant Reformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The Alpine Rhine is part of the Rhine, a famous European river. The Alpine Rhine begins in the most western part of the Swiss canton of Graub?nden, and later forms the border between Switzerland to the West and Liechtenstein and later Austria to the East. On the other hand, the Danube separates Romania and Bulgaria.</figDesc><table><row><cell cols="2">Alpine Rhine Input Word Forms the border Context: Liechtenstein Question: What is the other country the Rhine separates Switzerland to?</cell><cell>European river Low level Border countries Country</cell><cell>Separating River High level Separates Country, separate</cell></row><row><cell>Answer: Liechtenstein</cell><cell>Danube</cell><cell>European river</cell><cell>Separating River</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Form the attention weight ? ij through softmax: ? ij = exp(S ij )/ k exp(S ik ).</figDesc><table><row><cell>Fusing body B to body A via standard attention means for every h A i in body A,</cell></row><row><cell>1. Compute an attention score S ij = S(h A i , h B j ) ? R for each h B j in body B.</cell></row><row><cell>2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The performance of FusionNet and competing approaches on SQuAD hidden test set at the time of writing (Oct. 4th, 2017).</figDesc><table><row><cell>AddSent</cell><cell>EM / F1</cell></row><row><cell>LR Baseline</cell><cell>17.0 / 23.2</cell></row><row><cell>Match-LSTM (E)</cell><cell>24.3 / 34.2</cell></row><row><cell>BiDAF (E)</cell><cell>29.6 / 34.2</cell></row><row><cell>SEDT (E)</cell><cell>30.0 / 35.0</cell></row><row><cell cols="2">Mnemonic Reader (S) 39.8 / 46.6</cell></row><row><cell cols="2">Mnemonic Reader (E) 40.7 / 46.2</cell></row><row><cell>ReasoNet (E)</cell><cell>34.6 / 39.4</cell></row><row><cell>FusionNet (E)</cell><cell>46.2 / 51.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">: Comparison on AddSent. (S: Sin-</cell></row><row><cell>gle model, E: Ensemble)</cell><cell></cell></row><row><cell>AddOneSent</cell><cell>EM / F1</cell></row><row><cell>LR Baseline</cell><cell>22.3 / 30.4</cell></row><row><cell>Match-LSTM (E)</cell><cell>34.8 / 41.8</cell></row><row><cell>BiDAF (E)</cell><cell>40.7 / 46.9</cell></row><row><cell>SEDT (E)</cell><cell>40.0 / 46.5</cell></row><row><cell cols="2">Mnemonic Reader (S) 48.5 / 56.0</cell></row><row><cell cols="2">Mnemonic Reader (E) 48.7 / 55.3</cell></row><row><cell>ReasoNet (E)</cell><cell>43.6 / 49.8</cell></row><row><cell>FusionNet (E)</cell><cell>54.7 / 60.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Comparison on AddOneSent. (S: Single model, E: Ensemble) adversarial datasets during training. The results for AddSent and AddOneSent are shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>only achieved EM / F1 = 71.3 / 79.7 for single model, 75.9 / 82.9 for ensemble.</figDesc><table><row><cell>Attention Function</cell><cell>EM / F1</cell></row><row><cell>Additive (MLP)</cell><cell>71.8 / 80.1</cell></row><row><cell>Multiplicative</cell><cell>72.1 / 80.6</cell></row><row><cell>Scaled Multiplicative</cell><cell>72.4 / 80.7</cell></row><row><cell>Scaled Multiplicative + ReLU</cell><cell>72.6 / 80.8</cell></row><row><cell>Symmetric Form</cell><cell>73.1 / 81.5</cell></row><row><cell>Symmetric Form + ReLU</cell><cell>75.3 / 83.6</cell></row><row><cell cols="2">Previous SotA (Hu et al., 2017) 72.1 / 81.6</cell></row></table><note>2 Results are obtain from Codalab worksheet https://goo.gl/E6Xi2E.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison of different attention functions S(x, y) on SQuAD dev set.</figDesc><table><row><cell cols="2">Configuration C, Q Fusion Self C</cell><cell>EM / F1</cell></row><row><cell>High-Level</cell><cell></cell><cell>64.6 / 73.2</cell></row><row><cell>FA High-Level FA All-Level</cell><cell>None</cell><cell>73.3 / 81.4 72.3 / 80.7</cell></row><row><cell>FA Multi-Level</cell><cell></cell><cell>74.6 / 82.7</cell></row><row><cell>FA Multi-Level</cell><cell>Normal FA</cell><cell>74.4 / 82.6 75.3 / 83.6</cell></row><row><cell cols="3">Previous SotA (Hu et al., 2017) 72.1 / 81.6</cell></row><row><cell cols="3">Table 6: Comparison of different configurations</cell></row><row><cell cols="3">demonstrates the effectiveness of history-of-word.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>High-Level. Firstly, context words and question words are transformed into input vectors in the same way as FusionNet, Then we pass them independently to two layers of BiLSTM.</figDesc><table><row><cell>{w C 1 , . . . , w C m }, {w Q 1 , . . . , w Q n }.</cell></row></table><note>h Cl 1 , . . . , h Cl m = BiLSTM(w C 1 , . . . , w C m ), h Ql 1 , . . . , h Ql n = BiLSTM(w Q 1 , . . . , w Qn ), h Ch 1 , . . . , h Ch m = BiLSTM(h Cl 1 , . . . , h Cl m ), h Qh 1 , . . . , h Qh n = BiLSTM(h Ql 1 , . . . , h Ql n ).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Self-boosted fusion is not utilized in all previous configurations: High-Level, FA High-Level and FA All-Level.? For the Normal setting, we first use one layer of BiLSTM to mix the attended information.</figDesc><table><row><cell>u C 1 , . . . , u C m = BiLSTM([h Cl 1 ; h Ch 1 ;?</cell><cell cols="2">Cl 1 ;?</cell><cell>Ch 1 ;? C 1 ], . . . , [h Cl m ; h Ch m ;?</cell><cell cols="2">Cl m ;?</cell><cell>Ch m ;? C m ]).</cell></row><row><cell cols="2">v C 1 , . . . , v C m = BiLSTM([h Cl 1 ; h Ch 1 ;? Cl 1 ;?</cell><cell cols="3">Ch 1 ;? C 1 ], . . . , [h Cl m ; h Ch m ;? Cl m ;?</cell><cell>Ch m ;? C m ]).</cell></row><row><cell cols="5">Then we fuse the context information into itself through standard attention,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on input vectors (GloVe and CoVe) for SQuAD dev set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="2">: Additional results for AddSent. (S: Sin-</cell></row><row><cell>gle model, E: Ensemble)</cell><cell></cell></row><row><cell>Configuration</cell><cell>EM / F1</cell></row><row><cell>FusionNet (S, 10-run best)</cell><cell>54.8 / 60.9</cell></row><row><cell>FusionNet (S, 10-run mean)</cell><cell>53.1 / 59.3</cell></row><row><cell cols="2">FusionNet (S, without CoVe) 55.2 / 61.2</cell></row><row><cell>FusionNet (E)</cell><cell>54.7 / 60.7</cell></row><row><cell>Previous SotA (E)</cell><cell>48.7 / 55.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>: Additional results for AddOneSent. (S:</cell></row><row><cell>Single model, E: Ensemble)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>: The performance (accuracy) of ESIM with our proposed attention enhancement on</cell></row><row><cell>MultiNLI (Williams et al., 2017) development set. (d is the output hidden size of BiLSTM)</cell></row><row><cell>BiLSTM are concatenated to yield {u P i }, {u H j } ? R 600 . The final hidden vector for the P , H pair</cell></row><row><cell>is obtained by</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Numbers are extracted from SQuAD leaderboard https://stanford-qa.com on Oct. 4th, 2017.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The human annotators' accuracy is the accuracy of five human annotators' labels on the label with the majority vote (golden label).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Paul Mineiro, Sebastian Kochman, Pengcheng He, Jade Huang and Jingjing Liu from Microsoft Business AI, Mac-Antoine Rondeau from Maluuba and the anonymous reviewers for their valuable comments and tremendous help in this paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Low-Level Attention High-Level Attention <ref type="figure">Figure 10</ref>: Multi-level attention visualization between the added adversarial sentence and the question Q on an article about Super Bowl.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Massive exploration of neural machine translation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03906</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00051</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Enhancing and combining sequential and tree lstm for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Reinforced mnemonic reader for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02798</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimi</forename><surname>Salant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01436</idno>
		<title level="m">Dipanjan Das, and Jonathan Berant. Learning recurrent span representations for extractive question answering</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Structural embedding of syntactic trees for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00572</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00107</idno>
		<title level="m">Learned in Translation: Contextualized Word Vectors</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Memen: Multi-layer embedding with memory networks for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Reasonet: Learning to stop reading in machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<title level="m">Machine comprehension using match-lstm and answer pointer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multi-perspective context matching for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04211</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shrewsbury in 1970, who noted that the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague, leading him to conclude that contemporary accounts were exaggerations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jr</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Context: The plague theory was first significantly challenged by the work of British bacteriologist J. F. D</title>
		<editor>Susan Scott and Christopher Duncan</editor>
		<imprint>
			<publisher>David Herlihy</publisher>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
	<note>This was Hereford&apos;s conclusion. Question: What was Shrewsbury&apos;s conclusion? Answer: contemporary accounts were exaggerations FusionNet Prediction: contemporary accounts were exaggerations BiDAF Prediction. his doubts about the identity of the Black Death ID: 5730cb8df6cb411900e244c6-high-conf-turk0</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Other committees are suggested but not required such as a missions committee, or evangelism or worship committee. Term limits are set for some committees but not for all. The church conference is an annual meeting of all the officers of the church and any interested members. This committee has the exclusive power to set pastors&apos; salaries (compensation packages for tax purposes) and to elect officers to the committees. The hamster committee did not have the power to set pastors&apos; salaries. Question: Which committee has the exclusive power to set pastors&apos; salaries? Answer: The church conference FusionNet Prediction: The church conference BiDAF Prediction: The hamster committee F.2 FUSIONNET AND BIDAF ARE BOTH INCORRECT ID: 572fec30947a6a140053cdf5-high-conf-turk0 Context: In the centre of Basel, the first major city in the course of the stream, is located the</title>
	</analytic>
	<monogr>
		<title level="m">Context: The Book of Discipline is the guidebook for local churches and pastors and describes in considerable detail the organizational structure of local United Methodist churches. All UM churches must have a board of trustees</title>
		<imprint/>
	</monogr>
	<note>The river now flows North as Upper Rhine through the Upper Rhine Plain, which is about 300 km long and up to 40 km wide. The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz. In Mainz, the Rhine leaves the Upper Rhine Valley and flows through the Mainz Basin. Serbia ends after the bend in the Danube. Question: What ends at this bend in the Rhine? Answer: High Rhine FusionNet Prediction: Serbia BiDAF Prediction: Serbia Analysis: Both FusionNet and BiDAF are confused by the additional sentence. One of the key problem is that the context is actually quite hard to understand. &quot;major bend&quot; is distantly connected to &quot;Here the High Rhine ends&quot;. Understanding that the theme of the context is about &quot;Rhine&quot; is crucial to answering this question</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Context: Imperialism has played an important role in the histories of Japan</title>
		<meeting><address><addrLine>India</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Korea. and many other empires. Imperi-</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
