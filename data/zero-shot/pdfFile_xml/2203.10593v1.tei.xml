<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open-Vocabulary One-Stage Detection with Hierarchical Visual-Language Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyang</forename><surname>Ma</surname></persName>
							<email>mazongyang2020@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Gao</surname></persName>
							<email>jin.gao@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Brain Science Center</orgName>
								<orgName type="institution">Beijing Institute of Basic Medical Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoru</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congxuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Nanchang Hangkong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Open-Vocabulary One-Stage Detection with Hierarchical Visual-Language Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Open-vocabulary object detection aims to detect novel object categories beyond the training set. The advanced open-vocabulary two-stage detectors employ instance-level visual-to-visual knowledge distillation to align the visual space of the detector with the semantic space of the Pretrained Visual-Language Model (PVLM). However, in the more efficient one-stage detector, the absence of classagnostic object proposals hinders the knowledge distillation on unseen objects, leading to severe performance degradation. In this paper, we propose a hierarchical visual-language knowledge distillation method, i.e., Hi-erKD, for open-vocabulary one-stage detection. Specifically, a global-level knowledge distillation is explored to transfer the knowledge of unseen categories from the PVLM to the detector. Moreover, we combine the proposed globallevel knowledge distillation and the common instance-level knowledge distillation to learn the knowledge of seen and unseen categories simultaneously. Extensive experiments on MS-COCO show that our method significantly surpasses the previous best one-stage detector with 11.9% and 6.7% AP 50 gains under the zero-shot detection and generalized zero-shot detection settings, and reduces the AP 50 performance gap from 14% to 7.3% compared to the best twostage detector. Code will be released at this url 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The emerging trends in advanced detectors <ref type="bibr">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> have improved the speed and accuracy of traditional object detection tasks significantly, whereas the categories they can 1 https://github.com/mengqiDyangge/HierKD ? Corresponding authors. recognize are limited. Once the traditional detectors are expected to detect more object categories in the real-world scenarios, the usual solution falls on labeling more categories of objects in training sets. However, the cost may be unaffordable and the long-tail distribution will be exacerbated by increasing the unseen categories linearly according to Zipf's law <ref type="bibr" target="#b37">[38]</ref>. To overcome these limitations, zeroshot <ref type="bibr" target="#b10">[11]</ref> and open-vocabulary <ref type="bibr" target="#b12">[13]</ref> object detection tasks are proposed to recognize objects from unseen categories (novel categories) while the detector is only trained with annotations from seen categories (base categories). The main difference between these two tasks is that the openvocabulary detector might have seen a novel object during training though its instance-level annotation is not available.</p><p>Therefore, the open-vocabulary detector <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> has developed more rapidly recently, and their performance also lead the former by a large margin.</p><p>There have been some works attempting to redesign the traditional detectors to accomplish the above two detection tasks. These works can also be divided into two-stage <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref> methods and one-stage <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36]</ref> methods as in traditional detection. It is known that the traditional state-of-the-art one-stage detectors have comparable performance and more concise pipeline compared to traditional two-stage ones. However, in the open-vocabulary object detection, the current best two-stage method ViLD <ref type="bibr" target="#b15">[16]</ref> significantly surpasses the similar one-stage method <ref type="bibr" target="#b16">[17]</ref>. As such, it is encouraging to analyze the reason behind this phenomenon and find ways to narrow this performance gap, and then construct a high-performance openvocabulary one-stage detector.</p><p>We show pipelines of recent two-stage and one-stage open-vocabulary detection methods in <ref type="figure" target="#fig_0">Figure 1</ref> (a). It can be seen that both of them perform Instance-level visual-tovisual Knowledge Distillation (IKD) on possible instances of interest in the images. The key difference lies in the selection of instances, i.e., object proposals for two-stage methods and positive sample points for one-stage methods. Compared to the object proposals, there are severe inherent limitations in the positive sample points. We argue that these limitations cause the performance gap between twostage and one-stage methods.</p><p>Specifically, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (a), the positive sample points (red points) only cover the area of the objects from base categories (green boxes), so the one-stage methods can only learn the semantic knowledge about the base categories from the PVLM during the distillation. On the contrary, the class-agnostic proposals (red boxes) in twostage methods usually cover the regions of the objects from novel categories (purple boxes), which enables the twostage methods to implicitly learn the semantic knowledge of novel categories from the PVLM (See sec 4.3 for a clearer analysis). This advantage can effectively expand the semantic category space and further improve performance. What's more, the number of positive sample points is much less than the object proposals in most images, and each positive sample point only covers a smaller area on the feature maps than the proposals. This sparse sampling of the feature map areas during distillation also makes the semantic supervision from PVLM shrink a lot in one-stage methods.</p><p>To compensate for these inherent limitations, a straightforward approach is to make use of more sample points of the feature maps for knowledge distillation. Thus, in this work, we propose a weakly supervised global-level language-to-visual knowledge distillation method (GKD) to achieve this approach. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b), GKD exploits the visual captions that potentially contain seman-tic knowledge of novel categories, and performs languageto-visual knowledge distillation between caption representation and global-level image representation. In this way, GKD implicitly aligns all sample points in the image with the caption semantics, so that the sample points belonging to the novel categories can also learn their related semantic knowledge from the PVLM.</p><p>Finally, our proposed GKD is combined with the commonly used IKD to perform open-vocabulary one-stage detection in an end-to-end fashion, leading to a hierarchical knowledge distillation mechanism-based detector, namely HierKD. We summarize our contributions as follows:</p><p>? A weakly supervised global-level language-to-visual knowledge distillation method is explored to learn novel category knowledge beyond training labels for one-stage detection.</p><p>? An end-to-end hierarchical visual-language knowledge distillation mechanism is proposed to achieve a high-performing open-vocabulary one-stage detector.</p><p>? The proposed HierKD detector significantly surpasses the previous best open-vocabulary one-stage detector with 11.9% and 6.7% AP 50 gains under the zero-shot detection and generalized zero-shot detection settings respectively on MS-COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Zero-shot Learning: As the capability of image recognition with supervised learning has reached a high-level status, researchers begin to explore how well the classification models can recognize objects of novel categories beyond training sets, which is usually referred as zero-shot learning (ZSL). The earliest works start from modeling the attributes of objects by encoding the label space with binary attribute vectors for recognizing objects <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>, while the later works focus more on the semantic representation of the visual space <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>. Recently, PVLM, e.g., CLIP <ref type="bibr" target="#b14">[15]</ref>  The GKD aggregates all the multi-layer feature maps to directly align with the captions by cross attention. During inference, the knowledge distillation modules are removed and the CLIP textual embedding is initialized with the novel categories. It is noteworthy that the distillation has less impact on the regression branch thanks to the inherent characteristics of disentanglement in one-stage detectors.</p><p>projection layer for aligning visual space with textual semantic space based on PixelBERT <ref type="bibr" target="#b13">[14]</ref>. Xie et al. <ref type="bibr" target="#b16">[17]</ref> proposed to distill region-level visual features from CLIP. Another direction focuses on designing more efficient onestage detectors by modifying loss functions <ref type="bibr" target="#b11">[12]</ref>, introducing transductive learning <ref type="bibr" target="#b21">[22]</ref>, and synthesizing features for unseen objects <ref type="bibr" target="#b20">[21]</ref>. Gu et al. <ref type="bibr" target="#b15">[16]</ref> also distilled knowledge from CLIP with a baseline one-stage detector YOLO-v5 <ref type="bibr" target="#b4">[5]</ref>. A lthough it significantly surpasses the previous one-stage methods, there is still a large performance gap compared to the advanced two-stage methods. We have analyzed the reason behind the poorly performing one-stage methods during instance-level knowledge distillation, and concentrate on compensating for their inherent limitations. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the overall framework of our proposed open-vocabulary one-stage detector HierKD. It consists of a teacher pre-trained visual-language model and a student detector during the training phase. Here we employ a pre-trained visual-language model named CLIP 2 for its superior performance. The student model aims to learn the teacher model's zero-shot recognition ability by our proposed hierarchical visual-language knowledge distillation mechanism. In particular, the positive sample points learn from Image Encoder (IE) of the teacher model by instance-2 CLIP ViT-B/32 is selected for fair comparisons with other methods. level visual-to-visual knowledge distillation, and the multiscale feature maps from the detector directly transfer knowledge from Text Encoder (TE) of teacher by global-level language-to-visual knowledge distillation. Notations: The categories in the training set, i.e., base categories is denoted as C B , and the novel categories in the testing set is denoted as C N . In addition, TE and IE of CLIP are denoted as T and V, respectively. The textual embedding T B used in training is initialized offline by feeding each category in C B with a prompt, i.e. "a photo of a <ref type="bibr">[CLS]</ref>.", into the text encoder T . During inference, the only modification is to replace C B with C N or the union C B ? C N under different settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Choosing and Modifying a Base Detector</head><p>The first challenge is how to adapt an off-the-shelf onestage base detector to the open-vocabulary object detection task with necessary structural modifications. Choosing a Base One-stage Detector: We first leverage ATSS <ref type="bibr" target="#b8">[9]</ref> as the base one-stage detector for two reasons: <ref type="bibr">(1)</ref> The adaptive training sample selection mechanism makes it a top performer in the traditional object detection task; <ref type="bibr" target="#b1">(2)</ref> There is only one anchor at each location on the feature maps, which is important because modifying the classification layer (see below) will dramatically increase memory consumption as the number of anchors increases. Modifying the Base Detector: We then make two modifi-cations to the original ATSS, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>: <ref type="formula" target="#formula_0">(1)</ref> The original convolution-based classification layer is modified to the classification form of CLIP with the names or descriptions of the dataset's categories embedded by TE. A background embedding T bg is also required since this modification would lose the original detector's ability to distinguish the background samples. The T bg 3 is initialized by feeding "a photo of background." into T , which allows to learn the background in the training stage. The sigmoid function is also replaced with the softmax function, and the final classification loss is based on the softmax focal loss.</p><formula xml:id="formula_0">p i = SoftMax([? c ? (T B f T i ), ? c ? (T bg f T i )]) , L cls = 1 N pos N i=0 L focalloss (p i , y i ) ,<label>(1)</label></formula><p>where f i , p i and y i denote the anchor feature, classification result and label of the anchor respectively. ? c is a learnable temperature coefficient during training, and N pos is the number of positive sample points while N denotes the total number of positive and negativet samle points; <ref type="formula" target="#formula_1">(2)</ref> The centerness branch in ATSS is replaced with an IOU branch <ref type="bibr" target="#b9">[10]</ref> for mitigating the misalignment between classification task and regression task to a certain extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Instance-level Knowledge Distillation</head><p>We then introduce the instance-level knowledge distillation, which aims at transferring knowledge from the image encoder V. Following the common practice, only the features of positive samples are fetched for distillation. Since the positive sample points in ATSS may have relatively small IOU values with respect to the ground-truth boxes, we set a fixed IOU threshold to further filter out the positive samples with small IOUs and acquire the features for the remaining positive sample points {f 1 , f 2 , ..., f Npos }. Unlike ZSD-YOLO <ref type="bibr" target="#b16">[17]</ref>, we use the predicted boxes of regression branch instead of the ground-truth boxes to crop regions from image I for the sake of data augmentation. These cropped regions are then resized to 224 ? 224 to adapt to the input image size of V. We use a resizing method that can keep more image information, i.e., "Long side + padding", which resizes the long side to 224 and pads the short side with 0. Next, the features to be mimicked {V(I, r i ), r i ? R} can be obtained by feeding these resized regions R into the image encoder V. Finally, the knowledge is transferred from the CLIP image encoder to detectors with distillation as follows: <ref type="bibr" target="#b2">3</ref> We also try to randomly initialize it <ref type="bibr" target="#b16">[17]</ref> and set a fixed zero vector with a bias <ref type="bibr" target="#b12">[13]</ref>, but we finally get similar performance.</p><formula xml:id="formula_1">L ins = 1 N pos Npos i=1 f i f i 2 ? V(I, r i ) V(I, r i ) 2 1 .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max Pool</head><p>Caption: A woman is skating on the ski slope. We have also tried with L 2 norm for mimicking, and there are no obvious differences among different measures after adjusting the appropriate loss weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Global-level Knowledge Distillation</head><p>To overcome the limitation of only learning from the base categories, a weakly supervised GKD module is explored by exploiting the image captions to learn the semantic knowledge of novel categories beyond training labels. GKD mimics the contrastive learning in CLIP to match the image-caption pairs and aims at transferring CLIP's largescale semantic knowledge to the one-stage detector. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the overall process of GKD. Specifically, an arbitrary image denoted by I and its paired caption denoted by C are matched by Multi-Layer Cross Attention (MLCA). For the visual input, feature maps from different FPN layers are evenly divided into N ? N patches, and the Max Pooling operation is performed inside all patches of different feature maps to obtain the patch-level representations. The set of pooled patch features is denote by {P i,j I |i = 1, 2, 3, 4, 5, j = 1, ..., N ? N }, where i indicates the FPN layer and j is the patch location on the feature maps of each layer. Next, for the textual input, the whole caption C is encoded directly by text encoder T to represent the textual feature T C . As the CLIP model is great at extracting the overall high-level textual feature while not at word-level representation in some simple visual-grounding experiments, we choose to take the feature of the entire caption instead of each word.</p><p>After obtaining the textual feature and the set of multilayer patch features, the cross attention takes these multimodal inputs to aggregate the patch features. Specifically, the caption is regarded as query, all patches are regarded as keys, and the response between the query and each key can be calculated via cosine similarity. Hence, the aggregation of all patch features is obtained with the normalized simi-larities as follows:</p><formula xml:id="formula_2">e i,j = T C ? P i,j I T C P i,j I , P C I = 5,k i=j=1 exp(e i,j ) 5,k i =j =1 exp(e i ,j ) P i,j I ,<label>(3)</label></formula><p>where P C I represents the caption-aware visual feature aggregation, and e i,j is the response between the caption and the j th patch of i th layer. Finally, the matching score I, C between the image-caption pair (I, C) is:</p><formula xml:id="formula_3">I, C = P C I ? T C P C I T C .<label>(4)</label></formula><p>Since the aim of our global-level knowledge distillation is to transfer CLIP's large-scale semantic knowledge to the detector, it is naturally to mimic the contrastive learning in CLIP and also the recent self-supervised learning works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. The paired images and captions are regarded as positive pairs in a batch while the others are negative pairs. We introduce a symmetrical contrastive loss function to push the positive pairs and pull the negatives in semantic space:</p><formula xml:id="formula_4">L Glo?I = ?log exp(? m ? I, C ) b Ci=1 exp(? m ? I, C i ) , L Glo?C = ?log exp(? m ? I, C ) b Ii=1 exp(? m ? I i , C ) ,<label>(5)</label></formula><p>where ? m is a trainable temperature coefficient, and b denotes the batch size. Finally, the hierarchical knowledge distillation of our one-stage detector can be formulated by combining the instance-level knowledge distillation and global-level knowledge distillation:</p><formula xml:id="formula_5">L =? cls L cls + ? loc L loc + ? ins L ins + ? Glo (L Glo?I + L Glo?C ) .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Sampling the Negative Samples</head><p>Advanced one-stage detectors often combine focal Loss <ref type="bibr" target="#b6">[7]</ref> or its variants <ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref> with all negative samples to solve the imbalance problem between positive and negative samples. However, this setting is troubling in open-vocabulary detection, for the detectors will identify more foreground regions as background when generalizing to novel categories in experiments. On the other hand, sampling the negative samples to 1:1 with positive samples as in two-stage methods will boost the performance on novel categories, whereas it seriously affects the base categories. To make a trade-off between the above options, we adopt a sampling strategy by sampling 10% negative samples to boost the recall performance on novel categories while maintaining the performance on base categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Direct Inference Alternative with CLIP</head><p>As the zero-shot recognition ability of the proposed method is transferred from CLIP, we can thus measure the mimicking ability of our method by comparing the performance gap between our model and this CLIP direct inference. We design a simple CLIP direct inference way in algorithm 1. Essentially, it compares the difference in the classification results of the same sample points between the detector and the CLIP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation Protocol</head><p>We validate our method on the MS-COCO 2017 benchmark under both zero-shot detection (ZSD) and generalized zero-shot detection (GZSD) settings. In the previous ZSD literature, two different types of base/novel split settings are available: the 48/17 and the 65/15 base/novel splits by Bansal et al. <ref type="bibr" target="#b10">[11]</ref> and Rahman et al. <ref type="bibr" target="#b11">[12]</ref>, respectively. We evaluate both split settings in this paper. Our data preprocessing is the same as Rahman et al. <ref type="bibr" target="#b12">[13]</ref>. Following the most previous ZSD methods, we evaluate our method using mAP and Recall@100 at IOU=0.5, and mainly focus on the performance of novel categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Our implementation and hyper-parameter settings are based on MMdetection <ref type="bibr" target="#b38">[39]</ref>. A standard ResNet-50 [31] is adopted as the backbone, and all hyper-parameters remain the default settings unless otherwise specified. We set the thresholds of NMS and classification score to 0.4 and 0.0 respectively. The temperature coefficients ? c and ? m are initialized to 100 and 10 respectively. We also add a gradient clip at 10.0 during the training stage. For the knowledge distillation, the teacher model CLIP is frozen, and the feature maps of different FPN layers are divided into 3 ? 3 patches. We train the model on 4 Tesla V100 GPUs and use a batch IOU Base/Novel AR@100 AR@300 AR@1000 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Test on generalization ability of RPN</head><p>To more clearly illustrate the generalization ability of the RPN, we train the RPN on the base categories and directly transfer it to test on the novel categories. As shown in Table 1, the category-agnostic proposals in RPN of two-stage methods usually cover the regions of the novel objects, and AR is still up to 37.4 when generating 100 proposals and IOU=0.75, which contributes to feature learning on novel categories during knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We conduct ablation studies on the MS-COCO ZSD benchmark to verify the effectiveness of design choices. All the results are reported on the novel categories under the 48/17 base/novel split setting unless otherwise specified. Instance-level Knowledge Distillation: We compare the impact of different sub-module options in the instance-level knowledge distillation in <ref type="table" target="#tab_1">Table 2</ref>. Compared to our distillation using L 1 norm, replacing it with L 2 loss norm will cause a 1.8% AP 50 drop, and this gap can be reduced through increasing the loss weight. It is essentially because the distance between the features measured by the L2 norm requires a larger weight to be consistent with the result of the L1 norm. The cropped region factor used in knowledge distillation is not sensitive to using prediction boxes or ground-truth boxes. However, it can improve performance by cropping the 1.5? expanded box area to provide more contextual information. Global-level Knowledge Distillation: As shown in Table 3, the different choices of sub-modules have great impacts on the performance. First, We observe that the AP 50 achieved by using Average Pooling is only about half of Max Pooling. This is caused due to the loss of distinguishability of the patch features obtained through Average Pooling. Moreover, compared to dividing the feature maps into  a small number of patches, such as 3?3 or 4?4, dividing it into more patches, such as 8?8, brings a significant AP 50 drop. It can be attributed to the reason that more training iterations are required to converge for more patches. Additionally, there is no obvious difference between 8 bs/gpu and 4 bs/gpu. We infer that both of them we can afford are too small for contrastive learning to make a difference. Finally, replacing contrastive learning with only pushing the positive pairs brings a 2.8% AP 50 drop. This shows that contrastive learning can better transfer the zero-shot recognition capability of the PVLM. Distillation Module Analysis: We quantitatively verify the effectiveness of each distillation module and the compatibility of different modules. We additionally report the detection performance on small objects AP S , medium objects AP M and large objects AP L to perform a more detailed analysis. As shown in <ref type="table" target="#tab_3">Table 4</ref>, by adding IKD and GKD to the baseline, we can obtain 4.4% and 10.5% AP 50 gains as well as 10.0% and 8.7% AR 50 gains respectively. This validates the effectiveness of each distillation module. In addition, compared to applying IKD and GKD separately, the combination of IKD and GKD, i.e., HierKD, further brings 7.7% and 9.0% AR 50 gains respectively. This shows that the great compatibility of IKD and GKD. The AP S and AP M in HierKD are improved by 1.4% and 1.7% compared to GKD, which shows that HierKD has advantages in detecting small and medium objects. Finally, using IOU branch leads to more improvements on the medium and large objects than the small. It may be because objects with low classification scores and high IOUs generally do not appear on small objects.   We also visualize some classification score distribution and detection results for qualitative analysis. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates the spatial distribution of classification score in IKD and HierKD, respectively. We can see that IKD often fails to identify the objects of novel categories, e.g., the "umbrella" in the first row. In addition, IKD may also have low confidence in recognition of the objects of novel categories, such as the "cup" in the second row and the "cat" in the third row. By introducing GKD, the proposed Hi-erKD can recognize the "umbrella" in the first row, and also significantly increases the confidence in recognition of the "cup" in the second row and the "cat" in the third row. This shows that our HierKD can better transfer the novel category knowledge from CLIP and reduce missed detections while increasing detection confidence. We also show some detection results of novel categories in <ref type="figure" target="#fig_5">Figure 5</ref>. First, it can be seen that GKD and HierKD can identify more objects of novel categories compared to IKD, such as the "umbrella" in the second row and the "airplane" in the third row. Moreover, GKD and HierKD also have higher classification accuracy, such as correctly classifying the "elephant" in the first row instead of recognizing it as a "cow" like IKD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IKD HierKD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GKD</head><p>HierKD IKD   Compared to GKD, HierKD can suppress more meaningless detection results, such as the multiple partial "airplane" in the third row.</p><p>Sampling the Negative Samples: The impact of the sampling strategy for negative samples is shown in <ref type="table" target="#tab_5">Table 5</ref>. Taking 100% sampling as the baseline, we can see that 10% sampling does not cause a large AP 50 drop on the base categories in comparison with 1:1 sampling. When generalizing to novel categories, the AP 50 obtained by 10% sampling is not much worse than the 1:1 sampling in IKD while achieving the best in GKD. This validates the effectiveness of the 10% sampling strategy. Compared to Direct Inference with CLIP: The performance gap between the proposed method and direct inference with CLIP is shown in <ref type="table" target="#tab_7">Table 6</ref>. The IKD baseline has only about half of the AP 50 compared to direct inference with CLIP on all sizes of objects, while our proposed GKD achieves similar performance on medium and large objects.</p><p>The final HierKD has higher AR 50 than direct inference with CLIP. However, the AP S in HierKD lags behind the direct inference with CLIP a lot, which shows that our method has insufficient learning ability for small objects. Different Training Settings: As shown in  <ref type="table">Table 7</ref>. Comparison with other state-of-the-art methods: * denotes the state-of-the-art methods in various settings. "TS/MS" and "OS" are abbreviation of two-stage/multi-stage and one-stage detectors, respectively. "ZS" and "OV" indicate that the models belong to zero-shot and open-vocabulary detectors, respectively.</p><p>the base and novel categories. This validates that the proposed HierKD is compatible with the general detection performance improvement techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with the Start-of-the-Art</head><p>We compare our HierKD with the other two-stage methods and one-stage methods on the MS-COCO benchmark in <ref type="table">Table 7</ref>, all metrics reported in <ref type="table">Table 7</ref> are AP 50 . Limitation: we can not make a completely fair comparison like the traditional object detection because the factors of batch size, scale jitter, etc., used in some works (such as ViLD <ref type="bibr" target="#b15">[16]</ref>) are different from the general settings.</p><p>We can observe that under the 48/17 base/novel split setting, HierKD achieves 25.3% AP 50 on novel categories under the ZSD setting. HierKD significantly outperforms the previous best one-stage method ZSD-YOLO with 11.9% AP 50 gains, and also exceeds the most recent two-stage method OVD (trained without external Conceptual Caption dataset <ref type="bibr" target="#b36">[37]</ref>) by 8.6% AP 50 . Under the GZSD setting, Hi-erKD outperforms ZSD-YOLO with 6.7% gains on novel categories. HierKD also reduces the AP 50 performance gap from 14% to 7.3% compared to the best two-stage method ViLD. Under the GZSD setting, the AP 50 of HierKD on the novel categories is 5% lower than that of the ZSD. This is caused by the detection confidence of the novel categories is lower than that of the base categories, so some detection results of novel categories are suppressed during NMS.</p><p>Under another 65/15 base/novel split setting, HierKD surpasses the previous best method ZSD-YOLO with 10.1% and 2.5% AP 50 gains on novel categories under ZSD and GZSD settings respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Upper Bound Analysis</head><p>We can get the ideal upper bound of this type of distillation method by directly using CLIP to classify the instances in the ground-truth boxes and then evaluating the detection results, i.e., the classification results of ground-truth boxes. As shown in <ref type="table">Table 9</ref>, our method achieves a relatively high recall, while the total AP 50 and AP on objects of various sizes, i.e. AP S , AP M , AP M are still far from the upper bound. This shows that there is still much room to improve the mimicking ability of the proposed HierKD. In addition, this also reminds us of using techniques such as prompt learning <ref type="bibr" target="#b42">[43]</ref> to improve the zero-shot recognition ability of CLIP itself, thereby further improving the upper bound of model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have developed a hierarchical visuallanguage knowledge distillation method, namely HierKD, to obtain a top-performing one-stage open-vocabulary detector. HierKD uses image caption to distill knowledge in a language-to-visual manner. The rich vocabulary in captions enables HierKD to transfer the semantic knowledge of novel categories from CLIP during training. The results indicate that the proposed HierKD can identify novel objects more accurately and confidently, and significantly surpasses the previous methods. In the future, we will continue to explore more efficient and advanced distillation methods to transfer the zero-shot recognition ability of teacher models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Instance-level Knowledge Distillation GT boxes of base categories GT boxes of novel categories Proposals of two-stage methods Positive sample points of one-stage methods IE Image Encoder of the PVLM (b) Global-level Knowledge Distillation TE Text Encoder of the PVLM Aggregate IE Comparisons between instance-level and global-level knowledge distillation: (a) illustrates the pipelines of two-stage methods and one-stage methods with instance-level knowledge distillation. (b) illustrates our proposed global-level knowledge distillation, which directly distills the caption representation from PVLM to the global image representation from detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our open-vocabulary one-stage detector with hierarchical visual-language knowledge distillation: In the training stage, the classification branch is initialized with the CLIP textual embedding of base categories. For IKD, the aim is to minimize the distance between the features of sparse positive sample points on feature maps and the CLIP visual embedding of the cropped regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Global-level knowledge distillation: This GKD module takes the caption as textual input and the feature maps from multi layers as visual input, and learns to match the image-caption pairs by mimicking the contrastive learning in CLIP through the multi-layer cross-attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 : 3 A? 7 B,</head><label>137</label><figDesc>CLIP Direct Inference Input: CLIP image encoder V and text encoder T , novel categories C N , trained model M, test images D T Output: Detection boxes B 1 T N ? T (P rompt(C N )) and normalize; 2 for I ? D T do I ? M anchor (I); 4 A f ore I ? arg max k (M cls (A I ) ? M IOU (A I )); 5 V f ore I ? V(I, M loc (A f ore I )) and normalize; 6 S f ore I Sof tmax(? ? T N V f ore I ); ? B ? N M S(S f ore I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Spatial distribution of classification score. The red boxes in the images are ground-truth of the novel categories. The heatmaps in IKD and HierKD show the classification score of anchors at each location for the categories in the red boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of some detections on novel categories. Model CLIP AR 50 AP 50 AP S AP M AP L IKD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparisons between different sub-module options in IKD. pred and GT mean cropping regions from prediction boxes and ground-truth boxes, respectively. 1? and 1.5? represent cropping the original box and its 1.5? center expansion respectively. size of 16 in IKD and 32 in GKD and HierKD. The learning schedule follows the traditional object detection settings.</figDesc><table><row><cell>5</cell><cell>48/17</cell><cell>61.9</cell><cell></cell><cell>76.9</cell><cell>87.5</cell></row><row><cell>0.75</cell><cell>48/17</cell><cell>37.4</cell><cell></cell><cell>48.1</cell><cell>57.4</cell></row><row><cell></cell><cell cols="5">Table 1. Generalization ability of RPN</cell></row><row><cell cols="6">Norm Weight Region Area AR 50 AP 50</cell></row><row><cell>L 1</cell><cell>1</cell><cell>pred</cell><cell>1?</cell><cell>62.4</cell><cell>14.6</cell></row><row><cell>L 2</cell><cell>1</cell><cell>pred</cell><cell>1?</cell><cell>65.1</cell><cell>12.8</cell></row><row><cell>L 2</cell><cell>10</cell><cell>pred</cell><cell>1?</cell><cell>63.6</cell><cell>14.6</cell></row><row><cell>L 1</cell><cell>1</cell><cell>GT</cell><cell>1?</cell><cell>62.8</cell><cell>14.5</cell></row><row><cell>L 1</cell><cell>1</cell><cell>pred</cell><cell>1.5?</cell><cell>64.5</cell><cell>15.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Patch Pool Loss bs/gpu AR 50 AP 50 Comparisons between different sub-module options in GKD. Ave and Max represent using Average Pooling and Max Pooling to obtain patch features respectively. CL denotes training with the contrastive learning loss, while PL only considers the cosine similarities between positive pairs. bs/gpu is the batch size on each GPU during training.IKD GKD IOU b AR 50 AP 50 AP S AP M AP L</figDesc><table><row><cell>4</cell><cell></cell><cell>Ave</cell><cell>CL</cell><cell>8</cell><cell>59.2</cell><cell>12</cell></row><row><cell>4</cell><cell></cell><cell>Max</cell><cell>CL</cell><cell>8</cell><cell>64.2</cell><cell>20.1</cell></row><row><cell>3</cell><cell></cell><cell>Max</cell><cell>CL</cell><cell>8</cell><cell>61.1</cell><cell>20.7</cell></row><row><cell>8</cell><cell></cell><cell>Max</cell><cell>CL</cell><cell>8</cell><cell>60.8</cell><cell>13.7</cell></row><row><cell>3</cell><cell></cell><cell>Max</cell><cell>PL</cell><cell>8</cell><cell>60.9</cell><cell>17.9</cell></row><row><cell>3</cell><cell></cell><cell>Max</cell><cell>CL</cell><cell>4</cell><cell>65.6</cell><cell>20.5</cell></row><row><cell>-? ? ?</cell><cell>-? ? ?</cell><cell>-?</cell><cell cols="4">52.4 10.2 8.8 12.5 12.8 62.4 14.6 10.1 13.2 19.1 61.1 20.7 10.1 28.5 27.5 70.1 20.7 11.5 30.2 27.0 71.3 21.6 11.6 30.7 28.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Verify the effectiveness and compatibility of each module. The first row is the baseline, which is the exploited base detector trained with only classification loss and localization loss.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Comparisons between different sampling strategies for negative samples. 1:1, 10%, 100% mean sampling the same number of negative samples as the positive samples, sampling 10 % of the negative samples, and using all the negative samples.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Comparison with the direct inference with CLIP. Model and CLIP represent inference with the detector from distillation and direct inference with CLIP respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>, extend-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Backbone Schedule Scale JitterBase Novel AR 50 AP 50 AR 50 AP 50</figDesc><table><row><cell>ResNet-50</cell><cell>1?</cell><cell></cell><cell>74.8</cell><cell>44.7</cell><cell>71.3</cell><cell>21.6</cell></row><row><cell>ResNet-50 ResNet-50 ResNet-101</cell><cell>2? 3? 3?</cell><cell>? ?</cell><cell>77.5 80.0 80.8</cell><cell>49.0 51.8 53.5</cell><cell>69.8 70.0 71.4</cell><cell>23.1 25.3 27.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Verification of the compatibility with general detection performance improvement techniques.Base/Novel AR 50 AP 50 AP S AP M AP LTable 9. Comparison with the ideal upper bound. All reported metrics are results on the novel categories.</figDesc><table><row><cell>HierKD</cell><cell>48/17</cell><cell>71.4</cell><cell>27.3 11.4 39.5 37.3</cell></row><row><cell>Upper Bound</cell><cell>48/17</cell><cell>70.7</cell><cell>68.0 36.3 74.5 87.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic anchor assignment with iou prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<idno>2020. 4</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="384" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved visualsemantic alignment for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Openvocabulary object detection using captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="393" to="407" />
		</imprint>
	</monogr>
	<note>402. 1, 2, 4, 5, 8</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Zero-shot detection via vision and language knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13921</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Zsd-yolo: Zero-shot yolo detection using vision-language knowledgedistillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12066</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Zeroshot object detection by hybrid region embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06157</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Don&apos;t even look once: Synthesizing features for zero-shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="11" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transductive learning for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6082" to="6091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Zero shot recognition with unreliable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4327</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via semantic embeddings and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6857" to="6866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Zero-shot instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2593" to="2602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Zero-shot object detection with textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kanhere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8690" to="8697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Background learnable cascade for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gtnet: Generative transfer network for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Theory of Zipf&apos;s law and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Saichev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malevergne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sornette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">632</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno>pp. 21 002-21 012, 2020. 5</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">641</biblScope>
			<biblScope unit="page" from="11" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tood: Task-aligned one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="3510" to="3519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01134</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
