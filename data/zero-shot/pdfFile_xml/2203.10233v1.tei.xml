<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Dat</forename><surname>Truong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVIU Lab</orgName>
								<orgName type="institution">University of Arkansas</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc-Huy</forename><surname>Bui</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">FPT Software</orgName>
								<orgName type="institution">NextG</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Nhan</forename><surname>Duong</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Concordia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Seok</forename><surname>Seo</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Dep. of Food Science</orgName>
								<orgName type="institution">University of Arkansas</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><forename type="middle">Lam</forename><surname>Phung</surname></persName>
							<email>phung@uow.edu.au</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Wollongong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
							<email>xin.li@mail.wvu.edu</email>
							<affiliation key="aff5">
								<orgName type="institution">West Virginia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVIU Lab</orgName>
								<orgName type="institution">University of Arkansas</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human action recognition has recently become one of the popular research topics in the computer vision community. Various 3D-CNN based methods have been presented to tackle both the spatial and temporal dimensions in the task of video action recognition with competitive results. However, these methods have suffered some fundamental limitations such as lack of robustness and generalization, e.g., how does the temporal ordering of video frames affect the recognition results? This work presents a novel end-to-end Transformer-based Directed Attention (Direc-Former) framework 1 for robust action recognition. The method takes a simple but novel perspective of Transformerbased approach to understand the right order of sequence actions. Therefore, the contributions of this work are threefold. Firstly, we introduce the problem of ordered temporal learning issues to the action recognition problem. Secondly, a new Directed Attention mechanism is introduced to understand and provide attentions to human actions in the right order. Thirdly, we introduce the conditional dependency in action sequence modeling that includes orders and classes. The proposed approach consistently achieves the state-of-the-art (SOTA) results compared with the recent action recognition methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b74">74]</ref>, on three standard large-scale benchmarks, i.e. Jester, Kinetics-400 and Something-Something-V2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video understanding has recently become one of the popular research topics in the computer vision community. Video data has become ubiquitous and occurs in numerous daily activities and applications, e.g., movies and camera surveillance <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b61">61]</ref>. In the field of video understanding <ref type="bibr" target="#b60">[60]</ref>, action recognition has become a fundamental problem. In action recognition, there is a need to pay more attention to the temporal structures of the video sequences. Indeed, emphasis on temporal modeling is a common strategy among most methods. It can be considered as the main difference between video and images. These works include long-short term dependencies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>, temporal structure, low-level motion, and action modeling as a sequence of events or states.</p><p>The current methods in video action recognition utilize 3D or pseudo 3D convolution to extract the spatio-temporal features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b70">70]</ref>. However, these 3D CNN-based methods suffer from intensive computation with many parameters to be learned. Others attempt to adopt two-stream structures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b52">53]</ref> for accurate action recognition, since information from one branch could be fused to the other one. Some methods in this category require computing the optical flow first, which could be time-consuming and requires a large amount of storage. Others apply 3D convolution to avoid computing the optical flow. Nonetheless, this approach also requires a large amount of computational resources to implement.</p><p>Although prior methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b66">66]</ref> have achieved remarkable performance, they have several limitations related to the robustness of the models. In this paper, we therefore address two fundamental questions for current action recognition models. In the first question, given a set of video frames shuffled in a random order and different from the original one, will it be classified as the same label as the original recognition result? If it is the case, these models have been clearly overfitted or biased to other factors (e.g. scene background), rather than learned semantic information of the actions. In the second question, we want to understand whether these action recognition models are able to correct the incorrectly-ordered frames to the right ones and provide an accurate prediction? Finally, we introduce a new theory to improve the robustness and generalization of the action recognition models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contributions of this Work</head><p>In this work, we present a new end-to-end Transformerbased Directed Attention (DirecFormer) approach to robust action recognition. Our method takes a simple but novel perspective of Transformer-based approach to learn the right order of a sequence of actions. The contributions of this work are three-fold. First, we introduce the problem of ordered temporal learning in action recognition. Second, a new Directed Attention mechanism is introduced to provide human action attentions in the right order. Third, we introduce the conditional dependency in action sequence modeling that includes orders and classes. The proposed approach consistently achieves the State-of-the-Art results compared to the recent methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b72">72]</ref> on three standard action recognition benchmarks, i.e. Jester <ref type="bibr" target="#b44">[45]</ref>, Somethingin-Something V2 <ref type="bibr" target="#b22">[23]</ref> and Kinetics-400 <ref type="bibr" target="#b30">[31]</ref>, as in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video Action Classification. In recent years, video understanding has become a popular topic in Computer Vision due to its promising applications such as robotics, autonomous driving, camera surveillance or human behavior analysis. In the early days, many traditional approaches used hand-crafted features as a method to encode information of video sequences <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b67">67]</ref>. Among these approaches, iDT <ref type="bibr" target="#b62">[62]</ref> achieved very good performance by utilizing dense trajectory features and became one of the most popular hand-designed methods. Later, with the success of deep learning architecture <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b54">[54]</ref><ref type="bibr" target="#b55">[55]</ref><ref type="bibr" target="#b56">[56]</ref> using computing hardware, i.e., GPU, TPU, and the introduction of various large-scale datasets, e.g. Sport1M <ref type="bibr" target="#b29">[30]</ref>, Kinetics <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31]</ref> and AVA <ref type="bibr" target="#b23">[24]</ref>, video understanding, especially video action recognition, has become easier to approach in the research community, resulting in an introduction of a series of deep learning frameworks. These methods mainly focus on learning spatio-temporal representations in an end-to-end classification manner. <ref type="bibr" target="#b11">[12]</ref> proposed to model the temporal relationship using LSTM <ref type="bibr" target="#b25">[26]</ref> to corporate 2D CNN features. <ref type="bibr" target="#b29">[30]</ref> presented an approach that fuses the information from the temporal dimension while suggesting applying a single 2D CNN to each frame of the video sequence. However, this method cannot handle well the motion change and perform weaker than the hand-crafted features methods.</p><p>The remaining approaches for video action recognition could be divided into two categories. The first group contains models that adopt the conventional two-stream structure <ref type="bibr" target="#b52">[53]</ref> to improve the temporal modeling capability. A spatial 2D CNN is used to learn semantic features and a temporal 2D CNN is applied in the other branch to analyze the motion content of the video sequences using the optical flow as input. Both streams are trained in parallel and the scores are averaged to make the final predictions. <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> studied different combinations to fuse spatio-temporal information between both streams. TSN <ref type="bibr" target="#b64">[64]</ref> proposed sampling sparse frames from evenly divided segments of the video clip to capture long-range dependencies. These dualpath methods require additional computation of the optical flow, which is time-consuming and demand a considerable amount of storage. However, our proposed method can operate without the need of optical flow modality, thus, reducing the complexity of the network.</p><p>The second category for action recognition is 3D CNN based methods and the (2+1)D CNN variants. C3D <ref type="bibr" target="#b57">[57]</ref> was the first work to apply 3D convolutions to model the spatial and temporal features together. I3D <ref type="bibr" target="#b6">[7]</ref> was proposed to inflate 2D convolutional kernels into 3D to capture spatio-temporal features. However, the major drawback of 3D CNNs is the large number of parameters involved. To cope with the intensive computation of 3D CNN, various methods adopted the 2D + 1D paradigm. P3D <ref type="bibr" target="#b49">[50]</ref> decomposes 3D convolution into a pseudo-3D convolutional block. R(2+1)D <ref type="bibr" target="#b59">[59]</ref> and S3D-G <ref type="bibr" target="#b70">[70]</ref> factorize the 3D convolution to enhance accuracy and reduce the complexity. TRN <ref type="bibr" target="#b76">[76]</ref> introduced an interpretable relational module to replace the average pooling operation. TSM <ref type="bibr" target="#b41">[42]</ref> shifts part of the features forward and backward along the temporal dimension, allowing the network to achieve the performance of 3D CNN but maintains the complexity of 2D CNN. Nonlocal neural network <ref type="bibr" target="#b65">[65]</ref> proposed a special non-local operation for better capturing the long-range temporal dependencies between video frames. SlowFast <ref type="bibr" target="#b18">[19]</ref> adopted a dual-path network to model the spatio-temporal information at two different temporal rates, with mid-level features being interactively fused. In general, our method also learns to approximate the spatio-temporal representation at the feature level with the help of the knowledge distillation process.</p><p>More recently, significant improvement in terms of efficiency has been reported for action recognition in <ref type="bibr" target="#b7">[8]</ref>; it was found that 2D-CNN and 3DCNN models behave similarly in terms of spatio-temporal representation ability and transferability. More efficient action recognition can be achieved by focusing more on making the most of selected frames by dynamic knowledge propagation <ref type="bibr" target="#b31">[32]</ref> or exploiting spatiotemporal self-similarity <ref type="bibr" target="#b34">[35]</ref>. Most recent works such as elastic semantic network (Else-Net) <ref type="bibr" target="#b39">[40]</ref> and memory attention network (MAN) <ref type="bibr" target="#b38">[39]</ref> also reported promising improvement in terms of recognition accuracy. Video Ordering Several prior works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b71">71]</ref> have considered the frame orders into account. Although these prior works have partially addressed some aspects of order prediction, their losses only provide a weak supervision, i.e. binary label for in-or out-of-order <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b45">46]</ref> or sub-clip based order <ref type="bibr" target="#b71">[71]</ref>. Moreover, there is no explicit mechanism to enforce the model focus on the motion information rather than particular background information of the scene. Video Transformer Transformer approaches have filled an important role by acquiring competitive accuracy while maintaining computational resources compared with the traditional convolution method. A pure-transformer based model (ViViT) was demonstrated in <ref type="bibr" target="#b1">[2]</ref> handling spatiotemporal tokens from a long sequence of frames by factorizing space-time dimension inputs efficiently on both large and small datasets. The divided spatial and tempo-ral attention within each block, TimeSformer <ref type="bibr" target="#b3">[4]</ref> reduces training time while achieving comparable test efficiency. A Spatial-Temporal Transformer network (ST-TR) was developed in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b75">75]</ref> for skeleton-based action recognition. A transformer-based RGB-D egocentric action recognition framework called Trear was proposed in <ref type="bibr" target="#b40">[41]</ref> showing dramatic improvement over the existing state-of-the-art results. A multiscale pyramid network called MViT was proposed in <ref type="bibr" target="#b16">[17]</ref> to extract information from low-level to high levels of attention. When compared with many other successful applications of transformers, their potential in action recognition has still largely remained unexplored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head><p>Let x ? R T ?H?W ?3 be the input video and y be the corresponding label of the video x. H, W and T are the height, the width and the number of frames of a video, respectively. Let o ? N T be the permutation representing the reordering of video frames and i be the indexing associated with the permutation. Our goal is to learn a deep network to classify the actions and infer the permutation simultaneously as in Eqn. <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_0">arg max ? E x,y,o,i (log(p(y|x; ?)) + log(p(i|T (x, o); ?))) (1)</formula><p>where ? is the parameters of the deep neural network, and T is the permutation function. Given a video x and the permutation o, the goal is to learn the class label y of the ordered video and learn the ordering i of the video after permutation T (x, o).</p><p>To effectively predict the class label y and the indexing of the permutation i, a Transformer with Directed Attention is introduced to learn the directed attention in both spatial and temporal dimensions. The proposed DirecFormer is therefore formulated as in Eqn. <ref type="bibr" target="#b1">(2)</ref>.</p><formula xml:id="formula_1">y = ? cls ? G(x) i = ? ord ? G(T (x, o))<label>(2)</label></formula><p>where G is the proposed DirecFormer; ? cls and ? ord are the projections that map the token outputted from DirecFormer to the predicted class label? and the predicted ordering index?, respectively; and ? is the functional composition. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates our proposed framework. The proposed DirecFormer method will be described in detail in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Patch Representation</head><p>Given a video frame, it is represented by N nonoverlapped patches of P ? P (N = HW P 2 ) as in <ref type="bibr" target="#b12">[13]</ref>. Let us denote x s,t ? R 3P 2 as a vector representing the patch s of the video frame t, where s (1 ? s ? N ) denotes the spatial position and t represents the temporal dimension (1 ? t ? T ). To embed the temporal information into the representation, the raw patch representation is projected to the latent space with additive temporal representation as in Eqn. <ref type="bibr" target="#b2">(3)</ref>.</p><formula xml:id="formula_2">z (0) s,t = ?(x s,t ) + e s,t<label>(3)</label></formula><p>where ? is the embedding network and e s,t is the spatialtemporal embedding added into the patch representation. The output sequences {z (0) s,t } N,T s=1,t=1 represent the input tokens fed to our DirecFormer network. We also add one more learnable token z 0,0 in the first position, as in BERT <ref type="bibr" target="#b9">[10]</ref> to represent the classification token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Directed Attention Approach</head><p>The proposed DirecFormer consists of L encoding blocks. In particular, the current block l takes the output tokens of the previous block l ? 1 as the input and decomposes the token into the key k    </p><formula xml:id="formula_3">s,t = ? (l) k ? (l) k z (l?1) s,t v (l) s,t = ? (l) v ? (l) v z (l?1) s,t q (l) s,t = ? (l) q ? (l) q z (l?1) s,t (4) where ? (l) k , ? (l) v and ? (l)</formula><p>q represent the key, value, and query embedding, respectively; ?</p><formula xml:id="formula_4">(l) k , ? (l) v and ? (l) q</formula><p>are the layer normalization <ref type="bibr" target="#b2">[3]</ref>.</p><p>In the traditional self-attention approach, the attention matrix is computed by the scaled dot multiplication between key and query vectors. Although scaled dot attention has shown its potential performance in video classification, this attention is non-directed because it is unable to illustrate the direction of attention. In particular, the scaled dot attention simply indicates the correlations among tokens and ignores the temporal or spatial ordering among tokens. It is noticed that the ordering of frames in a video sequence does matter. The recognition of actions in a video is highly dependent on the ordering of video frames. For example, the same group of video frames, if ordered differently in time, may result in different actions, e.g. walking might become running. However, traditional Softmax attention can not fully exploit the ordering of video frames because it does not contain the directional information of the correlation.</p><p>Therefore, we propose a new Directed Attention using the cosine similarity. Formally, the attention weights a  s,t can be formulated as in Eqn. <ref type="bibr" target="#b4">(5)</ref>.</p><formula xml:id="formula_5">a (l) (s,t) = ? ? ?cos ? ? q (l) s,t ? D , k (l) 0,0 ? ? ? ? ? cos ? ? q (l) s,t ? D , k (l) s ? ,t ? ? ? ? ? ? N,T s ? =1,t ? =1 ? ? ? (5)</formula><p>where D is the dimensional length of the query vector q (l) s,t , a (l) p,t ? R N T +1 denotes the directed attention weights. This attention is computed over the spatial and temporal dimensions. As a result, this operator suffers a heavy computational cost. We therefore divide and conquer the Directed Attention in the spatial dimension and temporal dimension sequentially as in <ref type="bibr" target="#b3">[4]</ref>.</p><p>More specifically, we first implement the attention mechanism over the time dimension (a (l)?time (s,t) ) as in Eqn. <ref type="bibr" target="#b5">(6)</ref>.</p><formula xml:id="formula_6">a (l)?time (s,t) = ? ? ?cos ? ? q (l) s,t ? D , k (l) 0,0 ? ? ? ? ? cos ? ? q (l) s,t ? D , k (l) s,t ? ? ? ? ? ? T t ? =1 ? ? ?<label>(6)</label></formula><p>Then, the directed temporal attention information is accumulated to the current token representations as in Eqn. <ref type="bibr" target="#b6">(7)</ref>. tion as in Eqn. <ref type="bibr" target="#b7">(8)</ref>.</p><formula xml:id="formula_7">k ? (l) s,t = ? ?(l) k ? ?(l) k z ? (l)?time s,t v ? (l) s,t = ? ?(l) v ? ?(l) v z ? (l)?time s,t q ? (l) s,t = ? ?(l) q ? ?(l) q z ? (l)?time s,t<label>(8)</label></formula><p>Next, the Directed Attention over the spatial dimension (a</p><formula xml:id="formula_8">(l)?space (s,t)</formula><p>) can be computed as in Eqn. <ref type="bibr" target="#b8">(9)</ref>.</p><formula xml:id="formula_9">a (l)?space (s,t) = ? ? ?cos ? ? q ? (l) s,t ? D , k ?(l) 0,0 ? ? ? ? ? cos ? ? q ? (l) s,t ? D , k ?(l) s,t ? ? ? ? ? ? T t ? =1 ? ? ?<label>(9)</label></formula><p>The Spatial Directed Attention is then embedded to the temporal attentive features z ? (l)?time</p><formula xml:id="formula_10">s,t</formula><p>to obtain a new spatial</p><formula xml:id="formula_11">attentive feature z ? (l)?space s,t</formula><p>as in Eqn. <ref type="bibr" target="#b9">(10)</ref>.</p><formula xml:id="formula_12">s (l)?space s,t = a (l)?space (s,t),(0,0) v (l) 0,0 + N s ? =1 a (l)?space (s,t),(s ? ,t) v (l) s ? ,t z ? (l)?space s,t = z ? (l)?times s,t + ? (l)?space s (l)?space s,t<label>(10)</label></formula><p>Finally, the Spatial-Temporal Attentive features z ? (l)?space s,t are projected to the output token, getting ready for the next transformer block. Formally, the output of the current transformer block (z (l) s,t ) can be formed as in Eqn. <ref type="bibr" target="#b10">(11)</ref>.</p><formula xml:id="formula_13">z (l) s,t = ? (l) ? (l) z ? (l)?space s,t + z ? (l)?space s,t<label>(11)</label></formula><p>where ? (l) is a projection mapping implemented using a multi-layer perception network, and ? (l) denotes the layer normalization <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Classification Embedding</head><p>The final representation is obtained in the final block of DirecFormer. Then, the class index and the order index of the video are predicted using linear projections as follows:</p><formula xml:id="formula_14">y = ? cls ? cls z (L) 0,0 ? = ? odr ? odr z (L) 0,0<label>(12)</label></formula><p>where ? cls and ? ord are the classification projection and order projection, respectively; ? cls and ? ord are the layer normalization <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Self-supervised Guided Loss For Directed Temporal Attention Loss</head><p>In this stage, we are given the permutation of the current input video. To further reduce the burden of the network when learning the temporal attention, we propose a new self-supervised guided loss to enforce the temporal attention learning from the prior order knowledge. Formally, the self-supervised loss can be formulated as in Eqn. <ref type="bibr" target="#b12">(13)</ref>.</p><formula xml:id="formula_15">L self = 1 LN T 2 L l=1 N,T s=1,t=1 T t ? =1 1 ? a (l)?time (s,t),(s,t ? ) ?(ot, o t ? ) (13) where ?(o t , o t ? ) = 1 if the index o t &lt; o t ? , otherwise ?(o t , o t ? ) = ?1.</formula><p>The guided loss L self helps to indicate the attention learning the correct direction during the training process. Finally, the total loss function of DirecFormer is defined as in Eqn. <ref type="bibr" target="#b13">(14)</ref>.</p><formula xml:id="formula_16">L = ? cls L cls + ? ord L ord + ? self L self<label>(14)</label></formula><p>where L cls and L ord are the cross-entropy losses of the classification projection (? cls ) and order projection (? ord ), respectively; {? cls , ? ord , ? self } are the parameters controlling their relative importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present the evaluation results with Di-recFormer on three popular action recognition benchmarking datasets, i.e. Jester <ref type="bibr" target="#b44">[45]</ref>, Something-Something V2 <ref type="bibr" target="#b22">[23]</ref>, and Kinetics 400 <ref type="bibr" target="#b30">[31]</ref>. Firstly, we describe our implementation details and datasets used in our experiments. Secondly, we analyze our results with different settings shown in the ablation study on the Jester dataset. Lastly, we present our results on Something-Something V2 and Kinetics compared to prior state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>The architecture of DirecFormer consists of L = 12 blocks. The input video consists of T = 8 frames sampled at a rate of 1/32 and the resolution of each frame is 224?224 (H = W = 224). The patch size is set to 18?18; therefore, there are N = 224 2 16 2 = 196 patches in total for each frame. The embedding network ? is implemented by a linear layer in which the output dimension is set to 768. All values (?</p><formula xml:id="formula_17">(l) v , ? ?(l) v ), key (? (l) k , ? ?(l) k ), query (? (l) q , ? ?(l)</formula><p>q ) embedding networks, and projections (? (l)?time , ? (l)?space ) are also implemented by the linear layers. Similar to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>, we adopt the multi-head attention in our implementation, where the number of heads is set to 12. The network ? (l) is implemented as the residual-style multi-layer perceptron consisting of two fully connected layers followed by a normalization layer. Finally, the classification projection (? cls ) and the order projection (? ord ) are implemented as the linear layer. We set the control parameters of loss to 1.0, i.e. ? cls = ? ord = ? self = 1.0.</p><p>There will be a total of T ! permutations of the video frames. Therefore, learning with all permutations is ineffective. Moreover, the permutation set plays an important role. If these two permutations are very far from each other, the network may easily predict the order since the two permutations have significant differences. However, if all the permutations are close to each other, learning the temporal attention is more challenging since the two permutations have minor differences in order. Therefore, we select 1,000 random permutations from T ! = 8! permutations so that the Hamming distance between permutations is as minimum as possible. Similar to <ref type="bibr" target="#b4">[5]</ref>, we use a greedy algorithm to generate the set of permutations.</p><p>In the evaluation, following the protocol of other papers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, the single clip is sampled in the middle of the video. We use three spatial crops (top-left, center, and bottom-right) from the temporal clip and obtain the final result by averaging the prediction scores for these three crops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>Jester. <ref type="bibr" target="#b44">[45]</ref> This dataset is a large-scale gesture recognition real-world video dataset that includes 148, 092 videos of 27 actions. Each video is recorded for approximately 3 seconds. <ref type="figure" target="#fig_8">Fig 3(a)</ref> illustrates the video samples of Jester. Something-in-Something V2. <ref type="bibr" target="#b22">[23]</ref> The dataset is a largescale dataset to show humans performing predefined basic actions with everyday objects, which includes 174 classes. It contains 220, 847 videos, with 168, 913 videos in the training set, 24, 777 videos in the validation set, and 27, 157 videos in the testing set. Similar to other work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b77">77]</ref>, we report the accuracy on the validation set. <ref type="figure" target="#fig_8">Fig. 3(b</ref>  <ref type="figure" target="#fig_8">Fig. 3(c)</ref> illustrates the video examples of Kinetics-400. In our experiment, following the protocol of other papers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b68">68]</ref>, we report the accuracy on the validation set. The license of Kinetics is registered by Google Inc. under a Creative Commons Attribution 4.0 International License.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Effectiveness Of Directed Attention To show the effectiveness of our proposed Directed Attention, we consider three different types of the temporal-spatial attention: (i) Softmax Temporal Attention followed by Cosine Spatial Attention (DirecFormer S?C), (ii) Cosine Temporal Attention followed by Softmax Spatial Attention (DirecFormer C ? S), and (iii) Cosine Temporal Attention followed by Cosine Spatial Attention (DirecFormer C ?C). The method is also compared with TimeSformer where the softmax attention is applied for both time and space. <ref type="table" target="#tab_0">Table 1</ref> illustrates the results of the DirecFormer with different settings compared to TimeSFormer and other approaches. In all configurations, our proposed DirecFormer outperforms the prior methods.</p><p>Considering the effectiveness of the directed attention in time and space, the directions of the attention over the spatial dimension are important in some cases. For example, if A performs an action to B then B receives an action from A. Considering the mentioned example, the spatial attention should involve directions so that the model can learn <ref type="table">Table 2</ref>. Order Correction By Hamilton Algorithm Performance On Jester. X ?Y denotes for the attention types of temporal and spatial dimension, respectively. X (and Y ) could be either S: Softmax or C: Cosine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Attention the actor(s) performing actions in a video. However, the order of the temporal dimension plays a more important role in a video compared to the spatial dimension, since the order of the frames represents how the action is happening. As in <ref type="table" target="#tab_0">Table 1</ref>, the results of DirecFormer C ? S are better than DirecFormer S ? C confirming our hypothesis about the importance of time and space. When the Directed Attention is deployed in both temporal and spatial dimensions, the results of DirecFormer C ? C were significantly improved and achieved the SOTA performance on the Jester dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness Of Losses</head><p>With the order prediction loss L ord , the performance of the DirecFormer in all settings has been improved, since the prediction loss influences the way that network learns the Directed Temporal Attention. Moreover, the performance of DirecFormer is improved by employing the self-supervised guided loss L self . This selfsupervised loss further enhances the directed temporal attention learning during the training. Consequently, the performance of DirecFormer is consistently improved by using our proposed losses, as in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Order Correction To illustrate the ability of order learning of DirecFormer, we conduct an experiment in which, given a random temporal order video, we show our approaches can retrieve back the correct order of the video from the directed temporal attention. In this experiment, we use the temporal attention of the last block and average this temporal attention over the spatial dimension. Then, we perform a search algorithm to find the Hamiltonian path on the temporal attention to find the correct order. In particular, we consider the temporal attention as the adjacency matrix of the graph, in which each frame is the node of the graph. The Hamilton path is the path that goes through each node exactly once (no revisit). Since our attention represents both direction and correlation among the frames, the higher (positive) correlation is, the higher the possibility of correct order should be between frames. Therefore, the Hamilton path with maximum total weight is going to represent the order of the video should be. Let? be the order obtained by the Hamilton algorithm, the accuracy of the order retrieval can be defined as follows: <ref type="bibr" target="#b14">15)</ref> where LCS(?, o) is the longest common subsequence between? and o. In this evaluation, for each video, we randomly select a permutation of {1, ..., N } as the order of the input video. To be fair between benchmarks, we set the same random seed value at the beginning of the testing script so that every time we conduct the evaluation, we obtain the same permutation for each video. As shown in <ref type="table">Table 2</ref>, we use the Softmax attention of TimeSFomer to retrieve the order of the video. The order accuracy of the TimeSFormer is only 52.84. In other words, the Softmax attention of TimeSFormer can only predict the correct order of approximately 4 frames over 8 frames. With the support of order prediction loss, the order accuracy of TimeSFormer is improved to 72.57%. However, without the order prediction loss, our DirecFormer C ? S and Di-recFormer C ? C have already correctly predicted the order of approximately 6 frames over 8 frames (75.04% and 76.16%). When we further employ the order prediction and self-supervised guided losses, the performance of Di-recFormer is significantly improved. Particularly, with the order prediction loss only, DirecFormer in all settings gains more than 87.0% (which is approximately 7 frames over 8 frames). When both losses (L ord and L self ) are employed, the order accuracy of both DirecFormer C ? S and Direc-Former C ? C is improved to 90.02% and 90.19%, respectively. It should be noted that the performance of Direc-Former C ? C is only minorly greater than DirecFormer C ? S as the directed attention over the space does not largely affect the temporal order predictions.</p><formula xml:id="formula_18">OrderAcc = LCS(?, o) T ? 100<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-Art Results</head><p>Something-Something V2 <ref type="table">Table 3</ref> illustrates the performance of our proposed approaches evaluated on Something-Something V2 compared to prior SOTA approaches. In this experiment, similar to other approaches <ref type="bibr" target="#b3">[4]</ref>, we use the DirecFormer pretrained on ImageNet-1K <ref type="bibr" target="#b8">[9]</ref>. As in <ref type="table">Table 3</ref>, our results in all settings outperform <ref type="table">Table 3</ref>. Comparison with the SOTA methods on Something-Something V2. X ? Y denotes for the attention types of temporal and spatial dimension, respectively. X (and Y ) could be either S: Softmax or C: Cosine.   <ref type="bibr" target="#b3">[4]</ref>. It is noted that the prior methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> use 10 temporal clips with 3 spatial crops of a video in the evaluation phase. However, TimeSFormer and our DirecFormer use only 3 spatial crops of a video with a single clip to achieve the solid results. In particular, our method achieves the SOTA performance compared to prior methods as shown in <ref type="table" target="#tab_3">Table 4</ref>. The Top 1 accuracy of the best model is approximately 2% higher than TimeSFormer-L <ref type="bibr" target="#b3">[4]</ref> sitting at 82.75%. The effectiveness of the proposed directed attention has been also proved in these experiments, as the performance of DirecFormer is consistently improved when we deploy the directed attention over time and space. Network Size Comparison As shown in <ref type="table">Table 5</ref>, although the number of parameters and the GFLOPS of single view <ref type="table">Table 5</ref>. Network Size Comparison. We report the computational cost of the inference phase with a single "view" (temporal clip with spatial crop) ? the numbers of such views used (GFLOPs ? views). "NA" indicates the number is not available for us.</p><p>Model GFLOPS x Views Params I3D <ref type="bibr" target="#b6">[7]</ref> 108?NA 12.0M SlowFast 8x8 R50 <ref type="bibr" target="#b18">[19]</ref> 36.1 ? 30 34.4M SlowFast 8x8 R101 <ref type="bibr" target="#b18">[19]</ref> 106 ? 30 53.7M Nonlocal R50 <ref type="bibr" target="#b65">[65]</ref> 282 ? 30 35.3M X3D-XL <ref type="bibr" target="#b17">[18]</ref> 35.8 ? 30 11.0M X3D-XXL <ref type="bibr" target="#b17">[18]</ref> 143.5 ? 30 20.3M ViViT-L/16x2 320 <ref type="bibr" target="#b1">[2]</ref> 3980 ? 3 310.8M TimeSformer <ref type="bibr" target="#b3">[4]</ref> 196 ? 3 121.4M DirecFormer 196 ? 3 121.4M used in our DirecFormer are higher than the traditional 3D-CNN approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, we only use 3 views compared to 30 views of prior approaches and maintain competitive performance. In comparison with TimeSFormer, we gain the same performance in terms of network size and inference flops; however, we achieve better accuracy on three large-scale benchmarks as shown in <ref type="table" target="#tab_0">Tables 1, 3, and 4</ref>. Qualitative Results <ref type="figure" target="#fig_9">Fig. 4</ref> illustrates the Directed Attention of our proposed DirecFormer. We use a video on the validation set of Kinetics-400 and extract the attention map. We randomly permute the frames of video along the temporal dimension and correct the order of frames using the Hamilton algorithm. As in <ref type="figure" target="#fig_9">Fig. 4</ref>, we can successfully correct the frame order of the Parkour action video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have presented a new and simple Di-recFormer method with Directed Attention mechanism in Transformer over the temporal and spatial dimensions. The presented Directed Temporal-Spatial Attention not only learns the magnitude of the correlation between frames and tokens, but also exploits the direction of attention. Moreover, the self-supervised guided loss further enhances the directed learning capability of the Directed Temporal Attention. The intensive ablation study on the Jester dataset has shown the effectiveness of our proposed Directed Attention in both time and space. Furthermore, it has illustrated the impact of the proposed losses used in Directed Temporal Attention learning. The experiments on two other largescale datasets, i.e. Something-Something V2 and Kinetics 400, have further confirmed the high accuracy performance of our proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Result Preview. Top 1 accuracy against GLOPS ? Views of our DirecFormer compared to other methods. Direc-Former achieves SOTA performance while maintaining the low computational cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The Proposed Framework. (a) The Proposed DirecFormer. (b) Directed Temporal Attention. (c) Directed Spatial Attention. The green arrows in (b) and (c) denotes for positive correlation and the red arrows denotes for negative correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, and query q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>vectors as in Eqn.<ref type="bibr" target="#b3">(4)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(l) (s,t)for a query q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 .</head><label>3</label><figDesc>The Video Samples of Three Datasets: (a) Jester, (b) Something-Something V2, and (c) Kinetics-400.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of Order Correction by Finding Hamilton Path On Directed Temporal Attention Map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>) shows two examples of two different class of Something-Something V2. The licenses of Something-Something V2 and Jester datasets are registered by the TwentyBN team that are publicly available for academic research purposes. Kinetics-400.<ref type="bibr" target="#b30">[31]</ref> The dataset contains 400 human action classes, with at least 400 videos for each action. In particular, Kinetics-400 contains 234, 619 training videos and Ablation Study On Jester. X ? Y denotes for the attention types of temporal and spatial dimension, respectively. X (and Y ) could be either S: Softmax or C: Cosine.</figDesc><table><row><cell>Models</cell><cell>Attention Time-Space</cell><cell cols="4">L ord L self Top 1 Top 5</cell></row><row><cell>I3D [7]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">91.46 98.67</cell></row><row><cell>3D SqueezeNet [28]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>90.77</cell><cell>?</cell></row><row><cell>ResNet 50 [25]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>93.70</cell><cell>?</cell></row><row><cell>ResNet 101 [25]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>94.10</cell><cell>?</cell></row><row><cell>ResNeXt [69]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>94.89</cell><cell>?</cell></row><row><cell>PAN [72]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>96.70</cell><cell>?</cell></row><row><cell>STM [29]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>96.70</cell><cell>?</cell></row><row><cell>ViViT-L/16x2 320 [2]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">81.70 93.80</cell></row><row><cell>TimeSFormer [4]</cell><cell>S ? S</cell><cell>?</cell><cell>?</cell><cell cols="2">94.14 99.19</cell></row><row><cell>DirecFormer</cell><cell>S ? C</cell><cell></cell><cell></cell><cell cols="2">94.52 99.26</cell></row><row><cell>DirecFormer</cell><cell>S ? C</cell><cell>?</cell><cell></cell><cell cols="2">94.65 99.25</cell></row><row><cell>DirecFormer</cell><cell>C ? S</cell><cell></cell><cell></cell><cell cols="2">95.52 99.20</cell></row><row><cell>DirecFormer</cell><cell>C ? S</cell><cell>?</cell><cell></cell><cell cols="2">96.28 99.45</cell></row><row><cell>DirecFormer</cell><cell>C ? S</cell><cell>?</cell><cell>?</cell><cell cols="2">97.55 97.54</cell></row><row><cell>DirecFormer</cell><cell>C ? C</cell><cell></cell><cell></cell><cell cols="2">96.15 99.38</cell></row><row><cell>DirecFormer</cell><cell>C ? C</cell><cell></cell><cell></cell><cell cols="2">97.48 99.48</cell></row><row><cell>DirecFormer</cell><cell>C ? C</cell><cell>?</cell><cell>?</cell><cell cols="2">98.15 99.57</cell></row></table><note>19, 761 validation videos. The videos were downloaded from youtube and each video lasts for 10 seconds. There are different types of human actions: Person Actions (e.g. singing, smoking, sneezing, etc.); Person-Person Actions (e.g. wrestling, hugging, shaking hands, etc.); and Person- Object Actions (e.g. opening a bottle, walking the dog, us- ing a computer, etc.).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison with the SOTA methods on Kinetics 400. X ? Y denotes for the attention types of temporal and spatial dimension, respectively. X (and Y ) could be either S: Softmax or C: Cosine.</figDesc><table><row><cell>Models</cell><cell>Attention Time-Space</cell><cell cols="2">Top 1 Top 5</cell></row><row><cell>I3D NLN [7]</cell><cell>?</cell><cell>74.00</cell><cell>91.10</cell></row><row><cell>ip-CSN-152 [58]</cell><cell>?</cell><cell>77.80</cell><cell>92.80</cell></row><row><cell>LGD-3D-101 [51]</cell><cell>?</cell><cell>79.40</cell><cell>94.40</cell></row><row><cell>SlowFast [19]</cell><cell>?</cell><cell>77.00</cell><cell>92.60</cell></row><row><cell>SlowFast Multigrid [68]</cell><cell>?</cell><cell>76.60</cell><cell>92.70</cell></row><row><cell>X3D-M [18]</cell><cell>?</cell><cell>75.10</cell><cell>91.70</cell></row><row><cell>X3D-L [18]</cell><cell>?</cell><cell>76.90</cell><cell>92.50</cell></row><row><cell>X3D-XXL [18]</cell><cell>?</cell><cell>80.40</cell><cell>94.60</cell></row><row><cell>MViT [17]</cell><cell>?</cell><cell>78.40</cell><cell>93.50</cell></row><row><cell>TimeSFormer [4]</cell><cell>S ? S</cell><cell>77.90</cell><cell>93.20</cell></row><row><cell>TimeSFormer ? HR [4]</cell><cell>S ? S</cell><cell>79.70</cell><cell>94.40</cell></row><row><cell>TImeSFormer ? L [4]</cell><cell>S ? S</cell><cell>80.70</cell><cell>94.70</cell></row><row><cell>DirecFormer</cell><cell>S ? C</cell><cell>80.16</cell><cell>94.55</cell></row><row><cell>DirecFormer</cell><cell>C ? S</cell><cell>81.69</cell><cell>94.62</cell></row><row><cell>DirecFormer</cell><cell>C ? C</cell><cell>82.75</cell><cell>94.86</cell></row><row><cell cols="4">other candidates. With the simple design of the Transformer</cell></row><row><cell cols="4">network with the directed attention mechanisms over time</cell></row><row><cell cols="4">and space, our approaches achieve SOTA performance com-</cell></row><row><cell cols="4">pared to traditional 3D CNN approaches [19, 77] and other</cell></row><row><cell cols="4">Transformer approaches [4, 74] by a competitive margin.</cell></row><row><cell cols="4">Kinetics 400 We conduct the experiments on Kinetics 400</cell></row><row><cell cols="4">and compare our results with prior SOTA methods. The pre-</cell></row><row><cell cols="4">trained model on ImageNet-21K [9] for our DirecFormer</cell></row><row><cell>is used, similar to</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The implementation of DirecFormer is available at https : / / github.com/uark-cviu/DirecFormer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">s (l)?time s,t = a (l)?time (s,t),(0,0) v (l) 0,0 + T t ? =1 a (l)?time (s,t),(s,t ? ) v (l) s,t ? z ? (l)?time s,t = z (l?1) s,t + ? (l)?time s (l)?time s,t(7)where ? (l)?time denotes the temporal projection. Secondly, the temporally attentive vector z ? (l)?time s,t is projected to the new key, value, and query to drive Spatial Directed Atten-</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Maria</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio D&amp;apos;</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A short note about kinetics-600</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep analysis of cnn-based spatio-temporal representations for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu Richard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6165" to="6175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep appearance models: A deep boltzmann machine approach for face modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Nhan Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gia</forename><surname>Kha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien</forename><forename type="middle">D</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic face aging in videos via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Nhan Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gia</forename><surname>Kha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nghia</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien</forename><forename type="middle">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning from longitudinal face demonstration-where tractable deep modeling meets inverse reinforcement learning. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Nhan Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kha</forename><forename type="middle">Gia</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Hoang Ngan Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien D</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6201" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7445" to="7454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzy?ska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6047" to="6056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bhiksha Raj, Marios Savvides, and Zhiqiang Shen. Contrast and order representations for video self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7919" to="7929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;1mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>abs/1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient action recognition via dynamic knowledge propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Tae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrack</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13719" to="13728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<editor>Mark Everingham, Chris J. Needham, and Roberto Fraile, editors, BMVC</editor>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>British Machine Vision Association</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<editor>Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L?on Bottou, and Kilian Q. Weinberger</editor>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning self-similarity in space and time as generalized motion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13065" to="13075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="432" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3361" to="3368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reformulating level sets as deep recurrent neural network approach to semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoang Ngan Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kha</forename><forename type="middle">Gia</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Nhan</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Memory attention networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Else-net: Elastic semantic network for continual action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><forename type="middle">En</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13434" to="13443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Trear: Transformer-based rgb-d egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">TSM: temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7082" to="7092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">No frame left behind: Full video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><forename type="middle">L</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh</forename><surname>Karimi Nejadasl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Video swin transformer</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The jester dataset: A large-scale video dataset of human gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Shuffle and Learn: Unsupervised Learning using Temporal Order Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal non-volume preserving approach to facial age-progression and age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Nhan Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kha</forename><forename type="middle">Gia</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Temporal-relational crosstransformers for few-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Masullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilo</forename><surname>Burghardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Mirmehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Spatial temporal transformer network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiara</forename><surname>Plizzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cannici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Matteucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="694" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal representation with local and global diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Action bank: A high-level representation of activity in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreemanananth</forename><surname>Sadanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1234" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Zoubin Ghahramani</title>
		<meeting><address><addrLine>Max Welling, Corinna Cortes, Neil D</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio and Yann LeCun</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Bimal: Bijective maximum likelihood approach to domain adaptation in semantic scene segmentation. IICV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Dat</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Nhan</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><forename type="middle">Lam</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chase</forename><surname>Rainwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fast flow reconstruction via robust invertible n ? n convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Dat</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Nhan</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Internet</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Tdn: Temporal difference networks for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Action-net: Multipath excitation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Smolic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">An efficient dense and scale-invariant spatio-temporal interest point detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<editor>David A. Forsyth, Philip H. S. Torr, and Andrew Zisserman</editor>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">5303</biblScope>
			<biblScope unit="page" from="650" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Kaiming He, Christoph Feichtenhofer, and Philipp Kr?henb?hl. A Multigrid Method for Efficiently Training Video Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 6</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">11219</biblScope>
			<biblScope unit="page" from="318" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Pan: Towards fast action recognition via learning persistence of appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Gan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Temporal reasoning graph for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Vidtr: Video transformer without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Stst: Spatial-temporal specialized transformer for skeletonbased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3229" to="3237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint>
			<biblScope unit="volume">11205</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="831" to="846" />
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Msnet: A multilevel instance segmentation network for natural disaster damage assessment in aerial videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
