<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Nado</surname></persName>
							<email>znado@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Gilmer</surname></persName>
							<email>gilmer@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Shallue</surname></persName>
							<email>cshallue@cfa.harvard.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Anil</surname></persName>
							<email>rohananil@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
							<email>gdahl@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Google Research, Brain Team</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Center for Astrophysics | Harvard &amp; Smithsonian</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Google Research, Brain Team</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Google Research, Brain Team</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently the LARS and LAMB optimizers have been proposed for training neural networks faster using large batch sizes. LARS and LAMB add layer-wise normalization to the update rules of Heavy-ball momentum and Adam, respectively, and have become popular in prominent benchmarks and deep learning libraries. However, without fair comparisons to standard optimizers, it remains an open question whether LARS and LAMB have any benefit over traditional, generic algorithms. In this work we demonstrate that standard optimization algorithms such as Nesterov momentum and Adam can match or exceed the results of LARS and LAMB at large batch sizes. Our results establish new, stronger baselines for future comparisons at these batch sizes and shed light on the difficulties of comparing optimizers for neural network training more generally. * Equal contribution Preprint. Under review. arXiv:2102.06356v3 [cs.</p><p>LG] 9 Jun 2021 performance in less training time [e.g. Ying et al., 2018]. On an idealized data-parallel system with negligible overhead from increasing the batch size, they might hope to achieve perfect scaling, a proportional reduction in training time as the batch size increases.</p><p>However, achieving perfect scaling is not always straightforward. Changing the batch size changes the training dynamics, requiring the training hyperparameters (e.g. learning rate) to be carefully re-tuned in order to maintain the same level of validation performance. 2 In addition, smaller batch sizes provide implicit regularization from gradient noise that may need to be replaced by other forms of regularization when the batch size is increased. Finally, even with perfect tuning, increasing the batch size eventually produces diminishing returns. After a critical batch size, the number of training steps cannot be decreased in proportion to the batch size -the number of epochs must increase to match the validation performance of the smaller batch size. See Shallue et al. 2019 for a survey of the effects of data parallelism on neural network training. Once these effects are taken into account, there is no strong evidence that increasing the batch size degrades the maximum achievable performance on any workload. At the same time, the ever-increasing capacity for data parallelism presents opportunities for new regularization techniques that can replace the gradient noise of smaller batch sizes and new optimization algorithms that can extend perfect scaling to larger batch sizes by using more sophisticated gradient information <ref type="bibr" target="#b23">[Zhang et al., 2019]</ref>.</p><p>You et al. [2017]  proposed the LARS optimization algorithm in the hope of speeding up neural network training by exploiting larger batch sizes. LARS is a variant of stochastic gradient descent (SGD) with momentum <ref type="bibr" target="#b13">[Polyak, 1964]</ref> that applies layer-wise normalization before applying each gradient update. Although it is difficult to draw strong conclusions from the results presented in the LARS paper, 3 the MLPerf 4 Training benchmark 5 adopted LARS as one of two allowed algorithms in the closed division for ResNet-50 on ImageNet and it became the de facto standard algorithm for that benchmark task. With MLPerf entrants competing to find the fastest-training hyperparameters for LARS, the first place submissions in the two most recent MLPerf Training competitions used LARS to achieve record training speeds with batch sizes of 32,678 and 65,536, respectively. No publications or competitive submissions to MLPerf have attempted to match these results with a standard optimizer (e.g. Momentum or Adam). However, MLPerf entrants do not have a strong incentive (nor are necessarily permitted by the rules) to explore other algorithms because MLPerf Training is a systems benchmark that requires algorithmic equivalence between submissions to make fair comparisons. Moreover, since the main justification for LARS is its excellent performance on ResNet-50 at large batch sizes, more work is needed to quantify any benefit of LARS over standard algorithms at any batch size.</p><p>You et al. [2019]  later proposed the LAMB optimizer to speed up pre-training for BERT <ref type="bibr" target="#b5">[Devlin et al., 2018]</ref> using larger batch sizes after concluding that LARS was not effective across workloads. LAMB is a variant of Adam [Kingma and Ba, 2014] that adds a similar layer-wise normalization step to LARS. You et al. [2019]  used LAMB for BERT pre-training with batch sizes up to 65,536 and claimed that Adam cannot match the performance of LAMB beyond batch size 16,384.</p><p>In this paper, we demonstrate that standard optimizers, without any layer-wise normalization techniques, can match or improve upon the large batch size results used to justify LARS and LAMB. In Section 2, we show that Nesterov momentum <ref type="bibr" target="#b12">[Nesterov, 1983]</ref> matches the performance of LARS on the ResNet-50 benchmark with batch size 32,768. We are the first to match this result with a standard optimizer. In Section 3, contradicting the claims in <ref type="bibr" target="#b22">You et al. [2019]</ref>, we show that Adam obtains better BERT pre-training results than LAMB at the largest batch sizes, resulting in better downstream performance metrics after fine-tuning.</p><p>In addition, we establish a new state-of-the-art for BERT pretraining speed, reaching an F1 score of 90.46 in 7,818 steps using Adam at batch size 65,536 (we report training speed in steps because our focus is algorithmic efficiency, but since we compare LARS and LAMB to simpler optimizers, fewer training steps corresponds to faster wall-time in an optimized implementation -our BERT result 2 Although there are heuristics for adjusting the learning rate as the batch size changes, these heuristics inevitably break down sufficiently far from the initial batch size and it is also not clear how to apply them to other training hyperparameters (e.g. momentum). 3  The modified AlexNet on ImageNet benchmark did not have well-established accuracy targets from prior work and LARS used a more general learning rate schedule than the momentum baseline. For ResNet-50 on ImageNet, LARS achieved sub-par accuracy numbers and was not compared to any other optimizer at the same batch size, leaving open the possibility that a generic optimizer would scale just as well as LARS. 4  MLPerf is a trademark of MLCommons.org. 5 https://mlperf.org/training-overview</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, hardware systems employing GPUs and TPUs have enabled neural network training programs to process dramatically more data in parallel than ever before. The most popular way to exploit these systems is to increase the batch size in the optimization algorithm (i.e. the number of training examples processed per training step). On many workloads, modern systems can scale to larger batch sizes without significantly increasing the time per step <ref type="bibr" target="#b8">[Jouppi et al., 2017</ref>, thus proportionally increasing the number of training examples processed per second. If researchers can use this increased throughput to reduce the time required to train each neural network, then they should achieve better results by training larger models, using larger datasets, and by exploring new ideas more rapidly.</p><p>As the capacity for data parallelism continues to increase, practitioners can take their existing, well-tuned training configurations and re-train with larger batch sizes, hoping to achieve the same with Adam also improves upon the wall-time record of LAMB reported in <ref type="bibr" target="#b22">You et al. 2019)</ref>. Taken together, our results establish stronger training speed baselines for these tasks and batch sizes, which we hope will assist future work aiming to accelerate training using larger batch sizes.</p><p>In addition to the contributions mentioned above, we demonstrate several key effects that are often overlooked by studies aiming to establish the superiority of new optimization algorithms. We show that future work must carefully disentangle regularization and optimization effects when comparing a new optimizer to baselines. We also report several under-documented details used to generate the best LARS and LAMB results, a reminder that future comparisons should document any novel tricks and include them in baselines. Finally, our results add to existing evidence in the literature on the difficulty of performing independently rigorous hyperparameter tuning for optimizers and baselines. In particular, we show that the optimal shape of the learning rate schedule is optimizer-dependent (in addition to the scale), and that differences in the schedule can dominate optimizer comparisons at smaller step budgets and become less important at larger step budgets.</p><p>We made our code used for LARS experiments available at https://github.com/google/ init2winit, and the official BERT codebase used for LAMB experiments can be found at https://github.com/google-research/bert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>Shallue et al. <ref type="bibr">[2019]</ref> and <ref type="bibr" target="#b23">Zhang et al. [2019]</ref> explored the effects of data parallelism on neural network training for different optimizers, finding no evidence that larger batch sizes degrade performance and demonstrating that different optimizers can achieve perfect scaling up to different critical batch sizes. <ref type="bibr" target="#b21">You et al. [2017</ref><ref type="bibr" target="#b22">You et al. [ , 2019</ref> developed the LARS and LAMB optimizers in the hope of speeding up training by achieving perfect scaling beyond standard optimizers. Many other recent papers have proposed new optimization algorithms for generic batch sizes or larger batch sizes [see <ref type="bibr" target="#b14">Schmidt et al., 2020]</ref>. <ref type="bibr" target="#b4">Choi et al. [2019]</ref> and <ref type="bibr" target="#b14">Schmidt et al. [2020]</ref> demonstrated the difficulties with fairly comparing optimizers, showing that the hyperparameter tuning protocol is a key determinant of optimizer rankings. The MLPerf Training benchmark  provides a competitive ranking of neural network training systems, but does not shed much light on the relative performance of optimizers because entrants are limited in the algorithms they can use and the hyperparameters they can tune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Matching LARS on ImageNet</head><p>The MLPerf training benchmark for ResNet-50 v1.5 on ImageNet  aims to reach 75.9% validation accuracy in the shortest possible wall-clock time. In the closed division of the competition, entrants must choose between two optimizers, SGD with momentum or LARS, and are only allowed to tune a specified subset of the optimization hyperparameters, with the remaining hyperparameter values set by the competition rules. <ref type="bibr">6</ref> The winning entries in the two most recent competitions used LARS with batch size 32,768 for 72 training epochs 7 and LARS with batch size 65,536 for 88 training epochs, 8 respectively.  later improved the training time for batch size 32,768 by reaching the target accuracy in 64 epochs. These are currently the fastest published results on the ResNet-50 benchmark. However, it has been unclear whether LARS was necessary to achieve these training speeds since no recent published results or competitive MLPerf submissions have used another optimizer. In this section, we describe how we matched the 64 epoch, 32,768 batch size result of LARS using standard Nesterov momentum. 9 A fair benchmark of training algorithms or hardware systems must account for stochasticity in individual training runs. In the MLPerf competition, the benchmark metric is the mean wall-clock time of 5 trials after the fastest and slowest trials are excluded. Only 4 out of the 5 trials need to reach the target accuracy and there is no explicit limit on the number of times an entrant can try a different set of 5 trials. Since our goal is to compare algorithms, rather than systems, we aim to match the LARS result in terms of training steps instead (but since Nesterov momentum is computationally simpler than LARS, this would also correspond to faster wall-clock time on an optimized system). Specifically, we measure the median validation accuracy over 50 training runs with a fixed budget of 6 https://git.io/JtknD 7 https://mlperf.org/training-results-0-6 8 https://mlperf.org/training-results-0-7 9 The 88 epoch, 65,536 batch size result is faster in terms of wall-clock time but requires more training epochs, indicating that it is beyond LARS's perfect scaling regime. Although LARS obtains diminishing returns when increasing the batch size from 32,768 to 65,536, future work could investigate whether Nesterov momentum drops off more or less rapidly than LARS. 2,512 training steps 10 at a batch size of 32,768. When we ran the published LARS training pipeline, 11 LARS achieved a median accuracy of 75.97% and reached the target in 35 out of 50 trials. We consider the LARS result to be matched by another optimizer if the median over 50 trials exceeds the target of 75.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Nesterov momentum at batch size 32k</head><p>This section describes how we used the standard Nesterov momentum optimizer to train the ResNet-50 v1.5 on ImageNet to 75.9% validation accuracy in 2,512 update steps at a batch size of 32,768, matching the best published LARS result at this batch size. Although we implemented our own training program, the only logical changes we made to the published LARS pipeline were to the optimizer and the optimization hyperparameters. Our model implementation and data pre-processing pipeline were identical to those required under the MLPerf closed division rules (see Appendix B).</p><p>We present two Nesterov momentum hyperparameter configurations that achieve comparable performance to LARS. Configuration A achieved a median accuracy of 75.97% (the same as LARS) and reached the target accuracy in 34 out of 50 trials. Configuration B is a modified version of Configuration A designed to make as few changes as possible to the LARS hyperparameters; it achieved a median accuracy of 75.92% and reached the target in 29 out of 50 trials. See Appendix D.1 for the complete hyperparameter configurations.</p><p>To achieve these results, we tuned the hyperparameters of the training pipeline from scratch using Nesterov momentum. We ran a series of experiments, each of which searched over a hand-designed hyperparameter search space using quasi-random search <ref type="bibr">[Bousquet et al., 2017]</ref>. Between each experiment, we modified the previous search space and/or tweaked the training program to include optimization tricks and non-default hyperparameter values we discovered in the state-of-the-art LARS pipeline. The full sequence of experiments we ran, including the number of trials, hyperparameters tuned, and search space ranges, are provided in Appendix D.4. Once we had matched the LARS result with Configuration A, we tried setting each hyperparameter to its value in the LARS pipeline in order to find the minimal set of changes that still achieved the target result, producing Configuration B. The remainder of this section describes the hyperparameters we tuned and the techniques we applied on the journey to these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Nesterov Momentum Optimizer</head><p>Nesterov momentum is a variant of classical or "heavy-ball" momentum defined by the update rule</p><formula xml:id="formula_0">v t+1 = ?v t + ? (? t ), ? t+1 = ? t ? ? t (?v t+1 + ? (? t )) ,</formula><p>where v 0 = 0, ? t is the vector of model parameters after t steps, ? (? t ) is the gradient of the loss function (?) averaged over a batch of training examples, ? is the momentum, and ? t is the learning rate for step t. We prefer Nesterov momentum over classical momentum because it tolerates larger values of its momentum parameter <ref type="bibr" target="#b16">[Sutskever et al., 2013]</ref> and sometimes outperforms classical momentum, although the two algorithms perform similarly on many tasks <ref type="bibr" target="#b4">, Choi et al., 2019</ref>. We tuned the Nesterov momentum ? in Configurations A and B. We discuss the learning rate schedule {? t } separately in Section 2.1.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Batch normalization</head><p>The ResNet-50 v1.5 model uses batch normalization <ref type="bibr" target="#b7">[Ioffe and Szegedy, 2015]</ref>, defined as</p><formula xml:id="formula_1">BN(x (l) ) = x (l) ? mean(x (l) ) var(x (l) ) + ? ? (l) + ? (l) ,</formula><p>where x (l) is a vector of pre-normalization outputs from layer l, mean(?) and var(?) denote the element-wise sample mean and variance across the batch of training examples, 12 and ? (l) and ? (l) are trainable model parameters. <ref type="bibr">10</ref> Corresponding to 64 training epochs in . 11 https://git.io/JtsLQ 12 In a distributed training environment the mean and variance are commonly computed over a subset of the full batch. The LARS pipeline uses a "virtual batch size" of 64, which we also use to avoid changing the training objective <ref type="bibr" target="#b6">[Hoffer et al., 2017]</ref>.</p><p>Batch normalization introduces the following tuneable hyperparameters: , the small constant added to the sample variance; the initial values of ? (l) and ? (l) ; and ?, which governs the exponential moving averages of the scaling factors used in evaluation. The LARS pipeline uses = 10 ?5 and ? = 0.9. It sets the initial value of ? (l) to 0.0 everywhere, but the initial value of ? (l) depends on the layer: it sets ? (l) to 0.0 in the final batch normalization layer of each residual block, and to 1.0 everywhere else. In Configuration A, we tuned , ?, and ? 0 , the initial value of ? (l) in the final batch normalization layer of each residual block. In Configuration B, we used the same values as LARS for and ?, but we found that choosing ? 0 between 0.0 and 1.0 was important for matching the LARS result with Nesterov momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Regularization</head><p>In Configuration A, we tuned both the L2 regularization coefficient ? and label smoothing coefficient ? <ref type="bibr" target="#b17">[Szegedy et al., 2016]</ref>. The LARS pipeline uses ? = 10 ?4 and ? = 0.1. Crucially, the LARS pipeline does not apply L2 regularization to the bias variables of the ResNet model nor the batch normalization parameters ? (l) and ? (l) (indeed, the published LARS pipeline does not even apply LARS to these parameters -it uses Heavy-ball momentum). This detail is extremely important for both LARS and Nesterov momentum to achieve the fastest training speed. Configuration B used the same ? and ? as Configuration A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nesterov</head><p>LARS p warmup 2 1 ? peak 7.05 29.0 ? final 6 ? 10 ?6 10 ?4 1 ? ? 0.02397 0.071 ? 5.8 ? 10 ?5 10 ?4 ? 0.15 0.10 ? 0 0.4138 0.0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Learning rate schedule</head><p>The LARS pipeline uses a piecewise polynomial schedule</p><formula xml:id="formula_2">? t = ? ? ? ? init + (? peak ? ? init ) t twarmup pwarmup , t ? t warmup ? final + (? peak ? ? final ) T ?t T ?twarmup pdecay t &gt; t warmup ,</formula><p>with ? init = 0.0, ? peak = 29.0, ? final = 10 ?4 , p warmup = 1, p decay = 2, and t warmup = 706 steps. In Configuration A, we retuned all of these hyperparameters with Nesterov momentum. In Configuration B, we set ? init , p decay , and t warmup to the same values as LARS, changing only p warmup from 1 to 2 and rescaling ? peak and ? final . <ref type="table" target="#tab_0">Table 1</ref> shows the hyperparameter values for Configuration B that differ from the stateof-the-art LARS pipeline. Aside from re-tuning the momentum, learning rate scale, and regularization hyperparameters (whose optimal values are all expected to change with the optimizer), the only changes are setting p warmup to 2 instead of 1 and re-tuning ? 0 .  Aside from re-scaling, the only difference is setting the warmup polynomial power to 2 instead of 1. <ref type="figure" target="#fig_1">Figure 1</ref> shows the LARS learning rate schedule compared to the Nesterov momentum schedule. Even though these schedules are similar, we found that each optimizer had a different optimal value of the warmup polynomial power. As <ref type="table">Table 2</ref> shows, Nesterov momentum performs better with p warmup = 2 instead of 1, while the opposite is true with LARS. As discussed in <ref type="bibr" target="#b1">Agarwal et al. [2020]</ref>, optimizers can induce implicit step size schedules that strongly influence their training dynamics and solution quality, and it appears from <ref type="table">Table 2</ref> that the implicit step sizes of Nesterov momentum and LARS may evolve differently, causing the shapes of their optimal learning rate schedules to differ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5">Comparing Nesterov momentum and LARS</head><p>Although the main concern of a practitioner is validation performance, the primary task of an optimization algorithm is to minimize training loss. <ref type="table">Table 2</ref> shows that Nesterov momentum achieves higher training accuracy than LARS, despite similar validation performance. Thus, it may be more appropriate to consider the layerwise normalization of LARS to be a regularization technique, rather than an optimization technique.</p><p>Spending even more effort tuning LARS or Nesterov momentum would likely further improve the current state-of-the-art for that optimizer. Meaningful optimizer comparisons are only possible with independent and equally intensive tuning efforts, and we do not claim that either optimizer outperforms the other on this benchmark. That said, if the main evidence for LARS's utility as a "large-batch optimizer" is its performance on this particular benchmark, then more evidence is needed to quantify any benefit it has over traditional, generic optimizers like Nesterov momentum. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Lessons learned</head><p>In hindsight, it was only necessary to make a few changes to the LARS pipeline to match its performance at batch size 32,768 with Nesterov momentum. However, <ref type="table" target="#tab_0">Table 1</ref> does not accurately represent the effort required when attempting to match a highly tuned training-speed benchmark.</p><p>Firstly, as described in Sections 2.1.2 and 2.1.3, the strong results of LARS depend partly on a few subtle optimization tricks and non-default values of uncommonly-tuned hyperparameters. Fortunately, in this case we could discover these tricks by examining the open-source code required for MLPerf submissions, but machine learning research papers do not always report these important details. Researchers can easily waste a lot of experiments and produce misleading results before getting all of these details right. We demonstrate the importance of adding these tricks to our Nesterov momentum pipeline in Appendix C; without these tricks (or some new tricks), we likely would not have been able to match the LARS performance.</p><p>Secondly, the learning rate schedule really matters when trying to maximize performance with a relatively small step budget. Both LARS and Nesterov momentum are sensitive to small deviations from the optimized learning rate schedules in <ref type="figure" target="#fig_1">Figure 1</ref>, and neither schedule works as well for the other optimizer. Although relatively minor changes were sufficient to match LARS with Nesterov momentum, there is no way to know a priori how the optimal schedule will look for a new optimizer <ref type="bibr" target="#b19">Wu et al. [2018]</ref>. Even in toy settings where the optimal learning rate schedule can be derived, it does not fit into commonly used schedule families and depends strongly on the optimizer <ref type="bibr" target="#b23">Zhang et al. [2019]</ref>. Indeed, this problem applies to the other optimization hyperparameters as well: it is extremely difficult to know which are worth considering ahead of time. Finally, even when we narrowed down our hyperparemeter search spaces around the optimal point, the volume of our search spaces corresponding to near-peak performance was small, likely due to the small step budget . We investigate how these effects change with a less stringent step budget in Section 4.</p><p>3 Stronger BERT pretraining speed baselines <ref type="bibr" target="#b22">You et al. [2019]</ref> developed the LAMB optimizer in the hope of speeding up training for BERT-Large [Bidirectional Encoder Representations from Transformers, <ref type="bibr" target="#b5">Devlin et al., 2018]</ref>. BERT training consists of two phases. The "pretraining" phase has two objectives: (1) predicting masked tokens based on the rest of the sequence (a masked language model), and (2) predicting whether two given sentences follow one from another. Finally, the "fine-tuning" phase refines the model for a downstream task of interest. BERT pretraining takes a considerable amount of time (up to 3 days on 16 Cloud TPU-v3 chips Jouppi et al. <ref type="bibr">[2017]</ref>), whereas the fine-tuning phase is typically much faster. Model quality is typically assessed on the downstream metrics, not on pretraining loss, making BERT training a somewhat awkward benchmark for optimization research. <ref type="bibr" target="#b22">You et al. [2019]</ref> used LAMB for BERT pretraining with batch sizes up to 65,536 and claimed that LAMB outperforms Adam batch size 16,384 and beyond. The LAMB optimizer has since appeared in several NLP toolkits, including as Microsoft DeepSpeed and NVIDIA Multi-node BERT training, and as a benchmark task in MLPerf v0. <ref type="bibr">7. 13</ref> As shown in <ref type="table" target="#tab_3">Table 3</ref>, we trained Adam (with decoupled weight decay) baselines that achieve better results than both the LAMB and Adam results reported in <ref type="bibr" target="#b22">You et al. [2019]</ref>. Our new Adam baselines obtain better F1 scores on the development set of the SQuaD v1.1 task in the same number of training steps as LAMB for both batch size 32,768 and the hybrid 65,536-then-32,768 batch size training regime in <ref type="bibr" target="#b22">You et al. [2019]</ref>. We also ran Adam at batch size 65,536 to reach nearly the same F1 score as the hybrid batch size LAMB result, but in much fewer training steps. We believe 7,818 steps is a new state-of-the-art for BERT pretraining speed [in our experiments, it also improves upon the 76-minute record claimed in <ref type="bibr" target="#b22">You et al., 2019]</ref>   <ref type="bibr" target="#b22">You et al. [2019]</ref> in terms of F1 score on the downstream SQuaD v1.1 task.</p><p>We used the same experimental setup as <ref type="bibr" target="#b22">You et al. [2019]</ref>, including two pretraining phases with max sequence lengths of 128 and then 512. In order to match <ref type="bibr" target="#b22">You et al. [2019]</ref>, we reported the F1 score on the downstream SQuaD v1.1 task as the target metric, although this metric introduces potential confounds: optimization efficiency should be measured on the training task using training and held-out data sets. Fortunately, in this case better pretraining performance correlated a with higher F1 score after fine-tuning. See Appendix B.2 for additional experiment details. We tuned Adam hyperparameters independently for each pretraining phase, specifically learning rate ?, ? 1 , ? 2 , the polynomial power for the learning rate warmup p warmup , and weight decay ?, using quasi-random search <ref type="bibr">[Bousquet et al., 2017]</ref>. See Appendix D.2 for the search spaces.</p><p>In addition to hyperparmeter tuning, our improved Adam results at these batch sizes are also likely due to two implementation differences. First, the Adam implementation in <ref type="bibr" target="#b22">You et al. [2019]</ref> comes from the BERT open source code base, in which Adam is missing the standard bias correction. 14 The Adam bias correction acts as an additional step size warm-up, thereby potentially improving the stability in the initial steps of training. Second, the BERT learning rate schedule had a discontinuity at the start of the decay phase due to the learning rate decay being incorrectly applied during warm-up 15 (see <ref type="figure" target="#fig_2">Figure 2</ref> in Appendix B). This peculiarity is part of the official BERT release and is present in 3000+ copies of the BERT Training code on GitHub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Investigating a less stringent step budget</head><p>Part of what makes comparing optimizers so difficult is that the hyperparameter tuning tends to dominate the comparisons <ref type="bibr" target="#b4">[Choi et al., 2019]</ref>. Moreover, tuning becomes especially difficult when we demand a fixed epoch budget even when dramatically increasing the batch size . Fixing the epoch budget as the batch size increases is equivalent to demanding perfect scaling (i.e. that the number of training steps decreases by the same factor that the batch size is increased). We can view the role of hyperparameter tuning for large batch training as resisting the inevitable end of perfect scaling. For example, it might be possible to extend perfect scaling using delicately tuned learning rate schedules, but comparing optimizers under these conditions can make the learning rate schedule dominate the comparison by favoring some algorithms over others. Therefore, in order to better understand the behavior of LARS and LAMB compared to Nesterov Momentum and Adam, we ran additional ResNet-50 experiments with a more generous 6,000 step budget (vs 2,512 in Section 2) and a more simplistic cosine learning rate schedule. At batch size 32,768, this budget should let us reach better validation accuracy than the MLPerf target of 75.9%.</p><p>Although not mentioned in <ref type="bibr" target="#b21">You et al. [2017]</ref>, the state-of-the-art MLPerf pipeline for "LARS" actually uses both LARS and Heavy-ball Momentum, with Momentum applied to the batch normalization and ResNet bias parameters and LARS applied to the other parameters. <ref type="bibr" target="#b22">You et al. [2019]</ref> does not mention whether LAMB was only applied to some parameters and not others. If layerwise normalization can be harmful for some model parameters, this is critical information for practitioners using LARS or LAMB, since it might not be obvious which optimizer to apply to which parameters. To investigate this, we trained both pure LARS and LAMB configurations, as well as configurations that did not apply layerwise normalization to the batch normalization and ResNet bias parameters. Moreover, LAMB's underlying Adam implementation defaults to = 10 ?6 , rather than the typical 10 ?7 or 10 ?8 . In some cases, can be a critical hyperparameter for Adam <ref type="bibr" target="#b4">[Choi et al., 2019]</ref>, so we included Adam configurations with both = 10 ?6 and = 10 ?8 . <ref type="table" target="#tab_4">Table 4</ref> shows the validation accuracy of these different configurations after training for 6,000 steps with batch size 32,768. In every case, we used a simple cosine decay learning rate schedule and tuned the initial learning rate and weight decay using quasi-random search. We used momentum parameters of 0.98 for Nesterov momentum and 0.929 for LARS, respectively, based on the tuned values from Section 2. We used default hyperparameters for Adam and LAMB except where specified. We set all other hyperparameters to the same values as the state-of-theart LARS pipeline, except we set ? 0 = 1.0. See Appendix D.3 for more details. As expected, highly tuned learning rate schedules and optimizer hyperparameters are no longer necessary with a less stringent step budget. Multiple optimizer configurations in  <ref type="table" target="#tab_4">Table 4</ref>: Validation accuracy of ResNet-50 on Ima-geNet trained for 6,000 steps instead of 2,512. The second column is the optimizer that was applied to the batch norm and ResNet bias variables. We report the median top-1 accuracy over 5 seeds of the best hyperparameter setting in a refined search space. See Appendix D.3 for details.</p><p>In <ref type="table" target="#tab_4">Table 4</ref>, "pure LAMB" performs extremely poorly: LAMB only obtains reasonable results when it is not used on the batch normalization and ResNet bias parameters, suggesting that layerwise normalization can indeed be harmful on some parameters. "Pure LARS" and Nesterov momentum perform roughly the same at this step budget, but the MLPerf LARS pipeline, which is tuned for a more stringent step budget, does not use LARS on all parameters, at least suggesting that the optimal choice could be budget-dependent.</p><p>Many new neural net optimizers, including LAMB, are introduced alongside claims that the new optimizer does not require any-or at least minimal-tuning. Unfortunately, these claims require a lot of work to support, since they require trying the optimizer on new problems without using those problems during the development of the algorithm. Although our experiments here are not sufficient to determine which optimizers are easiest to tune, experiments like these that operate outside the regime of highly tuned learning rate schedules can serve as a starting point. In this experiment, LARS and LAMB do not appear to have an advantage in how easy they are to tune even on a dataset and model that were used in the development of both of those algorithms. LAMB is a variant of Adam and performs about the same as Adam with the same value of ; LARS is more analogous to Momentum and indeed Nesterov momentum and LARS have similar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Our results show that standard, generic optimizers suffice for achieving strong results across batch sizes. Therefore, any research program to create new optimizers for training at larger batch sizes must start from the fact that Momentum, Adam, and likely other standard methods work fine at batch sizes as large as those considered in this paper. The LARS and LAMB update rules have no more to do with the batch size (or "large" batches) than the Momentum or Adam update rules. Although <ref type="bibr" target="#b22">You et al. [2019]</ref> presented convergence rate bounds for LARS and LAMB to support their claims of superior performance, we show in Appendix A that Adam satisfies a similar bound to LAMB.</p><p>These bounds all rely on very unrealistic assumptions. <ref type="bibr">16</ref> Most of all, they are loose upper bounds on the worst case behavior of the algorithms, not accurate reflections of optimizer performance in reality. Whether layer-wise normalization can be useful for optimization or regularization remains an open question. However, if LARS and LAMB have any advantage over standard techniques, it is not that they work dramatically better on the tasks and batch sizes in <ref type="bibr" target="#b21">You et al. [2017</ref><ref type="bibr" target="#b22">You et al. [ , 2019</ref>. This is not to suggest that there is nothing interesting about studying neural network optimization at larger batch sizes. For example, as gradient noise decreases, there may be opportunities to harness curvature information and extend the region of perfect scaling <ref type="bibr" target="#b23">[Zhang et al., 2019]</ref>. However, there is currently no evidence that LARS and LAMB scale better than Momentum and Adam.</p><p>Our primary concern in this paper has been matching the state of the art-and establishing new baselines-for training speed measurements of the sort used to justify new techniques and algorithms for training with larger batch sizes. In contrast, many practitioners are more concerned with obtaining the best possible validation error with a somewhat flexible training time budget. Part of the reason why matching LARS at batch size 32,768 was non-trivial is because getting state of the art training speed requires several tricks and implementation details that are not often discussed. It was not obvious to us a priori which ones would prove crucial. These details do not involve changes to the optimizer, but they interact with the optimizer in a regime where all hyperparameters need to be well tuned to stay competitive, making it necessary to re-tune everything for a new optimizer.</p><p>In neural network optimization research, training loss is rarely discussed in detail and evaluation centers on validation/test performance since that is what practitioners care most about. However, although we shouldn't only consider training loss, it is counter-intuitive and counter-productive to elide a careful investigation of the actual objective of the optimizer. If a new optimizer achieves better test performance, but shows no speedup on training loss, then perhaps it is not a better optimizer so much as an indirect regularizer. 17 Indeed, in our experiments we found that Nesterov momentum achieves noticeably better training accuracy on ResNet-50 than the LARS configuration we used, despite reaching roughly the same validation accuracy. Properly disentangling possible regularization benefits from optimization speed-ups is crucial if we are to understand neural network training, especially at larger batch sizes where we lose some of the regularization effect of gradient noise. Hypothetically, if the primary benefit of a training procedure is regularization, then it would be better to compare the method with other regularization baselines than other optimizers.</p><p>Ultimately, we only care about batch size to the extent that higher degrees of data parallelism lead to faster training. Training with a larger batch size is a means, not the end goal. New optimizerswhether designed for generic batch sizes or larger batch sizes-have the potential to dramatically improve algorithmic efficiency across multiple workloads, but our results show that standard optimizers can match the performance of newer alternatives on the workloads we considered. Indeed, despite the legion of new update rule variants being proposed in the literature, standard Adam and Momentum remain the workhorses of practitioners and researchers alike, while independent empirical comparisons consistently find no clear winner when optimizers are compared across a variety of workloads <ref type="bibr" target="#b14">[Schmidt et al., 2020]</ref>. Meanwhile, as <ref type="bibr" target="#b4">Choi et al. [2019]</ref> and our results underscore, comparisons between optimizers crucially depend on the effort spent tuning hyperparameters for each optimizer. Given these facts, we should regard with extreme caution studies claiming to show the superiority of one particular optimizer over others. Part of the issue stems from current incentives in the research community; we overvalue the novelty of new methods and undervalue establishing strong baselines to measure progress against. This is particularly problematic in the study of optimizers, where the learning rate schedule is arguably more important than the choice of the optimizer update rule itself! As our results show, the best learning rate schedule is tightly coupled with the optimizer, meaning that tuning the learning rate schedule for a new optimizer will generally favor the new optimizer over a baseline unless the schedule of the baseline is afforded the same tuning effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we demonstrated that standard optimizers, without any layer-wise normalization techniques, can match or exceed the large batch size results used to justify LARS and LAMB. Future work attempting to argue that a new algorithm is useful by comparing to baseline methods or results, including those established in this paper, faces a key challenge in showing that the gains are due to the new method and not merely due to better tuning or changes to the training pipeline (e.g. regularization tricks). Although gains from tuning will eventually saturate, we can, in principle, always invest more effort in tuning and potentially get better results for any optimizer. However, our goal should be developing optimizers that work better across many different workloads when taking into account the amount of additional tuning they require.</p><p>Moving forward, if we are to reliably make progress we need to rethink how we compare and evaluate new optimizers for neural network training. Given how sensitive optimizer performance is to the hyperparameter tuning protocol and how difficult it is to quantify hyperparameter tuning effort, we can't expect experiments with self-reported baselines to always lead to fair comparisons. Ideally, new training methods would be evaluated in a standardized competitive benchmark, where submitters of new optimizers do not have full knowledge of the evaluation workloads. Some efforts in this direction have started, for instance the MLCommons Algorithmic Efficiency Working Group 18 , but more work needs to be done to produce incentives for the community to publish well-tuned baselines and to reward researchers that conduct the most rigorous empirical comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Convergence Proofs</head><p>To support our larger point about the irrelevance of this type of result when comparing LAMB and Adam, below we derive a convergence bound for Adam in a similar manner to the LAMB bound in <ref type="bibr" target="#b22">You et al. [2019]</ref>. Note that all of these bounds are loose upper bounds on the worst case behavior of the algorithms, so there is no reason that comparing them reflects the relative behaviors of optimizers in reality. For example in Equation 3 below, we follow similar operations as the LAMB bound derivation and simply switch a ? to a + for algebraic convenience.</p><p>We define the following as our optimization objective</p><formula xml:id="formula_3">min x?R d f (x) := E s?P [ (x, s)] + ? 2 x 2 ,<label>(1)</label></formula><p>with an optimal solution(s) x * . x ? R d are the neural network parameters, a smooth and possibly nonconvex loss function, P a data distribution, and ? the regularization strength.</p><p>Let T be the number of training steps, h the number of neural network layers, b the batch size, ? the learning rate, n the mini-batch size, and ?(v) : R + ? R + a function that is layerwise multiplied by the learning rate in LARS and LAMB updates. Let L be a vector of the layerwise Lipschitz constants for the neural network, and L avg the mean of L. Let s be a training step uniformly sampled from {1, 2, ..., T }.</p><p>We define the stochastic minibatch estimate of the true gradient as E[g (i) ] = ? i f (x) and assume that its variance is bounded by E g (i) ? ? i f (x) 2 ? ? 2 i layerwise for a vector of standard deviations ? := ? (1) , . . . ? (h) and elementwise for? := ? (1) , . . .? <ref type="bibr">(h)</ref> .</p><formula xml:id="formula_4">Next, let ? t = ? = 2(f (x1)?f (x * )) ? 2 u L 1T ?t ? [T ], b = T , ? l ? ?(v) ? ? u ?v &gt; 0, ? l , ? u &gt; 0.</formula><p>Crucially, additionally let b = T, ? 1 = 0, ? = 0. Under these conditions <ref type="bibr" target="#b22">You et al. [2019]</ref> show the convergence rate for LARS is</p><formula xml:id="formula_5">E 1 ? h h i=1 ? i f (x s ) 2 ? O (f (x 1 ) ? f (x * ))L avg T + ? 2 1 T h .</formula><p>They also derive the convergence rate of LAMB as</p><formula xml:id="formula_6">E[ ?f (x a ) 2 ] ? O G 2 d h(1 ? ? 2 ) ? 2(f (x 1 ) ? f (x * )) L 1 T + ? 1 ? T .</formula><p>Additionally, for ? 2 = 0, the convergence rate of LAMB can be derived as</p><formula xml:id="formula_7">E 1 ? d ?f (x a ) 1 2 ? O (f (x 1 ) ? f (x * ))L avg T + ? 2 1 T h ,</formula><p>Below we derive a similar bound for the ? 2 &gt; 0 case for Adam updates. We note that the ? 2 = 0 case where the bound depends on L avg instead of ||L|| 1 can be very similarly derived for Adam, but is also a very unrealistic condition in practice.</p><p>Proof. Under the assumption ? 1 = 0, ? = 0, one could write the Adam update rule as follows:</p><formula xml:id="formula_8">x (i) t+1 = x (i) t ? ? t 1 ? ? t 2 g (i) t v (i) t ,</formula><p>where</p><formula xml:id="formula_9">v t = ? 2 v t?1 + (1 ? ? 2 )g 2 t for all i ? [h].</formula><p>Since the function f is L-smooth, we have the following:</p><formula xml:id="formula_10">f (x t+1 ) ? f (x t ) + ? i f (x t ), x (i) t+1 ? x (i) t + h i=1 L i 2 x (i) t+1 ? x (i) t 2 ? f (x t ) ?? t h i=1 di j=1 [? i f (x t )] j 1 ? ? t 2 g (i) t,j v (i) t,j T1 + h i=1 L i ? 2 t d 2(1 ? ? 2 )<label>(2)</label></formula><p>Where the last term comes from the fact that 1 ? ? t 2 ? 1. We bound term T 1 in the following manner, in line with <ref type="bibr" target="#b22">[You et al., 2019]</ref>:</p><formula xml:id="formula_11">T 1 = ?? t h i=1 di j=1 [? i f (x t )] j 1 ? ? t 2 g (i) t,j v (i) t,j ? ?? t h i=1 di j=1 ? 1 ? ? 2 G [? i f (x t )] j g (i) t,j ? ? t h i=1 di j=1 ? ? [? i f (x t )] j 1 ? ? t 2 g (i) t,j v (i) t,j ? ? 1(sign([? i f (x t )] j ) = sign(g (i) t,j ))</formula><p>Relying on the following inequalities:</p><p>? v t ? G and 1 ? ? t 2 &gt; 1 ? ? 2 . Taking expectation, we have the following:</p><formula xml:id="formula_12">E[T 1 ] ? ?? t h i=1 di j=1 ? 1 ? ? 2 G E [? i f (x t )] j g (i) t,j ? ? t h i=1 di j=1 ? 1 ? ? 2 G E [? i f (x t )] j g (i) t,j 1(sign([? i f (x t )] j ) = sign(g (i) t,j )) E[T 1 ] ? ?? t h i=1 di j=1 ? 1 ? ? 2 G E [? i f (x t )] j g (i) t,j + ? t h i=1 di j=1 1 ? ? 2 E ([? i f (x t )] j ) |P(sign([? i f (x t )] j ) = sign(g (i) t,j )) (3)</formula><p>similarly what is shown in signsgd, we bound the probability by first relaxing the condition, then applying Markov's and then Jensen's inequality:</p><formula xml:id="formula_13">P(sign([? i f (x t )] j ) = sign(g (i) t,j )) ? P |[? i f (x t )] j ? g (i) t,j | ? |g (i) t,j | ? E |[? i f (x t )] j ? g (i) t,j | |[? i f (x t )] j | ? E ([? i f (x t )] j ? g (i) t,j ) 2 |[? i f (x t )] j | =? t,i |[? i f (x t )] j | ?? i ? n|[? i f (x t )] j |</formula><p>where the last inequality is from the fact that? t,j is the minibatch variance at time t with batch size n. Substituting this into our derivation of T 1</p><formula xml:id="formula_14">E[T 1 ] ? ?? t ? 1 ? ? 2 G ||?f (x t )|| 2 + ? t 1 ? ? 2 h i=1 di j=1? i ? n</formula><p>and replacing this with our definition of T 1 in Eq. <ref type="formula" target="#formula_10">(2)</ref> we get</p><formula xml:id="formula_15">E[f (x t+1 )] ? f (x t ) ? ? t ? 1 ? ? 2 G ||?f (x t )|| 2 + ? t 1 ? ? 2 ||?|| 1 ? n + ||L|| 1 ? 2 t d 2(1 ? ? 2 ) .<label>(4)</label></formula><p>We then arrive at the final bound by summing Eq. (4) to step T and cancelling consecutive terms via the telescoping sum, followed by rearranging and then multiplying through by</p><formula xml:id="formula_16">G T ?t ? 1??2 E[f (x T +1 )] ? f (x 1 ) ? ? t ? 1 ? ? 2 G T t=1 ||?f (x t )|| 2 + T ? t 1 ? ? 2 ||?|| 1 ? n + T ||L|| 1 ? 2 t d 2(1 ? ? 2 ) . ? t ? 1 ? ? 2 G T t=1 ||?f (x t )|| 2 ? f (x 1 ) ? f (x * ) + T ? t 1 ? ? 2 ||?|| 1 ? n + T ||L|| 1 ? 2 t d 2(1 ? ? 2 ) 1 T T t=1 ||?f (x t )|| 2 ? G f (x 1 ) ? f (x * ) T ? t ? 1 ? ? 2 + ||?|| 1 ? n + ||L|| 1 ? t d 2(1 ? ? 2 ) 3 2 Taking ? t = ? = 2(f (x1)?f (x * ))</formula><p>T ||L||1(1??2)d and letting n = T as is similarly done in <ref type="bibr" target="#b22">[You et al., 2019]</ref>, we can recover a bound that, up to some constants, is similar to the bound for LAMB:  </p><formula xml:id="formula_17">E[||?f (x t )|| 2 ] ? O ? ? G ? ? f (x 1 ) ? f (x * ) T 2(f (x1)?f (x * )) T ||L||1(1??2)d ? 1 ? ? 2 + ||?|| 1 ? n + ||L|| 1 2(f (x1)?f (x * )) T ||L||1(1??2)d d 2(1 ? ? 2 ) 3 2 ? ? ? ? = O G 1 2 2(f (x 1 ) ? f (x * ))||L|| 1 d T + ||?|| 1 ? n + 1 2(1 ? ? 2 ) 2 2(f (x 1 ) ? f (x * ))||L|| 1 d T = O G 1 + 1 (1 ? ? 2 ) 2 2(f (x 1 ) ? f (x * ))||L|| 1 d T + G||?|| 1 ? T B Additional</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 BERT pre-training</head><p>We used the same experimental setup as the official BERT codebase 20 and the standard train/test split from the previous literature. This matches the experimental setup of <ref type="bibr" target="#b22">You et al. [2019]</ref>. We trained on Google TPUs, using TPUv3-256 or TPUv3-512 for the 32,768 batch size experiments, and TPUv3-1024 for the 65,536 batch size experiments.</p><p>We trained the two pretraining objectives on the combined Wikipedia and Books corpus <ref type="bibr" target="#b24">[Zhu et al., 2015]</ref> datasets (2.5B and 800M words, respectively). We used sequence lengths of 128 and 512, respectively, for the pretraining tasks. We ran the fine-tuning phase on the SQuaD v1.1 question answering task. In order to match <ref type="bibr" target="#b22">You et al. [2019]</ref>, we report the F1-score on the dev set as the target metric. We followed the fine-tuning protocol described in the LAMB optimizer setup and did not perform any additional tuning for fine-tuning.</p><p>We tuned Adam hyperparameters using quasi-random search <ref type="bibr">[Bousquet et al., 2017]</ref> in a simple search space. Hyperparameters included learning rate ?, ? 1 , ? 2 , the polynomial power for the learning rate warmup p warmup , and weight decay ?. We fixed the in Adam to 10 ?11 for all BERT experiments. See Appendix D.2 for the search spaces. We selected the best trial using the masked language model accuracy over 10k examples from the training set. The number of training steps for each of the phases, as well as the warmup steps are identical to <ref type="bibr" target="#b22">You et al. [2019]</ref> and are listed in Appendix D.2. Each phase of pretraining used completely independent Adam hyperparameters. We found the final hyperparameters within 30 trials of random search for each of the phases, except for the second phase of 65,536 batch size which used 130 trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Nesterov ablations</head><p>To explore the sensitivity of our best Nesterov momentum configuration (Configuration A), we ablated several elements of the experiment pipeline, one at a time, and tested their impact on performance. <ref type="figure" target="#fig_3">Figure 4</ref> shows the results of these experiments. "Base" refers to Nesterov momentum Configuration A <ref type="table">(Table 5)</ref>. "ResNet version" is the same point as "Base" but with ResNet version 1.0 instead of version 1.5. "BN init" is the same point as "Base" but with ? 0 = 1.0 instead of 0.4138. "Virtual BN" is the same point as "Base" but with a virtual batch size of 256 instead of 64, which is the largest that fits in a single TPUv3 core. "BN &amp; LR tuning" is Configuration B <ref type="table">(Table 5)</ref>, the same point as "Base" but with p decay , t warmup , ? 0 , ?, set to their values in the LARS pipeline. Finally, "L2 variables" is the same point as "Base" but where the L2 regularization is applied to all variables. The only ablation We considered two configurations of Nesterov hyperparameters: Configuration A, where we tuned a wide set of hyperparameters in the experiment pipeline, and Configuration B, where we reverted the less impactful hyperparameters to the same values as the LARS baseline (or in the case of p warmup , a simpler value). We included Configuration B in order to demonstrate the minimal set of changes to the baseline necessary to still reach the target accuracy. The hyperparameter values for these configurations can be found in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Adam on BERT</head><p>The search space used to tune Adam on BERT for all phases of the pipeline can be found in <ref type="table">Table 6</ref>, which yielded our best Adam results on BERT in <ref type="table" target="#tab_7">Table 7</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Less stringent step budget on ResNet-50</head><p>All trials used a cosine decay learning rate schedule and tuned the initial learning rate ? and L2 regularization or weight decay parameter 21 ? according to <ref type="table" target="#tab_9">Table 8</ref>. We used 50 or more trials to search in the "Initial Range" and then 25 trials to search in the refined "Final Range." Finally, we ran the best point from the latter for 5 random seeds. When LARS or LAMB were used alongside a different optimizer for the batch normalization and ResNet-50 bias parameters, we set ? = 0 on the batch normalization and ResNet-50 bias parameters.   <ref type="table">Table 9</ref>: First search space of the Nesterov tuning journey. The search spaces were mostly by informed guesses by the authors. ? refers to weight decay, which is applied to all variables. Tuned for 251 trials. Trained for 2,815 steps ("72 epochs" as defined by MLPerf epoch calculations). We used a linear learning rate decay schedule that decays for all training steps, starting from ? 0 and ending at ? 0 ? ? decay f actor . Virtual batch size 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Nesterov ResNet50 search space chronology</head><p>Below we list the sequence of search spaces we used to arrive at our final values in <ref type="table">Table 5</ref>. Given that the final results reported in papers are rarely found in a single iteration of experiments, we believe that it is important to document the full journey to arriving at our results.</p><p>Note that although we tuned a wide range of hyperparameters to match the LARS result with Nesterov momentum, we later realized that many of these hyperparameters could be reverted to the values from the LARS pipeline (see <ref type="table">Table 5</ref>). We started tuning with a training budget of 2,815 steps, which is the number of steps in the MLPerf 0.6 submission. We sometimes would decrease this to 2,658 steps to test how decreasing the training budget would affect tuning performance, before eventually moving to the 2,512 steps used to generate the results in the main text.  <ref type="table" target="#tab_0">Table 11</ref>: ? refers to weight decay, which is now not applied to the bias and batch normalization variables. 50 trials. Trained for 2,658 steps. Linear learning rate decay schedule that decays for t decay steps, starting from ? 0 and ending at ? 0 ? ? decay f actor . Virtual batch size 128.  <ref type="table" target="#tab_0">Table 12</ref>: ? refers to weight decay, which is not applied to the bias and batch normalization variables. 50 trials. Trained for 2,658 steps. Linear warmup for 500 steps followed by a quadratic decay, which decays until step t decay , and then is constant at the final learning rate ? 0 ? ? decay f actor . Virtual batch size 128. We increased the max learning rate based off the larger learning rates used by LARS. We also ran two additional studies which were the same except with 250 and 977 warmup steps.  <ref type="table" target="#tab_0">Table 13</ref>: ? refers to weight decay, which is not applied to the bias and batch normalization variables. 50 trials. Trained for 2,815 steps. Linear warmup for 500 steps followed by a quadratic decay, which decays until step t decay , and then is constant at the final learning rate ? 0 ? ? decay f actor . Virtual batch size 128.  <ref type="table" target="#tab_0">Table 14</ref>: ? refers to weight decay, which is not applied to the bias and batch normalization variables. 50 trials. Trained for 2,815 steps. Linear warmup for 500 steps followed by a quadratic decay, which decays until step t decay , and then is constant at the final learning rate ? 0 ? ? decay f actor . Virtual batch size 128.  <ref type="table" target="#tab_0">Table 16</ref>: ? refers to weight decay, which is not applied to the bias and batch normalization variables. Trained for 2,815 steps. Virtual batch size 64. Using the best hyperparameters from Table 15, we swept over the peak learning rate in a discrete set of ten values per order of magnitude, each for three random seeds, to find the max stable learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Range</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Range</head><p>Scaling ? peak 4.118 ? decay factor 8.144 ? 10 ?5 t decay 2250 -1 ? ? 0.02397 ? {{0.5 ? 10 ? , 10 ? , ...} ?? ? {?3, ...0}} + {1.0, } Discrete ? 0.07786 - <ref type="table" target="#tab_0">Table 17</ref>: ? refers to weight decay, which is not applied to the bias and batch normalization variables. Trained for 2,815 steps. Virtual batch size 64. Using the best hyperparameters from <ref type="table" target="#tab_0">Table 15</ref>, we swept over the weight decay in a discrete set of twenty values per order of magnitude, to test how high the regularization has to be in this region of hyperparameter space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Range</head><p>Scaling ? peak 4.118 ? decay factor 8.144 ? 10 ?5 t decay 2250 -1 ? ? 0.02397 ? 0.009992 ? 0.07786 ? {0.0, 0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.995, 0.999} Discrete {10 ?7 , 10 ?6 , 10 ?5 , 10 ?4 , 10 ?3 , 10 ?2 , 10 ?1 } Discrete  <ref type="table" target="#tab_0">Table 19</ref>: ? refers to weight decay, which is not applied to the bias and batch normalization variables. 50 trials. Trained for 2,815 steps. Linear warmup for 500 steps followed by a quadratic decay, which decays until step t decay , and then is constant at the final learning rate ? 0 ? ? decay f actor . Virtual batch size 64. Peak learning rate range was consolidated based off the results of <ref type="table" target="#tab_0">Table 16</ref>. The weight decay range was consolidated based off the results of <ref type="table" target="#tab_0">Table 17.</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The learning rate schedules of LARS and Nesterov momentum Configuration B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>ForFigure 2 :</head><label>2</label><figDesc>ImageNet, we used the following sequence of TensorFlow functions for pre-processing: An illustration of the sudden drop in the BERT learning rate schedule in the official codebase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>runs starting from the same pretraining checkpoint to show the stability of our results, at each of the 32,768, mixed 65,536-32,768, and 65,536 batch size settings. Distributions over 50 training runs for each ablation study around our best Nesterov momentum configuration (Configuration A). The dotted red line is at the target accuracy of 75.9%, and the boxes show the min, max, and quartiles of the distribution of accuracies over the 50 training runs. whose median over 50 seeds continues to beat the target 75.9% accuracy (noted by the dotted red line) is "BN &amp; LR tuning", with the rest having between 0.1%-0.3% drops in median accuracy.D Hyperparameter tuning D.1 Nesterov momentum training speed on ResNet-50</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: The hyperparameters of</cell></row><row><cell>Configuration B that differ from</cell></row><row><cell>state-of-the-art LARS at batch size</cell></row><row><cell>32,768 [Kumar et al., 2019].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The best warmup schedule differs for Nesterov momentum and LARS. Values are medians over 50 training runs after setting p warmup without retuning other hyperparameters. (Right) Median train and test accuracies over 50 training runs for Nesterov momentum Configuration B and LARS.</figDesc><table><row><cell cols="2">p warmup Nesterov</cell><cell>LARS</cell><cell cols="3">Optimizer Train Acc Test Acc</cell></row><row><cell>1</cell><cell cols="2">75.79% 75.97%</cell><cell>Nesterov</cell><cell>78.97%</cell><cell>75.93%</cell></row><row><cell>2</cell><cell cols="2">75.92% 75.69%</cell><cell>LARS</cell><cell>78.07%</cell><cell>75.97%</cell></row><row><cell>Table 2: (Left)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. Additionally, at batch size 32,768 our Adam baseline got a better pretraining loss of 1.277 compared to LAMB's 1.342.</figDesc><table><row><cell cols="4">Batch size Step budget LAMB Adam</cell></row><row><cell>32k</cell><cell>15,625</cell><cell>91.48</cell><cell>91.58</cell></row><row><cell>65k/32k</cell><cell>8,599</cell><cell>90.58</cell><cell>91.04</cell></row><row><cell>65k</cell><cell>7,818</cell><cell>-</cell><cell>90.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Using Adam for pretraining exceeds the reported performance of LAMB in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>exceed the MLPerf target accuracy of 75.9% at batch size 32,768 with minimal tuning. Training with larger batch sizes is not fundamentally unstable: stringent step budgets make hyperparameter tuning trickier.</figDesc><table><row><cell>Weights Optimizer</cell><cell>Bias/BN Optimizer</cell><cell>Top-1</cell></row><row><cell>Nesterov</cell><cell>Nesterov</cell><cell>76.7</cell></row><row><cell>LARS</cell><cell>Momentum</cell><cell>76.9</cell></row><row><cell>LARS</cell><cell>LARS</cell><cell>76.9</cell></row><row><cell cols="2">Adam ( = 10 ?8 ) Adam ( = 10 ?8 )</cell><cell>76.2</cell></row><row><cell cols="2">Adam ( = 10 ?6 ) Adam ( = 10 ?6 )</cell><cell>76.4</cell></row><row><cell>LAMB</cell><cell>LAMB</cell><cell>27.3</cell></row><row><cell>LAMB</cell><cell>Adam ( = 10 ?8 )</cell><cell>76.3</cell></row><row><cell>LAMB</cell><cell>Adam ( = 10 ?6 )</cell><cell>76.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>All experiments were run on Google TPUs<ref type="bibr" target="#b8">[Jouppi et al., 2017]</ref>. We typically trained on TPUv2-256 or TPUv3-128 in order to accommodate the 32,768 batch size. The ResNet-50 experiments used Jax<ref type="bibr" target="#b3">[Bradbury et al., 2018]</ref> using the Flax library, with code released here. The BERT experiments were run using TensorFlow<ref type="bibr" target="#b0">[Abadi et al., 2015]</ref> version 1.15. We used the standard train/validation split from the previous literature and MLPerf competition.</figDesc><table><row><cell>experiment details</cell></row><row><cell>B.1 ResNet-50 training benchmark</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The search space used to tune Adam on BERT for all phases of the pipeline. ? refers to weight decay and p refers to the polynomial power in the learning rate schedule for both the warmup and decay phases.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="5">Configuration A Configuration B LARS</cell></row><row><cell></cell><cell></cell><cell>t warmup</cell><cell>638</cell><cell></cell><cell>706</cell><cell></cell><cell>706</cell></row><row><cell></cell><cell></cell><cell>p warmup</cell><cell>2.497</cell><cell></cell><cell>2.0</cell><cell></cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell>p decay</cell><cell>1.955</cell><cell></cell><cell>2.0</cell><cell></cell><cell>2.0</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>0.94</cell><cell></cell><cell>0.9</cell><cell></cell><cell>0.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>4 ? 10 ?6</cell><cell></cell><cell cols="2">10 ?5</cell><cell>10 ?5</cell></row><row><cell></cell><cell></cell><cell>? peak</cell><cell>7.05</cell><cell></cell><cell>7.05</cell><cell></cell><cell>29.0</cell></row><row><cell></cell><cell></cell><cell>? final</cell><cell>6 ? 10 ?6</cell><cell></cell><cell cols="2">6 ? 10 ?6</cell><cell>10 ?4</cell></row><row><cell></cell><cell></cell><cell>1 ? ?</cell><cell>0.02397</cell><cell></cell><cell cols="2">0.02397</cell><cell>0.071</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>5.8 ? 10 ?5</cell><cell></cell><cell cols="2">5.8 ? 10 ?5</cell><cell>10 ?4</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>0.15</cell><cell></cell><cell>0.15</cell><cell></cell><cell>0.10</cell></row><row><cell></cell><cell></cell><cell>? 0</cell><cell>0.4138</cell><cell></cell><cell cols="2">0.4138</cell><cell>0.0</cell></row><row><cell></cell><cell></cell><cell cols="6">Table 5: Nesterov momentum Configurations A and B.</cell></row><row><cell></cell><cell></cell><cell cols="2">Hyperparameter</cell><cell cols="2">Range</cell><cell>Scaling</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>p</cell><cell cols="2">{1, 2}</cell><cell>Discrete</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>?</cell><cell cols="2">[10 ?5 , 1.0]</cell><cell>Log</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 ? ? 1</cell><cell cols="2">[10 ?2 , 0.5]</cell><cell>Log</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 ? ? 2</cell><cell cols="2">[10 ?2 , 0.5]</cell><cell>Log</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>?</cell><cell cols="2">[10 ?3 , 10]</cell><cell>Log</cell><cell></cell></row><row><cell cols="4">Table 6: Batch size Phase Seq len Warmup</cell><cell>Train</cell><cell cols="2">Learning</cell><cell>? 1</cell><cell>? 2</cell><cell>?</cell><cell>p</cell></row><row><cell></cell><cell></cell><cell></cell><cell>steps</cell><cell>steps</cell><cell cols="2">rate</cell><cell></cell></row><row><cell>32,768</cell><cell>1</cell><cell>128</cell><cell>3,125</cell><cell cols="5">14,063 5.9415 ? 10 ?4 0.934271 0.989295 0.31466 1</cell></row><row><cell>32,768</cell><cell>2</cell><cell>512</cell><cell>781</cell><cell cols="5">1,562 2.8464 ? 10 ?4 0.963567 0.952647 0.31466 1</cell></row><row><cell>65,536</cell><cell>1</cell><cell>128</cell><cell>2,000</cell><cell cols="5">7,037 1.3653 ? 10 ?3 0.952378 0.86471 0.19891 2</cell></row><row><cell>32,768</cell><cell>2</cell><cell>512</cell><cell>781</cell><cell cols="5">1,562 2.8464 ? 10 ?4 0.952647 0.963567 0.19891 2</cell></row><row><cell>65,536</cell><cell>2</cell><cell>512</cell><cell>390</cell><cell>781</cell><cell cols="2">6.1951 ? 10 ?5</cell><cell>0.65322</cell><cell>0.82451 0.19891 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Best hyperparameters from tuning Adam on BERT-Large pretraining. ? refers to weight decay and p refers to the polynomial power in the learning rate schedule for both the warmup and decay phases. All trials used = 10 ?11 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>When LAMB was used all parameters, the majority of trials diverged during training -it took 67 trials to get 25 trials that did not NaN during training. Our trial budgets refer to the number of feasible trials, i.e. trials that do not diverge during training.</figDesc><table><row><cell cols="3">Weights Optimizer Bias/BN Optimizer Name</cell><cell>Initial Range</cell><cell>Final Range</cell><cell>Best</cell></row><row><cell>Nesterov</cell><cell>Nesterov</cell><cell>?</cell><cell>np.logspace(-.5, .5, 10)</cell><cell>[0.8, 3]</cell><cell>1.173</cell></row><row><cell>Nesterov</cell><cell>Nesterov</cell><cell>?</cell><cell>np.logspace(-4, -3, 10)</cell><cell>[3 ? 10 ?4 , 10 ?3 ]</cell><cell>3.026 ? 10 ?4</cell></row><row><cell>LARS</cell><cell>Heavy-ball momentum</cell><cell>?</cell><cell>np.logspace(0, 2, 10)</cell><cell>[10, 40]</cell><cell>14.49</cell></row><row><cell>LARS</cell><cell>Heavy-ball momentum</cell><cell>?</cell><cell cols="3">np.logspace(-5, -2, 10) [5 ? 10 ?5 , 2 ? 10 ?4 ] 1.708 ? 10 ?4</cell></row><row><cell>LARS</cell><cell>LARS</cell><cell>?</cell><cell>[1, 30]</cell><cell>[10, 30]</cell><cell>14.18</cell></row><row><cell>LARS</cell><cell>LARS</cell><cell>?</cell><cell>[10 ?4 , 10 ?1 ]</cell><cell cols="2">[5 ? 10 ?5 , 5 ? 10 ?4 ] 5.278 ? 10 ?5</cell></row><row><cell>Adam ( = 10 ?8 )</cell><cell>Adam ( = 10 ?8 )</cell><cell>?</cell><cell>[10 ?3 , 1]</cell><cell>[4 ? 10 ?3 , 2 ? 10 ?2 ]</cell><cell>0.004596</cell></row><row><cell>Adam ( = 10 ?8 )</cell><cell>Adam ( = 10 ?8 )</cell><cell>?</cell><cell>[10 ?2 , 4]</cell><cell>[2 ? 10 ?1 , 1]</cell><cell>0.6182</cell></row><row><cell>Adam ( = 10 ?6 )</cell><cell>Adam ( = 10 ?6 )</cell><cell>?</cell><cell>np.logspace(-3, 0, 10)</cell><cell>[3 ? 10 ?3 , 10 ?2 ]</cell><cell>3.332 ? 10 ?3</cell></row><row><cell>Adam ( = 10 ?6 )</cell><cell>Adam ( = 10 ?6 )</cell><cell>?</cell><cell>np.logspace(-2, 0.5, 6)</cell><cell>[0.5, 2]</cell><cell>1.055</cell></row><row><cell>LAMB</cell><cell>LAMB</cell><cell>?</cell><cell cols="2">np.logspace(-4, 0, 30) [4 ? 10 ?3 , 5 ? 10 ?2 ]</cell><cell>0.01134</cell></row><row><cell>LAMB</cell><cell>LAMB</cell><cell>?</cell><cell>np.logspace(-5, -2, 4)</cell><cell>[1 ? 10 ?2 , 0.1]</cell><cell>0.02657</cell></row><row><cell>LAMB</cell><cell>Adam ( = 10 ?8 )</cell><cell>?</cell><cell>[10 ?3 , 1]</cell><cell>[10 ?2 , 8 ? 10 ?2 ]</cell><cell>0.02569</cell></row><row><cell>LAMB</cell><cell>Adam ( = 10 ?8 )</cell><cell>?</cell><cell>[10 ?2 , 4]</cell><cell>[1, 8]</cell><cell>2.500</cell></row><row><cell>LAMB</cell><cell>Adam ( = 10 ?6 )</cell><cell>?</cell><cell>np.logspace(-3, 0, 10)</cell><cell>[10 ?2 , 8 ? 10 ?2 ]</cell><cell>0.03378</cell></row><row><cell>LAMB</cell><cell>Adam ( = 10 ?6 )</cell><cell>?</cell><cell>np.logspace(-2, 0.5, 6)</cell><cell>[1, 8]</cell><cell>4.197</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Search spaces used for the 6,000 step, cosine learning rate schedule experiments. All hyperparameters were tuned on a logarithmic scale, except for those which define a discrete sequence of points to evaluate such as "np.logspace".</figDesc><table><row><cell></cell><cell>Range</cell><cell>Scaling</cell></row><row><cell>? 0</cell><cell>[10 ?3 , 50.0]</cell><cell>Log</cell></row><row><cell cols="3">? decay factor {10 ?4 , 10 ?3 , 10 ?2 , 10 ?1 } Discrete</cell></row><row><cell>1 ? ?</cell><cell>[10 ?3 , 1.0]</cell><cell>Log</cell></row><row><cell>?</cell><cell>[10 ?5 , 10 ?1 ]</cell><cell>Log</cell></row><row><cell>?</cell><cell>[10 ?2 , 2 ? 10 ?1 ]</cell><cell>Linear</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Log ? decay factor {10 ?4 , 10 ?3 , 10 ?2 , 10 ?1 } Discrete</figDesc><table><row><cell></cell><cell>Range</cell><cell>Scaling</cell></row><row><cell>? 0</cell><cell>[10 ?3 , 50.0]</cell><cell></cell></row><row><cell>1 ? ?</cell><cell>[10 ?3 , 1.0]</cell><cell>Log</cell></row><row><cell>?</cell><cell>[10 ?5 , 10 ?1 ]</cell><cell>Log</cell></row><row><cell>?</cell><cell>[10 ?2 , 2 ? 10 ?1 ]</cell><cell>Linear</cell></row><row><cell cols="3">Table 10: Same as Table 9 but trained for 2,658 steps ("68 epochs" as defined by MLPerf epoch</cell></row><row><cell>calculations) for 50 trials.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Range</cell><cell>Scaling</cell></row><row><cell>? 0</cell><cell>[10 ?1 , 20.0]</cell><cell>Log</cell></row><row><cell cols="3">? decay factor {10 ?5 , 10 ?4 , 10 ?3 } Discrete</cell></row><row><cell>t decay</cell><cell>[2392, 2.658]</cell><cell>Linear</cell></row><row><cell>1 ? ?</cell><cell>[10 ?3 , 1.0]</cell><cell>Log</cell></row><row><cell>?</cell><cell>[10 ?5 , 2 ? 10 ?1 ]</cell><cell>Log</cell></row><row><cell>?</cell><cell>[10 ?2 , 2 ? 10 ?1 ]</cell><cell>Linear</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Log ? decay factor {10 ?5 , 10 ?4 , 10 ?3 } Discrete</figDesc><table><row><cell></cell><cell>Range</cell><cell>Scaling</cell></row><row><cell>? peak</cell><cell>[10 ?1 , 32.0]</cell><cell></cell></row><row><cell>t decay</cell><cell>[2392, 2.658]</cell><cell>Linear</cell></row><row><cell>1 ? ?</cell><cell>[10 ?4 , 10 ?1 ]</cell><cell>Log</cell></row><row><cell>?</cell><cell>[10 ?4 , 10 ?1 ]</cell><cell>Log</cell></row><row><cell>?</cell><cell>[5 ? 10 ?2 , 0.15]</cell><cell>Linear</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Range Scaling ? peak [10 ?1 , 32.0] Log ? decay factor [3 ? 10 ?5 , 3 ? 10 ?4 ]</figDesc><table><row><cell></cell><cell></cell><cell>Log</cell></row><row><cell>t decay</cell><cell>[2533, 2.815]</cell><cell>Linear</cell></row><row><cell>1 ? ?</cell><cell>[5 ? 10 ?3 , 10 ?1 ]</cell><cell>Log</cell></row><row><cell>?</cell><cell>[10 ?2 , 10 ?1 ]</cell><cell>Log</cell></row><row><cell>?</cell><cell>[5 ? 10 ?2 , 0.15]</cell><cell>Linear</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 :</head><label>15</label><figDesc>The same asTable 14except with virtual batch size 64. , 2 ? 10 ? , ..., 9 ? 10 ? } ?? ? {?3, ...2}} + {100, } Discrete</figDesc><table><row><cell></cell><cell>Range</cell><cell>Scaling</cell></row><row><cell>? peak</cell><cell>[10 ?1 , 32.0]</cell><cell>Log</cell></row><row><cell cols="2">? decay factor [3 ? 10 ?5 , 3 ? 10 ?4 ]</cell><cell>Log</cell></row><row><cell>t decay</cell><cell>[2533, 2.815]</cell><cell>Linear</cell></row><row><cell>1 ? ?</cell><cell>[5 ? 10 ?3 , 10 ?1 ]</cell><cell>Log</cell></row><row><cell>?</cell><cell>[10 ?2 , 10 ?1 ]</cell><cell>Log</cell></row><row><cell>?</cell><cell>[5 ? 10 ?2 , 0.15]</cell><cell>Linear</cell></row><row><cell></cell><cell>Range</cell><cell>Scaling</cell></row><row><cell>? peak {{10 ? decay factor</cell><cell>8.144 ? 10 ?5</cell><cell>-</cell></row><row><cell>t decay</cell><cell>2250</cell><cell>-</cell></row><row><cell>1 ? ?</cell><cell>0.02397</cell><cell>-</cell></row><row><cell>?</cell><cell>0.009992</cell><cell>-</cell></row><row><cell>?</cell><cell>0.07786</cell><cell>-</cell></row></table><note>?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 18 :</head><label>18</label><figDesc>? refers to weight decay, which is not applied to the bias and batch normalization variables. Trained for 2,815 steps. Virtual batch size 64. Using the best hyperparameters fromTable 15, we swept over batch normalization hyperparameters. ? 10 ?6 , 5 ? 10 ?5 ] Linear</figDesc><table><row><cell></cell><cell>Range</cell><cell>Scaling</cell></row><row><cell>? peak</cell><cell>[2.0, 8.0]</cell><cell>Log</cell></row><row><cell cols="3">? decay factor [4 ? 10 ?5 , 1.6 ? 10 ?4 ] Linear</cell></row><row><cell>t decay</cell><cell>[2100, 2400]</cell><cell>Linear</cell></row><row><cell>1 ? ?</cell><cell>[0.012, 0.04]</cell><cell>Log</cell></row><row><cell>?</cell><cell>[7 ? 10 ?3 , 7 ? 10 ?2 ]</cell><cell>Log</cell></row><row><cell>?</cell><cell>[0.04, 0.1]</cell><cell>Linear</cell></row><row><cell>?</cell><cell>[0.45, 0.55]</cell><cell>Linear</cell></row><row><cell></cell><cell>[5</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">We do not consider the MLPerf task in this paper since it is a warm-start, partial training task. 14 https://git.io/JtY8d 15 See https://git.io/JtnQW and https://git.io/JtnQ8.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">All convergence bounds assume no momentum is used, and the Lavg bound for LAMB also assumes ?2 = 0, when it is typically 0.999. Additionally, Lavg could still be large if L? is large, but we leave an empirical analysis of this to future work.17  Deep learning folk wisdom is that "any method to make training less effective can serve as a regularizer," whether it is a bug in gradients or a clever algorithm.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18">https://mlcommons.org/en/groups/research-algorithms/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t f . image . s a m p l e d i s t o r t e d b o u n d i n g b o x t f . image . d e c o d e a n d c r o p j p e g t f . image . r e s i z e t f . image . r a n d o m f l i p l e f t r i g h t t f . image . c o n v e r t i m a g e d t y p e 19 Full code available at https://git.io/JtgtE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20">https://github.com/google-research/bert</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21">As suggested in<ref type="bibr" target="#b22">You et al. [2019]</ref>, we used L2 regularization for LARS and weight decay for LAMB. For consistency, we used L2 regularization for Nesterov momentum (which is more analogous to LARS) and weight decay for Adam (which is more analogous to LAMB).</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table 20</ref><p>: Here we switched ? to refer to L2 regularization. We also began training for 2,512 steps, which is the final "64 epochs" used in the Nesterov results reported in the main text. Because of this more stringent step budget, we focused on the learning rate schedule. t decay was set to all remaining steps after the warmup was finished. Tuned for 229 trials. Virtual batch size 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Range</head><p>Scaling  <ref type="table">Table 23</ref>: Again we dial in more on a tighter tuning range for the L2 regularization. ? refers to L2. Trained for 2,512 steps steps. Tuned for 37 trials. Virtual batch size 64.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/.Softwareavailablefromtensorflow.org" />
		<editor>Man?, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi?gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Disentangling adaptive gradient methods from learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11803</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Critical hyperparameters: No random</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Vincent</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.03200" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://github.com/google/jax" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dami</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05446</idno>
		<title level="m">On empirical comparisons of optimizers for deep learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08741</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raminder</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bitorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiachen</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09756</idno>
		<title level="m">models on google tpu-v3 pods</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gu-Yeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bittorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debojyoti</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udit</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Hock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Ike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffery</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tayo</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gennady</forename><surname>Oguntebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pekhimenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01500</idno>
		<ptr target="https://arxiv.org/abs/1910.01500" />
		<title level="m">Cliff Young, and Matei Zaharia. MLPerf training benchmark</title>
		<editor>Vijay Janapa Reddi, Taylor Robie, Tom St. John, Tsuguchika Tabaru, Carole-Jean Wu, Lingjie Xu, Masafumi Yamazaki</editor>
		<meeting><address><addrLine>Lillian Pentecost</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate O(1/k?2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yurii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Dokl. akad. nauk Sssr</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Some methods of speeding up the convergence of iteration methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Descending through a crowded valleybenchmarking deep learning optimizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hennig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01547</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Measuring the effects of data parallelism on neural network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">112</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">Emma</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu-Yeon</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Brooks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10701</idno>
		<title level="m">Benchmarking tpu, gpu, and cpu platforms for deep learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Understanding short-horizon bias in stochastic meta-optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02021</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Image classification at supercomputer scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06992</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sushant</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8196" to="8207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.11</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.11" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
