<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">U-shape Transformer for Underwater Image Enhancement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lintao</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunli</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Bian</surname></persName>
						</author>
						<title level="a" type="main">U-shape Transformer for Underwater Image Enhancement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Underwater image enhancement</term>
					<term>Transformer</term>
					<term>Multi-color space loss function</term>
					<term>Underwater image dataset</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The light absorption and scattering of underwater impurities lead to poor underwater imaging quality. The existing data-driven based underwater image enhancement (UIE) techniques suffer from the lack of a large-scale dataset containing various underwater scenes and high-fidelity reference images. Besides, the inconsistent attenuation in different color channels and space areas is not fully considered for boosted enhancement. In this work, we built a large scale underwater image (LSUI) dataset, which covers more abundant underwater scenes and better visual quality reference images than existing underwater datasets. The dataset contains 4279 real-world underwater image groups, in which each raw image's clear reference images, semantic segmentation map and medium transmission map are paired correspondingly. We also reported an U-shape Transformer network where the transformer model is for the first time introduced to the UIE task. The U-shape Transformer is integrated with a channel-wise multi-scale feature fusion transformer (CMSFFT) module and a spatial-wise global feature modeling transformer (SGFMT) module specially designed for UIE task, which reinforce the network's attention to the color channels and space areas with more serious attenuation. Meanwhile, in order to further improve the contrast and saturation, a novel loss function combining RGB, LAB and LCH color spaces is designed following the human vision principle. The extensive experiments on available datasets validate the state-of-the-art performance of the reported technique with more than 2dB superiority. The dataset and demo code are available on https: //lintaopeng.github.io/ pages/UIE%20Project%20Page.html.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>U NDERWATER Image Enhancement (UIE) technology <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> is essential for obtaining underwater images and investigating the underwater environment, which has wide applications in ocean exploration, biology, archaeology, underwater robots <ref type="bibr" target="#b2">[3]</ref> and among other fields. However, underwater images frequently have problematic issues, such as color casts, color artifacts and blurred details <ref type="bibr" target="#b3">[4]</ref>. Those issues could be explained by the strong absorption and scattering effects on light, which are caused by dissolved impurities and suspended matter in the medium (water). Therefore, UIErelated innovations are of great significance for improving the visual quality and merit of images in accurately understanding the underwater world.</p><p>In general, the existing UIE methods could be categorized into three types, which are physical model-based, visual priorbased and data-driven methods, respectively. Among them, L. Peng, C. <ref type="bibr">Zhu</ref>   <ref type="figure">Fig. 1</ref>. Compared with the existing UIE methods, the image produced by our U-shape Transformer has the highest PSNR <ref type="bibr" target="#b4">[5]</ref> score and best visual quality.</p><p>visual prior-based UIE methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> mainly concentrated on improving the visual quality of underwater images by modifying pixel values from the perspectives of contrast, brightness and saturation. Nevertheless, the ignorance of the physical degradation process limits the improvement of enhancement quality. In addition, physicalmodel based UIE methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> mainly focus on the accurate estimation of medium transmission. With the estimated medium transmission and other key underwater imaging parameters such as the homogeneous background light, a clean image can be obtained by reversing a physical underwater imaging model. However, the performance of physical model-based UIE is restricted to complicated and diverse real-world underwater scenes. That is because, (1) model hypothesis is not always plausible with complicated and dynamic underwater environment; (2) evaluating multiple parameters simultaneously is challenging. More recently, as to the data-driven methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b3">[4]</ref>, which could be regarded as deep learning technologies in UIE domain, exhibit impressive performance on UIE task. However, the existing underwater datasets more-or-less have the disadvantages, such as a small number of images, few underwater scenes, or even not real-world scenarios, which limits the performance of the data-driven UIE method. Besides, the inconsistent attenuation of the underwater images in different color channels and space areas have not been unified in one framework.</p><p>In this work, we first built a large scale underwater image (LSUI) dataset, which covers more abundant underwater scenes (water types, lighting conditions and target categories) and better visual quality reference images than existing underwater datasets <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b21">[22]</ref>. The dataset contains 4279 real-world underwater images, and the corresponding clear images are generated as comparison references. We also provide arXiv:2111.11843v6 [cs.CV] 12 Jun 2022 the semantic segmentation map and medium transmission map for each image. Furthermore, with the prior knowledge that the attenuation of different color channels and space areas in underwater images is inconsistent, we designed a channelwise multi-scale feature fusion transformer (CMSFFT) and a spatial-wise global feature modeling transformer (SGFMT) based on the attention mechanism, and embedded them in our U-shape Transformer which is designed based on <ref type="bibr" target="#b32">[33]</ref>. Moreover, according to the color space selection experiment and <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b22">[23]</ref>, we designed a multi-color space loss function including RGB, LAB and LCH color space. <ref type="figure">Fig. 1</ref> shows the result of our UIE method and some comparison UIE methods, and the main contributions of this paper can be summarized as follows:</p><p>? We reported a novel U-shape Transformer dealing with the UIE task, in which the designed channel-wise and spatial-wise attention mechanism based on transformer enables to effectively remove color artifacts and casts. ? We designed a novel multi-color space loss function combing the RGB, LCH and LAB color-space features, which further improves the contrast and saturation of output images. ? We released a large-scale dataset containing 4279 real underwater images and the corresponding high-quality reference images, semantic segmentation maps, and medium transmission maps, which facilitates further development of UIE techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. UIE Methods</head><p>UIE is an indispensable step to improve the visual quality of recorded underwater images. A variety of methods have been proposed and can be categorized as visual prior, physical models, and data-driven methods.</p><p>UIE methods based on visual prior. This approach aims to restore a clear underwater image by modifying its pixel value. Typical methods involve: 1) Modify the pixel value with single metric. Such as contrast adjustment <ref type="bibr" target="#b34">[35]</ref>, histogram equalization <ref type="bibr" target="#b34">[35]</ref>, and white balance <ref type="bibr" target="#b10">[11]</ref>. For instance, Hitam et al. <ref type="bibr" target="#b34">[35]</ref> used contrast adjustment and adaptive histogram equalization methods in RGB color space and HSV color space to enhance the contrast of underwater images and reduce noise. 2) Modify the pixel value with multiple metrics. For example, fusion-based methods, which exhibit the final enhancement image via the weighted fusion of multiple traditional UIE methods. For example, Fang et al. <ref type="bibr" target="#b5">[6]</ref> first applied white balance and global contrast adjustments to enhance underwater images, and then the two enhancement results are combined into one image by weighted addition to obtain the final enhanced underwater image. 3) Retinex based UIE methods. Fu et al. <ref type="bibr" target="#b8">[9]</ref> proposed a retinex model-based UIE method including color correction, layer decomposition and enhancement. Furthermore, Zhang et al. <ref type="bibr" target="#b35">[36]</ref> proposed a multi-scale UIE method based on the retinex model.</p><p>The way of modifying pixel value has the inherent advantage of improving the contrast and saturation of the raw underwater image. However, as visual prior neglected the inconsistent attenuation degree of underwater images in varied color channels and space areas, it performs not well on real underwater images with complex underwater environments.</p><p>UIE methods based on physical models. This approach regard UIE as a problem of inversion, and researchers usually enhance underwater images based on the following three steps, 1) establishing the prior conditions of the hypothetical physical imaging model; 2) estimating the key parameters; 3) reversing the degradation process of the underwater imaging process to obtain a clear image.</p><p>Prior is the basis of the physical model based UIE, in which existing work includes underwater dark channel priors <ref type="bibr" target="#b12">[13]</ref>, attenuation curve priors <ref type="bibr" target="#b14">[15]</ref>, fuzzy priors <ref type="bibr" target="#b17">[18]</ref> and minimum information priors <ref type="bibr" target="#b19">[20]</ref>, etc. Early-stage research enhanced the underwater image by modifying the dark channel prior (DCP) <ref type="bibr" target="#b12">[13]</ref> algorithm. Chiang et al. <ref type="bibr" target="#b17">[18]</ref> restored the underwater image by combining the DCP with the wavelength compensation algorithm. Drews Jr et al. <ref type="bibr" target="#b12">[13]</ref> proposed an underwater dark channel prior algorithm (UDCP) based on the priori that the red channel in the underwater image is more attenuated. Carlevaris Bianca et al. <ref type="bibr" target="#b36">[37]</ref> used the attenuation difference prior between the three color channels in RGB color space to predict the transmission characteristics of the underwater scene, which feasibility is basically due to red light generally decays faster than green and blue light. In addition, Peng et al. <ref type="bibr" target="#b15">[16]</ref> proposed a depth map estimate method for underwater scenes based on the intrinsic characteristics of underwater image blurriness and light absorption to effectively recover underwater images. Li et al. <ref type="bibr" target="#b6">[7]</ref> integrate the minimum information loss and histogram distribution prior for depth estimation to recover underwater images.</p><p>This branch of UIE methods could achieve satisfactory results only when underwater scenes are in accordance with the selected physical imaging model. Therefore, the manually established priors restrain the model's robustness and scalability under the complicated and varied circumstances. Moreover, as the underwater physical imaging model does not taken human eye's perception characteristics into account, the visual quality of the restored images are of poor presentation effect. In recent years, underwater physical imaging models are gradually utilized in combination with data-driven methods <ref type="bibr" target="#b22">[23]</ref>.</p><p>Data-driven UIE methods. Current data-driven UIE methods can be divided into two main technical routes, (1) designing an end-to-end module; (2) utilizing deep models directly to estimate physical parameters, and then restore the clean image based on the degradation model. To alleviate the need for real-world underwater paired training data, Li et al. <ref type="bibr" target="#b21">[22]</ref> proposed a WaterGAN to generate underwater-like images from in-air images and depth maps in an unsupervised manner, in which the generated dataset is further used to train the WaterGAN. Moreover, <ref type="bibr" target="#b23">[24]</ref> exhibited a weakly supervised underwater color transmission model based on CycleGAN <ref type="bibr" target="#b37">[38]</ref>. Benefiting from the adversarial network architecture and multiple loss functions, that network can be trained using unpaired underwater images, which refines the adaptability of the network model to underwater scenes. However, images in the training dataset used by the above methods are not matched real underwater images, which leads to limited en-hancement effects of the above methods in diverse real-world underwater scenes. Recently, Li et al. <ref type="bibr" target="#b27">[28]</ref> proposed a gated fusion network named WaterNet, which uses gamma-corrected images, contrast-improved images, and white-balanced images as the inputs to enhance underwater images. Yang et al. <ref type="bibr" target="#b38">[39]</ref> proposed a conditional generative adversarial network (cGAN) to improve the perceptual quality of underwater images.</p><p>The methods mentioned above usually use existing deep neural networks for general purposes directly on UIE tasks and neglect the unique characteristics of underwater imaging. For example, <ref type="bibr" target="#b23">[24]</ref> directly used the CycleGAN <ref type="bibr" target="#b37">[38]</ref> network structure, and <ref type="bibr" target="#b27">[28]</ref> adopted a simple multi-scale convolutional network. Other models such as UGAN <ref type="bibr" target="#b29">[30]</ref>,WaterGAN <ref type="bibr" target="#b21">[22]</ref> and cGAN <ref type="bibr" target="#b38">[39]</ref>, still inherited the disadvantage of GANbased models, which produces unstable enhancement results. In addition, Ucolor <ref type="bibr" target="#b22">[23]</ref> combined the underwater physical imaging model and designed a medium transmission guided model to reinforce the network's response to areas with more severe quality degradation, which could improve the visual quality of the network output to a certain extent. However, physical models sometimes failed with varied underwater environments.</p><p>From above, our proposed network aims at generating high visual quality underwater images by properly accounting the inconsistent attenuation characteristics of underwater images in different color channels and space areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Underwater Image Datasets</head><p>The sophisticated and dynamic underwater environment results in extreme difficulties in the collection of matched underwater image training data in real-world underwater scenes. Present datasets can be classified into two types, they are, (1) Non-reference datasets. Liu et al. <ref type="bibr" target="#b31">[32]</ref> proposed the RUIE dataset, which encompasses varied underwater lighting, depth of field, blurriness and color cast scenes. Akkaynak et al. <ref type="bibr" target="#b25">[26]</ref> published a non-reference underwater dataset with a standard color comparison chart. Those datasets, however, cannot be used for end-to-end training for lacking matched clear reference underwater images. (2) Full-reference datasets. Li et al. <ref type="bibr" target="#b21">[22]</ref> presented an unsupervised network dubbed WaterGAN to produce underwater-like images using in-air images and depth maps. Similarly, Fabbri et al. <ref type="bibr" target="#b29">[30]</ref> used CycleGAN to generate distorted images from clean underwater images based on weakly-supervised distribution transfer. However, these methods rely heavily on training samples, which is easy to produce artifacts that are out of reality and unnatural. Li et al. <ref type="bibr" target="#b27">[28]</ref> constructed a real UIE benchmark UIEB, including 890 images pairs, in which reference images were handcrafted using the existing optimal UIE methods. Although those images are authentic and reliable, the number, content and coverage of underwater scenes are limited. In contrast, our LSUI dataset contains 4279 real-world underwater images with more abundant underwater scenes (water types, lighting conditions and target categories) than existing underwater datasets <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b21">[22]</ref>, and the corresponding clear images are generated as comparison references. We also provide the semantic segmentation map and medium transmission map for each raw underwater image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Transformers</head><p>Although CNN-based UIE methods <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b22">[23]</ref> achieved significant improvement compared with traditional UIE methods. There are still two aspects that limit its further promotion, (1) uniform convolution kernel is not able to characterize the inconsistent attenuation of underwater images in different color channels and spatial regions; (2) the CNN architecture concerns more on local features, while ineffective for long-dependent and global feature modeling.</p><p>Recently, transformer <ref type="bibr" target="#b39">[40]</ref> has gained more and more attention, its content-based interactions between image content and attention weights can be interpreted as spatially varying convolution, and the self-attention mechanism is efficient at modeling long-distance dependencies and global features. Benefiting from these advantages, transformers have shown outstanding performance in several vision tasks <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Compared with previous CNN-based UIE networks, our CMSFFT and SGFMT modules designed based on the transformer can guide the network to pay more attention to the more serious attenuated color channels and spatial areas. Moreover, by combining CNN with transformer, we achieve better performance with a relatively small amount of parameters. Data Collection. We have collected 8018 underwater images, which is composed of images collected by ourself and from other existing public datasets <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b29">[30]</ref> (All images have been licensed and used only for academic purposes). Real underwater images with rich water scenes, water types, lighting conditions and target categories, are selected to the extent possible, for further generating clear reference images. Reference Image Generation. The reference images were selected with two round subjective and objective evaluations, to eliminating the potential bias to the extent possible. In the first round, inspired by ensemble learning <ref type="bibr" target="#b44">[45]</ref> that multiple weak classifiers could form a strong one, we firstly use 18 existing optimal UIE methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> to process the collected underwater images successively,  . Example images in the LSUI dataset. Our LSUI dataset contains 4279 real-world underwater images with more abundant underwater scenes (water types, lighting conditions and target categories) than existing underwater datasets <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b21">[22]</ref>, and the corresponding clear images are generated as comparison references. We also provide the semantic segmentation map and medium transmission map for each raw underwater image. The top of each image group is the clear reference image, followed by the raw underwater image, semantic segmentation map, and medium transmission map. and a set with 18 * 8018 images is generated for the next-step optimal reference dataset selection. Unlike <ref type="bibr" target="#b27">[28]</ref>, to reducing the number of images that need to be selected manually, nonreference metrics UIQM <ref type="bibr" target="#b47">[48]</ref> and UCIQE <ref type="bibr" target="#b48">[49]</ref> are adopted to score all generated images with equal weights. Then, the top-three reference images of each original one form a set with the size 3 * 8018. Considering individual differences, 20 volunteers with image processing experience were invited to rate images according to 5 most important judgments (contrast; saturation; color correction effects; artifacts degree; over or under-enhancement degree) of UIE tasks with a score from 0-10, where the higher score represents the more contentedness. And the total score of each reference picture is 100 (5 * 20) after normalizing each score to 0-1. The top-one reference image of each raw underwater image was chosen with the highest summation value. In addition, images with the highest summation lower than 70 have been removed from the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED DATASET AND METHOD A. LSUI Dataset</head><p>After the first round, some of the generated reference images still have problems such as blur, color cast and noise. So in the second round, we invited volunteers to vote on each reference picture again to select its existing problems and determine the corresponding optimization method, and then use appropriate image enhancement methods <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> to process it. Next, all volunteers were invited to conduct another round of voting to remove image pairs that more than half of the volunteers were dissatisfied with. To improve the utility of the LSUI dataset, we also hand-labeled a segmentation map and generated a medium transmission map for each image. Eventually, our LSUI dataset contains 4279 images and the corresponding high-quality reference images, semantic segmentation maps, and medium transmission maps for each image.</p><p>As shown in <ref type="figure" target="#fig_0">Fig .2</ref>, compared with UIEB <ref type="bibr" target="#b27">[28]</ref>, our LSUI dataset contains large number of images with richer underwater scenes and object categories. In particular, our LSUI dataset includes deep-sea scenes and underwater cave scenes that are not available in previous underwater datasets. We provide some examples of our LSUI dataset in <ref type="figure" target="#fig_2">Fig .3</ref>, which includes varied underwawter scenes, water types, lighting conditions and target categories. As to the authors' best knowledge, LSUI is the largest real underwater image dataset with high quality reference images at the present time, which could facilitate the further development of the UIE methods.</p><p>B. U-shape Transformer 1) Overall Architecture: The overall architecture of the U-shape Transformer is shown as <ref type="figure">Fig. 4</ref>, which includes a CMSFFT &amp; SGFMT based generator and a discriminator.</p><p>In the generator, (1) Encoding: Except being directly input to the network, the original image will be downsampled three times respectively. Then after 1*1 convolution, the three scale feature maps are input into the corresponding scale convolution block. The outputs of four convolutional blocks are the inputs of the CMSFFT and SGFMT; <ref type="formula" target="#formula_2">(2)</ref>  </p><formula xml:id="formula_0">+ GT Output Fig. 4.</formula><p>The network structure of U-shape Transformer. CMSFFT and SGFMT modules specially designed for UIE tasks reinforce the network's attention to the more severely attenuated color channels and spatial regions. The multi-scale connections of the generator and the discriminator make the gradient flow freely between the generator and the discriminator, therefore making the training process more stable.</p><p>to the first convolutional block. Meanwhile, four convolutional blocks with varied scales will receive the four outputs from CMSFFT.</p><p>In the discriminator, the input of the four convolutional blocks includes: the feature map output by its own upper layer, the feature map of the corresponding size from the decoding part and the feature map generated by 1 * 1 convolution after downsampling to the corresponding size using the reference image. With the described multi-scale connections, the gradient flow can flow freely on multiple scales between the generator and the discriminator, such that a stable training process could be obtained, details of the generated images could be enriched. The detailed structure of SGFMT and CMSFFT in the network will be described in the following two subsections.</p><p>2) SGFMT: The SGFMT (as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>) is used to replace the original bottleneck layer of the generator, which can assist the network to model the global information and reinforce the network's attention on severely degraded parts. Assuming the size of the input feature map is F in ? R For the expected one-dimensional sequence of the transformer, linear projection is used to stretch the two-dimensional feature map into a feature sequence S in ? R HW 256 * C . For preserving the valued position information of each region, learnable position embedding is merged directly, which can be expressed as,</p><formula xml:id="formula_1">S in = W * F in + PE,<label>(1)</label></formula><p>where W * F i represents a linear projection operation, PE represents a position embedding operation. Then we input the feature sequence S in to the transformer block, which contains 4 standard transformer layers <ref type="bibr" target="#b39">[40]</ref>. Each transformer layer contains a multi-head attention block (MHA) and a feed-forward network (FFN). The FFN includes a normalization layer and a fully connected layer. The output of the l-th(l ? [1, 2, . . . ., l]) layer in the transformer block can be calculated by,   According to the prior that underwater images are inconsistently attenuated in different color channels, we propose a novel channel-wise multi-scale feature fusion transformer(CMSFFT). Specifically, CMSFFT replaces the skip connection of the generator, uses the channel-wise self-attention mechanism to perform channel-wise multi-scale feature fusion on the features output by the encoder of the generator, and transmits the fusion results to the decoder efficiently, so as to reinforce the network's attention to the color channels with more serious attenuation and realize accurate UIE.</p><formula xml:id="formula_2">S l = MHA(LN(S l?1 )) + S l?1<label>(2)</label></formula><formula xml:id="formula_3">S l = FFN(LN(S l )) + S l ,<label>(3)</label></formula><p>where LN represents layer normalization, and S l represents the output sequence of the l-th layer in the transformer block. The output feature sequence of the last transformer block is S l ? R HW 256 * C , which is restored to the feature map of F out ? R H 16 * W 16 * C after feature remapping.</p><p>3) CMSFFT: To reinforce the network's attention on the more serious attenuation color channels, inspired by <ref type="bibr" target="#b51">[52]</ref>, we designed the CMSFFT block to replace the skip connection of the original generator's encoding-decoding architecture <ref type="figure" target="#fig_6">(Fig.6</ref>), which consists of the following three parts. Multi-Scale Feature Encoding. The inputs of CMSFFT are the feature maps F i ? R H 2 i * W 2 i * Ci (i = 0, 1, 2, 3) with different scales. Differs from the linear projection in Vit <ref type="bibr" target="#b52">[53]</ref> which is applied directly on the partitioned original image, we use convolution kernels with related filter size P 2 i * P 2 i (i = 0, 1, 2, 3) and step size P 2 i (i = 0, 1, 2, 3), to conduct linear projection on feature maps with varied scales. In this work, P is set as 32.</p><p>After that, four feature sequence S i ? R d * Ci (i = 1, 2, 3, 4) could be obtained, where d ? HW P 2 . Those four convolution kernels divide feature maps into the same number of blocks, while the number of channels C i (i = 1, 2, 3, 4) remains unchanged. Then, four query vectors Q i ? R d * Ci (i = 1, 2, 3, 4), K ? R d * C and V ? R d * C can be obtained by Eq.(4).</p><formula xml:id="formula_4">Q i = S i W Qi K = SW K V = SW V ,<label>(4)</label></formula><p>where W Qi ? R d * Ci (i = 1, 2, 3, 4), W K ? R d * C and W V ? R d * C stands for learnable weight matrices; S is generated by concatenating S i ? R d * Ci (i = 1, 2, 3, 4) via the channel dimension, where C = C 1 + C 2 + C 3 + C 4 . In this work, C 1 , C 2 , C 3 , and C 4 are set as 64, 128, 256, 512, respectively. Channel-Wise Multi-Head Attention(CMHA). The CMHA block has six inputs, which are K ? R d * C , V ? R d * C and Q i ? R d * Ci (i = 1, 2, 3, 4). The output of channel-wise attention CA i ? R Ci * d (i = 1, 2, 3, 4) could be obtained by,</p><formula xml:id="formula_5">CA i = SoftMax(IN( Q T i K 2 ? C ))V T ,<label>(5)</label></formula><p>where IN represents the instance normalization operation. This attention operation performs along the channel-axis instead of the classical patch-axis <ref type="bibr" target="#b52">[53]</ref>, which can guide the network to pay attention to channels with more severe image quality degradation. In addition, IN is used on the similarity maps to assist the gradient flow spreads smoothly. The output of the i-th CMHA layer can be expressed as,</p><formula xml:id="formula_6">CMHA i = (CA i 1 + CA i 2 + ......., +CA i N )/N + Q i ,<label>(6)</label></formula><p>where N is the number of heads, which is set as 4 in our implementation.</p><p>Feed-Forward Network(FFN). Similar to the forward propagation of <ref type="bibr" target="#b52">[53]</ref>, the FFN output can be expressed as,</p><formula xml:id="formula_7">O i = CMHA i + MLP(LN(CMHA i )),<label>(7)</label></formula><p>where O i ? R d * Ci (i = 1, 2, 3, 4); MLP stands for multilayer perception. Here, The operation in Eq. (7) needs to be repeated l (l=4 in this work) times in sequence to build the l-layer transformer. Finally, feature remappings are performed on the four different output feature sequences O i ? R Ci * d (i = 1, 2, 3, 4) to reorganize them into four feature maps F i ? R H 2 i * W 2 i * Ci (i = 0, 1, 2, 3) , which are the input of convolutional block in the generator's decoding part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss Function</head><p>To take advantage of the LAB and LCH color spaces' wider color gamut representation range and more accurate description of the color saturation and brightness, we designed a multi-color space loss function combining RGB, LAB and LCH color spaces to train our network. The image from RGB space is firstly converted to LAB and LCH space, and reads,</p><formula xml:id="formula_8">L G(x) , A G(x) , B G(x) = RGB2LAB(G(x)) L y , A y , B y = RGB2LAB(y),<label>(8)</label></formula><formula xml:id="formula_9">L G(x) , C G(x) , H G(x) = RGB2LCH(G(x)), L y , C y , H y = RGB2LCH(y),<label>(9)</label></formula><p>where x, y and G(x) represents the original inputs, the reference image, and the clear image output by the generator, respectively. Loss functions in the LAB and LCH space are written as Eq.(10) and Eq. <ref type="bibr" target="#b10">(11)</ref>.</p><formula xml:id="formula_10">Loss LAB (G(x), y) = E x,y [(L y ? L G(x) ) 2 ? n i=1 Q(A y i )log(Q(A G(x) i )) ? n i=1 Q(B y i )log(Q(B G(x) i ))],<label>(10)</label></formula><formula xml:id="formula_11">Loss LCH (G(x), y) = E x,y [? n i=1 Q(L y i )log(Q(L G(x) i )) + (C y ? C G(x) ) 2 + (H y ? H G(x) ) 2 ],<label>(11)</label></formula><p>where Q stands for the quantization operator.</p><p>L 2 loss in the RGB color space Loss RGB and the perceptual loss Loss per <ref type="bibr" target="#b53">[54]</ref> , as well as Loss LAB and Loss LCH are the four loss functions for the generator.</p><p>Besides, standard GAN loss function is introduced for minimizing the loss between generated and reference pictures, and written as,</p><formula xml:id="formula_12">L GAN (G, D) = E y [logD(y)] + E x [log(1 ? D(G(x)))],<label>(12)</label></formula><p>where D represents the discriminator. D aims at maximizing L GAN (G, D), to accurately distinguish the generated image from the reference image. And the goal of generator G is to minimize the loss between generated pictures and reference pictures.</p><p>Then, the final loss function is expressed as,</p><formula xml:id="formula_13">G * = arg min G max D L GAN (G, D) + ?Loss LAB (G(x), y) + ?Loss LCH (G(x), y) + ?Loss RGB (G(x), y) + ?Loss per (G(x), y),<label>(13)</label></formula><p>where ?, ?, ?, ? are hyperparameters, which are set as 0.001, 1, 0.1, 100, respectively, with numerous experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first introduce the training details of the U-shape Transformer and the detailed settings of the experiment. Next, we conduct experiments on the selection of color space. Then we retrain some network models we collected on the existing underwater datasets and the LSUI dataset to evaluate our proposed dataset. Moerover, we also compare our UIE method with state-of-the-arts on five datasets. Finally, series of ablation studies are conducted to demonstrate the effectiveness of each component in U-shape Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>The LSUI dataset was randomly divided as Train-L (3879 images) and Test-L400 (400 images) for training and testing, respectively. The training set was enhanced by cropping, rotating and flipping the existing images. All images were adjusted to a fixed size (256*256) when input to the network, and the pixel value will be normalized to [0,1].</p><p>We use python and pytorch framework via NVIDIA RTX3090 on Ubuntu20 to implement the U-shape Transformer. Adam optimization algorithm is utilized for the total of 800 epochs training with batchsize set as 6. The initial learning rate is set as 0.0005 and 0.0002 for the first 600 epochs and the last 200 epochs, respectively. Besides, the learning rate decreased 20% every 40 epochs. For Loss RGB , L 2 loss is used for the first 600 epochs, and L 1 loss is used for the last 200 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment Settings</head><p>Benchmarks. Besides Train-L, the second training set Train-U contains 800 pairs of underwater images from UIEB <ref type="bibr" target="#b27">[28]</ref> and 1,250 synthetic underwater images from <ref type="bibr" target="#b54">[55]</ref>; the third training set Train-E contains the paired training images in the EUVP <ref type="bibr" target="#b2">[3]</ref> dataset. Testing datasets are categorized into two types, (1) full-reference testing dataset: Test-L400 and Test-U90 (remaining 90 pairs in UIEB); (2) non-reference testing dataset: Test-U60 and SQUID. Here, Test-U60 includes 60 non-reference images in UIEB; 16 pictures from SQUID <ref type="bibr" target="#b25">[26]</ref> forms the second non-reference testing dataset. Compared Methods. We compare U-shape Transformer with 10 UIE methods to verify our performance superiority. It includes two physical-based models (UIBLA <ref type="bibr" target="#b15">[16]</ref>, UDCP <ref type="bibr" target="#b12">[13]</ref>), three visual prior-based methods (Fusion <ref type="bibr" target="#b5">[6]</ref>, retinex based <ref type="bibr" target="#b8">[9]</ref>, RGHS <ref type="bibr" target="#b9">[10]</ref>), and five data-driven methods (WaterNet <ref type="bibr" target="#b27">[28]</ref>, FUnIE <ref type="bibr" target="#b2">[3]</ref>, UGAN <ref type="bibr" target="#b29">[30]</ref>, UIE-DAL <ref type="bibr" target="#b30">[31]</ref>, Ucolor <ref type="bibr" target="#b22">[23]</ref>). Evaluation Metrics. For the testing dataset with reference images, we conducted full-reference evaluations using PSNR <ref type="bibr" target="#b4">[5]</ref> and SSIM <ref type="bibr" target="#b55">[56]</ref> metrics. Those two metrics reflect the proximity to the reference, where a higher PSNR value represents closer image content, and a higher SSIM value reflects a more similar structure and texture. For images in the non-reference testing dataset, non-reference evaluation metrics UCIQE <ref type="bibr" target="#b48">[49]</ref> and UIQM <ref type="bibr" target="#b47">[48]</ref> are employed, in which higher UCIQE or UIQM score suggests better human visual perception. For UCIQE and UIQM cannot accurately measure the performance  in some cases <ref type="bibr" target="#b27">[28]</ref> [57], we also conducted a survey following <ref type="bibr" target="#b22">[23]</ref>, which results are stated as "perception score (PS)". PS ranges from 1-5, with higher scores indicating higher image quality. Moreover, NIQE <ref type="bibr" target="#b57">[58]</ref>, which lower value represents a higher visual quality, is also adopted as the metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Color Space Selection</head><p>In order to select the appropriate color space to form the multi-color space loss function, we use the mixed loss function composed of the single color space loss function and other loss functions to train the U-shape transformer. We use Train-L to train the network, and then test and calculate PSNR on Test-L400 and Test-U90 data sets, respectively. The results are shown in Tab. I, As in Tab. I, We note that the LAB, LCH, and RGB color spaces achieve the top-3 PSNR scores on both test datasets. In RGB color space, image is easy to store and display because of its strong color physical meaning, but these three components (R, G, and B) are highly correlated and easily affected by brightness, shadows, noise, and other factors. Compared with other color spaces, LAB color space is more consistent with the characteristics of human visual, can express all colors that human eyes can perceive, and the color distribution is more uniform. LCH color space can intuitively express brightness, saturation, and hue. Combined with the experimental results and the above analysis, we choose LAB, LCH, and RGB color space to form our multi-color space loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Dataset Evaluation</head><p>The effectiveness of LSUI is evaluated by retraining the compared methods (U-net <ref type="bibr" target="#b58">[59]</ref>, UGAN <ref type="bibr" target="#b29">[30]</ref> and U-shape Transformer) on Train-L, Train-U and Train-E. The trained network was tested on Test-L400 and Test-U90. As shown in Tab.II, the model trained on our dataset is the best of PSNR and SSIM. It could be explained that LSUI contains richer underwater scenes and better visual quality reference images than existing underwater image datasets, which could improve the enhancement and generalization ability of the tested network. <ref type="figure" target="#fig_7">Fig. 7</ref> is the sampled enhancement results of U-shape transformer trained on different underwater datasets, which is a supplement of the Data Evaluation part of the paper. Enhancement results training on Train-L (a portion of our LSUI dataset) demonstrates the highest PSNR value and preferable visual quality, while results training on other datasets show a certain degree of color cast. For the high-quality reference images and rich underwater scenes (lighting conditions, water types and target categories), our constructed LSUI dataset could improve the imaging quality and generalization performance of the UIE network.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Network Architecture Evaluation</head><p>Full-Reference Evaluation. The Test-L400 and Test-U90 datasets were used for evaluation. The statistical results and visual comparisons are summarized in Tab. III and <ref type="figure">Fig. 8</ref>. We also provide the running time (image size is 256*256) of all UIE methods in Tab. III, as well as the FLOPs and parameter amount of each data-driven UIE method. And we retrianed the 5 open-sourced deep learning-based UIE methods on our dataset. As in Tab.III, our U-shape Transformer demonstrates the best performance on both PSNR and SSIM metrics with relatively few parameters, FLOPs, and running time. The potential limitations of the performance of the 5 data-driven methods are analyzed as follows. The strength of FUnIE <ref type="bibr" target="#b2">[3]</ref> lies in achieving fast, lightweight,and fewer parameter models, while naturally limits its scalability on complex and distorted testing samples. UGAN <ref type="bibr" target="#b29">[30]</ref> and UIE-DAL <ref type="bibr" target="#b30">[31]</ref> did not consider the inconsistent characteristics of the underwater images. Ucolor's media transmission map prior can not effectively represent the attenuation of each area, and simply introducing the concept of multi-color space into the network's encoder part cannot effectively take advantage of it, which causes unsatisfactory results in terms of contrast, brightness, and detailed textures.</p><p>The visual comparisons shown in <ref type="figure">Fig. 8</ref> reveal that enhancement results of our method are the closest to the reference image, which has fewer color artifacts and high-fidelity object Ours Ucolor <ref type="bibr" target="#b22">[23]</ref> UGAN <ref type="bibr" target="#b29">[30]</ref> FUnIE <ref type="bibr" target="#b2">[3]</ref> Retinex <ref type="bibr" target="#b8">[9]</ref> UIBLA <ref type="bibr">[</ref>  <ref type="bibr" target="#b15">[16]</ref>, Retinex based <ref type="bibr" target="#b8">[9]</ref>, FUnIE <ref type="bibr" target="#b2">[3]</ref>, UGAN <ref type="bibr" target="#b29">[30]</ref>, Ucolor <ref type="bibr" target="#b22">[23]</ref>, our U-shape Transformer and the reference image (recognized as ground truth (GT)). The highest PSNR value of each raw is marked in yellow.</p><p>areas. Five selected methods tend to produce color artifacts that deviated from the original color of the object. Among the methods, UIBLA <ref type="bibr" target="#b15">[16]</ref> exhibits severe color casts. Retinex based <ref type="bibr" target="#b8">[9]</ref> could improve the image contrast to a certain extent, but cannot remove the color casts and color artifacts effectively. The enhancement result of FUnLE <ref type="bibr" target="#b2">[3]</ref> is yellowish and reddish overall. Although UGAN <ref type="bibr" target="#b29">[30]</ref> and Ucolor <ref type="bibr" target="#b27">[28]</ref> could provide relatively good color appearance, they are often affected by local over-enhancement, and there are still some color casts in the result.</p><p>Non-reference Evaluation. The Test-U60 and SQUID datasets were utilized for the non-reference evaluation, in which statistical results and visual comparisons are shown in Tab. IV and <ref type="figure">Fig. 9</ref>. As in Tab. IV, our method achieved the highest scores on PS and NIQE metrics, which confirmed the initial idea to contemplate the human eye's color perception and better generalization ability to varied real-world underwater scenes. Note that UCIQE and UIQM of all deep learning-based UIE methods are weaker than physical model-based or visual priorbased, also reported in <ref type="bibr" target="#b22">[23]</ref>. Those two metrics are of valuable reference, but cannot as absolute justifications <ref type="bibr" target="#b27">[28]</ref>[57], for they are non-sensitive to color artifacts &amp; casts and biased to some features.</p><p>As in <ref type="figure">Fig. 9</ref>, enhancement results of our method have the highest PS value, which index reflects the visual quality. Generally, compared methods are unsatisfactory, which includes undesirable color artifacts, over-saturation and unnat-ural color casts. Among the methods, results of the UIBLA <ref type="bibr" target="#b15">[16]</ref> and FUnIE <ref type="bibr" target="#b2">[3]</ref> have a certain degree of color cast. Retinex based <ref type="bibr" target="#b8">[9]</ref> method introduces artifacts and unnatural colors. UGAN <ref type="bibr" target="#b29">[30]</ref> and UIE-DAL <ref type="bibr" target="#b30">[31]</ref> have the issue of local over-enhancement and color artifacts, which main reason is they ignore the inconsistent attenuation characteristics of the underwater images in the different space areas and the color channels. Although Ucolor <ref type="bibr" target="#b22">[23]</ref> introduces the transmission medium prior to reinforcing the network's attention on the spatial area with severe attenuation, it still ignores the inconsistent attenuation characteristics of the underwater image in different color channels, which results in the problem of overall color cast. In our method, the reported CMSFFT and SGFMT modules could reinforce the network's attention to the color channels and spatial regions with serious attenuation, therefore obtaining high visual quality enhancement results without artifacts and color casts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Color Restoration Performance Evaluation</head><p>To demonstrate the robustness and accuracy of our UIE method for color correction, we compare the color correction ability of 10 UIE methods on the Color-Checker7 dataset. The Color-Checker7 dataset contains 7 underwater images taken from a shallow swimming pool with different cameras. Color checker is also photographed in each image. It provides a good path to demonstrate the robustness of our method to different imaging devices and the accuracy of color restoration.   <ref type="figure">Fig. 9</ref>. Visual comparison of the non-reference evaluation sampled from the Test-U60(UIEB <ref type="bibr" target="#b27">[28]</ref>) dataset. From left to right are raw underwater images, results of UIBLA <ref type="bibr" target="#b15">[16]</ref>, Retinex based <ref type="bibr" target="#b8">[9]</ref>, FUnIE <ref type="bibr" target="#b2">[3]</ref>, UGAN <ref type="bibr" target="#b29">[30]</ref>, UIE-DAL <ref type="bibr" target="#b30">[31]</ref>, Ucolor <ref type="bibr" target="#b22">[23]</ref> and our U-shape Transformer. The score in the upper right corner of each image is the perception score(PS), and the highest PS value of each raw is marked in yellow.</p><p>We follow Ancuti et al. <ref type="bibr" target="#b59">[60]</ref> to employ CIEDE2000 <ref type="bibr" target="#b60">[61]</ref> to measure the relative differences between the corresponding color patches of ground-truth Macbeth Color Checker and the enhancement results of these comparison methods. The experimental results are shown in Tab. V and <ref type="figure" target="#fig_9">Fig .10</ref>.</p><p>As in Tab. V, for the cameras of Pentax W60, Pentax W80, Cannon D10, Fuji Z33, Panasonic TS1 and Olympus T6000, our U-shape Transformer obtains the lowest color dissimilarity. Moreover, our U-shape Transformer achieves the best average score. Such results demonstrate the superiority of our method for underwater color correction. It is worth mentioning that some comparable methods acquired lower score than that of the raw image, which reflected that those methods are incapable of recovering the real color and even break the inherent color.</p><p>As shown in <ref type="figure" target="#fig_10">Fig. 11</ref>, the professional underwater camera (Fuji Z33) also inevitably introduces various color casts. Among all the UIE methods involved in the comparison, our U-shape Transformer achieves the highest CIEDE 2000 score, which means our UIE method has the best color correction ability. The results of UDCP and UIBLA are bluish, and Retinex has the problem of color distortion. UGAN and UIE-DAL suffer from low saturation and excessive reddish compensation. Although FUnIE and Ucolor could remove the color cast to a certain extent, there are still problems of low contrast and saturation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Ablation Study</head><p>To prove the effectiveness of each component, we conduct a series of ablation studies on the Test-L400 and Test-U90. Four factors are considered including the CMSFFT, the SGFMT, the multi-scale gradient flow mechanism (MSG), and the multicolor space loss function (MCSL). Experiments are all trained by Train-L. Statistical results are shown in Tab. VI, in which baseline model (BL) refers to <ref type="bibr" target="#b32">[33]</ref>, full models is the complete U-shape Transformer. In Tab. VI, our full model achieves the best quantitative performance on the two testing dataset, which reflects the effectiveness of the combination of CMSFFT, SGFMT, MSG, and MCSL modules. As in <ref type="figure" target="#fig_10">Fig .11</ref>, the enhancement result of the full model has the highest PSNR and best visual quality. The results of BL+MSG have less noise and artifacts than the BL module because the MSG mechanism helps to reconstruct local details. Thanks to the multi-color space loss function, the overall Input Reference UIBLA <ref type="bibr" target="#b15">[16]</ref> Retinex <ref type="bibr" target="#b8">[9]</ref> FUnIE <ref type="bibr" target="#b2">[3]</ref> UDCP <ref type="bibr" target="#b12">[13]</ref> UGAN <ref type="bibr" target="#b29">[30]</ref> UIE-DAL <ref type="bibr" target="#b30">[31]</ref> Ucolor <ref type="bibr" target="#b22">[23]</ref> Ours    color of BL+MCSL's result is close to the reference image. The unevenly distributed visualization and artifacts in local areas of BL+MCSL are due to the lack of efficient attention guidance. Although the enhanced results of BL+CMSFFT and BL+SGFMT are evenly distributed, the overall color is not accurate. The investigated four modules have their particular functionality in the enhancement process, which integration could improve the overall performance of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this work, we released a large scale underwater image (LSUI) dataset, which contains 4279 real-world underwater images with more abundant underwater scenes (water types, lighting conditions and target categories) than existing underwater datasets <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b21">[22]</ref>, and the corresponding clear images are generated as comparison references. We also provide the semantic segmentation map and medium transmission map for each raw underwater image. Besides, we reported an U-shape Transformer network for state-of-the-art UIE performance. The network's CMSFFT and SGFMT modules could solve the inconsistent attenuation issue of underwater images in different color channels and space regions, which has not been considered among existing methods. Extensive experiments validate the superior ability of the network to remove color artifacts and casts. Combined with the multicolor space loss function, the contrast and saturation of output images are further improved. Nevertheless, it is impossible to collect images of all the complicated scenes such as deepocean low-light scenarios. Therefore, we will introduce other general enhancement techniques such as low-light boosting <ref type="bibr" target="#b61">[62]</ref> for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Statistics of our LSUI dataset and the existing underwater dataset UIEB<ref type="bibr" target="#b27">[28]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3. Example images in the LSUI dataset. Our LSUI dataset contains 4279 real-world underwater images with more abundant underwater scenes (water types, lighting conditions and target categories) than existing underwater datasets [32], [28], [26], [22], and the corresponding clear images are generated as comparison references. We also provide the semantic segmentation map and medium transmission map for each raw underwater image. The top of each image group is the clear reference image, followed by the raw underwater image, semantic segmentation map, and medium transmission map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Data flow diagram of the SGFMT module. Based on the prior that underwater images are not uniformly degraded in different spatial regions, we designed a novel spatial-wise global feature modeling transformer (SGFMT) based on the spatial self-attention mechanism to replace the original bottleneck layer of the generator. It can accurately model the global feature of underwater images and reinforce the network's attention to the space areas with more serious attenuation, thus achieving uniform UIE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Detailed structure of the CMSFFT module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Enhancement results of U-shape transformer trained on different underwater datasets. (a): Input images; (b): Enhanced results using the model trained on the Train-U; (c): Enhanced results using the model trained on the Train-E; (d): Enhanced results using the model trained by our proposed dataset Train-L; (e): Reference images(recognized as ground truth (GT)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>PS</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Visual comparison of the color restoration performance evaluation. The input image is sampled from color-check7 dataset and it's taken by Fuji Z33. The values of CIEDE2000 metric for the regions of Color Checker are reported on the top-left corner of the images (the smaller, the better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Visual comparison of the ablation study sampled from the Test-L400 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and L. Bian are with the Advanced Research Institute of Multidisciplinary Science &amp; School of Information and Electronics, Beijing Institute of Technology, Beijing, China. Correspondence to L. Bian: bian@bit.edu.cn.</figDesc><table><row><cell>PSNR 14.62</cell><cell>PSNR 20.15</cell><cell>PSNR 17.58</cell><cell>PSNR 13.27</cell></row><row><cell>Input</cell><cell>UIBLA [16]</cell><cell>FUnIE [3]</cell><cell>Retinex[9]</cell></row><row><cell>PSNR 21.54</cell><cell>PSNR 22.06</cell><cell>PSNR 26.96</cell><cell></cell></row><row><cell>UGAN[30]</cell><cell>Ucolor [23]</cell><cell>Ours</cell><cell>Reference</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Decoding: After feature remapping, the SGFMT output is directly sent+</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CMSFFT</cell><cell></cell></row><row><cell>Input</cell><cell>Linear Projection</cell><cell>+ PE</cell><cell></cell><cell>CMHA</cell><cell>????</cell><cell></cell><cell>CMHA</cell><cell>Feature Mapping</cell></row><row><cell></cell><cell>Linear Projection</cell><cell>PE</cell><cell></cell><cell></cell><cell>Transformer Layer</cell><cell>Transformer Layer</cell><cell></cell><cell>Feature Mapping</cell></row><row><cell cols="2">Element-wise Addition</cell><cell></cell><cell></cell><cell cols="3">SGFMT</cell><cell></cell></row><row><cell cols="2">PE Position Embedding Multi-Layer Perceptron MLP 2 Times Down Sampling 1*1 Convolution</cell><cell>Layer Norm</cell><cell>Attention</cell><cell>Multi-Head</cell><cell>+</cell><cell>Layer Norm</cell><cell>MLP</cell><cell>+</cell></row><row><cell>CMHA</cell><cell>Channel-wise Multi-Head Attention</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I STATISTICAL</head><label>I</label><figDesc>RESULTS OF COLOR SPACE SELECTION EXPERIMENTS. WE TEST U-SHAPE TRANSFORMERS TRAINED WITH DIFFERENT COLOR SPACE LOSSFUNCTIONS ON TEST-L400 AND TEST-U90 DATASETS, RESPECTIVELY, AND THE COLOR SPACES THAT OBTAIN THE TOP THREE PSNR SCORES ARE MARKED WITH RED, GREEN, AND BLUE, RESPECTIVELY.</figDesc><table><row><cell>Color Space</cell><cell>RGB</cell><cell>HSV</cell><cell>HSI</cell><cell>XYZ</cell><cell>LAB</cell><cell>LUV</cell><cell>LCH</cell><cell>YUV</cell></row><row><cell>Tset-L400</cell><cell cols="4">23.79 23.32 23.37 22.63</cell><cell>23.86</cell><cell>22.81</cell><cell>23.62</cell><cell>23.43</cell></row><row><cell>Test-U90</cell><cell cols="4">22.72 22.01 22.17 21.69</cell><cell>22.53</cell><cell>21.77</cell><cell>22.49</cell><cell>22.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II DATASET</head><label>II</label><figDesc>EVALUATION RESULTS. THE HIGHEST PSNR AND SSIM SCORES ARE MARKED IN RED.</figDesc><table><row><cell>Methods</cell><cell>Training Data</cell><cell cols="4">Test-U90 PSNR SSIM PSNR SSIM Test-L400</cell></row><row><cell></cell><cell>Train-U</cell><cell>17.07</cell><cell>0.76</cell><cell>19.19</cell><cell>0.79</cell></row><row><cell>U-net[59]</cell><cell>Train-E</cell><cell>17.46</cell><cell>0.76</cell><cell>19.45</cell><cell>0.78</cell></row><row><cell></cell><cell>Ours</cell><cell>20.14</cell><cell>0.81</cell><cell>20.89</cell><cell>0.82</cell></row><row><cell></cell><cell>Train-U</cell><cell>20.71</cell><cell>0.82</cell><cell>19.89</cell><cell>0.79</cell></row><row><cell>UGAN[30]</cell><cell>Train-E</cell><cell>20.72</cell><cell>0.82</cell><cell>19.82</cell><cell>0.78</cell></row><row><cell></cell><cell>Ours</cell><cell>21.56</cell><cell>0.83</cell><cell>21.74</cell><cell>0.84</cell></row><row><cell></cell><cell>Train-U</cell><cell>21.25</cell><cell>0.84</cell><cell>22.87</cell><cell>0.85</cell></row><row><cell>Ours</cell><cell>Train-E</cell><cell>21.75</cell><cell>0.86</cell><cell>23.01</cell><cell>0.87</cell></row><row><cell></cell><cell>Ours</cell><cell>22.91</cell><cell>0.91</cell><cell>24.16</cell><cell>0.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III QUANTITATIVE</head><label>III</label><figDesc>COMPARISON AMONG DIFFERENT UIE METHODS ON THE FULL-REFERENCE TESTING SET. THE HIGHEST SCORES OF PSNR AND SSIM ARE MARKED IN RED, AND ALL UIE METHODS ARE TESTED ON A PC WITH AN INTEL(R) I5-10500 CPU, 16.0GB RAM, A NVIDIA GEFORCE RTX 1660 SUPER. AMONG DIFFERENT UIE METHODS ON THE NON-REFERENCE TESTING SET. THE HIGHEST SCORES ARE MARKED IN RED.</figDesc><table><row><cell>Methods</cell><cell cols="6">Test-L400 PSNR? SSIM? PSNR? Test-U90 SSIM?</cell><cell>FLOPs?</cell><cell>#param.?</cell><cell>time?</cell></row><row><cell>UIBLA[16]</cell><cell></cell><cell>13.54</cell><cell>0.71</cell><cell>15.78</cell><cell>0.73</cell><cell></cell><cell>?</cell><cell>?</cell><cell>42.13s</cell></row><row><cell>UDCP[13]</cell><cell></cell><cell>11.89</cell><cell>0.59</cell><cell>13.81</cell><cell>0.69</cell><cell></cell><cell>?</cell><cell>?</cell><cell>30.82s</cell></row><row><cell>Fusion[6]</cell><cell></cell><cell>17.48</cell><cell>0.79</cell><cell>19.04</cell><cell>0.82</cell><cell></cell><cell>?</cell><cell>?</cell><cell>6.58s</cell></row><row><cell>Retinex based[9]</cell><cell></cell><cell>13.89</cell><cell>0.74</cell><cell>14.01</cell><cell>0.72</cell><cell></cell><cell>?</cell><cell>?</cell><cell>1.06s</cell></row><row><cell>RGHS[10]</cell><cell></cell><cell>14.21</cell><cell>0.78</cell><cell>14.57</cell><cell>0.79</cell><cell></cell><cell>?</cell><cell>?</cell><cell>8.92s</cell></row><row><cell>WaterNet[28]</cell><cell></cell><cell>17.73</cell><cell>0.82</cell><cell>19.81</cell><cell>0.86</cell><cell></cell><cell>193.7G</cell><cell>24.81M</cell><cell>0.61s</cell></row><row><cell>FUnIE[3]</cell><cell></cell><cell>19.37</cell><cell>0.84</cell><cell>19.45</cell><cell>0.85</cell><cell></cell><cell>10.23G</cell><cell>7.019M</cell><cell>0.09s</cell></row><row><cell>UGAN[30]</cell><cell></cell><cell>19.79</cell><cell>0.78</cell><cell>20.68</cell><cell>0.84</cell><cell></cell><cell>38.97G</cell><cell>57.17M</cell><cell>0.05s</cell></row><row><cell>UIE-DAL[31]</cell><cell></cell><cell>17.45</cell><cell>0.79</cell><cell>16.37</cell><cell>0.78</cell><cell></cell><cell>29.32G</cell><cell>18.82M</cell><cell>0.07s</cell></row><row><cell>Ucolor[23]</cell><cell></cell><cell>22.91</cell><cell>0.89</cell><cell>20.78</cell><cell>0.87</cell><cell cols="2">443.85G</cell><cell>157.4M</cell><cell>2.75s</cell></row><row><cell>Ours</cell><cell></cell><cell>24.16</cell><cell>0.93</cell><cell>22.91</cell><cell>0.91</cell><cell></cell><cell>66.2G</cell><cell>65.6M</cell><cell>0.07s</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">TABLE IV</cell><cell></cell><cell></cell><cell></cell></row><row><cell>QUANTITATIVE COMPARISON Methods</cell><cell>PS?</cell><cell cols="3">Test-U60 UIQM? UCIQE?</cell><cell>NIQE?</cell><cell>PS?</cell><cell cols="2">SQUID UIQM? UCIQE?</cell><cell>NIQE?</cell></row><row><cell>input</cell><cell>1.46</cell><cell>0.82</cell><cell>0.45</cell><cell></cell><cell>7.16</cell><cell>1.23</cell><cell>0.81</cell><cell>0.43</cell><cell>4.93</cell></row><row><cell>UIBLA[16]</cell><cell>2.18</cell><cell>1.21</cell><cell>0.60</cell><cell></cell><cell>6.13</cell><cell>2.45</cell><cell>0.96</cell><cell>0.52</cell><cell>4.43</cell></row><row><cell>UDCP[13]</cell><cell>2.01</cell><cell>1.03</cell><cell>0.57</cell><cell></cell><cell>5.94</cell><cell>2.57</cell><cell>1.13</cell><cell>0.51</cell><cell>4.47</cell></row><row><cell>Fusion[6]</cell><cell>2.12</cell><cell>1.23</cell><cell>0.61</cell><cell></cell><cell>4.96</cell><cell>2.89</cell><cell>1.29</cell><cell>0.61</cell><cell>5.01</cell></row><row><cell>Retinex based[9]</cell><cell>2.04</cell><cell>0.94</cell><cell>0.69</cell><cell></cell><cell>4.95</cell><cell>2.33</cell><cell>1.01</cell><cell>0.66</cell><cell>4.86</cell></row><row><cell>RGHS[10]</cell><cell>2.45</cell><cell>0.66</cell><cell>0.71</cell><cell></cell><cell>4.82</cell><cell>2.67</cell><cell>0.82</cell><cell>0.73</cell><cell>4.54</cell></row><row><cell>WaterNet[28]</cell><cell>3.23</cell><cell>0.92</cell><cell>0.51</cell><cell></cell><cell>6.03</cell><cell>2.72</cell><cell>0.98</cell><cell>0.51</cell><cell>4.75</cell></row><row><cell>FUnIE[3]</cell><cell>3.12</cell><cell>1.03</cell><cell>0.54</cell><cell></cell><cell>6.12</cell><cell>2.65</cell><cell>0.98</cell><cell>0.51</cell><cell>4.67</cell></row><row><cell>UGAN[30]</cell><cell>3.64</cell><cell>0.86</cell><cell>0.57</cell><cell></cell><cell>6.74</cell><cell>2.79</cell><cell>0.90</cell><cell>0.58</cell><cell>4.56</cell></row><row><cell>UIE-DAL[31]</cell><cell>2.03</cell><cell>0.72</cell><cell>0.54</cell><cell></cell><cell>4.99</cell><cell>2.21</cell><cell>0.79</cell><cell>0.57</cell><cell>4.88</cell></row><row><cell>Ucolor[23]</cell><cell>3.71</cell><cell>0.84</cell><cell>0.53</cell><cell></cell><cell>6.21</cell><cell>2.82</cell><cell>0.82</cell><cell>0.51</cell><cell>4.32</cell></row><row><cell>Ours</cell><cell>3.91</cell><cell>0.85</cell><cell>0.73</cell><cell></cell><cell>4.74</cell><cell>3.23</cell><cell>0.89</cell><cell>0.67</cell><cell>4.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V THE</head><label>V</label><figDesc>COLOR DISSIMILARITY COMPARISONS OF DIFFERENT METHODS ON COLOR-CHECK7 IN TERMS OF THE CIEDE2000. THE BEST SCORES ARE MARKED IN RED.</figDesc><table><row><cell>Methods</cell><cell>Pen W60</cell><cell cols="3">Pen W80 Can D10 Fuj Z33</cell><cell>Oly T6000</cell><cell>Oly T8000</cell><cell>Pan TS1</cell><cell>Avg</cell></row><row><cell>input</cell><cell>14.21</cell><cell>16.92</cell><cell>17.14</cell><cell>16.03</cell><cell>15.02</cell><cell>22.43</cell><cell>18.65</cell><cell>17.2</cell></row><row><cell>UIBLA[16]</cell><cell>13.45</cell><cell>16.31</cell><cell>14.48</cell><cell>14.29</cell><cell>12.46</cell><cell>14.91</cell><cell>20.13</cell><cell>15,15</cell></row><row><cell>UDCP[13]</cell><cell>15.32</cell><cell>24.12</cell><cell>16.53</cell><cell>13.21</cell><cell>12.65</cell><cell>16.78</cell><cell>12.85</cell><cell>15.92</cell></row><row><cell>Fusion[6]</cell><cell>12.65</cell><cell>13.54</cell><cell>14.43</cell><cell>12.31</cell><cell>11.78</cell><cell>10.97</cell><cell>11.15</cell><cell>12.41</cell></row><row><cell>Retinex based[9]</cell><cell>13.08</cell><cell>19.25</cell><cell>17.13</cell><cell>18.85</cell><cell>17.18</cell><cell>19.45</cell><cell>20.62</cell><cell>17.94</cell></row><row><cell>RGHS[10]</cell><cell>11.07</cell><cell>12.73</cell><cell>15.92</cell><cell>13.47</cell><cell>14.26</cell><cell>18.73</cell><cell>12.06</cell><cell>14.03</cell></row><row><cell>WaterNet[28]</cell><cell>12.54</cell><cell>19.82</cell><cell>15.71</cell><cell>12.73</cell><cell>17.75</cell><cell>21.87</cell><cell>18.91</cell><cell>17.05</cell></row><row><cell>FUnIE[3]</cell><cell>12.81</cell><cell>11.81</cell><cell>12.39</cell><cell>12.76</cell><cell>12.46</cell><cell>16.74</cell><cell>19.28</cell><cell>14.04</cell></row><row><cell>UGAN[30]</cell><cell>20.49</cell><cell>21.75</cell><cell>22.63</cell><cell>26.49</cell><cell>21.63</cell><cell>22.05</cell><cell>20.73</cell><cell>22.25</cell></row><row><cell>UIE-DAL[31]</cell><cell>12.94</cell><cell>16.73</cell><cell>14.64</cell><cell>12.93</cell><cell>16.78</cell><cell>17.21</cell><cell>18.34</cell><cell>15.65</cell></row><row><cell>Ucolor[23]</cell><cell>9.12</cell><cell>11.14</cell><cell>12.43</cell><cell>10.02</cell><cell>8.31</cell><cell>14.18</cell><cell>13.41</cell><cell>11.23</cell></row><row><cell>Ours</cell><cell>7.87</cell><cell>9.70</cell><cell>9.96</cell><cell>8.23</cell><cell>7.71</cell><cell>11.14</cell><cell>9.81</cell><cell>9.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI STATISTICAL</head><label>VI</label><figDesc>RESULTS OF ABLATION STUDY ON THE TEST-L400 AND THE TEST-U90. THE HIGHEST SCORES ARE MARKED IN RED.</figDesc><table><row><cell>Models</cell><cell cols="4">Test-L400 PSNR SSIM PSNR Test-U90 SSIM</cell></row><row><cell>BL</cell><cell>19.34</cell><cell>0.79</cell><cell>19.36</cell><cell>0.81</cell></row><row><cell>BL+CMSFFT</cell><cell>22.47</cell><cell>0.88</cell><cell>21.72</cell><cell>0.86</cell></row><row><cell>BL+SGFMT</cell><cell>21.78</cell><cell>0.86</cell><cell>21.36</cell><cell>0.87</cell></row><row><cell>BL+MSG</cell><cell>20.11</cell><cell>0.82</cell><cell>21.24</cell><cell>0.85</cell></row><row><cell>BL+MCSL</cell><cell>21.51</cell><cell>0.82</cell><cell>20.16</cell><cell>0.81</cell></row><row><cell>Full Model</cell><cell>24.16</cell><cell>0.93</cell><cell>22.91</cell><cell>0.91</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An in-depth survey of underwater image enhancement and restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rohde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="123" to="638" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on underwater image enhancement techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCA</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast underwater image enhancement for improved visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sattar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Underwater image processing: State of the art of restoration and image enhancement methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Corchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP. J. Adv. Signal Process</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Peak signal-to-noise ratio revisited: Is simple beautiful?&quot; in QoMEX</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhancing underwater images and videos by fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bekaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Underwater image enhancement by dehazing with minimum information loss and histogram distribution prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-M</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5664" to="5677" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Underwater image quality enhancement through composition of dual-intensity images and rayleighstretching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S A</forename><surname>Ghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A M</forename><surname>Isa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCE</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="219" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A retinex-based enhancing approach for single underwater image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shallowwater image enhancement using relative global histogram stretching based on adaptive parameter acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sequeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mavromatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMM</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhancing the low quality images using unsupervised colour correction method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Odetayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Salam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z H</forename><surname>Talib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Syst. Man. Cybern</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1703" to="1709" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1956" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transmission estimation in underwater single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Drews</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moraes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Botelho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Underwater depth estimation and image restoration based on single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Drews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Botelho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Montenegro Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single underwater image restoration using adaptive attenuation-curve prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits. Syst. I. Regul. Pap</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="992" to="1002" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Underwater image restoration based on image blurriness and light absorption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Cosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Underwater image enhancement by dehazing with minimum information loss and histogram distribution prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-M</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Underwater image enhancement by wavelength compensation and dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic redchannel underwater image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galdran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pic?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alvarez-Gila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JVCIR</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Underwater image restoration based on minimum information loss principle and optical properties of underwater imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Underwater image enhancement using a multiscale dense generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. OCEANIC. ENG</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="862" to="870" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Watergan: Unsupervised generative network to enable real-time color correction of monocular underwater images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Skinner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Eustice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Underwater image enhancement via medium transmission-guided multi-color space embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Emerging from water: Underwater image color correction based on weakly supervised color transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Low complexity underwater image enhancement based on dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Shiau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IBICA</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sea-thru: A method for removing water from underwater images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Akkaynak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Two-step approach for single underwater image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An underwater image enhancement benchmark dataset and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A rapid scene depth estimation model based on underwater light attenuation prior for underwater image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tjondronegoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PCM</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enhancing underwater imagery using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sattar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">All-in-one underwater image enhancement using domain-adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Uplavikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-world underwater enhancement: Challenges, benchmarks, and solutions under natural light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits. Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An improved image enhancement method based on lab color space retinex algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<editor>ICGIP, C. Li, H. Yu, Z. Pan, and Y. Pu</editor>
		<imprint>
			<date type="published" when="2019" />
			<publisher>SPIE</publisher>
			<biblScope unit="volume">11069</biblScope>
			<biblScope unit="page" from="756" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mixture contrast limited adaptive histogram equalization for underwater image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Hitam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Awalludin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">N</forename><surname>Jawahir Hj Wan Yussof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bachok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCAT</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Underwater image enhancement via extended multi-scale retinex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">245</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Initial results in underwater single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlevaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eustice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OCEANS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Underwater image enhancement based on conditional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">115723</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Signal Process</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Restormer: Efficient transformer for high-resolution image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>2021. 3, 4</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page" from="16" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Polikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ensemble machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Sguie-net: Semantic attention guided underwater image enhancement with multiscale perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02832,2022.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A wavelet-based dual-stream network for underwater image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08758</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Human-visual-system-inspired underwater image quality measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Panetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agaian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Ocean. Eng</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An underwater color image quality evaluation metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sowmya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="1833" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Perceiving and modeling density is all you need for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<idno>2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Uctransnet: Rethinking the skip connections in u-net from a channel-wise perspective with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaiane</surname></persName>
		</author>
		<idno>2021. 6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>abs/2010.11929</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Underwater scene prior inspired deep underwater image and video enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">107038</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hor?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
		<title level="m">Image Quality Metrics: PSNR vs. SSIM,&quot; in ICPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2366" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Underwater single image color restoration using haze-lines and a new quantitative dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T PATTERN ANAL</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Color balance and fusion for underwater image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bekaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The ciede2000 color-difference formula: Implementation notes, supplementary test data, and mathematical observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COLOR RES APPL</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3291" to="3300" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
