<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data-to-text Generation with Variational Sequential Planning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
							<email>r.puduppully@sms.ed.ac.ukyao.fu@ed.ac.ukmlap@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data-to-text Generation with Variational Sequential Planning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the task of data-to-text generation, which aims to create textual output from non-linguistic input. We focus on generating long-form text, i.e., documents with multiple paragraphs, and propose a neural model enhanced with a planning component responsible for organizing high-level information in a coherent and meaningful way. We infer latent plans sequentially with a structured variational model, while interleaving the steps of planning and generation. Text is generated by conditioning on previous variational decisions and previously generated text. Experiments on two data-to-text benchmarks (ROTOWIRE and MLB) show that our model outperforms strong baselines and is sample efficient in the face of limited training data (e.g., a few hundred instances). V(B.Keller) KANSAS CITY, Mo. -Brad Keller kept up his recent pitching surge . . . V(B.Keller) V(C.Mullins) V(Royals) V(Orioles) Keller gave up a home run to the first batter of the game -Cedric Mullins but quickly settled . . . V(B.Keller) Keller (7-5) gave up two runs and four hits with two walks and four strikeouts to improve . . . . . . . . .</p><p>Figure 2: Conceptual sequence of interleaved planning and generation steps. The paragraph plan and its corresponding paragraph have the same color.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data-to-text generation refers to the task of generating textual output from non-linguistic input such as database tables, spreadsheets, or simulations of physical systems <ref type="bibr">Dale, 1997, 2000;</ref><ref type="bibr" target="#b11">Gatt and Krahmer, 2018)</ref>. Recent progress in this area <ref type="bibr" target="#b37">(Mei et al., 2016;</ref><ref type="bibr" target="#b28">Lebret et al., 2016;</ref><ref type="bibr" target="#b69">Wiseman et al., 2017)</ref> has been greatly facilitated by the very successful encoder-decoder neural architecture <ref type="bibr" target="#b61">(Sutskever et al., 2014)</ref> and the development of large scale datasets. ROTOWIRE <ref type="bibr" target="#b69">(Wiseman et al., 2017)</ref> and MLB <ref type="bibr" target="#b45">(Puduppully et al., 2019b)</ref> constitute such examples. They both focus on the sports domain which has historically drawn attention in the generation community <ref type="bibr" target="#b1">(Barzilay and Lapata, 2005;</ref><ref type="bibr" target="#b62">Tanaka-Ishii et al., 1998;</ref><ref type="bibr" target="#b53">Robin, 1994)</ref> and consider the problem of generating long target texts from database records. <ref type="figure" target="#fig_0">Figure 1</ref> (reproduced from Puduppully and Lapata, 2021) provides a sample from the MLB dataset which pairs human written summaries <ref type="table">(Table C)</ref> with major league baseball game statistics. These are mostly scores (collectively referred to as box score) which summarize the performance of teams and players, e.g., batters, pitchers, or fielders (Table A) and a play-by-play description of the most important events in the game <ref type="table">(Table B</ref>). Game summaries in MLB are relatively long (540 tokens on average) with multiple paragraphs (15 on average). The complexity of the input and the length of the game summaries pose various challenges to neural models which, despite producing fluent output, are often imprecise, prone to hallucinations, and display poor content selection <ref type="bibr" target="#b69">(Wiseman et al., 2017)</ref>. Attempts to address these issues have seen the development of special-purpose modules which keep track of salient entities <ref type="bibr" target="#b18">(Iso et al., 2019;</ref><ref type="bibr" target="#b45">Puduppully et al., 2019b)</ref>, determine which records (see the rows in <ref type="table">Tables A and B)</ref> should be mentioned in a sentence and in which order <ref type="bibr" target="#b44">(Puduppully et al., 2019a;</ref>, and reconceptualize the input in terms of paragraph plans <ref type="bibr" target="#b46">(Puduppully and Lapata, 2021)</ref> to facilitate document-level planning (see <ref type="table">Table D</ref> in <ref type="figure" target="#fig_0">Figure 1</ref>). Specifically, <ref type="bibr" target="#b46">Puduppully and Lapata (2021)</ref> advocate the use of macro plans for improving the organization of document content and structure. A macro plan is a sequence of paragraph plans, and each paragraph plan corresponds to a document paragraph. A macro plan is shown in Table E <ref type="figure" target="#fig_0">(Figure 1)</ref>. Examples of paragraph plans are given in <ref type="table">Table D</ref> where &lt;V(entity)&gt; verbalizes records pertaining to entities and &lt;V(inning-T/B)&gt; verbalizes records for the Top/Bottom side of an inning. Verbalizations are sequences of record types followed by their values. Document paragraphs are shown in <ref type="table">Table C</ref> and have the same color as their corresponding plans in <ref type="table">Table E</ref>. During training, <ref type="bibr" target="#b46">Puduppully and Lapata (2021)</ref> learn to predict a macro plan from a pool of paragraph plans, and produce a game summary based on it. Continuing with our example in <ref type="figure" target="#fig_0">Figure 1,</ref>  Inn1: runs in innings, TR: team runs, TH: team hits, E: errors, H/V: home/visiting, AB: at-bats, BR: batter runs, BH: batter hits, RBI: runs-batted-in, W: wins, L: losses, IP: innings pitched, PH: hits given, PR: runs given, ER: earned runs, BB: walks, K: strike outs, <ref type="bibr">INN:</ref> inning with (T)op/(B)ottom, PL-ID: play id, SCR: score of Royals.</p><p>(C) KANSAS CITY, Mo. -Brad Keller kept up his recent pitching surge with another strong outing. &lt;P&gt; Keller gave up a home run to the first batter of the game -Cedric Mullins -but quickly settled in to pitch eight strong innings in the Kansas City Royals' 9-2 win over the Baltimore Orioles in a matchup of the teams with the worst records in the majors. &lt;P&gt; Keller (7-5) gave up two runs and four hits with two walks and four strikeouts to improve to 3-0 with a 2.16 ERA in his last four starts. &lt;P&gt; Ryan O'Hearn homered among his three hits and drove in four runs, Whit Merrifield scored three runs, and Hunter Dozier and Cam Gallagher also went deep to help the Royals win for the fifth time in six games on their current homestand. &lt;P&gt; With the score tied 1-1 in the fourth, Andrew Cashner (4-13) gave up a sacrifice fly to Merrifield after loading the bases on two walks and a single. Dozier led off the fifth inning with a 423-foot home run to left field to make it 3-1. &lt;P&gt; The Orioles pulled within a run in the sixth when Mullins led off with a double just beyond the reach of Dozier at third, advanced to third on a fly ball and scored on Trey Mancini's sacrifice fly to the wall in right. &lt;P&gt; . . .   <ref type="bibr" target="#b46">Puduppully and Lapata (2021)</ref> with the authors' permission. <ref type="table">Table A</ref> is typically referred to as box score. It summarizes the data of the game per team and player. <ref type="table">Table B</ref> reports statistics pertaining to innings or play-by-play scores. <ref type="table">Table C</ref> contains the game summary. Paragraphs in <ref type="table">Table C</ref> are separated with &lt;P&gt; delimiters. <ref type="table">Table D contains paragraph plans obtained from Tables A  and B</ref>. Paragraph plans in the first column correspond to a single entity or event. Paragraph plans in the second column describe combinations of entities or events. &lt;V(entity)&gt; verbalizes records pertaining to entities and &lt;V(inning-T/B)&gt; verbalizes records for the Top/Bottom side of an inning. Paragraph plans correspond to paragraphs in <ref type="table">Table C</ref>. <ref type="table">Table E</ref> contains the macro plan for the document in <ref type="table">Table C</ref>. A macro plan is a sequence of paragraph plans. Plan-document correspondences are highlighted using the same color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(B)</head><formula xml:id="formula_0">(D) V(Orioles), V(Royals), V(C.Mullins), V(J.Villar), V(W.Merrifield), V(R.O'Hearn), V(A.Cashner), V(B.Keller), V(H.Dozier), . . . , V(1-T), V(1-B), V(2-T), V(2-B), V(3-T), V(3-B), . . . V(Royals) V(Orioles), V(Orioles) V(C.Mullins), V(Orioles) V(J.Villar), V(Royals) V(W.Merrifield), V(Royals) V(R.O'Hearn), V(Orioles) V(A.Cashner), V(Royals) V(B.Keller), . . . , V(C.Mullins) V(Royals) V(Orioles), V(J.Villar) V(Royals) V(Orioles), . . .</formula><p>from paragraph plans (D), to give rise to game summary (C).</p><p>The intermediate macro plan renders generation more interpretable (differences in the output can be explained by differences in macro planning). It also makes modeling easier, the input is no longer a complicated table but a sequence of paragraph plans which in turn allows us to treat data-to-text generation as a sequence-to-sequence learning problem. Nevertheless, decoding to a long document remains challenging for at least two rea-sons. Firstly, the macro plan may be encoded as a sequence but a very long one (more than 3,000 tokens) which the decoder has to attend to at each time step in order to generate a summary tokenby-token. Secondly, the prediction of the macro plan is conditioned solely on the input (i.e., pool of paragraph plans (D) in <ref type="figure" target="#fig_0">Figure 1</ref>) and does not make use of information present in the summaries. We hypothesize that planning would be more accurate were it to consider information available in the table (and corresponding paragraph plans) and the generated summary, more so because the plans are coarse-grained and there is a one-to-many relationship between a paragraph plan and its realization. For example, we can see that the plan for &lt;V(B.Keller)&gt; results in two very different realizations in the summary in <ref type="figure" target="#fig_0">Figure 1</ref> (see first and third paragraph).</p><p>In this work, we present a model which interleaves macro planning with text generation (see <ref type="figure">Figure 2</ref> for a sketch of the approach). We begin by selecting a plan from a pool of paragraph plans (see <ref type="table">Table D</ref> in <ref type="figure" target="#fig_0">Figure 1</ref>), and generate the first paragraph by conditioning on it. We select the next plan by conditioning on the previous plan and the previously generated paragraph. We generate the next paragraph by conditioning on the currently selected plan, the previously predicted plan, and generated paragraph. We repeat this process until the final paragraph plan is predicted. We model the selection of paragraph plans as a sequential latent variable process which we argue is intuitive since content planing is inherently latent. Contrary to <ref type="bibr" target="#b46">Puduppully and Lapata (2021)</ref>, we do not a priori decide on a global macro plan. Rather our planning process is incremental and as a result less rigid. Planning is informed by generation and vice versa, which we argue should be mutually beneficial (they are conditioned on each other).</p><p>During training, the sequential latent model can better leverage the summary to render paragraph plan selection more accurate and take previous decisions into account. We hypothesize that the interdependence between planning and generation allows the model to cope with diversity. In general, there can be many ways in which the input table can be described in the output summary, i.e., different plans give rise to equally valid game summaries. The summary in <ref type="figure" target="#fig_0">Figure 1</ref>  <ref type="table">(Table C)</ref> focuses on the performance of Brad Keller who is a high scoring pitcher (first three paragraphs). An equally plausible summary might have discussed a high scoring batter first (e.g., Ryan O'Hearn). Also notice that the summary describes innings in chronological order. However, another ordering might have been equally plausible, for example, describing innings where the highest runs are scored first or innings which are important in flipping the outcome of the match. In the face of such diversity, there may never be enough data to learn an accurate global plan. It is easier to select a paragraph plan from the pool once some of the summary is known, and different plans can be predicted for the same input. In addition, the proposed model is end-to-end differentiable and gradients for summary prediction also inform plan prediction.</p><p>Our contributions can be summarized as follows: (1) we decompose data-to-text generation into sequential plan selection and paragraph generation. The two processes are interleaved and generation proceeds incrementally. We look at what has been already generated, make a plan on what to discuss next, realize the plan, and repeat; (2) in contrast to previous models <ref type="bibr" target="#b44">(Puduppully et al., 2019a;</ref><ref type="bibr" target="#b46">Puduppully and Lapata, 2021)</ref> where content plans are monolithic and determined in advance, our approach is more flexible, it simplifies modeling (we do not need to learn alignments between paragraph plans and summary paragraphs), and leads to sample efficiency in low resource scenarios; (3) our approach scales better for tasks involving generation of long multi-paragraph texts, as we do not need to specify the document plan in advance; (4) experimental results on English and German ROTOWIRE <ref type="bibr" target="#b69">(Wiseman et al., 2017;</ref><ref type="bibr" target="#b17">Hayashi et al., 2019)</ref>, and MLB <ref type="bibr" target="#b45">(Puduppully et al., 2019b)</ref> show that our model is well-suited to long-form generation and generates more factual, coherent, and less repetitive output compared to strong baselines.</p><p>We share our code and models in the hope of being useful for other tasks (e.g., story generation, summarization) 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A long tradition in natural language generation views content planning as a central component to identifying important content and structuring it appropriately <ref type="bibr" target="#b51">(Reiter and Dale, 2000)</ref>. Earlier work has primarily made use of hand-crafted content plans with some exceptions which pioneered learning-based approaches. For instance, <ref type="bibr" target="#b5">Duboue and McKeown (2001)</ref> learn ordering constraints on the content plan, while <ref type="bibr" target="#b20">Kan and McKeown (2002)</ref> learn content planners from semantically annotated corpora, and <ref type="bibr" target="#b26">Konstas and Lapata (2013)</ref> predict content plans using grammar rules whose probabilities are learnt from training data.</p><p>More recently, there have been attempts to equip encoder-decoder models <ref type="bibr" target="#b0">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b69">Wiseman et al., 2017)</ref> with content planning modules. <ref type="bibr" target="#b44">Puduppully et al. (2019a)</ref> introduce micro planning: they first learn a content plan corresponding to a sequence of records, and then generate a summary conditioned on it.  treat content selection as a task similar to extractive summarization. Specifically, they post-process Pudupully et al.'s (2019a) micro-plans with special tokens identifying the beginning and end of a sentence. Their model first extracts sentence plans and then verbalizes them one-by-one by conditioning on previously generated sentences. <ref type="bibr">Moryossef et al. (2019b,a)</ref> propose a two-stage approach which first predicts a document plan and then generates text based on it. The input to their model is a set of RDF Subject, Object, Predicate tuples. Their document plan is a sequence of sentence plans where each sentence plan contains a subset of tuples in a specific order. Text generation is implemented using a sequence-to-sequence model enhanced with attention and copy mechanisms <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>. They evaluate their model on the WebNLG dataset <ref type="bibr" target="#b10">(Gardent et al., 2017)</ref> where the outputs are relatively short (24 tokens on average).</p><p>Our approach is closest to Puduppully and Lapata (2021) who advocate macro planning as a way of organizing high-level document content. Their model operates over paragraph plans which are verbalizations of the tabular input and predicts a document plan as a sequence of paragraph plans. In a second stage, the summary is generated from the predicted plan making use of attention enriched with a copy mechanism. We follow their formulation of content planning as paragraph plan prediction. Our model thus operates over larger content units compared to related work <ref type="bibr" target="#b44">(Puduppully et al., 2019a;</ref> and performs the tasks of micro-and macro-planning in one go. In contrast to <ref type="bibr" target="#b46">Puduppully and Lapata (2021)</ref>, we predict paragraph plans and their corresponding paragraphs jointly in an incremental fashion. Our approach is reminiscent of psycholinguistic models of speech production <ref type="bibr" target="#b30">(Levelt, 1993;</ref><ref type="bibr" target="#b63">Taylor and Taylor, 1990;</ref><ref type="bibr" target="#b15">Guhe, 2020)</ref> which postulate that different levels of processing (or modules) are responsible for language generation; these modules are incremental, each producing output as soon as the information it needs is available and the output is processed immediately by the next module.</p><p>We assume plans form a sequence of paragraphs which we treat as a latent variable and learn with a structured variational model. Sequential latent variables <ref type="bibr" target="#b3">(Chung et al., 2015;</ref><ref type="bibr" target="#b8">Fraccaro et al., 2016;</ref><ref type="bibr" target="#b14">Goyal et al., 2017)</ref> have previously found application in modeling attention in sequence-to-sequence networks <ref type="bibr" target="#b59">(Shankar and Sarawagi, 2019)</ref>, document summarization <ref type="bibr" target="#b32">(Li et al., 2017)</ref>, controllable generation <ref type="bibr" target="#b33">(Li and Rush, 2020;</ref><ref type="bibr" target="#b9">Fu et al., 2020)</ref>, and knowledge-grounded dialogue <ref type="bibr" target="#b22">(Kim et al., 2020)</ref>. In the context of data-to-text generation, latent variable models have been primarily used to inject diversity in the output. <ref type="bibr" target="#b60">Shao et al. (2019)</ref> generate a sequence of groups (essentially a subset of the input) which specifies the content of the sentence to be generated. Their plans receive no feedback from text generation, they cover a small set of input items, and give rise to relatively short documents (approximately 100 tokens long). <ref type="bibr" target="#b73">Ye et al. (2020)</ref> use latent variables to disentangle the content from the structure (operationalized as templates) of the output text. Their approach generates diverse output output by sampling from the template-specific sample space. They apply their model to singlesentence generation tasks <ref type="bibr" target="#b28">(Lebret et al., 2016;</ref><ref type="bibr" target="#b49">Reed et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Following Puduppully and Lapata (2021), we assume that at training time our model has access to a pool of paragraph plans E (see <ref type="table">Table D</ref> in <ref type="figure" target="#fig_0">Figure 1</ref>) which represent a clustering of records. We explain how paragraph plans are created from tabular input in Section 4. Given E, we aim to generate a sequence of paragraphs y = [y 1 , ..., y T ] that describe the data following a sequence of chosen plans z = [z 1 , ..., z T ]. Let y t denote a paragraph, which can consist of multiple sentences, and T the count of paragraphs in a summary. With a slight abuse of notation, superscripts denote indices rather than exponentiation. So, y t i refers to the i-th word in the t-th paragraph.</p><formula xml:id="formula_1">A plan z = [z 1 , ..., z T ] is a y 1:t?1 z 1:t?1 LSTMtext LSTMplan h y t-1 h y t h z t h z t-1 LSTMplan LSTMtext</formula><p>Step 1 to t-1</p><p>Step t</p><p>Step t+1 and after list of discrete variables where z t = j means that we choose the j-th item from pool E of candidate plans to guide the generation of paragraph y t .</p><formula xml:id="formula_2">z t ? p ? (z t | z &lt;t , y &lt;t ) y t ? p ? (y t | z t , z &lt;t , y &lt;t ) z t ? q ? (z t | z &lt;t , y 1:t ) y t z t Random Variables</formula><p>Generation with Latent Plans The core technique of our model is learning the sequence of latent plans that guides long document generation. We consider a conditional generation setting where the input E is a set of paragraph plans and the output y 1:T are textual paragraphs verbalizing the selected sequence z = z 1:T . Our goal is to induce variables z that indicate which paragraphs are being talked about and in which order. Similar to previous work <ref type="bibr" target="#b33">(Li and Rush, 2020;</ref><ref type="bibr" target="#b9">Fu et al., 2020)</ref>, we model this process as a conditional generative model that produces both y and z and factorizes as:</p><formula xml:id="formula_3">p ? (y, z|E) = t p ? (z t |y &lt;t , z &lt;t , E)p ? (y t |y &lt;t , z 1:t , E) (1)</formula><p>where ? denotes the model parameters and &lt; t all indices smaller than t. We believe this formulation is intuitive, simulating incremental document generation: inspect y &lt;t (what has been already said), make a plan z t about what to say next, realize this plan by generating a new paragraph y t , and so on.</p><p>Inference Model We are interested in the posterior distribution p ? (z|y, E), i.e., the probability over plan sequences z for a known text y and input E. This distribution is intractable to compute in general as the summation of all possible plan sequences z is exponentially complex:</p><formula xml:id="formula_4">p ? (z|y, E) = p ? (y, z|E) z p ? (y, z|E)<label>(2)</label></formula><p>We use variational inference <ref type="bibr" target="#b23">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b52">Rezende et al., 2014)</ref> to approximate the posterior with a parametrized distribution q ? (z|y, E) from which we sample values of z that are likely to produce y (see Doersch 2016 for a tutorial on this topic). Specifically, we employ an autoregressive inference model factorized as:</p><formula xml:id="formula_5">q ? (z|y, E) = t q ? (z t |y 1:t , z &lt;t , E)<label>(3)</label></formula><p>Note that a major difference between q above and p in Equation <ref type="formula">(1)</ref> is that p generates y t under the guidance of z t (conceptually z t ? y t ) while q infers z t given observed y t (conceptually y t ? z t ).</p><p>Neural Parametrization At step t, we start with the encoding of previous paragraphs y &lt;t and plans z &lt;t (see <ref type="figure" target="#fig_1">Figure 3</ref> left). Following <ref type="bibr" target="#b72">Yang et al. (2016)</ref>, we use a Bi-directional LSTM (BiLSTM) with a self-attention layer to encode paragraph y t as a vector r t y at step t:</p><formula xml:id="formula_6">r t y = Attn(q text , BiLSTM(y t ))<label>(4)</label></formula><p>where q text is a trainable query vector, which is randomly initialized and learnt along with the rest of the parameters. Attn(?) returns the attention probability and output vector over BiLSTM representation y t with query vector q text . 2 Our model uses the output vector. Next, we encode r &lt;t y with LSTM text as:</p><formula xml:id="formula_7">h &lt;t y = LSTM text (r &lt;t y )<label>(5)</label></formula><p>We encode candidate plans in pool E = [e 1 , .., e N ] with a BiLSTM, similar to the paragraph encoding shown in Equation <ref type="formula" target="#formula_6">(4)</ref>, and select one of them at each step. Let r t z denote a plan embedding at step t. We encode r &lt;t z using LSTM plan as:</p><formula xml:id="formula_8">h &lt;t z = LSTM plan (r &lt;t z )<label>(6)</label></formula><p>The currently selected plan is parametrized as:</p><formula xml:id="formula_9">h t?1 = FF plan ([h t?1 z ; h t?1 y ]) (7) p ? (z t |y &lt;t , z &lt;t , E) = Attn(h t?1 , E)<label>(8)</label></formula><p>where h t?1 summarizes information in y &lt;t and z &lt;t , FF plan (?) denotes a feed-forward layer, and Attn(?) returns the attention probability (and output vector) of choosing a plan from E with current state h t?1 . Here, we use the attention distribution, which serves essentially as a copy mechanism. Then, a plan z t is sampled from p (we use greedy decoding in our experiments), and its representation r t z is used to update LSTM plan <ref type="figure" target="#fig_1">(Figure 3 right)</ref>:</p><formula xml:id="formula_10">h t z = LSTM plan (r t z , h t?1 z )<label>(9)</label></formula><p>We guide the generation of y t with current plan z t and decode each word y t i sequentially with an LSTM gen decoder which makes use of beam search. Let s i denote the i-th decoder state (initialized with the plan encoding). We update it as:</p><formula xml:id="formula_11">s i = LSTM gen (y t i?1 , s i?1 , h t?1 y )<label>(10)</label></formula><p>Note that we feed h t?1 y , representing the context of previous paragraphs, as additional input similar to <ref type="bibr" target="#b58">Serban et al. (2017)</ref>. Let r t z,1 , ..., r t z,l denote the encoding of tokens of the current plan where r t z,k is the output of the BiLSTM plan encoder and l the length of the chosen plan. We generate the next word as:</p><formula xml:id="formula_12">c i = Attn(s i , [r t z,1 , ..., r t z,l ]) (11) p ? (y t i |z t , y t 1:i?1 , y &lt;t , z &lt;t , E) = softmax(FF gen ([s i ; c i ])) (12)</formula><p>where c denotes the context vector. In Equation 11, we use the output vector from Attn(?). FF gen (?) represents a feed-forward layer. In addition, we equip the decoder with copy attention <ref type="bibr" target="#b56">(See et al., 2017)</ref> to enable copying tokens from z t . As part of this, we learn a probability for copy based on s i <ref type="bibr" target="#b12">(Gehrmann et al., 2018)</ref>. Once paragraph y t has been generated, we obtain its encoding r t y with</p><p>Equation <ref type="formula" target="#formula_6">(4)</ref>, and update LSTM text <ref type="figure" target="#fig_1">(Figure 3</ref> middle):</p><formula xml:id="formula_13">h t y = LSTM text (r t y , h t?1 y )<label>(13)</label></formula><p>We parametrize the variational model so that it shares the LSTMs for encoding y and E with the generative model:</p><formula xml:id="formula_14">h t = FF v ([h t?1 z ; h t y ]) (14) q ? (z t |y 1:t , z &lt;t , E) = Attn(h t , E)<label>(15)</label></formula><p>where FF v (?) represents a feed-forward layer. Note that Equation <ref type="formula" target="#formula_6">(14)</ref> differs from Equation <ref type="formula">(7)</ref> in that it uses the updated h t y instead of the previous h t?1 y because now y t is observed. The variational distribution is again parametrized by the attention probability. Essentially, p and q are strongly tied to each other with the shared LSTM encoders.</p><p>Although we primarily focus on the inference, and how the latent plan can improve the generation of long documents, we note that the model sketched above could be parametrized differently, e.g., by replacing the encoder and decoder with pretrained language models like BART <ref type="bibr" target="#b31">(Lewis et al., 2020)</ref>. However, we leave this to future work.</p><p>Training We optimize the standard evidence lower bound (ELBO) loss:</p><formula xml:id="formula_15">L 0 = log p ? (y|E) ? D(q ? (z|y, E) p ? (z|y, E)) = E q ? (z|y,E) [log p ? (y, z|E) ? log q ? (z|y, E)] =E q ? (z|y,E) t log p ? (y t |z t , y &lt;t , z &lt;t , E)+ log p ? (z t |y &lt;t , z &lt;t , E) q ? (z t |y 1:t , z &lt;t , E)<label>(16)</label></formula><p>where log p ? (y|E) is the log-evidence from the data, and D(q ? (z|y, E) p ? (z|y, E)) is the Kullback-Leibler divergence between q ? and the true posterior p ? . The objective eventually decomposes to a summation of the reconstruction probability p ? (y t |?) and the ratio between p ? (z t |?) and q ? (z t |?) at each step.</p><p>Advantageously, we can exploit oracle plans (see <ref type="table">Table E</ref> in <ref type="figure" target="#fig_0">Figure 1</ref> and the description in Section 4 for how these were created) to obtain weak labels z * which we use as distant supervision to the inference model: Such distant supervision is essential for stabilizing training (it would be extremely challenging to optimize the model in a fully unsupervised way) and for mitigating posterior collapse. We use <ref type="bibr">Gumbel-Softmax (Maddison et al., 2017;</ref><ref type="bibr" target="#b19">Jang et al., 2017)</ref> for differentiable sampling (reparameterization) from q. The model is trained with scheduled sampling , and follows the curriculum learning strategy using linear decay scheduling. During earlier stages of training predicted plans are less accurate, and we thus sample from oracle plans at a rate which decays linearly with training:</p><formula xml:id="formula_16">L 1 = E z * [log q ? (z * |y, E)]<label>(17)</label></formula><formula xml:id="formula_17">L = L 0 + ?L 1<label>(18)</label></formula><formula xml:id="formula_18">k = max(0, 1 ? c * k)<label>(19)</label></formula><p>where c is the slope of the decay at training step k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Data We performed experiments on the RO-TOWIRE <ref type="bibr" target="#b69">(Wiseman et al., 2017)</ref> and MLB (Puduppully et al., 2019b) datasets and the German RO-TOWIRE provided as part of the WNGT 2020 DGT shared task on "Document-Level Generation and Translation" <ref type="bibr" target="#b17">(Hayashi et al., 2019)</ref>. Statistics on these datasets are shown in All three datasets were preprocessed following the method of <ref type="bibr" target="#b46">Puduppully and Lapata (2021)</ref>. A paragraph plan for an entity is constructed by verbalizing its records in a fixed sequence of record type followed by its value. For example, pitcher B.Keller from <ref type="figure" target="#fig_0">Figure 1</ref> would be verbalized as &lt;PLAYER&gt; B.Keller &lt;H/V&gt; V &lt;W&gt; 7 &lt;L&gt; 5 &lt;IP&gt; 8 &lt;PH&gt; 4 . . . . We denote this using the shorthand &lt;V(B.Keller)&gt;. The paragraph plan for an event is the verbalization of the players in the event followed by the verbalization of play-byplays. Candidate paragraph plans E are obtained by enumerating entities and events and their combinations (see <ref type="table">Table D</ref> in <ref type="figure" target="#fig_0">Figure 1</ref>). Oracle macro plans are obtained by matching the mentions of entities and events in the gold summary with the input table. We make use of these oracle macro plans during training. The versions of MLB and ROTOWIRE released by <ref type="bibr" target="#b46">Puduppully and Lapata (2021)</ref> contain paragraph delimiters for gold summaries; we preprocessed the German ROTOWIRE in a similar fashion.</p><p>Table 1 also shows the average length of the macro plan in terms of the number of paragraph plans it contains. This is 10.6 for ROTOWIRE, 15.1 for MLB, and 9.5 for German RotoWire.</p><p>Training Configuration We train our model with the AdaGrad optimizer <ref type="bibr" target="#b6">(Duchi et al., 2011)</ref> and tune parameters on the development set. We use a learning rate of 0.15. We learn a joint subword vocabulary <ref type="bibr" target="#b57">(Sennrich et al., 2016)</ref> for paragraph plans and summaries with 6K merge operations for ROTOWIRE, 16K merge operations for MLB, and 2K merge operations for German RO-TOWIRE. The model is implemented on a fork of OpenNMT-py <ref type="bibr" target="#b24">(Klein et al., 2017)</ref>. For efficiency, we batch using summaries instead of individual paragraphs. Batch sizes for MLB, ROTOWIRE, and German-ROTOWIRE are 8, 5, and 1 respectively. We set ? to 2 in Equation <ref type="formula" target="#formula_9">(18)</ref>. In Equation <ref type="formula" target="#formula_10">(19)</ref>, c is 1/100000 for MLB, 1/50000 for ROTOWIRE, and 1/30000 for German-ROTOWIRE. We set the temperature of Gumbel-Softmax to 0.1.</p><p>During inference in MLB, similar to Puduppully and Lapata (2021), we block the repetition of paragraph plan bigrams (i.e., we disallow the repetition of (z t , z t+1 )) and select the paragraph plan with the next higher probability in Equation <ref type="formula" target="#formula_9">(8)</ref>. In addition, we block consecutive repetitions, and more than two repetitions of a unigram. During training we observed high variance in the length of paragraphs y t since the same plan can result in a shorter or longer paragraph. For example, &lt;V(B.Keller)&gt; corresponds to two paragraphs (first and third paragraph) with different lengths in <ref type="figure" target="#fig_0">Figure 1</ref>. We found that this encourages the model to be conservative and generate relatively short output. We control the paragraph length <ref type="bibr" target="#b7">(Fan et al., 2018)</ref> by creating discrete bins, each containing approximately an equal number of paragraphs. During training, we prepend the embedding of the bin to the current plan r t z (see Equation <ref type="formula">(11)</ref>). For inference, bins are tuned on the validation set.</p><p>We run inference for 15 paragraphs on RO-TOWIRE and German ROTOWIRE, and for 20 paragraphs on MLB; we stop when the model predicts the end of paragraph plan token EOP. Unlike previous work <ref type="bibr" target="#b69">(Wiseman et al., 2017;</ref><ref type="bibr">Puduppully et al., 2019a,b</ref>, inter alia), we do not make use of truncated Back Propagation Through Time (BPTT; <ref type="bibr" target="#b68">Williams and Peng, 1990</ref>), as we incrementally generate paragraphs instead of long documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Comparisons We compared our model with: (1) a Template-based generator which creates a document consisting of template sentences. We used Wiseman et al.'s (2017) system on RO-TOWIRE and Puduppully et al.'s (2019b) system on MLB. They are both similar in that they describe team scores followed by player specific statistics and a concluding statement. In MLB, the template additionally describes play-by-play details. We also created a template system for German ROTOWIRE following a similar approach.</p><p>(2) ED+CC, the best performing model of <ref type="bibr" target="#b69">Wiseman et al. (2017)</ref>. It consists of an encoder-decoder model equipped with attention and copy mechanisms. (3) NCP+CC, the micro planning model of <ref type="bibr" target="#b44">Puduppully et al. (2019a)</ref>. It first creates a content plan by pointing to input records through the use of Pointer Networks . The content plan is then encoded with a BiLSTM and decoded using another LSTM with an attention and copy mechanism. (4) ENT, the entity model of <ref type="bibr" target="#b45">Puduppully et al. (2019b)</ref>. It creates entity-specific representations which are updated dynamically. At each time step during decoding, their model makes use of hierarchical attention by attending over entity representations and the records corresponding to these. (5) MACRO, the two-stage planning model of <ref type="bibr" target="#b46">Puduppully and Lapata (2021)</ref>, which first makes use of Pointer Networks  to predict a macro plan from a set of candidate paragraph plans. The second stage takes the predicted plan as input and generates the game summary with a sequence-to-sequence model enhanced with attention and copy mechanisms. In addition, we compare with a variant of Macro enhanced with length control (+Bin).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Our experiments were designed to explore how the proposed model compares to related approaches which are either not enhanced with planning modules or non-incremental. We also investigated the sample efficiency of these models and the quality of the predicted plans when these are available. The majority of our results focus on automatic evaluation metrics. We also follow previous work <ref type="bibr" target="#b69">(Wiseman et al., 2017;</ref><ref type="bibr">Puduppully et al., 2019a,b;</ref><ref type="bibr" target="#b46">Puduppully and Lapata, 2021)</ref> in eliciting judgments to evaluate system output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Automatic Evaluation</head><p>We evaluate model output using BLEU <ref type="bibr" target="#b43">(Papineni et al., 2002)</ref> with the gold summary as a reference. We also report model performance against the Information Extraction (IE) metrics of Wiseman et al. <ref type="formula" target="#formula_4">(2017)</ref> which are defined based on the output of an IE model which extracts entity (team and player names) and value (numbers) pairs from the summary and predicts the type of relation between them.</p><p>Let? be the gold summary and y be the model output. Relation Generation (RG) measures the precision and count of relations obtained from y that are found in the input table. Content Selection (CS) measures the precision, recall, and F-measure of relations extracted from y also found in?. And Content Ordering (CO) measures the complement of the Damerau-Levenshtein distance between relations extracted from y and?. Higher values are better for RG Precision, CS F-measure, CO, and BLEU. We reuse the IE model from <ref type="bibr" target="#b44">Puduppully et al. (2019a)</ref> for ROTOWIRE, Puduppully and Lapata (2021) for MLB, and <ref type="bibr" target="#b17">Hayashi et al. (2019)</ref> for German ROTOWIRE. Our computation of IE metrics for all systems includes duplicate records <ref type="bibr" target="#b46">(Puduppully and Lapata, 2021)</ref>.</p><p>In addition to IE-based metrics, we report the number of errors made by systems according to Number (incorrect number in digits, number spelled in words, etc.), Name (incorrect names of teams, players, days of week, etc.), and Word (errors in usage of words) following the classification of <ref type="bibr" target="#b64">Thomson and Reiter (2020)</ref>. We detect such errors automatically using the system of <ref type="bibr" target="#b21">Kasner et al. (2021)</ref>    <ref type="table" target="#tab_5">Table 2</ref>). This version obtains lower values compared to SeqPlan across all metrics underscoring the importance of sequential planning. We also present two variants of SeqPlan (a) one which makes use of oracle (instead of predicted) plans during training to generate y t ; essentially, it replaces z t with z * in Equation (12) (row w(ith) Oracle in <ref type="table" target="#tab_5">Table 2</ref>) and (b) a two stage model which trains the planner (Equation <ref type="formula" target="#formula_7">(15)</ref>) and generator (Equation <ref type="formula" target="#formula_4">(12)</ref>) separately (row 2-stage in <ref type="table" target="#tab_5">Table 2</ref>); in this case, we use greedy decoding to sample z t from Equation <ref type="formula" target="#formula_7">(15)</ref> instead of Gumbel-Softmax and replace z t with z * in Equation <ref type="formula" target="#formula_4">(12)</ref>. Both variants are comparable to SeqPlan in terms of RG P but worse in terms of CS F, CO, and BLEU.</p><p>Furthermore, we evaluate the accuracy of the inferred plans by comparing them against oracle plans, using the CS and CO metrics (computed over the entities and events in the plan) 4 . <ref type="table" target="#tab_9">Table 4</ref> shows that SeqPlan achieves higher CS F and CO scores than Macro. Again, this indicates planning is beneficial, particularly when taking the table and the generated summary into account.</p><p>English and German ROTOWIRE Results on ROTOWIRE are presented in <ref type="table" target="#tab_6">Table 3</ref> (top). In addition to Templ, ED+CC, NCP+CC, and ENT, we compare with the models of <ref type="bibr" target="#b69">Wiseman et al. (2017)</ref> (WS-2017) and <ref type="bibr" target="#b48">Rebuffel et al. (2020</ref><ref type="bibr">) (RBF-2020</ref>. WS-2017 is the best performing model of <ref type="bibr" target="#b69">Wiseman et al. (2017)</ref>. Note that ED+CC is an improved re-implementation of WS-2017. RBF-2020 represents the current state-of-the-art on ROTOWIRE, and comprises of a Transformer encoder-decoder architecture <ref type="bibr" target="#b66">(Vaswani et al., 2017)</ref> with hierarchical attention on entities and their records. The models of <ref type="bibr" target="#b55">Saleh et al. (2019)</ref>, <ref type="bibr" target="#b18">Iso et al. (2019), and</ref><ref type="bibr" target="#b13">Gong et al. (2019)</ref> are not comparable as they make use of information additional to the table such as previous/next games or the author of the game summary. The model of  is also not comparable as it relies on a pretrained language model <ref type="bibr" target="#b54">(Rothe et al., 2020)</ref> to generate the summary sentences.  try in the WNGT 2019 shared task 5 <ref type="bibr" target="#b17">(Hayashi et al., 2019)</ref>, and our implementation of Templ, ED+CC, ENT, Macro and RBF-2020. <ref type="bibr" target="#b55">Saleh et al. (2019)</ref> are not comparable as they pretrain on 32M parallel and 420M monolingual data. Likewise, Puduppully et al. (2019c) make use of a jointly trained multilingual model by combining ROTOWIRE with German ROTOWIRE. We find that SeqPlan achieves highest RG P amongst neural models, and performs on par with Macro (it obtains higher BLEU but lower CS F and CO scores). The +Bin variant of Macro performs better on BLEU but worse on other metrics. As in <ref type="table" target="#tab_5">Table 2</ref>, w Uniform struggles across metrics corroborating our hypothesis that latent sequential planning improves generation performance. The other two variants (w Oracle and 2-Stage) are worse than SeqPlan in RG P and CS F, comparable in CO, <ref type="bibr">5</ref> We thank Hiroaki Hayashi for providing us with the output of the NCP+CC system.  and slightly higher in terms of BLEU.</p><p>On German, our model is best across metrics achieving an RG P of 91.8% which is higher by 42% (absolute) compared to of Macro. In fact, the RG P of SeqPlan is superior to <ref type="bibr" target="#b55">Saleh et al. (2019)</ref> whose model is pretrained with additional data and is considered state of the art <ref type="bibr" target="#b17">(Hayashi et al., 2019)</ref>. RG# is lower mainly because of a bug in the German IE which excludes number records. RG# for NCP+CC and Macro is too high because the summaries contain a lot of repetition. The same record will repeat at least once with NCP+CC and three times with Macro, whereas only 7% of the records are repeated with SeqPlan. <ref type="table" target="#tab_9">Table 4</ref> evaluates the quality of the plans inferred by our model on the ROTOWIRE dataset. As can be seen, SeqPlan is slightly worse than Macro in terms of CS F and CO. We believe this is because summaries in ROTOWIRE are somewhat formulaic, with a plan similar to Templ: an opening statement is followed by a description of the top scoring players, and a conclusion describing the next match. Such plans can be learnt well by Macro without access to the summary. MLB texts show a lot more diversity in terms of length, and the sequencing of entities and events. The learning problem is also more challenging, supported by the fact that the template system does not do very well in this domain (i.e., it is worse in BLEU, CS F, and CO compared to ROTOWIRE). In German ROTOWIRE, SeqPlan plans achieve higher CS F and CO than Macro. <ref type="table" target="#tab_11">Table 5</ref> reports complementary automatic metrics on English ROTOWIRE aiming to assess the factuality of generated output. We find that Templ has the least Number, Name, and double-double errors. This is expected as it simply reproduces   facts from the table. SeqPlan and Macro have similar Number errors, and both are significantly better than other neural models. SeqPlan has significantly more Name errors than Macro, and significantly fewer than other neural models. Inspection of Name errors revealed that these are mostly due to incorrect information about next games. Such information is not part of the input and models are prone to hallucinate. SeqPlan fares worse as it attempts to discuss next games for both teams while Macro focuses on one team only. In terms of double-double errors, SeqPlan is comparable to Macro, ENT and NCP+CC, and significantly better than WS-2017, ED+CC, and RBF-2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sample Efficiency</head><p>We also evaluated whether SeqPlan is more sample efficient in comparison to Macro, by examining how RG P varies with (training) data size. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, the difference between SeqPlan and Macro is more pronounced when relatively little data is available.  <ref type="table">Table 6</ref>: Average number of supported (#Supp) and contradicting (#Contra) facts in game summaries and bestworst scaling evaluation for Coherence (Coher), Conciseness (Concis), and Grammaticality (Gram). Lower is better for contradicting facts; higher is better for Coherence, Conciseness, and Grammaticality. Systems significantly different from SeqPlan are marked with an asterisk * (using a one-way ANOVA with posthoc Tukey HSD tests; p ? 0.05).</p><p>. ference in RG P decreases. The slope of increase in RG P for Macro is higher for ROTOWIRE than MLB. We hypothesize this is because MLB has longer summaries with more paragraphs, and is thus more difficult for Macro to learn alignments between paragraph plans and text paragraphs in the game summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Human Evaluation</head><p>We used the Amazon Mechanical Turk (AMT) crowdsourcing platform for our judgment elicitation study. To ensure consistent ratings <ref type="bibr" target="#b29">(van der Lee et al., 2019)</ref> we required that raters have completed at least 1,000 tasks, and have at least 98% approval rate. Participants were restricted to English speaking countries (USA, UK, Canada, Australia, Ireland, or New Zealand) and were allowed to provide feedback or ask questions. Raters were paid an average of 0.35$ for each task, ensuring that the remuneration is higher than the minimum wage per hour in the US. We compared SeqPlan with Gold, Templ, ED+CC, and Macro; we did not compare against ENT as previous work <ref type="bibr" target="#b46">(Puduppully and Lapata, 2021)</ref> has shown that it performs poorly against Macro. For ROTOWIRE, we additionally compared against RBF-2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supported and Contradicted Facts</head><p>Our first eliciation study provided raters with box scores (and play-by-plays in the case of MLB), along with sentences randomly extracted from game summaries. We asked them to count supported and contradicting facts (ignoring hallucinations). Participants were given a cheatsheet to help them understand box score and play-by-play statistics as well as examples of sentences with the correct count of supported and contradicting facts. This evaluation was conducted on 40 summaries (20 for each dataset), with four sentences per summary, each rated by three participants. For MLB, this resulted in 300 tasks (5 systems ? 20 summaries ? 3 raters) and for ROTOWIRE in 360 (6 systems ? 20 summaries ? 3 raters). Altogether, we had 177 participants. The agreement between raters using Krippendorff's ? for supported facts and contradicting facts was 0.43. <ref type="table">Table 6</ref> (columns #Supp and #Contra) presents our results. Lower is better for contradicting facts. In case of supporting facts, the count should neither be too high nor too low. A high count of supporting facts indicates indicates poor content selection. A low count of supporting facts with a high count of contradicting facts indicates low accuracy of generation.</p><p>Templ achieves the lowest count of contradicting facts and the highest count of supported facts for both the datasets. This is no surprise as it essentially regurgitates facts (i.e., records) from the table. On MLB, all systems display a comparable count of supported facts (differences are not statistically significant), with the exception of Templ which contains significantly more. In terms of contradicting facts, SeqPlan performs on par with Macro, Gold and Templ, and is significantly better than ED+CC. On ROTOWIRE, in terms of supported facts, Seq-Plan performs on par with the other neural models, is significantly higher than Gold, and significantly lower than Templ. In terms of contradicting facts, SeqPlan performs on par with Macro, Gold and Templ, and significantly better than ED+CC and RBF-2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coherence, Grammaticality, and Conciseness</head><p>In our second study, raters were asked to choose the better summary from a pair of summaries based on Coherence (is the summary well structured and well organized and does it have a natural ordering of the facts?), Conciseness (does the summary avoid unnecessary repetition including whole sentences, facts or phrases?), and Grammaticality (is the summary written in well-formed English?). For this study, we required that the raters be able to comfortably comprehend summaries of NBA/MLB games. We obtained ratings using Best-Worst scaling <ref type="bibr" target="#b35">(Louviere and Woodworth, 1991;</ref><ref type="bibr" target="#b34">Louviere et al., 2015)</ref>, an elicitation paradigm shown to be more accurate than Likert scales. The score for a system is obtained by the number of times it is rated best minus the number of times it is rated worst <ref type="bibr" target="#b42">(Orme, 2009</ref>). Scores range between ?100 (absolutely worst) and +100 (absolutely best); higher is better. We assessed 40 summaries from the test set (20 for each dataset). Each summary pair was rated by three participants. For MLB, we created 1,800 tasks (10 system pairs ? 20 summaries ? 3 raters ? 3 dimensions) and 2,700 for ROTOWIRE (15 pairs of systems ? 20 summaries ? 3 raters ? 3 dimensions). Altogether, 377 raters participated in this task. The agreement between the raters using Krippendorff's ? was 0.49.</p><p>On MLB, SeqPlan is significantly more coherent than ED+CC and Templ, and is comparable with Gold and Macro. A similar picture emerges with grammaticality. SeqPlan is as concise as Gold, Macro and Templ, and significantly better than ED+CC. On ROTOWIRE, SeqPlan is significantly more coherent than Templ and ED+CC, but on par with Macro, RBF-2020 and Gold. In terms of conciseness, SeqPlan is comparable with Gold, Macro, RBF-2020, and ED+CC, and significantly better than Templ. In terms of grammaticality, SeqPlan is comparable with Macro, RBF-2020, and ED+CC, significantly better than Templ, and significantly worse than Gold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this work, we proposed a novel sequential latent variable model for joint macro planning and generation. Key in our approach is the creation of a latent plan in a sequential manner, while interleaving the prediction of plans and the generation of corresponding paragraphs. We proposed to deconstruct monolithic long document generation into smaller units (paragraphs in our case) which affords flexibility and better communication between planning and generation. Taken together, the results of automatic and human evaluation suggest that SeqPlan performs best in terms of factuality and coherence, it generates diverse, and overall fluent summaries and is less data-hungry compared to strong systems like Macro and NCP+CC. As SeqPlan does not have to learn alignments between the macro plan and the output text, it is better suited &lt;V(Cardinals)&gt; ? &lt;V(9-T)&gt; ? &lt;V(Cardinals)&gt; ? &lt;V(Cardinals)&gt; &lt;V(Brewers)&gt;? &lt;V(9-T)&gt; ? &lt;V(8-B)&gt; ? &lt;V(8-T)&gt; ? &lt;V(8-B)&gt; ? &lt;V(9-B)&gt; ? &lt;V(Brewers)&gt;? &lt;V(Adam Wainwright)&gt;? &lt;V(Brewers)&gt;? &lt;V(3-T)&gt; ? &lt;V(3-B)&gt; ? &lt;V(Carlos Villanueva)&gt; ST. LOUIS -The St. Louis Cardinals have been waiting for their starting rotation. &lt;P&gt; Skip Schumaker drove in the go-ahead run with a double in the ninth inning, and the Cardinals beat the Milwaukee Brewers 4-3 on Wednesday night to avoid a three-game sweep. &lt;P&gt; The Cardinals have won four of five, and have won four in a row. &lt;P&gt; The Cardinals have won four of five, including a threegame sweep by the Brewers. &lt;P&gt; Brian Barton led off the ninth with a pinch-hit double off Derrick Turnbow (0-1) and moved to third on Cesar Izturis' sacrifice bunt. Schumaker drove in Barton with a double down the left-field line. &lt;P&gt; Ryan Braun, who had two hits, led off the eighth with a double off Ryan Franklin (1-1). Braun went to third on a wild pitch and scored on Corey Hart's triple into the right-field corner. &lt;P&gt; Albert Pujols was intentionally walked to load the bases with one out in the eighth, and Guillermo Ankiel flied out. Troy Glaus walked to load the bases for Kennedy, who hit a sacrifice fly off Guillermo Mota. &lt;P&gt; Ryan Franklin (1-1) got the win despite giving up a run in the eighth. Ryan Braun led off with a double and scored on Corey Hart's one-out triple. &lt;P&gt; Jason Isringhausen pitched a perfect ninth for his seventh save in nine chances. He has converted his last six save opportunities and has n't allowed a run in his last three appearances. &lt;P&gt; The Brewers lost for the seventh time in eight games. &lt;P&gt; Wainwright allowed two runs and four hits in seven innings. He walked four and struck out six. &lt;P&gt; Brewers manager Ron Roenicke was ejected by home plate umpire Bill Miller for arguing a called third strike. &lt;P&gt; The Cardinals took a 2-0 lead in the third. Albert Pujols walked with two outs and Rick Ankiel walked. Glaus then lined a two-run double into the left-field corner. &lt;P&gt; The Brewers tied it in the third. Jason Kendall led off with a double and scored on Rickie Weeks' double. Ryan Braun's RBI single tied it at 2. &lt;P&gt; Villanueva allowed two runs and three hits in seven innings. He walked four and struck out one. <ref type="table">Table 7</ref>: Predicted macro plan (top) and generated output from our model. Transitions between paragraph plans are shown using ?. Paragraphs are separated with &lt;P&gt; delimiters. Entities and events in the summary corresponding to the macro plan are boldfaced.</p><p>for long-form generation. Potential applications include summarizing books <ref type="bibr" target="#b27">(Kry?ci?ski et al., 2021)</ref> where the output can be longer than 1,000 tokens or generating financial reports <ref type="bibr" target="#b25">(Kogan et al., 2009;</ref><ref type="bibr" target="#b16">H?ndschke et al., 2018)</ref> where the output exceeds 9,000 tokens. Existing approaches for long-form generation summarize individual paragraphs independently <ref type="bibr" target="#b27">(Kry?ci?ski et al., 2021)</ref> or adopt a hierarchical approach <ref type="bibr">(Wu et al., 2021)</ref> where summaries of paragraphs form the basis of chapter summaries which in turn are composed into a book summary. <ref type="table">Table 7</ref> gives an example of SeqPlan output. We see that the game summary follows the macro plan closely. In addition, the paragraph plans and the paragraphs exhibit coherent ordering. Manual inspection of SeqPlan summaries reveals that a major source of errors in MLB relate to attention diffusing over long paragraph plans. As an example, consider the following paragraph produced by SeqPlan "Casey Kotchman had three hits and three RBIs , including a two-run double in the second inning that put the Angels up 2-0. Torii Hunter had three hits and drove in a run ." In reality, Torii Hunter had two hits but the model incorrectly generates hits for Casey Kotchman. The corresponding paragraph plan is 360 tokens long and attention fails to discern important tokens. A more sophisticated encoder, e.g., based on Transformers <ref type="bibr" target="#b66">(Vaswani et al., 2017)</ref>, could make attention more focused. In RO-TOWIRE, the majority of errors involve numbers (e.g., team attributes) and numerical comparisons. Incorporating pre-executed operations such as min, max <ref type="bibr" target="#b41">(Nie et al., 2018)</ref> could help alleviate these errors.</p><p>Finally, it is worth mentioning that although the template models achieve highest RG precision for both MLB and ROTOWIRE <ref type="table" target="#tab_5">(Tables 2 and 3)</ref>, this is mainly because they repeat facts from the table. Template models score low against CS F, CO, and BLEU metrics. In addition, they obtain lowest scores in Grammaticality and Coherence (Table 6) which indicates that they are poor at selecting records from the table and ordering them correctly in a fluent manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Keller)&lt;P&gt;V(B.Keller) V(C.Mullins) V(Royals) V(Orioles)&lt;P&gt;V(B.Keller)&lt;P&gt; V(R.O'Hearn) V(W.Merrifield) V(H.Dozier) V(C.Gallagher) &lt;P&gt;V(4-B, 5-B) &lt;P&gt; V(6-T)&lt;P&gt; Example from the MLB dataset reproduced from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Model workflow. Solid arrows show dependencies between random variables. Dashed arrows show the computation graph whose backbone consists of an LSTM text and an LSTM plan . Note that the variational model and the generative model are tied closely with the shared LSTM. To generate long documents, the model observes what has been already generated, decides on a plan about what to discuss next, uses this plan to guide next stage generation, and repeats until the end.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Sample efficiency for (a) MLB and (b) RO-TOWIRE datasets. SeqPlan and Macro are trained on different portions (%) of the training dataset and performance is measured with RG P%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>plan (E) is obtained arXiv:2202.13756v1 [cs.CL] 28 Feb 2022 TEAM Inn1 Inn2 Inn3 Inn4 . . . TR TH E . . . . . . 2 4 0 . . . . . . 9 14 1 . . . BATTER H/V AB BR BH RBI TEAM . . . Royals . . . . . . . . . . . . . . . . . . . . . . . . PITCHER H/V W L IP PH PR ER BB K . . . . . . . . . . . . . . . . . . . . . . . . . . .</figDesc><table><row><cell>(A)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Orioles 0 0 Royals 1 0 1 0 0 3 C.Mullins H 4 2 2</cell><cell>1 Orioles . . .</cell></row><row><cell>J.Villar</cell><cell>H 4</cell><cell>0 0</cell><cell>0 Orioles . . .</cell></row><row><cell cols="2">W.Merrifield V 2</cell><cell>3 2</cell><cell>1 Royals . . .</cell></row><row><cell cols="4">R.O'Hearn 4 A.Cashner H 4 13 5.1 9 4 4 3 1 . . . V 5 1 3</cell></row><row><cell>B.Keller</cell><cell>V 7</cell><cell cols="2">5 8.0 4 2 2 2 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>. We used</cell></row><row><cell>the official train/dev/test splits: 3,398/727/728 for</cell></row><row><cell>ROTOWIRE, 22,821/1,739/1,744 for MLB, and</cell></row><row><cell>242/240/241 for German ROTOWIRE. The latter</cell></row><row><cell>is considerably smaller than its English counter-</cell></row><row><cell>part and MLB, and serves to illustrate our model's</cell></row><row><cell>sample efficiency when training data is scarce.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc>Highest and . . . . . . . . second . . . . . . . . . highest generation models are highlighted.</figDesc><table><row><cell>: MLB results (test set); relation generation (RG)</cell></row><row><cell>count (#) and precision (P%), content selection (CS) pre-</cell></row><row><cell>cision (P%), recall (R%), and F-measure (F%), content</cell></row><row><cell>ordering (CO) as complement of normalized Damerau-</cell></row><row><cell>Levenshtein distance (DLD%), and BLEU. Reiter, 2021). We only report these metrics for</cell></row><row><cell>English ROTOWIRE, since error annotations (for</cell></row><row><cell>automatic metric learning) are not available for</cell></row><row><cell>other datasets. Moreover, with regard to Word er-</cell></row><row><cell>rors, we only report errors for incorrect usage of</cell></row><row><cell>the word double-double. 3 We found such errors to</cell></row><row><cell>be detected reliably in contrast to Word errors as a</cell></row><row><cell>whole for which the precision of the system of Kas-</cell></row><row><cell>ner et al. (2021) is~50%. Lower values are better</cell></row><row><cell>for the Number, Name, and double-double errors.</cell></row><row><cell>We note metrics such as RG precision, Number,</cell></row><row><cell>Name, and double-double errors directly compute</cell></row><row><cell>the accuracy of the generation model. Metrics such</cell></row><row><cell>as CS, CO, and BLEU measure how similar model</cell></row><row><cell>output is against a reference summary. Thus, CS,</cell></row><row><cell>CO and BLEU measure generation accuracy indi-</cell></row><row><cell>rectly under the assumption that gold summaries</cell></row><row><cell>are accurate.</cell></row><row><cell>MLB Dataset Table 2 summarizes our results on</cell></row><row><cell>MLB. Our sequential planning model (SeqPlan)</cell></row><row><cell>has the highest RG P among neural models and</cell></row><row><cell>performs best in terms of CS F, CO, and BLEU.</cell></row><row><cell>The variant of Macro with length control (+Bin)</cell></row><row><cell>performs comparably or worse than Macro.</cell></row><row><cell>To examine the importance of latent sequential</cell></row><row><cell>planning, we also present a variant of our model</cell></row><row><cell>which uniformly samples a plan from the pool E in-</cell></row><row><cell>stead of Equation (8) (see row w(ith) Uniform in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>(bottom) shows our results on German</cell></row><row><cell>ROTOWIRE. We compare against NCP+CC's en-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc>Highest and . . . . . . . . second . . . . . . . . . highest generation models are highlighted.</figDesc><table><row><cell>: Evaluation on ROTOWIRE (RW) and German</cell></row><row><cell>ROTOWIRE (DE-RW) test sets; relation generation (RG)</cell></row><row><cell>count (#) and precision (P%), content selection (CS) pre-</cell></row><row><cell>cision (P%), recall (R%), and F-measure (F%), content</cell></row><row><cell>ordering (CO) as complement of normalized Damerau-</cell></row><row><cell>Levenshtein distance (DLD%), and BLEU.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Evaluation of macro planning stage (test set);</cell></row><row><cell>content selection (CS) precision (P%), recall (R%), and</cell></row><row><cell>F-measure (F%), content ordering (CO) as complement</cell></row><row><cell>of normalized Damerau-Levenshtein distance (DLD%).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Number, Name, and double-double (Word)</cell></row><row><cell>errors per example. Systems significantly different from</cell></row><row><cell>SeqPlan are marked with an asterisk * (using a one-way</cell></row><row><cell>ANOVA with posthoc Tukey HSD tests; p ? 0.05).</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ratishsp/ data2text-seq-plan-py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In our notation neural network layers are described by math functions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A double-double occurs when a player scores 10 points or more in two record types: points, rebounds, assists, steals, and blocked shots.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">To compute the accuracy of macro plans, entities and events from the model's plan need to be compared against entities and events in the oracle macro plan.<ref type="bibr" target="#b46">Puduppully and Lapata (2021)</ref> obtained the entities and events for the oracle macro plan by extracting these from reference summaries. We noted that this includes coreferent or repeat mentions of entities and events within a paragraph. We instead extract entities and events directly from the oracle macro plan.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the Action Editor, Ehud Reiter, and the anonymous reviewers for their constructive feedback. We also thank Parag Jain for helpful discussions. We acknowledge the financial support of the European Research Council (award number 681760, "Translating Multiple Modalities into Text").</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collective content selection for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
	<note>NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<idno>abs/1606.05908</idno>
		<title level="m">Tutorial on variational autoencoders. CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Empirically estimating order constraints for content planning in generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">A</forename><surname>Duboue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073012.1073035</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 39th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Controllable abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-2706</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequential neural models with stochastic layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>S?ren Kaae S? Nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Latent template induction with gumbel-crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20259" to="20271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Creating training corpora for NLG microplanners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1017</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Survey of the state of the art in natural language generation: Core tasks, applications and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.5477</idno>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="65" to="170" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bottom-up abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1443</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4098" to="4109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Table-to-text generation with effective hierarchical encoder on three dimensions (row, column and time)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Heng Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1310</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3143" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Z-forcing: Training stochastic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-Alexandre</forename><surname>C?t?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Nan Rosemary Ke</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Incremental conceptualization for language production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Guhe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A corpus of corporate annual and social responsibility reports: 280 million tokens of balanced organizational writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>H?ndschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buechel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Goldenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Poschmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udo</forename><surname>Walgenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hahn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-3103</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Economics and Natural Language Processing</title>
		<meeting>the First Workshop on Economics and Natural Language Processing<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Findings of the third workshop on neural generation and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5601</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Neural Generation and Translation</title>
		<meeting>the 3rd Workshop on Neural Generation and Translation<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to select, track, and generate for data-to-text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayate</forename><surname>Iso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yui</forename><surname>Uehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Ishigaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiji</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2102" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Categorical reparametrization with gumble-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Corpus-trained text generation for summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Natural Language Generation Conference</title>
		<meeting>the International Natural Language Generation Conference<address><addrLine>Harriman, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Text-in-context: Token-level error detection for table-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zden?k</forename><surname>Kasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Du?ek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Natural Language Generation</title>
		<meeting>the 14th International Conference on Natural Language Generation<address><addrLine>Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="259" to="265" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequential latent knowledge selection for knowledge-grounded dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongchang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Open-NMT: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
		<meeting>ACL 2017, System Demonstrations<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predicting risk from financial reports with regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Kogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitry</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">R</forename><surname>Routledge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><forename type="middle">S</forename><surname>Sagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="272" to="280" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inducing document plans for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1503" to="1514" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Booksum: A collection of datasets for long-form narrative summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Kry?ci?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazneen</forename><surname>Rajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Agarwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Caiming Xiong, and Dragomir Radev</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural text generation from structured data with application to the biography domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1203" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Best practices for the human evaluation of automatically generated text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Van Der Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Emiel Van Miltenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Wubben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahmer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-8643</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Natural Language Generation</title>
		<meeting>the 12th International Conference on Natural Language Generation<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Speaking: From intention to articulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Willem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levelt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>MIT press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep recurrent generative decoder for abstractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1222</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Posterior control of blackbox generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2731" to="2743" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Louviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony Alfred John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marley</surname></persName>
		</author>
		<title level="m">Best-worst scaling: Theory, methods and applications</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Best-worst scaling: A model for the largest difference judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George G</forename><surname>Louviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodworth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>University of Alberta: Working Paper</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using LSTMs with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1086</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving quality and efficiency in planbased neural data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Moryossef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-8645</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Natural Language Generation</title>
		<meeting>the 12th International Conference on Natural Language Generation<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="377" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Step-by-step: Separating planning from realization in neural data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Moryossef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1236</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2267" to="2277" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stepwise extractive summarization and planning with structured transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Adamek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaz</forename><surname>Bratanic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mc-Donald</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.339</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4143" to="4159" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Operation-guided neural networks for high fidelity data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1422</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3879" to="3889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Maxdiff analysis: Simple counting, individual-level logit, and hb. Sawtooth Software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Orme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Data-to-text generation with content selection and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd AAAI Conference on Artificial Intelligence, Honolulu</title>
		<meeting>the 33rd AAAI Conference on Artificial Intelligence, Honolulu<address><addrLine>Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Data-to-text generation with entity modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1195</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2023" to="2035" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Datato-text generation with macro planning. Transactions of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno>abs/2102.02723</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">University of Edinburgh&apos;s submission to the document-level generation and translation shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5630</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Neural Generation and Translation</title>
		<meeting>the 3rd Workshop on Neural Generation and Translation</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="268" to="272" />
		</imprint>
		<respStmt>
			<orgName>Hong Kong. Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A hierarchical model for data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Rebuffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Scoutheeten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="65" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Can neural generators for dialogue learn sentence planning and discourse structuring?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shereen</forename><surname>Oraby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6535</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation<address><addrLine>Tilburg University, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="284" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Building applied natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324997001502</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="87" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Building natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-26" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Revision-based generation of Natural Language Summaries providing historical Background</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Robin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
	<note>Ph. D. thesis</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Leveraging pre-trained checkpoints for sequence generation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00313</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="264" to="280" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Naver labs Europe&apos;s systems for the document-level generation and translation task at WNGT 2019</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahimeh</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioan</forename><surname>Calapodescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5631</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Neural Generation and Translation</title>
		<meeting>the 3rd Workshop on Neural Generation and Translation<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="273" to="279" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="3295" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Posterior attention models for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Long and diverse text generation with planning-based hierarchical variational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1321</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3257" to="3268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Reactive content selection in the generation of real-time soccer commentary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumiko</forename><surname>Tanaka-Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koiti</forename><surname>Hasida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itsuki</forename><surname>Noda</surname></persName>
		</author>
		<idno type="DOI">10.3115/980691.980778</idno>
	</analytic>
	<monogr>
		<title level="m">36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1282" to="1288" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Book reviews: Speaking: From intention to articulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Insup</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A gold standard methodology for evaluating accuracy in data-to-text systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Natural Language Generation</title>
		<meeting>the 13th International Conference on Natural Language Generation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="158" to="168" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Generation challenges: Results of the accuracy evaluation shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Natural Language Generation</title>
		<meeting>the 14th International Conference on Natural Language Generation<address><addrLine>Aberdeen, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">An efficient gradient-based algorithm for on-line training of recurrent network trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1990.2.4.490</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="490" to="501" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1239</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2253" to="2263" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisan</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Recursively summarizing books with human feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
		<idno>abs/2109.10862</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1174</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Variational template machine for data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
