<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Robust Real-Time Automatic License Plate Recognition Based on the YOLO Detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayson</forename><surname>Laroca</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evair</forename><surname>Severo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz</forename><forename type="middle">A</forename><surname>Zanlorensi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">Resende</forename><surname>Gon?alves</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Federal University of Minas Gerais (UFMG)</orgName>
								<address>
									<addrLine>Belo Horizonte</addrLine>
									<region>MG</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Robson Schwartz</surname></persName>
							<email>william@dcc.ufmg.br</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Federal University of Minas Gerais (UFMG)</orgName>
								<address>
									<addrLine>Belo Horizonte</addrLine>
									<region>MG</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Menotti</surname></persName>
							<email>menotti@inf.ufpr.brgabrielrg</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Federal University of Paran? (UFPR)</orgName>
								<address>
									<settlement>Curitiba</settlement>
									<region>PR</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Robust Real-Time Automatic License Plate Recognition Based on the YOLO Detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic License Plate Recognition (ALPR) has been a frequent topic of research due to many practical applications. However, many of the current solutions are still not robust in real-world situations, commonly depending on many constraints. This paper presents a robust and efficient ALPR system based on the state-of-the-art YOLO object detector. The Convolutional Neural Networks (CNNs) are trained and finetuned for each ALPR stage so that they are robust under different conditions (e.g., variations in camera, lighting, and background). Specially for character segmentation and recognition, we design a two-stage approach employing simple data augmentation tricks such as inverted License Plates (LPs) and flipped characters. The resulting ALPR approach achieved impressive results in two datasets. First, in the SSIG dataset, composed of 2,000 frames from 101 vehicle videos, our system achieved a recognition rate of 93.53% and 47 Frames Per Second (FPS), performing better than both Sighthound and OpenALPR commercial systems (89.80% and 93.03%, respectively) and considerably outperforming previous results (81.80%). Second, targeting a more realistic scenario, we introduce a larger public dataset 1 , called UFPR-ALPR dataset, designed to ALPR. This dataset contains 150 videos and 4,500 frames captured when both camera and vehicles are moving and also contains different types of vehicles (cars, motorcycles, buses and trucks). In our proposed dataset, the trial versions of commercial systems achieved recognition rates below 70%. On the other hand, our system performed better, with recognition rate of 78.33% and 35 FPS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Automatic License Plate Recognition (ALPR) has been a frequent topic of research <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> due to many practical applications, such as automatic toll collection, traffic law enforcement, private spaces access control and road traffic monitoring.</p><p>ALPR systems typically have three stages: License Plate (LP) detection, character segmentation and character recognition. The earlier stages require higher accuracy or almost perfection, since failing to detect the LP would probably lead to a failure in the next stages either. Many approaches search first for the vehicle and then its LP in order to reduce processing time and eliminate false positives.</p><p>Although ALPR has been frequently addressed in the literature, many studies and solutions are still not robust enough on real-world scenarios. These solutions commonly depend on certain constraints, such as specific cameras or viewing angles, simple backgrounds, good lighting conditions, search in a fixed region, and certain types of vehicles (they would not detect LPs from vehicles such as motorcycles, trucks or buses).</p><p>Many computer vision tasks have recently achieved a great increase in performance mainly due to the availability of large-scale annotated datasets (i.e., ImageNet <ref type="bibr" target="#b3">[4]</ref>) and the hardware (GPUs) capable of handling a large amount of data. In this scenario, Deep Learning (DL) techniques arise. However, despite the remarkable progress of DL approaches in ALPR <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, there is still a great demand for ALPR datasets with vehicles and LPs annotations. The amount of training data is determinant for the performance of DL techniques. Higher amounts of data allow the use of more robust network architectures with more parameters and layers. Hence, we propose a larger benchmark dataset, called UFPR-ALPR, focused on different real-world scenarios.</p><p>To the best of our knowledge, the SSIG SegPlate Database (SSIG) <ref type="bibr" target="#b7">[8]</ref> is the largest public dataset of Brazilian LPs. This dataset contains less than 800 training examples and has several constraints such as: it uses a static camera mounted always in the same position, all images have very similar and relatively simple backgrounds, there are no motorcycles and only a few cases where the LPs are not well aligned.</p><p>When recording the UFPR-ALPR dataset, we sought to eliminate many of the constraints found in ALPR applications by using three different non-static cameras to capture 4,500 images from different types of vehicles (cars, motorcycles, buses, trucks, among others) with complex backgrounds and under different lighting conditions. The vehicles are in different positions and distances to the camera. Furthermore, in some cases, the vehicle is not fully visible on the image. To the best of our knowledge, there are no public datasets for ALPR with annotations of cars, motorcycles, LPs and characters. Therefore, we can point out two main challenges in our dataset. First, usually, car and motorcycle LPs have different aspect ratios, not allowing ALPR approaches to use this constraint to filter false positives. Also car and motorcycle LPs have different layouts and positions.</p><p>As great advances in object detection were achieved through YOLO-inspired models <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, we decided to fine-tune it for ALPR. YOLOv2 <ref type="bibr" target="#b10">[11]</ref> is a state-of-the-art real-time object detection that uses a model with 19 convolutional layers and 5 maxpooling layers. On the other hand, Fast-YOLO <ref type="bibr" target="#b11">[12]</ref> is a model focused on a speed/accuracy trade-off that uses fewer convolutional layers (9 instead of 19) and fewer filters in those layers. Therefore, Fast-YOLO is much faster but less accurate than YOLOv2.</p><p>In this work, we propose a new robust real-time ALPR system based on the YOLO object detection Convolutional Neural Networks (CNNs). Since we are processing video frames, we also employ temporal redundancy such that we process each frame independently and then combine the results to create a more robust prediction for each vehicle.</p><p>The proposed system outperforms previous results and two commercial systems in the SSIG dataset and also in our proposed UFPR-ALPR. The main contributions of this paper can be summarized as follows:</p><p>? A new real-time end-to-end ALPR system using the stateof-the-art YOLO object detection CNNs 2 ; ? A robust two-stage approach for character segmentation and recognition mainly due to simple data augmentation tricks for training data such as inverted LPs and flipped characters. ? A public dataset for ALPR with 4,500 fully annotated images (over 30,000 LP characters) focused on usual and different real-world scenarios, showing that our proposed ALPR system yields outstanding results in both scenarios. ? A comparative evaluation among the proposed approach, previous works in the literature and two commercial systems in the UFPR-ALPR dataset. This paper is organized as follows. We briefly review related work in Section II. The UFPR-ALPR dataset is introduced in Section III. Section IV presents the proposed ALPR system using object detection CNNs. We report and discuss the results of our experiments in Section V. Conclusions and future work are given in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we briefly review several recent works that use DL approaches in the context of ALPR. For relevant studies using conventional image processing techniques, please refer to <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b18">[19]</ref>. More specifically, we discuss works related to each ALPR stage, and specially studies works that not fit into the other subsections. This section concludes with final remarks. LP Detection: Many authors have addressed the LP detection stage with object detection CNNs. Montazzolli and Jung <ref type="bibr" target="#b19">[20]</ref> used a single CNN arranged in a cascaded manner to detect both car frontal-views and its LPs, achieving high recall and precision rates. Hsu et al. <ref type="bibr" target="#b20">[21]</ref> customized CNNs exclusively for LP detection and demonstrated that the modified versions perform better. Rafique et al. <ref type="bibr" target="#b21">[22]</ref> applied Support Vector Machines (SVM) and Region-based CNN (RCNN) for LP detection, noting that RCNNs are best suited for real-time systems.</p><p>Li and Chen <ref type="bibr" target="#b4">[5]</ref> trained a CNN based on characters cropped from general text to perform a character-based LP detection, achieving higher recall and precision rates than previous approaches. Bulan et al. <ref type="bibr" target="#b2">[3]</ref> first extracts a set of candidate LP regions using a weak Sparse Network of Winnows (SNoW) classifier and then filters them using a strong CNN, significantly improving the baseline method. Character Segmentation: ALPR systems based on DL techniques usually address the character segmentation and recognition together. Montazzolli and Jung <ref type="bibr" target="#b19">[20]</ref> propose a CNN to segment and recognize the characters within a cropped LP. They have segmented more than 99% of the characters correctly, outperforming the baseline by a large margin.</p><p>Bulan et al. <ref type="bibr" target="#b2">[3]</ref> achieved very high accuracy in LP recognition jointly performing the character segmentation and recognition using Hidden Markov Models (HMMs) where the most likely LP was determined by applying the Viterbi algorithm. Character Recognition: Menotti et al. <ref type="bibr" target="#b22">[23]</ref> proposed the use of random CNNs to extract features for character recognition, achieving a significantly better performance than using image pixels or learning the filters weights with back-propagation. Li and Chen <ref type="bibr" target="#b4">[5]</ref> proposed to perform the character recognition as a sequence labelling problem. A Recurrent Neural Network (RNN) with Connectionist Temporal Classification (CTC) is employed to label the sequential data, recognizing the whole LP without the character-level segmentation.</p><p>Although Svoboda et al. <ref type="bibr" target="#b23">[24]</ref> have not perform the character recognition itself, they achieved high quality LP deblurring reconstructions using a text deblurring CNN, which can be very useful in character recognition. Miscellaneous: Masood et al. <ref type="bibr" target="#b6">[7]</ref> presented an end-to-end ALPR system using a sequence of deep CNNs. As this is a commercial system, little information is given about the used CNNs. Li et al. <ref type="bibr" target="#b5">[6]</ref> propose a unified CNN that can locate LPs and recognize them simultaneously in a single forward pass. In addition, the model size is highly decreased by sharing many of its convolutional features. Final Remarks: Many papers only address part of the ALPR pipeline (e.g., LP detection) or perform their experiments on datasets that do not represent real-world scenarios, making it difficult to accurately evaluate the presented methods. In addition, most of the approaches are not capable of recognizing LPs in real-time, making it impossible for them to be applied in some applications. In this sense, we employ the YOLO object detection CNNs in each stage to create a robust and efficient end-to-end ALPR system. In addition, we perform data augmentation for character recognition, since this stage is the bottleneck in some ALPR systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE UFPR-ALPR DATASET</head><p>The dataset contains 4,500 images taken from inside a vehicle driving through regular traffic in an urban environment. These images were obtained from 150 videos with duration of 1 second and frame rate of 30 Frames Per Second (FPS). Thus, the dataset is divided into 150 vehicles, each with 30 images with only one visible LP in the foreground. It is noteworthy that no stabilization method was used. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the diversity of the dataset.</p><p>The images were acquired with three different cameras and are available in the Portable Network Graphics (PNG) format with size of 1,920 ? 1,080 pixels. The cameras used were: GoPro Hero4 Silver, Huawei P9 Lite and iPhone 7 Plus. Images obtained with different cameras do not necessarily have the same quality, although they have the same resolution and frame rate. This is due to different camera specifications, such as autofocus, bit rate, focal length and optical image stabilization.</p><p>There are minor variations in the camera position due to repeated mountings of the camera and also to simulate a real condition, where the camera is not always placed in exactly the same position.</p><p>We collected 1,500 images with each camera, divided as follows: 900 of cars with gray LP, 300 of cars with red LP and 300 of motorcycles with gray LP. In Brazil, the LPs have size and color variations depending on the type of the vehicle and its category. Cars' LPs have a size of 40cm ? 13cm, while motorcycles LPs have 20cm ? 17cm. Private vehicles have gray LPs, while buses, taxis and other transportation vehicles have red LPs. There are other color variations for specific categories such as official or older cars. <ref type="figure" target="#fig_1">Fig. 2</ref> shows some of the different types of LPs found in the dataset. The dataset is split as follows: 40% for training, 40% for testing and 20% for validation, using the same protocol division proposed by Gon?alves et al. <ref type="bibr" target="#b7">[8]</ref> in the SSIG dataset. The dataset distribution was made so that each split has the same number of images obtained with each camera, taking into account the type and position of the vehicle, the color and the characters of the vehicle's LP, the distance of the vehicle from the camera (based on the height of the LP in pixels) such that each split is as representative as possible.</p><p>The heat maps of the distribution of the vehicles and LPs for the image frame in both SSIG and UFPR-ALPR datasets are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. As can be seen, the vehicles and LPs are much better distributed in our dataset. In Brazil, each state uses particular starting letters for its LPs which results in a specific range. In Paran? (where the dataset was collected), LPs range from AAA-0001 to BEZ-9999. Therefore, the letters A and B have many more examples than the others, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Every image has the following annotations available in a text file: the camera in which the image was taken, the vehicle's position and information such as: type (car or motorcycle), manufacturer, model and year; the identification and position of the LP, as well as the position of its characters. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the bounding boxes of different types of vehicle and LPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROPOSED ALPR APPROACH</head><p>This section describes the proposed approach and it is divided into four subsections, one for each of the ALPR stages (i.e., vehicle and LP detection, character segmentation and character recognition) and one for temporal redundancy. <ref type="figure" target="#fig_4">Fig. 5</ref> illustrates the ALPR pipeline, explained throughout this section.</p><p>We use specific CNNs for each ALPR stage. Thus, we can tune the parameters separately in order to improve the performance for each task. The models used are: Fast-YOLO, YOLOv2 and CR-NET <ref type="bibr" target="#b19">[20]</ref>, an architecture inspired by Fast-YOLO for character segmentation and recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Vehicle and LP Detection</head><p>We train two CNNs in this stage: one for vehicle detection in the input image and other for LP detection in the detected vehicle. Recent works <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref> also performed the vehicle detection first.</p><p>We evaluated both Fast-YOLO and YOLOv2 models at this stage to be able to handle simpler (i.e., SSIG) and more realistic (i.e., UFPR-ALPR) data. For simpler scenarios, the Fast-YOLO should be able to detect the vehicles and their LPs correctly in much less time. However, for more realistic scenarios it might not be deep enough to perform these tasks.</p><p>In order to use both YOLO models 3 , we need to change the number of filters in the last convolutional layer to match the number of classes. YOLO uses A anchor boxes to predict bounding boxes (we use A = 5) each with four coordinates (x, y, w, h), confidence and C class probabilities <ref type="bibr" target="#b10">[11]</ref>, so the number of filters is given by</p><formula xml:id="formula_0">f ilters = (C + 5) ? A.<label>(1)</label></formula><p>In a dataset such as the SSIG dataset, we intend to detect only one class in both vehicle and LP detection (first the car and then its LP), so the number of filters in each task has been reduced to 30. On the other hand, the UFPR-ALPR dataset includes images from cars and motorcycles (two classes), so the number of filters in the vehicle detection task must be 35. In our tests, the results were better when using two classes (instead of just one class called 'vehicle'). The Fast-YOLO's architecture used in both tasks is shown in <ref type="table" target="#tab_0">Table I</ref>. The same changes were made in the YOLOv2 model architecture (not shown due to lack of space). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Character Segmentation</head><p>Cropped Characters</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Frame Character Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABC-1234</head><p>LPs Patches While the entire frame and the vehicle coordinates are used as inputs to train the vehicle detection CNN, the vehicle patch (with a margin) and the coordinates of its LP are used to learn the LP detection network. The size of the margin is defined as follows. We evaluated, in the validation set, the required margin so that all LPs would be completely within the bounding boxes of the vehicles found by the vehicle detection CNN. This is done to avoid losing LPs in cases where the vehicle is not very well detected/segmented. By default, YOLO only returns objects detected with a confidence of 0.25 or higher. In the validation set, we evaluated the best threshold in order to detect all vehicles having the lowest false positive rate. A negative recognition result is given in cases where no vehicle is found. For LP detection we use threshold equal 0, as there might be cases where the LP is detected with very low confidence (e.g., 0.1). We keep only the detection with the largest confidence in cases where more than one LP is detected, since each vehicle has only one LP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple Frames</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABC-1234 ABG-1284</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Character Segmentation</head><p>Once the LP has been detected, we employ the CNN proposed by Montazzolli and Jung <ref type="bibr" target="#b19">[20]</ref> (CR-NET) for character segmentation and recognition. However, instead of performing both stages at the same time through an architecture with 35 classes (0-9, A-Z, where the letter O is detected jointly with the digit 0), we chose to first use a network to segment the characters and then another two to recognize them. Knowing that all Brazilian LPs have the same format: three letters and four digits, we use 26 classes for letters and 10 classes for digits. As pointed out by Gon?alves et al. <ref type="bibr" target="#b24">[25]</ref>, this reduces the incorrect classification.</p><p>The character segmentation CNN (architecture described in <ref type="table" target="#tab_0">Table II</ref>) is trained using the LP patch (with a margin) and the characters coordinates as inputs. As in the previous stage, this margin is defined based on the validation set to ensure that all characters are completely within its predicted LP.</p><p>The CNN input size (240 ? 80) was chosen based on the LP's ratio of Brazilian cars (3 ? 1), however the motorcycles LPs are nearly square (1.17 ? 1). That way, we enlarged horizontally all detected LPs (to 2.75 ? 1) before performing the character segmentation.</p><p>We also create a negative image of each LP, thereby doubling the number of training samples. Since the color of the characters in the Brazilian LPs depends on the category of the vehicle (e.g., private or commercial), the negative images simulate characters from other categories.</p><p>In some cases, more than 7 characters might be detected. If there are no overlaps (Intersection over Union (IoU) ? 0.25), we discard the ones with the lowest confidence levels. Otherwise, we perform the union between the overlapping characters, turning them into a single character. As motorcycle LPs can be very tilted, we use a higher threshold (IoU ? 0.75) to consider the overlap between its characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Character Recognition</head><p>Since many characters might not be perfectly segmented, containing missing parts, and as each character is relatively small, even one pixel difference between the ground truth and the prediction might impair the character's recognition. Therefore, we evaluate different padding values (1-3 pixels) in the segmented characters to achieve higher recognition rates. As <ref type="figure" target="#fig_5">Fig. 6</ref> illustrates, the more padding pixels the more noise information is added (e.g., portions of other characters or the LP frame). As previously mentioned, we use two networks for character recognition. For training these networks, the characters and their labels are passed as input. For digit recognition, we removed the first four layers of the character segmentation CNN, since in our tests the results were similar, but with a lower computational cost. However, for letter recognition (more classes and fewer examples) we still use the entire architecture of the character segmentation CNN. The networks for digit and letter recognition have 75 and 155 filters in the last convolutional layer, respectively (see Eq. 1).</p><p>The use of two networks allows the tuning of network parameters (e.g., input/output size) for each task. The best network sizes found in our experiments are 42 ? 26 ? 21 ? 13 and 270 ? 80 ? 33 ? 10 for digits and letters, respectively.</p><p>Having knowledge of the specific LP country layout (e.g., the Brazilian layout), we know which characters are letters and which are digits by their position. We sort the segmented characters by their horizontal and vertical positions for cars and motorcycles, respectively. The first three characters correspond to the letters and the last four to the digits, even in cases where the LP is considerably tilted. It is worth noting that a country (e.g., USA) might have different LP layouts, so this approach would not be suitable in such cases.</p><p>In addition to performing the training with the characters available in the training set, we also perform data augmentation in two ways. First, we create negative images to simulate characters from other vehicle categories (as in the character segmentation stage) and then, we also check which characters can be flipped both horizontally and vertically to create new instances. <ref type="table" target="#tab_0">Table III</ref> shows which characters can be flipped in each direction. As in the LP detection step, we use confidence threshold = 0 and consider only the detection with the largest confidence. Hence, we ensure that a class is predicted for every segmented character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Temporal Redundancy</head><p>After performing the LP recognition on single frames, we explore the temporal redundancy information through the union of all frames belonging to the same vehicle. Thus, the final recognition is composed of the most frequently predicted character at each LP position (majority vote).</p><p>Temporal information has already been explored previously in ALPR <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. In both studies, the use of majority voting has greatly increased recognition rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>In this section, we conduct experiments to verify the effectiveness of the proposed ALPR system. All the experiments were performed on a NVIDIA Titan XP GPU (3,840 CUDA cores and 12 GB of RAM) using the Darknet framework <ref type="bibr" target="#b26">[27]</ref>.</p><p>We consider as correct only the detections with IoU ? 0.5. This value was chosen based on previous works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>. In addition, the following parameters were used for training the networks: 80k iterations (max batches) and learning rate = [1 -3 , 1 -4 , 1 -5 ] with steps at 25k and 35k iterations.</p><p>Experiments were conducted in two datasets: SSIG and UFPR-ALPR. We report the results obtained by the proposed system and compare with previous work and two commercial systems <ref type="bibr" target="#b3">4</ref> : Sighthound <ref type="bibr" target="#b6">[7]</ref> and OpenALPR 5 <ref type="bibr" target="#b27">[28]</ref>. According to the authors, both are robust in the detection and recognition of Brazilian LPs.</p><p>It is important to emphasize that although the commercial systems were not tuned for these datasets, they use much larger private datasets, which is a great advantage especially in DL approaches.</p><p>In the OpenALPR system we choose which LP's style we want to detect (i.e., Brazilian) and we do not need to make any changes. On the other hand, Sighthound uses a single model for LPs from different countries. Therefore, we made some adjustments in its prediction so that it fits the Brazilian LPs format, such as swapping 0 by O and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation on the SSIG Dataset</head><p>The SSIG dataset <ref type="bibr" target="#b7">[8]</ref> is composed of 2,000 images of 101 vehicles with the following annotations: the position of the vehicle's LP, its identification (e.g., ABC-1234) and each character's position.</p><p>The high resolution images (1,920 ? 1,080 pixels) were acquired with a static digital camera and are available in the PNG format. A sample frame of the dataset is shown in <ref type="figure">Fig. 7</ref>.</p><p>The SSIG dataset uses the following evaluation protocol: 40% of the dataset to training, 20% to validation and 40% to test. According to the authors, this protocol was adopted because many character segmentation approaches do not require model estimation and a larger test set allows the reported results to be more statistically significant. <ref type="figure">Fig. 7</ref>. A sample frame of the SSIG dataset. It should be noted that there are vehicles in the background that do not have annotations. The LPs were blurred due to privacy constraints.</p><p>We report only the results obtained with the Fast-YOLO model in the vehicle and LP detection subsections, since it achieved impressive recall and precision rates in both tasks.</p><p>1) Vehicle Detection: Since the SSIG dataset does not have vehicle annotations, we manually label the vehicle's bounding box on each image of the dataset. Another possible approach would be to train a vehicle detector using the large-scale CompCars dataset <ref type="bibr" target="#b28">[29]</ref>, but that way many vehicles (including those in the background) would also be detected.</p><p>To perform the vehicle detection, we first evaluate different confidence thresholds. We started with confidence of 0.5, however some vehicles were not detected. All 407 vehicles in the validation set were successfully detected when the threshold was reduced to 0.25. Based on that, we decided to use half of this value (i.e., 0.125) in the test set to increase the chance that all vehicles are detected. With this threshold, we achieved a recall of 100% and precision above 99% (only 7 false positives).</p><p>2) LP Detection: Every vehicle in the validation set was well segmented with its LP completely within the predicted bounding box. Therefore, we use the vehicle patches without any margin to train the LP detection network. As expected, all LPs were correctly detected in both validation and test sets (recall and precision = 100%).</p><p>3) Character Segmentation: A margin of 5% (of the bounding box size) is required so each detected LP contains all its characters fully. Therefore, we double this value (i.e., 10%) in the test set and in the training of the character segmentation CNN.</p><p>We evaluated, in the validation set, the following confidence thresholds: 0.5, 0.25 and 0.1, but the recall achieved was 99.89%, regardless. Therefore, we chose to use a lower threshold (i.e., 0.1) in the test set to miss as few characters as possible. That way, we achieved 99.75% (5,614/5,628) recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Character Recognition:</head><p>The padding values that yielded the best recognition rates in the validation set were 2 pixels for letters and 1 pixel for digits. In addition, data augmentation with flipped characters only improved letter recognition, hampering digit recognition. We believe that a greater padding and data augmentation improve letter recognition because each class have far fewer training examples, compared to digits.</p><p>We first analyzed the results without temporal redundancy information. The proposed system achieved recognition rate of 85.45%, recognizing all three letters and all four digits in 86.32% and 98.63% of the time, respectively.</p><p>The results are greatly improved when taking advantage of temporal redundancy information. The final recognition rate is 93.53%, since the digits are correctly recognized in all vehicles and the letters in 93.53% of them. This result is given based on the number of frames correctly recognized, thereby vehicles with more frames have greater weight in the final result.</p><p>The recognition rates accomplished by the proposed system were considerably better than those obtained in previous works (81.8% ? 93.53%), as shown in <ref type="table" target="#tab_0">Table IV</ref>. As expected, the commercial systems have also achieved great recognition rates, but only the proposed system was able to recognize correctly at least 6 of the 7 characters in all LPs. This is particularly important since the LP's identification can be combined with the vehicle's manufacturer/model <ref type="bibr" target="#b29">[30]</ref> or its appearance <ref type="bibr" target="#b24">[25]</ref> to further enhance the recognition. According to our experiments, the great improvement in our system lies on separating the letter and digits recognition on two networks, so each one is tuned specifically for its task. Moreover, data augmentation was essential for letter recognition, since some classes (e.g., C, V) have less than 20 training examples.</p><p>In <ref type="table" target="#tab_4">Table V</ref>, we report the recall/accuracy rate achieved in each ALPR stage separately, as well as the time required for the proposed system to perform each stage. The reported time is the average time spent processing all inputs in each stage, assuming that the network weights are already loaded. Since the same model is used for vehicle and LP detection, the time required for both stages is very similar. The same is true for character segmentation and recognition, but the latter is performed 7 times (one time for each character). The average processing time for each frame was 21.31 seconds, an average of 47 FPS.</p><p>Our system had no difficulty recognizing red LPs, even with less training examples. According to our experiments, this is due to the negative images used in the training of the character segmentation and recognition CNNs. Due to the agreement terms of the SSIG dataset, we can not show qualitative results. Only a few LPs (all from the training set) can be shown for illustrations of publications.</p><p>B. Evaluation on the UFPR-ALPR Dataset 1) Vehicle Detection: We first evaluated the Fast-YOLO model, but the recognition rates achieved were not satisfactory. After evaluations with different confidence thresholds, the best recall rate achieved was 97.33%. This was expected since this dataset has greater variability in vehicle types and positions.</p><p>We chose to use the YOLOv2 model for vehicle detection, despite its higher computational cost. We evaluated several confidence thresholds, being 0.125 the best one, as in the SSIG dataset. The recall and precision rates achieved were 100% and 99%, respectively. <ref type="figure" target="#fig_6">Fig. 8</ref> shows a motorcycle and a car detected with the YOLOv2 model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) LP Detection:</head><p>We note that in more challenging images (usually of motorcycles), the vehicle's LP is not entirely within its predicted bounding box, requiring a small margin (5% in the validation set) so that the entire LP is completely within the predicted vehicle's bounding box. Therefore, we use a 10% margin in the test set and in the training of the LP detection CNN.</p><p>The recognition rates obtained by both YOLO models were very similar (less than half a percent difference). Thus, we use the Fast-YOLO model for LP detection. The recall rate attained was 98.33% (1,770/1,800). We were not able to detect the LP in just one vehicle (in its 30 frames), because a false positive was predicted with greater confidence than the actual LP, as shown in <ref type="figure">Fig. 9</ref>.</p><p>We could use the character segmentation CNN to perform a post-processing in cases where more than one LP is detected, for example: evaluate on each detected LP if there are 7 characters or consider only the LP where the characters' confidence is greater. However, since the actual LP can be detected with very low confidence levels (i.e., ? 0.1), many <ref type="figure">Fig. 9</ref>. A sample frame from the UFPR-ALPR dataset where the actual LP was not predicted with the highest confidence. The predicted position and ground truth are outlined in red and green, respectively. The LP was blurred due to privacy constraints. false negatives would have to be analyzed, increasing the overall computational cost of the system.</p><p>3) Character Segmentation: In the validation set, a margin of 10% is required so each detected LP contains all its characters fully. We decided not to double the margin in the test set, as 20% would add a considerable amount of noise and background in the LPs patches.</p><p>The recall obtained was 97.59% when disregarding the LPs not detected in the previous stage and 95.97% when considering the whole test set. We accomplished better results in the SSIG dataset, but it is worth noting that our dataset has different LPs types and many of them are tilted. <ref type="figure" target="#fig_0">Fig. 10</ref> depicts some LPs from different categories properly segmented, even when the LP is tilted or in presence of shadows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Character Recognition:</head><p>The best results were obtained with 1 pixel of padding and data augmentation, for both letters and digits. The proposed system achieved a recognition rate of 64.89% when processing frames individually and 78.33% (47/60 vehicles) with temporal redundancy.</p><p>Despite the great results obtained in the previous dataset, both commercial systems did not achieve satisfactory results in the UFPR-ALPR dataset. Analyzing the results we noticed that a substantial part of the errors were in motorcycles images, highlighting this constraint in both systems. This suggests that those systems are not so well trained for motorcycles. OpenALPR performed better than Sighthound, attaining a recognition rate of 70% when exploring temporal redundancy information. <ref type="table" target="#tab_0">Table VI</ref> shows all results obtained in the UFPR-ALPR dataset.</p><p>We report the recall/accuracy rate achieved in each ALPR stage separately in <ref type="table" target="#tab_0">Table VII</ref>, as well as the time required for the proposed system to perform each stage. The vehicle detection stage is more time-consuming in this dataset, as we use a larger CNN architecture (i.e., YOLOv2). It is worth noting that despite using a deeper CNN model in vehicle detection (i.e., YOLOv2), our system is still able to process images at 35 FPS (against 47 FPS using Fast-YOLO). This is sufficient for real-time usage, as commercial cameras generally record videos at 30 FPS. <ref type="figure" target="#fig_0">Fig. 11</ref> illustrates some of the recognition results obtained by the proposed system in the UFPR-ALPR dataset. It is noteworthy that our system can generalize well and correctly recognize LPs under different lighting conditions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we have presented a robust real-time endto-end ALPR system using the state-of-the-art YOLO object detection CNNs. We trained a network for each ALPR stage, except for the character recognition where letters and digits are recognized separately (with two distinct CNNs).</p><p>We also introduced a public dataset for ALPR that includes 4,500 fully annotated images (with over 30,000 LP characters) from 150 vehicles in real-world scenarios where both vehicle and camera (inside another vehicle) are moving. Compared to the largest Brazilian dataset (SSIG) for this task, our dataset has more than twice the images and contains a larger variety in different aspects.</p><p>At present, the bottleneck of ALPR systems is the character segmentation and recognition stages. In this sense, we performed several approaches to increase recognition rates in both stages, such as data augmentation to simulate LPs from other vehicle's categories and to increase characters with few instances in the training set. Although simple, these strategies were essential to accomplish outstanding results.</p><p>Our system was capable to achieve a full recognition rate of 93.53% (85.45% without temporal redundancy) in the SSIG dataset, considerably outperforming previous results (81.8% with temporal redundancy <ref type="bibr" target="#b24">[25]</ref> and 63.18% without <ref type="bibr" target="#b19">[20]</ref>) and presenting a performance slightly better than commercial systems (93.03%). In addition, the proposed system was the only to correctly recognize at least 6 characters in all LPs.</p><p>We also evaluated our proposed ALPR system and two commercial systems as baselines on the new dataset. The results demonstrated that the UFPR-ALPR dataset is very challenging since both commercial systems reached recognition rates below 70%. Our system performed better, with recognition rate of 78.33%. However, this result is still not satisfactory for some real-world ALPR applications.</p><p>As future work, we intend to explore new CNN architectures to further optimize (in terms of speed) vehicle and LP detection stages. We also intend to correct the alignment of inclined LPs and characters in order to improve the character segmentation and recognition. Additionally, we plan to explore the vehicle's manufacturer and model in the ALPR pipeline as our new dataset provides such information. Although our system was conceived and evaluated on two country-specific datasets from Brazil, we believe that the proposed ALPR system is robust to locate vehicle, LPs and alphanumeric characters from any other country. In this direction, aiming a fully robust system we just need to design a character recognition module that is independent of the LP layout.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Sample images of the UFPR-ALPR dataset. First three rows show the variety in backgrounds, lighting conditions, as well as vehicle/LP positions and types. Fourth row shows examples of vehicle and LP annotations. The LPs were blurred due to privacy constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Examples of the different LP types found in the UFPR-ALPR dataset. In Brazil, cars' LPs have 3 letters and 4 digits in the same row and motorcycles' LPs have 3 letters in one row and 4 digits in another.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Heat maps illustrating the distribution of vehicles and LPs in the SSIG and UFPR-ALPR datasets. The heat maps are log-normalized, meaning the distribution is even more concentrated than it appears.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>AFig. 4 .</head><label>4</label><figDesc>B C D E F G H I J K L M N O P Q R S T U V W X Y Letters distribution in the UFPR-ALPR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>An usual ALPR pipeline having temporal redundancy at the end.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Comparison of different values of padding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Examples of the detection obtained with the YOLOv2 model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>LPs from different categories properly segmented.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Qualitative results obtained by the proposed ALPR system in the UFPR-ALPR dataset. The first two rows shows examples of correctly detected and incorrectly recognized LPs, while the following rows show samples of LPs (from different categories) successfully recognized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I FAST15 detection Vehicle Detection LP Detection Temporal Redundancy Majority Vote ABG-1284 APC-7231 ABC-1234 NBC-1734 ABC-1234 License Plate Recognition Vehicles Patches</head><label>I</label><figDesc>-YOLO NETWORK USED IN BOTH VEHICLE AND LP DETECTION. THERE ARE EITHER 30 OR 35 FILTERS IN THE LAST CONVOLUTIONAL LAYER TO DETECT ONE OR TWO CLASSES, RESPECTIVELY.</figDesc><table><row><cell></cell><cell>Layer</cell><cell>Filters</cell><cell>Size</cell><cell>Input</cell><cell>Output</cell></row><row><cell>0</cell><cell>conv</cell><cell>16</cell><cell>3 ? 3/1</cell><cell>416 ? 416 ? 3</cell><cell>416 ? 416 ? 16</cell></row><row><cell>1</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 416 ? 416 ? 16</cell><cell>208 ? 208 ? 16</cell></row><row><cell>2</cell><cell>conv</cell><cell>32</cell><cell cols="2">3 ? 3/1 208 ? 208 ? 16</cell><cell>208 ? 208 ? 32</cell></row><row><cell>3</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 208 ? 208 ? 32</cell><cell>104 ? 104 ? 32</cell></row><row><cell>4</cell><cell>conv</cell><cell>64</cell><cell cols="2">3 ? 3/1 104 ? 104 ? 32</cell><cell>104 ? 104 ? 64</cell></row><row><cell>5</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 104 ? 104 ? 64</cell><cell>52 ? 52 ? 64</cell></row><row><cell>6</cell><cell>conv</cell><cell>128</cell><cell>3 ? 3/1</cell><cell>52 ? 52 ? 64</cell><cell>52 ? 52 ? 128</cell></row><row><cell>7</cell><cell>max</cell><cell></cell><cell>2 ? 2/2</cell><cell>52 ? 52 ? 128</cell><cell>26 ? 26 ? 128</cell></row><row><cell>8</cell><cell>conv</cell><cell>256</cell><cell>3 ? 3/1</cell><cell>26 ? 26 ? 128</cell><cell>26 ? 26 ? 256</cell></row><row><cell>9</cell><cell>max</cell><cell></cell><cell>2 ? 2/2</cell><cell>26 ? 26 ? 256</cell><cell>13 ? 13 ? 256</cell></row><row><cell>10</cell><cell>conv</cell><cell>512</cell><cell>3 ? 3/1</cell><cell>13 ? 13 ? 256</cell><cell>13 ? 13 ? 512</cell></row><row><cell>11</cell><cell>max</cell><cell></cell><cell>2 ? 2/1</cell><cell>13 ? 13 ? 512</cell><cell>13 ? 13 ? 512</cell></row><row><cell>12</cell><cell>conv</cell><cell>1024</cell><cell>3 ? 3/1</cell><cell>13 ? 13 ? 512</cell><cell>13 ? 13 ? 1024</cell></row><row><cell>13</cell><cell>conv</cell><cell>1024</cell><cell cols="2">3 ? 3/1 13 ? 13 ? 1024</cell><cell>13 ? 13 ? 1024</cell></row><row><cell>14</cell><cell>conv</cell><cell>30/35</cell><cell></cell><cell></cell><cell></cell></row></table><note>1 ? 1/1 13 ? 13 ? 1024 13 ? 13 ? 30/35</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II CHARACTER</head><label>II</label><figDesc>SEGMENTATION CNN, PROPOSED IN [20]. WE CHANGED THE NUMBER OF FILTERS IN THE LAST CONVOLUTIONAL LAYER TO 30, AS WE WANT TO FIRST SEGMENT THE CHARACTER (ONE CLASS).</figDesc><table><row><cell></cell><cell>Layer</cell><cell>Filters</cell><cell>Size</cell><cell>Input</cell><cell>Output</cell></row><row><cell>1</cell><cell>conv</cell><cell>32</cell><cell>3 ? 3/1</cell><cell>240 ? 80 ? 3</cell><cell>240 ? 80 ? 32</cell></row><row><cell>2</cell><cell>max</cell><cell></cell><cell cols="3">2 ? 2/2 240 ? 80 ? 32 120 ? 40 ? 32</cell></row><row><cell>3</cell><cell>conv</cell><cell>64</cell><cell cols="3">3 ? 3/1 120 ? 40 ? 32 120 ? 40 ? 64</cell></row><row><cell>4</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 120 ? 40 ? 64</cell><cell>60 ? 20 ? 64</cell></row><row><cell>5</cell><cell>conv</cell><cell>128</cell><cell>3 ? 3/1</cell><cell>60 ? 20 ? 64</cell><cell>60 ? 20 ? 128</cell></row><row><cell>6</cell><cell>conv</cell><cell>64</cell><cell cols="2">1 ? 1/1 60 ? 20 ? 128</cell><cell>60 ? 20 ? 64</cell></row><row><cell>7</cell><cell>conv</cell><cell>128</cell><cell>3 ? 3/1</cell><cell>60 ? 20 ? 64</cell><cell>60 ? 20 ? 128</cell></row><row><cell>8</cell><cell>max</cell><cell></cell><cell>2</cell><cell></cell><cell></cell></row></table><note>? 2/2 60 ? 20 ? 128 30 ? 10 ? 128 9 conv 256 3 ? 3/1 30 ? 10 ? 128 30 ? 10 ? 256 10 conv 128 1 ? 1/1 30 ? 10 ? 256 30 ? 10 ? 128 11 conv 256 3 ? 3/1 30 ? 10 ? 128 30 ? 10 ? 256 12 conv 512 3 ? 3/1 30 ? 10 ? 256 30 ? 10 ? 512 13 conv 256 1 ? 1/1 30 ? 10 ? 512 30 ? 10 ? 256 14 conv 512 3 ? 3/1 30 ? 10 ? 256 30 ? 10 ? 512 15 conv 30 1 ? 1/1 30 ? 10 ? 512 30 ? 10 ? 30 16 detection</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III THE</head><label>III</label><figDesc>CHARACTERS THAT CAN BE FLIPPED IN EACH DIRECTION TO CREATE NEW INSTANCES. WE ALSO USE THE NUMBERS 0 AND 1 AS TRAINING EXAMPLES FOR THE LETTERS O AND I, RESPECTIVELY.</figDesc><table><row><cell>Flip Direction</cell><cell>Characters</cell></row><row><cell>Vertical</cell><cell>0, 1, 3, 8, B, C, D, E, H, I, K, O, X</cell></row><row><cell>Horizontal</cell><cell>0, 1, 8, A, H, I, M, O, T, U, V, W, X, Y</cell></row><row><cell>Both</cell><cell>0, 1, 6(9), 8, 9(6), H, I, N, O, S, X, Z</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV RECOGNITION</head><label>IV</label><figDesc>RATES OBTAINED BY THE PROPOSED ALPR SYSTEM, PREVIOUS WORK AND COMMERCIAL SYSTEMS IN THE SSIG DATASET.</figDesc><table><row><cell>ALPR</cell><cell cols="2">? 6 characters All correct (vehicles)</cell></row><row><cell>Montazzolli and Jung [20]</cell><cell>90.55%</cell><cell>63.18%</cell></row><row><cell>Sighthound [7]</cell><cell>89.05%</cell><cell>73.13%</cell></row><row><cell>Proposed</cell><cell>99.38%</cell><cell>85.45%</cell></row><row><cell>OpenALPR [28]</cell><cell>92.66%</cell><cell>87.44%</cell></row><row><cell>Gon?alves et al. [25] (with redundancy)</cell><cell>?</cell><cell>81.80% (32/40)</cell></row><row><cell>Sighthound (with redundancy)</cell><cell>99.13%</cell><cell>89.80% (35/40)</cell></row><row><cell>OpenALPR (with redundancy)</cell><cell>95.77%</cell><cell>93.03% (37/40)</cell></row><row><cell>Proposed (with redundancy)</cell><cell>100.00%</cell><cell>93.53% (37/40)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V RESULTS</head><label>V</label><figDesc>OBTAINED AND THE COMPUTATIONAL TIME REQUIRED IN EACH ALPR STAGE IN THE SSIG DATASET. RECALL STANDS FOR DETECTION AND SEGMENTATION, AND ACCURACY STANDS FOR RECOGNITION.</figDesc><table><row><cell>ALPR Stage</cell><cell>Recall/Accuracy</cell><cell>Time (ms)</cell><cell>FPS</cell></row><row><cell>Vehicle Detection</cell><cell>100.00%</cell><cell>4.0746</cell><cell>245</cell></row><row><cell>License Plate Detection</cell><cell>100.00%</cell><cell>4.0654</cell><cell>246</cell></row><row><cell>Character Segmentation</cell><cell>99.75%</cell><cell>1.6555</cell><cell>604</cell></row><row><cell>Character Recognition</cell><cell>97.83%</cell><cell>1.6452 ? 7</cell><cell>87</cell></row><row><cell>ALPR (all correct) ALPR (with redundancy)</cell><cell>85.45% 93.53%</cell><cell>21.3119</cell><cell>47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI RECOGNITION</head><label>VI</label><figDesc>RATES OBTAINED BY THE PROPOSED ALPR SYSTEM AND COMMERCIAL SYSTEMS IN THE UFPR-ALPR DATASET.</figDesc><table><row><cell>ALPR</cell><cell cols="3">? 6 characters All correct (vehicles)</cell></row><row><cell>Sighthound [7]</cell><cell>62.50%</cell><cell>47.39%</cell><cell></cell></row><row><cell>OpenALPR [28]</cell><cell>54.72%</cell><cell>50.94%</cell><cell></cell></row><row><cell>Proposed</cell><cell>87.33%</cell><cell>64.89%</cell><cell></cell></row><row><cell>Sighthound (with redundancy)</cell><cell>76.67%</cell><cell cols="2">56.67% (34/60)</cell></row><row><cell>OpenALPR (with redundancy)</cell><cell>73.33%</cell><cell cols="2">70.00% (42/60)</cell></row><row><cell>Proposed (with redundancy)</cell><cell>88.33%</cell><cell cols="2">78.33% (47/60)</cell></row><row><cell></cell><cell>TABLE VII</cell><cell></cell><cell></cell></row><row><cell cols="4">RESULTS OBTAINED AND THE COMPUTATIONAL TIME REQUIRED IN EACH</cell></row><row><cell cols="4">STAGE IN THE UFPR-ALPR DATASET. RECALL STANDS FOR DETECTION</cell></row><row><cell cols="4">AND SEGMENTATION, AND ACCURACY STANDS FOR RECOGNITION.</cell></row><row><cell>ALPR Stage</cell><cell>Recall/Accuracy</cell><cell>Time (ms)</cell><cell>FPS</cell></row><row><cell>Vehicle Detection</cell><cell>100.00%</cell><cell>11.1578</cell><cell>90</cell></row><row><cell>License Plate Detection</cell><cell>98.33%</cell><cell>3.9292</cell><cell>255</cell></row><row><cell>Character Segmentation</cell><cell>95.97%</cell><cell>1.6548</cell><cell>604</cell></row><row><cell>Character Recognition</cell><cell>90.37%</cell><cell>1.6513 ? 7</cell><cell>87</cell></row><row><cell>ALPR (all correct) ALPR (with redundancy)</cell><cell>64.89% 78.33%</cell><cell>28.3011</cell><cell>35</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The UFPR-ALPR dataset is publicly available to the research community at https://web.inf.ufpr.br/vri/databases/ufpr-alpr/ subject to privacy restrictions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The entire ALPR system, i.e., the architectures and weights, is publicly available for academic purposes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For training YOLOv2 and Fast-YOLO we used convolutional weights pretrained on ImageNet<ref type="bibr" target="#b3">[4]</ref>, available at https://pjreddie.com/darknet/yolo/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">OpenALPR and Sighthound systems have Cloud APIs available at https:// www.openalpr.com/cloud-api.html and https://www.sighthound.com/products/ cloud, respectively. The results presented here were obtained on January, 2018.<ref type="bibr" target="#b4">5</ref> Although it has an open-source version, the commercial version uses different algorithms for OCR trained with larger datasets to improve accuracy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We thank the NVIDIA Corporation for the donation of the GeForce GTX Titan XP Pascal GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic license plate recognition (ALPR): A state-of-the-art review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Badawy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2013-02" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="311" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vehicle license plate recognition based on extremal regions and restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1096" to="1107" />
			<date type="published" when="2016-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segmentationand annotation-free license plate recognition with deep localization and failure identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bulan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kozitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shreve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2351" to="2363" />
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reading car license plates using deep convolutional neural networks and LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno>abs/1601.05610</idno>
		<ptr target="http://arxiv.org/abs/1601.05610" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards end-to-end car license plates detection and recognition with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno>abs/1709.08828</idno>
		<ptr target="http://arxiv.org/abs/1709.08828" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">License plate detection and recognition using deeply learned convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Ortiz</surname></persName>
		</author>
		<idno>abs/1703.07330</idno>
		<ptr target="http://arxiv.org/abs/1703.07330" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Benchmark for license plate character segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P G</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<idno>pp. 053 034-053 034</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatially supervised recurrent convolutional neural networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2017-05" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SqueezeDet: Unified, small, low power fully convolutional neural networks for real-time object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="446" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">License plate recognition from still images and video sequences: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N E</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Psoroulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Loumos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kayafas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="391" />
			<date type="published" when="2008-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Application-oriented license plate recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="552" to="561" />
			<date type="published" when="2013-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An iranian license plate recognition system based on color features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Ashtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Nordin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1690" to="1705" />
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time automatic license plate recognition for CCTV forensic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Elahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zafar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Edirisinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Real-Time Image Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="285" to="295" />
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accurate detection and recognition of dirty vehicle plate numbers for high-speed applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gholampour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="767" to="779" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A robust and efficient approach to license plate detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1102" to="1114" />
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic license plate detection in hazardous condition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Islam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="172" to="186" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time brazilian license plate detection and recognition using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Montazzolli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 30th SIBGRAPI Conference on Graphics, Patterns and Images</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust license plate detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ambikapathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vehicle license plate detection using region-based convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rafique</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vehicle license plate recognition with random convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chiachia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Falc?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J O</forename><surname>Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 27th SIBGRAPI Conference on Graphics, Patterns and Images</title>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="298" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CNN for license plate motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hradi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mar??k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zemc?k</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="page" from="3832" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">License plate recognition based on temporal redundancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="2577" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detecting, tracking and recognizing license plates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV</title>
		<editor>Y. Yagi, S. B. Kang, I. S. Kweon, and H. Zha</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Darknet: Open source neural networks in C</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<ptr target="http://pjreddie.com/darknet" />
		<imprint>
			<biblScope unit="page" from="2013" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Openalpr</forename><surname>Cloud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Api</forename></persName>
		</author>
		<ptr target="http://www.openalpr.com/cloud-api.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for fine-grained categorization and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="3973" to="3981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Recognizing cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dlagnekov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>San Diego</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science and Engineering, University of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
