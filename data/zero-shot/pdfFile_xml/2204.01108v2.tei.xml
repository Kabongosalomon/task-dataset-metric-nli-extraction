<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADJUSTING FOR BIAS WITH PROCEDURAL DATA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayan</forename><surname>Shesh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>02120</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">Bear</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>02120</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
							<email>ni.brown@northeastern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>02120</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ADJUSTING FOR BIAS WITH PROCEDURAL DATA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D software's are now capable of producing highly realistic images that look nearly indistinguishable from the real images. This raises the question: can real datasets be enhanced with 3D rendered data ? We investigate this question. In this paper we demonstrate the use of 3D rendered data, procedural, data for the adjustment of bias in image datasets. We perform error analysis of images of animals which shows that the misclassification of some animal breeds is largely a data issue. We then create procedural images of the poorly classified breeds and that model further trained on procedural data can better classify poorly performing breeds on real data. We believe that this approach can be used for the enhancement of visual data for any underrepresented group, including rare diseases, or any data bias potentially improving the accuracy and fairness of models. We find that the resulting representations rival or even out-perform those learned directly from real data, but that good performance requires care in the 3D rendered/procedural data generation. 3D image data set can be viewed as a compressed and organized copy of a real dataset, and we envision a future <ref type="bibr" target="#b18">[19]</ref> where more and more procedural data proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is available on our project page https://github.com/aiskunks/AI_Research/tree/main/Adjustin_bias_using_proced ural_data .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The last few years have seen great progress in the diversity and quality of 3D rendering and modeling software like Unreal Engine, Blender, SideFX Houdini and many more. 3D renderin g software can produce realistic samples of images and video that is shot a t any angle, in any lighting condition, in any pose, and in any environment. This raises an intriguing possibility: can we now enhance real data with 3D rendered data? <ref type="figure">Figure 1</ref>: Procedural data, 3D rendered data from game and 3D engines, has the potential to address some of computer visions most fundamental data issues If so, there would be immediate advantages. These 3D images are highly accurate have better lighting, angles and are compressed compared to the real datasets they represent and therefore easier to share and store. 3D image data also circumvents some of the concerns around privacy and usage rights that limit the distribution of real datasets <ref type="bibr" target="#b5">[6]</ref>, and rendered images and videos can further be edited to censor sensitive attributes <ref type="bibr" target="#b6">[7]</ref>. Perhaps because of these advantages, it is becoming increasingly common for 3D images and 3D image data to be shared online freely for anyone to use. This approach has been taken by individuals who may not have the resources or intellectual property rights to release the original data and to counter for rights/political issues.</p><p>Our work therefore targets a problem setting that has received little prior attention: given access to a 3D and game engines, and access to the real dataset with limited number of images and of low quality, can we learn effective visual representations and improve the accuracy of CNN classifiers to correctly classify the image class to which it belongs and in turn remove any unwanted bias in the model?</p><p>To this end, we provide an exploratory study of adjusting the bias in real data in the setting of 3D image data taken from various 3D rendering software's: we analyzed what is the accuracy of CNN classifier when trained only with real animal images in 5 categories namely bea r, sheep, dog, elephant and horse, how well they work, and how they can be improved to make use of the highquality 3D image data generated by 3D software. <ref type="figure">Figure.</ref>2 lays out the framework we study:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: Study framework</head><p>We took around 10,000 real images of 5 animal categories namely bear, dog, sheep, horse and elephant and trained a CNN to classify the unseen animal images to its correct categories. We then studied the accuracy of the CNN which is the misclassification rates of each animal category and ask under what conditions they can lead to enhanced accuracy. We then compared these results with the accuracy results of the same CNN but trained on the data after adding around 200 3D rendered images of bear category. Further, we studied the accuracy result of the same CNN trained on data which contained 3D rendered images of bear and sheep category (3D rendered images of sheep were added on top of 3D rendered images of bear) to identify if we can adjust the misclassification that is if we ca n adjust the bias using 3D rendered images and increase the overall accuracy of the CNN model. We asked: whether adding 3D rendered images helped to adjust the bias in data and if the model now can better classify poorly performing breeds on real data ? Our main findings are: 1. 3D image data i.e., the 3D rendered images can be used to adjust the bias in the data but properly matching the distribution of the real data with the 3D data can affect the results and needs further study.</p><p>2. These 3D image data can be combined with the real-world data to achieve better accuracy and lower misclassification in the CNN model than on the real-world data alone. 3. The lighting, quality and angles of the 3D rendered images plays a key role in adjusting the bias.</p><p>Tools that analyze the real data and suggest which poses and lighting conditions are needed for the 3D data is a crucial next step in this work. 4. 3D rendering software's can potentially produce an unbounded number of samples to train on; we find that performance improves by training on more samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Adding 3D image data may decrease the accuracy, we believe this happens when the rendered images are less representees of the real image distributions, but this requires further study. Tools that analyze the real data and suggest the parameters of the 3D images is a clear next step in this work.</p><p>Below tables summarize the accuracies of the model obtained by testing on 500 bootstrap samples containing 200 images of each animal.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Learning from 3D image data. Using 3D image data has been a prominent method for learning in different domains of science and engineering, with different goals including privacy-preservation and alternative sample generation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. In computer vision, 3D image data has been extensively used as a source for training models, for example in semantic segmentation <ref type="bibr" target="#b11">[12]</ref>, human pose estimation <ref type="bibr" target="#b10">[11]</ref> or self-supervised multitask learning <ref type="bibr" target="#b12">[13]</ref>. In most prior work, the 3D image data comes from a traditional simulation pipeline, e.g., via rendering a 3D world with a graphics engine, or recently a noise generator <ref type="bibr" target="#b13">[14]</ref>.</p><p>Animal Parsing. Though there exist large scale datasets containing animals for classification, detection, and instance segmentation, these datasets only cover a tiny portion of animal species in the world. Due to the lack of annotations, 3D image data has been widely used to address the problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. Similar to SMPL models <ref type="bibr" target="#b4">[5]</ref> for humans, <ref type="bibr" target="#b3">[4]</ref> proposes a method to learn articulated SMAL shape models for animals. Later, <ref type="bibr" target="#b2">[3]</ref> extracts more 3D shape details and is able to model new species. Unfortunately, these methods are built on manually extracted silhouettes and key point annotations. Recently, <ref type="bibr" target="#b1">[2]</ref> proposes to copy texture from real animals and predicts 3D mesh of animals in an end-to-end manner. Most related to our method is <ref type="bibr" target="#b0">[1]</ref>, where authors propose a method to estimate animal poses on real images using synthetic silhouettes. Different from <ref type="bibr" target="#b2">[3]</ref> which focuses on estimating animal poses, our strategy is to use 3D image data to adjust the bias in model classification for underrepresented animal breeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We consider the problem under unsupervised framework with two datasets. We name our real animal image dataset (XR) and 3D rendered animal image dataset (Xs) as the source dataset and real world animal images (different from the real world animal source dataset) as the target dataset Xt. The goal is to trian a. model f to predict categories/label for the target data Xt. We simply start with training our first model fR with real world data (XR) using Vgg16 architecture and transfer learning. Then we used the trained model to perform the prediction on the test dataset Xt. We then added our 3D image data set and trained the model f(S+R)1, f(S+R)2 in the same setting but on dataset that contained both 3D rendered images and real images. The 3D image data however was added one animal category each time for two animal categories, so at the end, we had 3 models: fR trained model with only real images of 5 animal categories, f(S+R)1 trained model with real images and 3D rendered images of bear and f(S+R)2 trained model with real images and 3D rendered images of bear and sheep. Finally, we compared accuracy of all the models and studied the accuracy on all animal categories. <ref type="table" target="#tab_3">Table 4</ref> displays the comparison results of all the three models   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We experiment on a single CNN with 3 training datasets as described in section 3 . We evaluate our model on the test image set which contained 1000 images belonging to 5 animal categories under study. 3D image datasets. We explain the details of our data generation parameters as follows. The virtual camera has a resolution of 640?480 and field of view of 90. We generated 200 images with random texture for bear and sheep animal categories.</p><p>Real image datasets. We explain the details of our data generation parameters as follows. The real images of the animals in the categories were taken from Imges.cv ( https://images.cv/ ) website and Kaggle animal dataset ( https://www.kaggle.com/datasets/alessiocorrado99/animals10 ). We took around 10000 images in 5 animal categories. We split the training set and validation set with a ratio of 4:1, resulting in 8,000 images for training and 2,000 for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We investigated how to reduce the bias in the dataset. 3D rendering software's make it possible to generate multiple views of similar image content with variety of angles, lighting and poses. These 3D rendered images can be used for training the model along with the real-world images to remove the bias and improve the accura cy. Our results demonstrate that leveraging 3D image data can improve accuracy beyond learning from real world images alone. However, this is not as simple as adding more pictures of bears, if a model is doing poorly on bear classification. Adding large quantities of 3D image data sometimes undermines the accuracy of categories which have some semblance amongst each other.</p><p>Tools need to be developed which suggest what types of 3D images need to be rendered is crucial. The next step in this research is to study why adding 3D image helps and when it can hurt so that 3D images can be generated that are specifically for that data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ETHICS STATEMENT</head><p>This work studies how to adjust the bias and learn useful image representations given data generated from 3D rendering software's as opposed to real data. This framework can further provide several societal advantages currently faced in real datasets, including protecting the privacy and usage rights of real images <ref type="bibr" target="#b5">[6]</ref>, or removing sensitive attributes <ref type="bibr" target="#b6">[7]</ref> and therefore learning fairer representations. At the same time, learning representations from 3D rendered images brings several challenges and risks, that need to be addressed. Rendered images can in some cases have the angles, lighting which are not the accurate representation of the real-world image. They can also amplify biases in datasets <ref type="bibr" target="#b14">[15]</ref> which can lead to negative societal impacts if they are not audit ed <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, or they are used in the wrong contexts. Making good use of the 3D image data will require addressing the above issues by studying mitigation strategies, as well as methods to audit implicit rendered images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX f (S+R)2 loss and accuracy</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3</head><label>3</label><figDesc>represents the visual comparison of the results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparison result for the 3 models *Note: fR = top_model_weights_no_3d (blue), f(S+R)1 = top_model_weights_with_3d (orange), and f(S+R)2 = top_model_weights_with_3d_2 (green)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :Figure 6 :Figure 7 :Figure 8 :Figure 9 :</head><label>456789</label><figDesc>Model Accuracy Model Loss f (S+R)1 loss and accuracy Model Accuracy Model Accuracy f R loss and accuracy Model Accuracy Model Accuracy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Confidence interval and accuracies of model fR, bootstrapped over 500 times</figDesc><table><row><cell>Model</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Confidence</figDesc><table><row><cell></cell><cell></cell><cell cols="2">interval and accuracies of model f(S+R)1, bootstrapped over 500 times</cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell>f (S+R)1</cell></row><row><cell cols="2">Animal/Stats min</cell><cell>mean max</cell><cell>Confidence interval (with 95% confidence)</cell></row><row><cell>bear</cell><cell cols="3">0.56 0.648 0.745 (0.6454438770018157, 0.6513561229981849)</cell></row><row><cell>dog</cell><cell cols="2">0.72 0.823 0.9</cell><cell>(0.8205027976389079, 0.8253572023610929)</cell></row><row><cell>elephant</cell><cell cols="3">0.725 0.813 0.89 (0.8100958715575248, 0.8150841284424766)</cell></row><row><cell>horse</cell><cell cols="3">0.58 0.664 0.75 (0.6612672056725515, 0.6668927943274474)</cell></row><row><cell>sheep</cell><cell cols="3">0.645 0.742 0.82 (0.7394166283453301, 0.7446033716546727)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Model</cell></row></table><note>Confidence interval and accuracies of model f(S+R)2, bootstrapped over 500 times</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison result for the mean of 3 model accuracies, bootstrapped over 500 times</figDesc><table><row><cell>Animal/Model f R</cell><cell>f (S+R)1</cell><cell>f (S+R)2</cell><cell></cell></row><row><cell>bear</cell><cell>0.5348</cell><cell>0.6484</cell><cell>0.64904</cell></row><row><cell>dog</cell><cell>0.87062</cell><cell>0.82293</cell><cell>0.86376</cell></row><row><cell>elephant</cell><cell>0.80166</cell><cell>0.81259</cell><cell>0.80783</cell></row><row><cell>horse</cell><cell>0.70566</cell><cell>0.66408</cell><cell>0.65252</cell></row><row><cell>sheep</cell><cell>0.68271</cell><cell>0.74201</cell><cell>0.75633</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Network Architecture .</head><label>Architecture</label><figDesc>We use Convolutional Neural Network with Leaky ReLU activation function and VGG16 architecture for our model. The initial step aims at creation of features with VGG16 model. Application of Image Processing along with Loading, Testing, Training, and Validating the dataset before the training step helps to remove the noise, obstacles, distortion and dirt from the images. The next step uses Convolutional Neural Network along with Leaky ReLU to train the model to accurately and precisely cla ssify animal classes. In order to avoid the problem of Dying ReLU, where some ReLU neurons essentially die for all inputs and remain inactive no matter what input is supplied, here no gradient flows and if large number of dead neurons are there in a neural. network its performance is affected. To resolve this issue, we make use of what is called Leaky ReLU, where slope is changed left of x=0 and thus causing a leak and extending the range of ReLU. We trained the model for 100 ephocs and used RMSPROP optimizer wih the learning rate of 1e-4  . After training the model, we graph the model's training and validation accuracy and loss to have insights about how well the model is trained. The accuracy and loss graphs for all the 3 models under study are provided in the Appendix.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Author Nicholas Bear Brown for their support. Special thanks to Shaunit Narayan Gupta for motivating me and encouraging me to continue work at nights as well.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Creatures great and SMAL: recovering the shape and motion of animals from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Biggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1811.05804</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Three-d safari: Learning to estimate zebra pose, shape, and texture from images&quot; in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><forename type="middle">Y</forename><surname>Berger-Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>abs/1908.07201</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lions and tigers and bears: Capturing non-rigid, 3d, articulated shape from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3955" to="3963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d menagerie: Modeling the 3d shape and pose of animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5524" to="5532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SMPL: a skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>248:1 -248:16</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating high -fidelit y synthetic patient data for assessing machine learning healthcare software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ylenia</forename><surname>Rotalinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puja</forename><surname>Myles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning generative adversarial representations (gap) under fairness and censoring constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachun</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lalitha</forename><surname>Sankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00411</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Synthetic observations from deep generative models and binary omics data with limited sample size. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nu?berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Boesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">Maria</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hess</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3D image data for small area estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sakshaug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trivellore E Raghunathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Privacy in Statistical Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="162" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evolving a psychophysical distance metric for generative design exploration of diverse shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahroz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkan</forename><surname>Gunpinar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Moriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nd Hiromasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mechanical Design</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast pose estimation with parameter sensitive hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>IEEE</publisher>
			<biblScope unit="page">750</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross-domain self-supervised multi-task feature learning using synthetic imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="762" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to see by looking at noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manel</forename><surname>Baradad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Imperfect imaganation: Implications of gans exacerbating biases on facial data augmentation and snapchat selfie lenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niharika</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Olmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sailik</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lydia</forename><surname>Manikonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subbarao</forename><surname>Kambhampati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09528</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Saving face: Investigating the ethical concerns of facial recognition auditing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Inioluwa Deborah Raji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="145" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Closing the ai accountability gap: Defining an end-to-end framework for internal algorithmic auditing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Inioluwa Deborah Raji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">N</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamila</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smith-Loud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parker</forename><surname>Theron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 conference on fairness, accountability, and transparency</title>
		<meeting>the 2020 conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Model cards for model reporting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zaldivar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vasserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><surname>Inioluwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on fairness, accountability, and transparency</title>
		<meeting>the conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05258</idno>
		<title level="m">Generative Models as a Data Source for Multiview Representation Learning</title>
		<imprint/>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
