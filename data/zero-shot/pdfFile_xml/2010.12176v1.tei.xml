<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Delving into the Cyclic Mechanism in Semi-supervised Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>See</surname></persName>
							<email>johnsee@mmu.edu.my</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
							<email>wylin@sjtu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Adobe Research</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Tencent Youtu Lab Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Multimedia University Selangor</orgName>
								<address>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Delving into the Cyclic Mechanism in Semi-supervised Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address several inadequacies of current video object segmentation pipelines. Firstly, a cyclic mechanism is incorporated to the standard semisupervised process to produce more robust representations. By relying on the accurate reference mask in the starting frame, we show that the error propagation problem can be mitigated. Next, we introduce a simple gradient correction module, which extends the offline pipeline to an online method while maintaining the efficiency of the former. Finally we develop cycle effective receptive field (cycle-ERF) based on gradient correction to provide a new perspective into analyzing object-specific regions of interests. We conduct comprehensive experiments on challenging benchmarks of DAVIS17 and Youtube-VOS, demonstrating that the cyclic mechanism is beneficial to segmentation quality. * equal contribution</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video object segmentation (VOS) is garnering more attention in recent years due to its widespread application in the area of video editing and analysis. Among all the VOS scenarios, semi-supervised video object segmentation is the most practical and widely researched. Specifically, a mask is provided in the first frame indicating the location and boundary of the objects, and the algorithm should accurately segment the same objects from the background in subsequent frames.</p><p>A natural solution toward the problem is to process videos in sequential order; this exploits the information from previous frames and guides the segmentation process in the current frame. In most practical scenarios, the video is processed in an online manner where only previous knowledge is available. Due to this reason, most state-of-the-art pipelines <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> follow a sequential order for segmentation in both training and inference stages. Ideally, if the masks predicted for intermediate frames are sufficiently accurate, they can provide more helpful object-specific features and position prior to segmentation. Besides, the existence of prediction errors at intermediate frames can be problematic -these masks can mislead the segmentation procedure in future frames. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates an example of such error propagation risk in sequential video object segmentation pipelines. As the algorithm is misled by another camel of similar appearance in the background, the segmented background camel will serve as erroneous guidance to future frames. Consequently the algorithm will gradually focus on both the foreground and background objects in upcoming new frames.</p><p>Based on these observations, in this paper, we propose to train and apply a segmentation network in cyclical fashion. In contrast to the predicted reference masks, the initial reference mask provided in the starting frame is always perfectly accurate and reliable (under semi-supervised mode). This inspires us to explicitly bridge the relationship between the initial reference mask and objective frame by taking the first reference mask as a measurement of prediction. This way, we can further refine the intermediate mask prediction and guide the network to learn more robust feature representation of cross frame correspondence, which is less prone to background distractors.</p><p>In this work, we apply a forward-backward data flow to form a cyclical structure, and train our segmentation network at both the objective frame and starting frame to help our model learn more robust correspondence relationship between predictions and the initial reference mask. Further to this, at the inference stage, we design a gradient correction module to selectively refine the predicted mask based on the gradient backward from the starting frame. In this way, we are able to naturally extend the offline trained model to an online scheme with marginal increase in time latency. Furthermore, we train our model under such cyclic consistency constraint without additional annotated data from other tasks. The trained models are evaluated in both online and offline schemes on common object segmentation benchmarks: DAVIS17 <ref type="bibr" target="#b9">[10]</ref> and Youtube-VOS <ref type="bibr" target="#b10">[11]</ref>, in which we achieve results that are competitive to other state-of-the-art methods while keeping the efficiency on par with most offline approaches.</p><p>Additionally, inspired by the process of gradient correction, we develop a new receptive field visualization method called cycle effective receptive field (cycle-ERF), which gradually updates an empty objective mask to show the strong response area w.r.t. the reference mask. In our experiments, we utilize the cycle-ERF to analyze how the cyclic training scheme affects the support regions of objects. This visualization method provides a fresh perspective for analyzing how the segmentation network extracts regions of interests from guidance masks.</p><p>In a nutshell, the contribution of this paper can be summarized as follows:</p><p>? We incorporate cycle consistency into the training process of a semi-supervised video object segmentation network to mitigate the error propagation problem. We achieved competitive results on mainstream benchmarks.</p><p>? We design a gradient correction module to extend the offline segmentation network to an online approach, which boosts the model performance with marginal increase in computation cost.</p><p>? We develop cycle-ERF, a new visualization method to analyze the important regions for object mask prediction which offers explainability on the impact of cyclic training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works 2.1 Semi-supervised video object segmentation</head><p>Semi-supervised video object segmentation has been widely researched in recent years with the rapid development of deep learning techniques. Depending on the presence of a learning process during inference stage, the segmentation algorithms can be generally divided into online methods and offline methods. OVOS <ref type="bibr" target="#b1">[2]</ref> is the first online approach to exploit deep learning for the VOS problem, where a multi-stage training strategy is design to gradually shrink the focus of network from general objects to the one in reference masks. Subsequently, OnAVOS <ref type="bibr" target="#b8">[9]</ref> improved the online learning process with an adaptive mechanism. MaskTrack <ref type="bibr" target="#b5">[6]</ref> introduced extra static image data with mask annotation and employed data synthesized through affine transformation, to fine-tune the network before inference. All of these online methods require explicit parameter updating during inference. Although high performance can be achieved, these methods are usually time-consuming with a real-time FPS of less than 1, rendering them unfeasible for practical deployment.</p><p>On the other hand, there are a number of offline methods that are deliberately designed to learn generalized correspondence feature and they do not require a online learning process during inference time. RGMP <ref type="bibr" target="#b0">[1]</ref> designed an hourglass structure with skip connections to predict the objective mask based on the current frame and previous information. S2S <ref type="bibr" target="#b10">[11]</ref> proposed to model video object segmentation as a sequence-to-sequence problem and proceeds to exploit a temporal modeling module to enhance the temporal coherency of mask propagation. Other works like <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4]</ref> resorted to using state-of-the-art instance segmentation or tracking pipeline <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> while attempting to design matching strategies to associate the mask over time. A few recent methods FEELVOS <ref type="bibr" target="#b6">[7]</ref> and AGSS-VOS <ref type="bibr" target="#b2">[3]</ref> mainly exploited the guidance from the initial reference and the last previous frame to enhance the segmentation accuracy with deliberately designed feature matching scheme or attention mechanism. STM <ref type="bibr" target="#b4">[5]</ref> further optimized the feature matching process with external feature memory and an attention-based matching strategy. Compared with online methods, these offline approaches are more efficient. However, to learn more general and robust feature correspondence, these data-hungry methods may require backbones pretrained on extra data with mask annotations from other tasks such as instance segmentation <ref type="bibr" target="#b13">[14]</ref> or saliency detection <ref type="bibr" target="#b14">[15]</ref>. Without these auxiliary help, the methods might well be disrupted by distractions from similar objects in the video, which then propagates erroneous mask information to future frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cycle consistency</head><p>Cycle consistency is widely researched in unsupervised and semi-supervised representation learning, where a transformation and its inverse operation are applied sequentially on input data, the consistency requires that the output representation should be close to the original input data in feature space. With this property, cycle consistency can be applied to different types of correspondence-related tasks. <ref type="bibr" target="#b15">[16]</ref> combined patch-wise consistency with a weak tracker to construct a forward-backward data loop and this guides the network to learn representative feature across different time spans. <ref type="bibr" target="#b16">[17]</ref> exploited the cycle consistency in unsupervised optical flow estimation by designing a bidirectional consensus loss during training. On the other hand Cycle-GAN <ref type="bibr" target="#b17">[18]</ref> and Recycle-GAN <ref type="bibr" target="#b18">[19]</ref> and other popular examples of how cyclic training can be utilized to learn non-trivial cross-domain mapping, yielding reliable image-to-image transformation across different domains.</p><p>Our method with cyclic mechanism is different from the works mentioned above in two main aspects. First, we incorporate cycle consistency with network training in a fully supervised manner, without requiring large amounts of unlabeled data as <ref type="bibr" target="#b15">[16]</ref>. Second, our cyclic structure is not only applicable during training, but also useful in the inference stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem formulation</head><p>Given a video of length T , X t is the t-th frame (t ? [1, T ]) in temporal sequential order, and Y t is its corresponding annotation mask. S ? is an object segmentation network parameterized by learnable weights ?. In terms of the sequential processing order of the video, the segmentation network should achieve the function as in Equation <ref type="formula" target="#formula_0">(1)</ref> below:</p><formula xml:id="formula_0">Y t = S ? (X t?1 , Y t?1 , X t ) t ? [2, T ]<label>(1)</label></formula><p>where Y t denotes the predicted object mask at t-th frame. X t?1 ? {X i |i ? [1, t ? 1]} is the reference frame set, which is a subset of all frames appearing before objective frame X t . Similarly, Y t?1 is a set containing reference object masks corresponding to the reference frames in X t?1 . However, in the semi-supervised setting, only the initial reference mask at the first frame is available. Therefore, in <ref type="figure">Figure 2</ref>: Overview of the proposed cyclic mechanism in both training and inference stages of the segmentation network. For simplicity, we take the situation where</p><formula xml:id="formula_1">X t?1 = {X 1 }, Y t?1 = {Y 1 }, X t = {X t } and Y t = { Y t } as an example.</formula><p>the inference stage, the corresponding predicted mask Y t is used as the approximation of the reference mask. Hence, we have</p><formula xml:id="formula_2">Y t?1 ? {Y 1 } { Y i |i ? [2, t ? 1]}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cycle consistency loss</head><p>For the sake of mitigating error propagation during training, we incorporate the cyclical process into the offline training process to explicitly bridge the relationship between the initial reference and predicted masks. To be specific, as illustrated in <ref type="figure">Figure 2</ref>, after obtaining the predicted output mask Y t at frame t, we construct a cyclic reference set for frames and mask set, respectively denoted as</p><formula xml:id="formula_3">X t ? {X i |i ? [2, t]}, Y t ? { Y i |i ? [2, t]}.</formula><p>With the cyclic reference set, we can obtain the prediction for the initial reference mask in the same manner as sequential processing:</p><formula xml:id="formula_4">Y 1 = S ? X t , Y t , X 1<label>(2)</label></formula><p>Consequently, we apply mask reconstruction loss (in <ref type="table">Equation 3</ref>) during supervision, optimizing on both the output t-th frame and the backward prediction Y 1 .</p><formula xml:id="formula_5">L cycle,t = L( Y t , Y t ) + L( Y 1 , Y 1 )<label>(3)</label></formula><p>In implementation, we utilize the combination of cross-entropy loss and mask IOU loss as supervision at both sides of the cyclic loop, which can be formulated as,</p><formula xml:id="formula_6">L( Y t , Y t ) = 1 |?| u?? (1 ? Y t,u ) log(1 ? Y t,u ) + Y t,u log( Y t,u ) ? ? u?? min( Y t,u , Y t,u ) u?? max( Y t,u , Y t,u )<label>(4)</label></formula><p>where ? denotes the set of all pixel coordinates in the mask while Y t,u and Y t,u are the normalized pixel values at coordinate u of the masks, ? is a hyperparameter to balance between the two loss components. It should also be noted that the cyclic mechanism in <ref type="figure">Figure 2</ref> indirectly applies data augmentation on the training data by reversing the input clips in temporal order, helping the segmentation network to learn more general feature correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Gradient correction</head><p>After training with the cyclic loss as Equation <ref type="formula" target="#formula_5">(3)</ref>, we can directly apply the offline model in the inference stage. However, inspired by the cyclic structure in training process, we can take the accurate initial reference mask as a measurement to evaluate the segmentation quality of current frame and proceed to refine the output results based on the evaluation results. In this way, we can explicitly reduce the effect of error propagation during inference time.</p><p>To achieve this goal, we design a gradient correction block to update segmentation results iteratively as illustrated in <ref type="figure">Figure 2</ref>. Since only the initial mask Y 1 is available in inference stage, we apply the predicted mask Y t to infer the initial reference mask in the same manner as Equation <ref type="formula" target="#formula_4">(2)</ref>, and we evaluate the segmentation quality of Y t with the loss function in Equation <ref type="formula" target="#formula_6">(4)</ref>. Intuitively, a more accurate prediction mask Y t will result in a smaller reconstruction error for Y 1 ; therefore, the gradient descent method is adopted to refine the mask Y t . To be specific, we start from an output mask Y 0 t = Y t , and then update the mask for N iterations:</p><formula xml:id="formula_7">Y l+1 t = Y l t ? ? ?L S ? {X t }, { Y l t }, X 1 , Y 1 ? Y l t (5)</formula><p>where ? is a predefined correction rate for mask update and N is the iteration times. With this iterative refinement, we naturally extend the offline model to an online inference algorithm. However, the gradient correction approach can be time-consuming since it requires multiple times of network forward-backward pass. Due to this reason, we only apply gradient correction once per K frames to achieve good performance-runtime trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Cycle-ERF</head><p>The cyclic mechanism with gradient update in Equation <ref type="formula">(5)</ref> is not only helpful for the output mask refinement, but it also offers a new aspect of analyzing the region of interests of specific objects segmented by the pretrained network. In detail, we construct a reference set as X l = {X l } and Y l = {0} as the guidance, where 0 denotes an empty mask of the same size as X l but is filled with zeros. We take these references to predict objects at the t-th frame Y t . To this end, we can obtain the prediction loss L( Y t , Y t ). To minimize this loss, we conduct the gradient correction process as in Equation <ref type="formula">(5)</ref> to gradually update the empty mask for M iterations. Finally, we take the ReLU function to preserve the positively activated areas of the objective mask as our final cycle-ERF representation.</p><formula xml:id="formula_8">cycle-ERF(Y l ) = ReLU Y M l<label>(6)</label></formula><p>As we will show in our experiments, the cycle-ERF is capable of properly reflecting the support region of specific objects for the segmentation task. Through this analysis, the pretrained model can be shown to be particularly concentrated on certain objects in video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment setup</head><p>Datasets. We train and evaluate our method on two widely used benchmarks for semi-supervised video object segmentation, DAVIS17 <ref type="bibr" target="#b9">[10]</ref> and Youtube-VOS <ref type="bibr" target="#b10">[11]</ref>. DAVIS17 contains 120 video sequences in total with at most 10 objects in a video. The dataset is split into 60 sequences for training, 30 for validation and the other 30 for test. The Youtube-VOS is larger in scale and contains more object categories. There are a total of 3,471 video sequences for training and 474 videos for validation in this dataset with at most 12 objects in a video. Following the training procedure in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, we construct a hybrid training set by mixing the data from two training sets. Furthermore, we also report the results of other methods with Youtube-VOS pretraining if available.</p><p>Metrics. For evaluation on DAVIS17 validation and test set, we adopt the metric following standard DAVIS evaluation protocol <ref type="bibr" target="#b9">[10]</ref>. The Jaccard overlap J is adopted to evaluate the mean IOU between predicted and groundtruth masks. The contour F-score F computes the F-measurement in terms of the contour based precision and recall rate. The final score is obtained from the average value of J and F. The evaluation on Youtube-VOS follows the same protocol except that the two metrics are computed on seen and unseen objects respectively and averaged together.</p><p>Baseline. We take the widely used Space Time Memory Network (STM) <ref type="bibr" target="#b4">[5]</ref> as our baseline model due to its flexibility in adjusting the reference sets X t and Y t . However, since the original STM model involves too much external data and results in unfair comparison, we retrain our implemented model with only the training data in DAVIS17 and Youtube-VOS in all experiments. In order to adapt   <ref type="table">Table 2</ref>: Comparison with state-of-the-art method on Youtube-VOS validation set. The subscript S and U denote the seen and unseen categories. G is the global mean. "-" indicates unavailable results.</p><p>to the time-consuming gradient correction process, we take the lightweight design by reducing the intermediate feature dimension, resizing the input to half of the original work and upsampling the output to original size by nearest interpolation. For ease of representation, we denote the one trained with cyclic scheme as "STM-cycle".</p><p>Implementation details. The training and inference procedures are deployed on an NVIDIA TITAN Xp GPU. Within an epoch, for each video sequence, we randomly sample 3 frames as the training samples; the frame with the smallest timestamp is regarded as the initial reference frame. Similar to <ref type="bibr" target="#b4">[5]</ref>, the maximum temporal interval of sampling increases by 5 every 20 training epochs. We set the hyperparameters as ? = 1.0, N = 10, K = 5, and M = 50. The Resnet50 <ref type="bibr" target="#b19">[20]</ref> pretrained on ImageNet <ref type="bibr" target="#b20">[21]</ref> is adopted as our backbone in baseline. The network is trained with a batch size of 4 for 240 epochs in total and is optimized by the Adam optimizer <ref type="bibr" target="#b21">[22]</ref> of learning rate 10 ?5 and ? 1 = 0.9, ? 2 = 0.999. In both training and inference stages, the input frames are resized to the resolution of 240 ? 427. The final output is upsampled to the original resolution by nearest interpolation. For simplicity, we directly use X t and Y t to construct the cyclic reference sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main results</head><p>DAVIS17. The evaluation results on DAVIS17 validation and test-dev set are reported in <ref type="table" target="#tab_1">Table 1</ref>  <ref type="table">Table 3</ref>: Ablation study on the effectiveness of different component. "GC" is short for gradient correction.  <ref type="table">Table 5</ref>: Comparison with low-quality reference mask on DAVIS17 validation set. methods and even performs better than the method with online learning <ref type="bibr" target="#b8">[9]</ref>. When combined with the online gradient correction process, our method further improves. In terms of the runtime speed, although gradient correction increases the computation cost, our method still runs at a speed comparable to other offline methods <ref type="bibr" target="#b2">[3]</ref> due to our efficient implementation. Although there is still a performance gap between our approach and the state-of-the-art online learning method <ref type="bibr" target="#b3">[4]</ref> and official STM <ref type="bibr" target="#b4">[5]</ref>, our method is far more efficient and it does not aggressively collect extra data from instance segmentation tasks as training samples. It should also be noticed that our cyclic scheme is not structure-specific and can be potentially complementary to a general video object segmentation framework, e.g. the offical STM model can get at most 1.9 J &amp;F gain when combined with gradient correction. This indicates that our scheme can potentially boost the performance of a general segmentation pipeline. The combination of our work and other VOS pipelines will be left as our future study.</p><formula xml:id="formula_9">X t?1 Y t?1 baseline +cycle ? {X 1 } {Y 1 } 65.2 67.6 +2.4 {X t?1 } { Y t?1 } 56.8 61.2 +4.4 {X 1 , X t?1 } {Y 1 , Y t?</formula><p>Youtube-VOS. The evaluation results on Youtube-VOS validation set are reported in <ref type="table">Table 2</ref>. On this benchmark, our model also outperforms some offline methods and their online learning counterparts <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8]</ref>. It is also noticeable that compared to the performance on seen objects, the one on unseen objects has improved more using our gradient correction strategy. Our final pipeline, however, performs slightly worse than <ref type="bibr" target="#b2">[3]</ref> on Youtube-VOS, but our model runs faster even when added with gradient correction. We think this could be due to the lesser average number of objects in this benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head><p>In this section, we conduct ablation studies to analyze the impact of different components in our method, with all the experiments performed on the DAVIS17 validation set.</p><p>Effectiveness of each component. We first demonstrate the effectiveness of cyclic training and gradient correction in <ref type="table">Table 3</ref>, where the baseline method <ref type="bibr" target="#b4">[5]</ref> is implemented and retrained by ourselves. From this table, both components are helpful in boosting the performance. In particular, the incorporated cycle mechanism improves the contour score F more than the overlap, signifying that the proposed scheme is likely to be more useful for fine-grained mask prediction. Improvement with different reference sets. Due to the flexibility of our baseline method in configuring its reference sets during inference, we tested how our cyclic training strategy impacts the performance using different reference sets. We conduct the test under four types of configuration: (1) Only the initial reference mask and its frame are utilized for predicting other frames. (2) Only the prediction of the last frame Y t?1 and the last frame are used. (3) Both the initial reference and last frame prediction are utilized, which is the most common configuration in other state-of-the-art works. (4) The external memory strategy (denoted as MEM) in <ref type="bibr" target="#b4">[5]</ref> is used where the reference set is dynamically updated by appending new prediction and frames at a specific frequency of 5Hz. In the results reported in <ref type="table">Table 4</ref>, we observe that the cyclic training is helpful under all configurations. It is also interesting to see that our scheme achieves the maximum improvement (+4.6 J &amp;F) with   Sensitivity analysis. Finally, we evaluate how the hyperparameters in our algorithm affect the final results. In <ref type="figure" target="#fig_1">Figure 3</ref>, we show the performance-runtime trade-off w.r.t. the correction iteration time N . We find that the J &amp;F score saturates when N approaches 10; above which, the score improvement is somewhat marginal but at the expense of decreasing efficiency. Therefore we take N = 10 as the empirically optimal iteration number for gradient correction. Additionally, we also analyze the impact of correction rate ? as shown in <ref type="figure" target="#fig_2">Figure 4</ref>. We find the performance variation is not sensitive to the change of correction rate ?, reflecting that our update scheme is robust and can accommodate variations to this parameter well.</p><p>Robustness to coarser reference. Additionally, We further investigate how gradient correction process mitigate the effect of low-quality reference masks. To do this, our model is running with the MEM strategy as <ref type="bibr" target="#b4">[5]</ref> by dynamically appending a predicted mask and its frame into the reference set. However, in this case, the predicted masks to be appended are manually replaced by a low-quality version. In our experiments, we take two adjustment schemes. (1) We replace the predicted mask Y t from baseline model on the same frame. This scheme is denoted as "baseline predict" (2) We replace the predicted mask Y t with a coarse level groundtruth mask where all pixels in the bounding box of objects are set to be 1. This scheme is denoted as "bounding box". For each scheme, we conduct another experiment with gradient correction on replaced masks before appending to the memory as the control group. From <ref type="table">Table 5</ref>, we see the gradient correction is helpful for both low-quality reference condition. Especially, the improvement is much more obvious under the case of "bounding box", this indicates that gradient correction is more helpful when the intermediate reference mask is coarser but properly covers the object area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative results</head><p>Segmentation results. In <ref type="figure" target="#fig_3">Figure 5</ref>, we show some segmentation results using the STM model trained with and without our cycle scheme. From comparison on the first sequences, we observe that the cyclic mechanism suppresses the accumulative error from problematic reference masks. From the second video, we see the cyclic model can depicts the boundary between foreground objects more precisely, which is consistent with the quantitative results. Further, our method can successfully segment some challenging small objects (caught by the left woman's hand). Readers can refer to our supplementary material for more qualitative comparison.</p><p>Cycle-ERF analysis. We further analyze the cycle-ERF defined as Equation <ref type="formula" target="#formula_8">(6)</ref> on different approaches. We take the initial mask as the objects to be predicted and take a random intermediate frame and an empty mask as reference. <ref type="figure" target="#fig_6">Figure 8</ref> visualizes the cycle-ERFs of some samples. Compared with baseline, our cyclic training scheme helps the network concentrate more on the foreground objects with stronger responses. This indicates that our model learns more robust object-specific correspondence. It is also interesting to see that only a small part of the objects is crucial for reconstructing the same objects that were in the initial frames as the receptive field focuses on the outline or skeleton of the objects. This can be used to explain the greater improvement of contour accuracy using our method, and also provide cues on the extraction from reference masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper incorporates the cycle mechanism with semi-supervised video segmentation network to mitigate the error propagation problem in current approaches. When combined with an efficient and flexible baseline, the proposed cyclic loss and gradient correction module achieve competitive performance-runtime trade-off on two challenging benchmarks. Further explanations can be drawn from a new perspective of cycle-ERF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>As we can foresee, with the development of 5G communication, video-based media industry will develop fast in the future. The advancement of accurate and fast semi-supervised segmentation will be helpful in modern video editing software and provide real-time online segmentation solution to stream media in video live applications. Consequently the online user experience can be improved. However, there also exists the risk that video segmentation technology is utilized in the scenario of illegal shoot and malicious edit, thus the personal privacy are more likely to be exposed and tracked. In our implementation, we set the cyclic reference set for training as X t = {X t }, Y t = { Y t } for simplicity. We compare this configuration with a more complicated cyclic reference set by combining all predicted masks and its corresponding frames appearing before the t-th frame as the cyclic reference set. The results on DAVIS17 validation set <ref type="bibr" target="#b9">[10]</ref> are shown in <ref type="table" target="#tab_4">Table 6</ref>. We see there is no distinctive difference between the performance of models trained under the two schemes. The model trained with more reference in the cycle achieves higher IOU with groundtruth, while the simplified training scheme results in better contour accuracy.</p><formula xml:id="formula_10">X t Y t J (%) F(%) J &amp;F(%) {X t } { Y t }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Qualitative Improvement with gradient correction</head><p>A more detailed comparison on gradient correction can be found in <ref type="figure" target="#fig_5">Figure 7</ref>. From the results, we observe that correction process can effectively suppress some false segmentation area and append segmentation of small part of objects. This observation indicates the gradient correction is beneficial to detailed segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Mask reconstruction with cycle-ERF</head><p>We go further to investigate the cycle-ERF in this section. From <ref type="figure" target="#fig_4">Figure 6</ref> in the main paper, we see the regions with high response intensity mainly focus on the outline of the objects in our method. In contrast, the cycle-ERF map of the baseline model is more dispersive, with some high response areas in the background. Since the cycle-ERF is obtained by the error minimization process of initial mask, this phenomenon indicates that the feature learnt by the baseline model is not robust enough thus incorrectly associate some background information with the foreground. To demonstrate this claim, we conduct an experiment by reconstructing the mask of the initial frame with the reference of selected frame and part of its cycle-ERF information.</p><p>Formally, we define the reference frame set as X l = {X l }, and define two different reference mask set,</p><formula xml:id="formula_11">Y ex l = {ReLU ( Y M l ) (1 ? Y l )} and Y in l = {ReLU ( Y M l ) Y l },</formula><p>where Y l is the groundtruth mask on frame X l , denotes elementwise production. Therefore, Y ex l denotes the cycle-ERF not covered by the specific objects and Y in l is the cycle-ERF inside the objects. In <ref type="figure" target="#fig_6">Figure 8</ref>, we show some qualitative reconstruction results and comparison with baseline methods on DAVIS17 validation set. We observe that when only the cycle-ERF out of objects are allowed as reference, the baseline can roughly segment the whole objects while ours can only recover a small part. In contrast, when only the cycle-ERF covered by specific objects is taken into account, our method performs better than baseline. This comparison results further demonstrate that baseline method extract more background information to help segment objects, while our model trained with cycle consistency learns more accurate and robust object-to-object correspondence to deal with VOS problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 More qualitative results and failure cases</head><p>In <ref type="figure">Figure 9</ref>, we show some additional qualitative segmentation results on DAVIS17 validation and test-dev set, showing that our method are suitable for multiple close objects and objects with fast and large motion. <ref type="figure" target="#fig_0">Figure 10</ref> shows some failure cases of our method, although combined with cyclic loss and gradient correction, the network can not handle extremely narrow and small objects (e.g. the brassie in the man's hand in the second row), meanwhile, as shown in the first row, our method can suffer from cases where the specified objects are severely occluded by obstacles in the foreground.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of error propagation risk during the inference time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Performance-runtime trade-off with different iteration size N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Performance with different correction rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results shows the improvement of cyclic training over the baseline. the configuration X t?1 = {X t?1 }, Y t?1 = { Y t?1 }, since this case is the most vulnerable to error propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Cycle-ERF of frames w.r.t. the initial reference object masks in DAVIS17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results of gradient correction, the right column shows the zoom up areas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of initial mask reconstruction results with different settings. The specified objects are car and goat respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Additional qualitative results on DAVIS17 validation and test-dev set Failure cases of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>%) FPS</cell></row></table><note>Comparison with state-of-the-art method on DAVIS17 validation and test-dev set. "Extra data" indicates the method is pretrained with extra data with mask annotations. "OL" denotes online learning or update process. "GC" is short for gradient correction.Method Extra data OL JS (%) JU (%) FS (%) FU (%) G(</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>{X i |i ? [2, t]} { Y i |i ? [2,t]} Comparison between different cyclic reference set for training Appendix A.1 Performance with different cyclic reference set</figDesc><table><row><cell>68.7</cell><cell>74.7</cell><cell>71.7</cell></row><row><cell>69.0</cell><cell>74.1</cell><cell>71.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Funding in direct support of this paper: China Major Project for New Generation of AI Grant (No.2018AAA0100400), National Natural Science Foundation of China (No. 61971277) and Adobe Gift Funding.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Agss-vos: Attention guided single-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dmm-net: Differentiable mask-matching network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Youtube-vos: Sequence-to-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gkioxari</forename><surname>Georgia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dollar</forename><surname>Piotr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girshick</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<meeting><address><addrLine>Z?rich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical image saliency detection on extended cssd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="717" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycleconsistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<meeting><address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recycle-gan: Unsupervised video retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rvos: End-to-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreu</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
