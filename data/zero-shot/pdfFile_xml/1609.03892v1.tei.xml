<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VIPLFaceNet: An Open Source Deep Face Recognition SDK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanglong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VIPLFaceNet: An Open Source Deep Face Recognition SDK</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Front.Comput.Sci</title>
					</monogr>
					<note>RESEARCH ARTICLE</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Face Recognition</term>
					<term>Open Source</term>
					<term>VIPLFaceNet</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robust face representation is imperative to highly accurate face recognition. In this work, we propose an open source face recognition method with deep representation named as VIPLFaceNet, which is a 10-layer deep convolutional neural network with 7 convolutional layers and 3 fully-connected layers. Compared with the well-known AlexNet, our VIPLFaceNet takes only 20% training time and 60% testing time, but achieves 40% drop in error rate on the real-world face recognition benchmark LFW. Our VIPLFaceNet achieves 98.60% mean accuracy on LFW using one single network. An open-source C++ SDK based on VIPLFaceNet is released under BSD license. The SDK takes about 150ms to process one face image in a single thread on an i7 desktop CPU. VIPLFaceNet provides a state-of-the-art start point for both academic and industrial face recognition applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Face recognition, as one of the typical problems in computer vision and machine learning, plays an important role in many applications, such as video surveillance, access control, computer-human interface and mobile entertainments <ref type="bibr" target="#b0">[1]</ref>. Generally speaking, a conventional face recognition system consists of four modules, face detection, face alignment, face representation and identity</p><p>Received month 02, 2016; accepted month 08, 2016 classification. In this pipeline, the key component for accurate face recognition is the third module, i.e. extracting the representation of an input face, which this paper mainly focuses on.</p><p>The main challenges of face representation lie in the small inter-person appearance difference caused by similar facial configurations, as well as the large intra-person appearance variations due to large intrinsic variations and diverse extrinsic imaging factors, such as head pose, expression, aging, and illumination. In the past decades, face representation is mostly based on hand-crafted local descriptors <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref> and shallow learning-based representation models <ref type="bibr" target="#b8">[9]</ref><ref type="bibr">[10]</ref><ref type="bibr" target="#b9">[11]</ref><ref type="bibr" target="#b10">[12]</ref><ref type="bibr" target="#b11">[13]</ref><ref type="bibr" target="#b12">[14]</ref>. As the development of deep learning technology, it becomes a more potent approach for face representation learning, especially in the real-word scenarios. Compared with the previous hand-crafted routine, deep face representation is learned in a data-driven style which can guarantee better performance as validated in <ref type="bibr" target="#b13">[15]</ref><ref type="bibr" target="#b14">[16]</ref><ref type="bibr" target="#b15">[17]</ref><ref type="bibr" target="#b16">[18]</ref><ref type="bibr" target="#b17">[19]</ref>. Taking the de-facto real-world face recognition benchmark LFW as an example, hand-crafted descriptor recorded 95.17% set by high-dimensional LBP <ref type="bibr" target="#b3">[4]</ref>, while 99.63% accuracy achieved by the latest deep FaceNet in <ref type="bibr" target="#b17">[19]</ref>.</p><p>In spite of many decades of research and development on face recognition, few open-source face recognition systems are publicly available yet. An open-source SDK with high accuracy in general scenarios is in great need for both academic research and industrial applications. So in this work, we meet this requirement and propose a deep face recognition model named as VIPLFaceNet, which is released as a BSD-license open source software with VIPLFaceNet is a powerful deep network for face representation with ten layers including 7 convolutional layers and 3 fully-connected layers. As a BSD-license open source software, VIPLFaceNet allows both academic research and industrial face recognition applications in different software and hardware platforms for free.</p><p>The contributions of this paper are summarized as follows: 1. We propose and release an open source deep face recognition model, VIPLFaceNet, with high-accuracy and low computational cost, which is a 10-layer deep convolutional neural network that achieves 98.60% mean accuracy on the real-world face recognition benchmark LFW.</p><p>2. We investigate the network architecture design and simplification. By careful design, VIPLFaceNet reduces 40% computation cost and cuts down 40% error rate on LFW compared with the AlexNet <ref type="bibr" target="#b18">[20]</ref>.</p><p>3. The VIPLFaceNet SDK code is written in pure C++ code under the BSD license. It is free and easy to be deployed in various software or hardware platforms for both academic research and industrial face recognition applications.</p><p>In summary, VIPLFaceNet is an open source deep face recognition SDK with high accuracy in general scenarios, which is built for facilitating the academic and industrial application of various real-world face recognition tasks. The rest of this paper is organized as follows. Section 2 presents the related works on face representation learning and introduce the face recognition benchmarks. Section 3 presents the network architecture design and technical details of our VIPLFaceNet. Section 4 conducts the experimental evaluation with comprehensive discussions and section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>In this section, we give a brief review of the related works on face representation learning. Moreover, we give a brief review of the face recognition benchmarks and discuss the performance evolution on the de-facto real-world face recognition benchmark LFW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Face Representation before Deep Learning</head><p>In the past decades, numerous hand-crafted local features were proposed for face representation, e.g.</p><p>Gabor wavelets <ref type="bibr" target="#b1">[2]</ref>, Local Binary Pattern (LBP) <ref type="bibr" target="#b2">[3]</ref> and its high (SIFT) <ref type="bibr" target="#b7">[8]</ref>, Histogram of Oriented Gradients (HOG) <ref type="bibr" target="#b4">[5]</ref>, patterns of oriented edge magnitudes (POEM) <ref type="bibr" target="#b5">[6]</ref>, Local Quantized Pattern (LQP) <ref type="bibr" target="#b6">[7]</ref> etc. However, designing an effective local descriptor demands considerable domain specific knowledge and a great deal of efforts.</p><p>Besides the hand-crafted local features, learning-based representation is also popular and reports promising accuracy. In <ref type="bibr" target="#b8">[9]</ref> and [10], filters are learned to maximize the discriminative power for face recognition. In <ref type="bibr" target="#b19">[21]</ref>, faces are represented from its responses to many pre-trained object filters. In <ref type="bibr" target="#b9">[11]</ref>, <ref type="bibr" target="#b10">[12]</ref>, <ref type="bibr" target="#b11">[13]</ref> and <ref type="bibr" target="#b20">[22]</ref>, codebook learning technologies are utilized for robust face representation. More recently, faces are represented with mid-level or high-level semantic information. For instance, the attributes and simile classifier <ref type="bibr" target="#b21">[23]</ref> represent faces by the mid-level face attributes and so-called simile feature. Tom-vs-Pete classifier <ref type="bibr" target="#b12">[14]</ref> encodes faces with high-level semantic information by the output scores of a large number of person-pair classifiers. Different from the deep learning approaches, the above methods are still shallow models and mostly rely on hand-crafted local features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Face Representation Learning</head><p>In recent years, deep learning methods are exploited to learn hierarchical representation and report state-of-the-art performance on LFW <ref type="bibr" target="#b13">[15]</ref><ref type="bibr" target="#b14">[16]</ref><ref type="bibr" target="#b15">[17]</ref><ref type="bibr" target="#b16">[18]</ref><ref type="bibr" target="#b17">[19]</ref><ref type="bibr" target="#b22">24]</ref>.</p><p>DeepFace is an early attempt of applying deep convolutional neural network in real-world face recognition. There are four highlights in DeepFace: 1) A 3D model based face alignment to frontalize facial images with large pose. 2) A very large scale training set with 4 million face images of 4,000 identities. 3) Deep convolutional neural network with the local connected layer that learns separate kernel for each spatial position. 4) A Siamese network architecture to learn deep metric based on the features of the deep convolutional network.</p><p>The DeepID <ref type="bibr" target="#b14">[16]</ref>, DeepID2 <ref type="bibr" target="#b15">[17]</ref> and DeepID2+ <ref type="bibr" target="#b16">[18]</ref> are a series of works, which provide a very good example of deep network evolution. In DeepID, 25 CNN networks are trained on each face patch independently. Besides, Joint Bayesian method <ref type="bibr" target="#b23">[25]</ref> is applied to learn robust face similarity metric. Finally, an ensemble of 25 deep networks achieve 97.45% mean accuracy on LFW. The DeepID2 introduces the joint identification and verification losses. The performance of DeepID2 on LFW is improved to 99.15%. The DeepID2+ just makes the network deeper and adds auxiliary loss signal embedding layer is also studied as sparse, selective and robust. The mean accuracy of DeepID2+ on LFW is 99.47% with 25 CNN models.</p><p>Learning face representation from scratch <ref type="bibr" target="#b22">[24]</ref> presents a semi-automatic way to collect face images from the internet and builds a large scale dataset CASIA-Web containing about 494,414 images of 10,575 subjects. Then a 13-layer deep network with 10 convolutional layers and 3 fully-connected layer is trained with joint identification and verification losses, reporting 97.73% accuracy on LFW.</p><p>Another promising deep neural network is FaceNet <ref type="bibr" target="#b17">[19]</ref> proposed by Google, which uses a super large scale face dataset containing 200 million face images of 8 million face identities to train a GoogLeNet network. Given such a large number of identities, the classical Softmax loss which needs the same number of 8 millon output nodes consumes too much GPU memory. So instead, a triplet loss which does not consume extra memory is introduced in FaceNet to directly optimize the embedding feature and achieves 99.63% mean accuracy on LFW.</p><p>All the above deep learning methods achieve quite promising face recognition accuracy on the challenging LFW dataset. The superiority demonstrates the superiority of the favorable feature learning ability of the deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evolution of Benchmarks</head><p>In early years, most datasets for face recognition were collected in controlled environment, e.g.</p><p>ORL <ref type="bibr" target="#b24">[26]</ref>, AR <ref type="bibr" target="#b25">[27]</ref>, FERET <ref type="bibr" target="#b26">[28]</ref>, PIE <ref type="bibr" target="#b27">[29]</ref>, FRGC <ref type="bibr" target="#b28">[30]</ref>, Extended-Yale-B <ref type="bibr" target="#b29">[31]</ref>, CAS-PEAL <ref type="bibr" target="#b30">[32]</ref> and MultiPIE <ref type="bibr" target="#b31">[33]</ref>. Among these datasets, AR is specially regarded as a benchmark to study occlusion robust face recognition. FERET, CAS-PEAL, PIE and MultiPIE are often used as general benchmarks to evaluate different factors of face recognition, such as aging, pose, expression, illumination, accessories etc. Yale-B is often cited as a lighting robust face recognition benchmark. Among these datasets, FRGC is widely used as a more challenging face recognition benchmark as it consist of over 50,000 images collected in varying lighting condition, e.g. atria, hallways, or outdoors.</p><p>In recent years, lots of real-world face datasets have been released, such as Labeled Face in the Wild <ref type="bibr" target="#b32">[34]</ref>, PubFig <ref type="bibr" target="#b21">[23]</ref>, CelebFaces <ref type="bibr" target="#b14">[16]</ref>, WDRef <ref type="bibr" target="#b23">[25]</ref>, SFC <ref type="bibr" target="#b13">[15]</ref>, CACD <ref type="bibr" target="#b33">[35]</ref>, WLFDB <ref type="bibr" target="#b34">[36]</ref>, CASIA-Web <ref type="bibr" target="#b22">[24]</ref>, MSRA-CFW <ref type="bibr" target="#b35">[37]</ref> etc. These datasets are built for different motivations. Among to train model for testing on LFW <ref type="bibr" target="#b32">[34]</ref>, but only CASIA-Web is a public dataset. WLFDB is a weakly labeled dataset without accurate identity annotation and acts as a search-based face tagging benchmark. CACD is built for studying cross-age face recognition, but only 10% of the subjects in CACD are manually annotated. The MSRA-CFW dataset is built for face retrieval evaluation and the face identity is automatically annotated by algorithms.</p><p>In the past several years, LFW has become the de-facto benchmark for real-world face recognition. According to the standard LFW protocol, the performance measurement should be the mean accuracy over 10-fold face verification task with each fold containing 300 inter-class and 300 intra-class face pairs. Besides the standard verification protocol, a face identification protocol is also available in <ref type="bibr" target="#b36">[38]</ref>. A brief history of the performance evolution of LFW is demonstrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Representative methods including LDML-MkNN <ref type="bibr" target="#b37">[39]</ref>, Multishot <ref type="bibr" target="#b38">[40]</ref>, LE <ref type="bibr" target="#b10">[12]</ref>, Associate-Predict <ref type="bibr" target="#b39">[41]</ref>, Tom-vs-Pete <ref type="bibr" target="#b12">[14]</ref>, Fisher Vector Face <ref type="bibr" target="#b20">[22]</ref>, High-dim LBP <ref type="bibr" target="#b3">[4]</ref>, TL Joint Bayesian <ref type="bibr" target="#b40">[42]</ref>, DeepFace <ref type="bibr" target="#b13">[15]</ref>, DeepID <ref type="bibr" target="#b14">[16]</ref>, DeepID2 <ref type="bibr" target="#b15">[17]</ref>, Gaussian Face <ref type="bibr" target="#b41">[43]</ref>, VGGFace <ref type="bibr" target="#b42">[44]</ref>, DeepID2+ <ref type="bibr" target="#b16">[18]</ref> and FaceNet <ref type="bibr" target="#b17">[19]</ref> are shown. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the evolution of face recognition techniques along with the accuracy increases: from early hand-crafted local features to shallow representation learning until recent deep representation learning. The first performance breakthrough is made by LDML-MkNN <ref type="bibr" target="#b37">[39]</ref>, which is a metric learning method, then the representation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed VIPLFaceNet</head><p>This section presents the details of our proposed VIPLFaceNet. Firstly, we introduce the network architecture design and simplification. Then, to accelerate the training of the deep network, we introduce the fast normalization layer. Finally, we present the technical details of face pre-processing and deep network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>The network architecture is the essential part of a deep model. Recently, some network architectures has been well recognized, such as AlexNet <ref type="bibr" target="#b18">[20]</ref>, GoogLeNet <ref type="bibr" target="#b43">[45]</ref> and VGGNet <ref type="bibr" target="#b44">[46]</ref>. Among them, the AlexNet is the simplest, with 5 convolutional layer and 3 fully-connected layers <ref type="bibr" target="#b18">[20]</ref>. Besides the convolutional layer and fully-connected layer, the ReLU layer and the dropout operation, proposed in AlexNet, build the basis of the latest deep convolutional neural networks. Another important component of AlexNet is its local response normalization layer (LRN) which can improve the generalization ability of AlexNet. Aiming to go deeper, GoogLeNet is designed to be a 22-layer deep network and introduces an inception structure which extracts 3 ? 3 convolutional kernel and the stride is always set to 1 <ref type="bibr" target="#b44">[46]</ref>, while the feature map size is reduced only by the pooling operation.</p><p>Among the above three networks, VGGNet is the slowest, while AlexNet is the simplest.</p><p>Design and simplification of VIPLFaceNet Network. Considering the success and efficiency of AlexNet, our network is designed by adapting AlexNet to incorporate some recent new findings. Compared with AlexNet, our VIPLFaceNet design has six main features: 1) we use 9 ? 9 size for the first convolutional layer rather than 11 ? 11, to reduce the computational cost. 2) We remove all local response normalization layers, as we found it unnecessary provided proper parameter initialization <ref type="bibr" target="#b45">[47]</ref>.</p><p>3) we decompose the second 5 ? 5 convolutional layer of AlexNet to two 3 ? 3 layers, inspired by He et al.'s work <ref type="bibr" target="#b46">[48]</ref>. 4) Specially, we remove all group structures in AlexNet as we exploit a more efficient way to do parallel training, i.e. asynchronous stochastic gradient descent <ref type="bibr" target="#b47">[49]</ref>. 5) Further, we reduce the number of feature maps in each layer and add one more convolutional layer. 6) The number of nodes in the FC2 fully-connected layer is reduced to 2,048 from 4,096 inspired by the experimental analysis in <ref type="bibr" target="#b48">[50]</ref>.</p><p>The above six features lead to our VIPLFaceNetFull network consisting of 7 convolutional layers followed by 3 fully-connected layers, as shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Its computational cost is almost 90% of AlexNet, which is still very high. To further reduce the computational cost, we simplified VIPLFaceNetFull to VIPLFaceNet by reducing in <ref type="table" target="#tab_0">Table 1</ref>. Eventually, the VIPLFaceNet network consumes only 60% computations of AlexNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fast Normalization Layer</head><p>As shown in LeCun et al.'s work <ref type="bibr" target="#b49">[51]</ref>, data normalization can speed up convergence, which is recently extended as the batch normalization algorithms <ref type="bibr" target="#b50">[52]</ref>. Inspired by these works, we also exploit a fast normalization layer in our VIPLFaceNet before the ReLU layer to speed up the convergence and improve the generalization.</p><p>Specifically, the fast normalization layer aims at normalizing the output of each network node to be of zero mean and unit variance. Unlike the batch normalization in <ref type="bibr" target="#b50">[52]</ref>, our fast normalization layer does not have the recovery operation and thus consumes less GPU memory and computation cost. Suppose the output of the network consists of C dimensions, and the normalization is applied to each dimension independently. Next we take the k-th dimension as an example for illustrating and omit k for simplicity. The k-th dimension of the network output for all N training samples in a mini-batch is denoted as B x = x 1 , x 2 , ..., x N . We denote the fast normalization layer (FNL) as:</p><p>FNL : x 1 , x 2 , ..., x N ?? 1 ,? 2 , ...,? N , ?i,? i ? N(0, 1), <ref type="bibr" target="#b0">(1)</ref> where N(0, 1) denotes the standard normal distribution with zero mean and unit variance. We present the detail of fast normalization layer (FNL) in Algorithm 1. In the algorithm, ? x is initialized as 0 and ? x is initialized as 1, and ? is the momentum value and set as 0.99 by default. In the test phase, ? x and ? x obtained in the final training stage are directly adopted.</p><p>During training, the fast normalization layer backpropagates the gradient using the chain rule as follows:</p><formula xml:id="formula_0">?L ?? = ? 1 2 N i=1 ?L ?? i (x i ? ?)? ?3/2 . ?L ?? = ( N i=1 ?L ?? i ?1 ? ? ) + ?L ?? ?2 N i=1 (x i ? ?) N . ?L ?x i = ?L ?? i 1 ? ? + ?L ?? 2(x i ? ?) N + 1 N ?L ?? .<label>(2)</label></formula><p>Additionally, as observed from extensive experiments, the dropout operation can be safely removed for deep network with fast normalization layer. It is observed that not only the  generalization ability is improved. In the experimental sections, we will validate the effectiveness of the fast normalization layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Technical Details</head><p>In all experiments, the face images are preprocessed with three steps including face detection, facial landmark localization and face normalization.</p><p>Face Detection: In face detection stage, we adopt the face detection toolkit developed by VIPL lab of CAS [53]. One can refer to <ref type="bibr" target="#b52">[54]</ref> for more details.</p><p>Facial Landmark Localization: We apply the Coarse-to-Fine Auto-Encoder Networks (CFAN) <ref type="bibr" target="#b53">[55]</ref> to detect the five facial landmarks in the face, i.e. the left and right center of the eyes, the nose tip, the left and right corner of mouth.</p><p>Face Normalization: As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the face image is normalized to 256 ? 256 pixels using five facial landmarks.</p><p>Training details In all experiments, for those deep networks without fast normalization layer , the base_lr is set as 0.01 and the learning rate is reduced following polynomial curve with gamma value equal to 0.5. For those deep networks with the fast normalization layer, the base_lr is set as 0.04. The momentum is set as 0.9 and the weight decay is set as 0.0005. All the experiments are conducted in Titan-X GPU with 12GB memory using a modified Caffe </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SDK Architecture</head><p>As mentioned previously, the source code of our VIPLFaceNet is released under the BSD license, and now available on https://github.com/seetaface. In this section, we introduce the highlights and architecture of the SDK for a better understanding of the source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Highlights of VIPLFaceNet SDK</head><p>This open-source SDK provides a powerful toolkit for testing and deploying face recognition applications, setting a good starting point for researchers and developer to experience the state-of-the-art face recognition technology. High-performance. The VIPLFaceNet achieves state-of-the-art performance on the LFW benchmark with only single network. Further performance improvement can be expected by using metric leaning approaches, e.g. Joint Bayesian method <ref type="bibr" target="#b23">[25]</ref> and MRMD <ref type="bibr" target="#b55">[57]</ref> or classifier ensemble approaches, e.g. LibD3C <ref type="bibr" target="#b56">[58]</ref>.</p><p>Object-oriented. The VIPLFaceNet SDK is designed from the beginning to be an object-oriented software, allowing easy extension to new network layers and any user-defined network architecture. It can also be easily integrated into industrial face recognition systems for various tasks.</p><p>Configurable Network Architecture. VIPLFaceNet SDK facilities the network architecture configuration saved in the model file using pre-defined format. VIPLFaceNet supports network architectures in the form of arbitrary directed acyclic graphs.</p><p>Upon initialization, VIPLFaceNet parses the network architecture from the model file and loads the network parameters into memory.</p><p>Pure C++ Code. The VIPLFaceNet is implemented in fully C++ code. It is very efficient to deploy VIPLFaceNet in multiple hardware platforms and operation systems.</p><p>Community Cooperation. We will put our source code in the GitHub and leverage the whole community to improve the SDK for better performance and flexibility. More features, such as python interface and PHP interface can be expected with the support of the whole community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>In this part, we will introduce the software architecture of the VIPLFaceNet. In <ref type="figure" target="#fig_3">Figure 3</ref>, the architecture of VIPLFaceNet SDK is presented. Main components in VIPLFaceNet SDK includes Blob, Command, Layer and Network Parser.</p><p>Blob. The blob is a container to hold the matrix in deep convolutional neural network. The Blob provides a mapping of the logic multi-dimensional matrix to physical one-dimensional array.</p><p>Command. The command is an interface that provides basic network elements, e.g. convolution, rectifier linear unit(ReLU), max pooling or mean pooling, and inner-product operation. As a SDK implementation for deployment, the implementation of the loss layers is unnecessary.</p><p>deep neural network. In VIPLFaceNet, a layer is composed by one or more commands, e.g. a convolutional layer is composed by a convolution command and a ReLU command.</p><p>Network Parser. To facilitate the definition of network architecture, VIPLFaceNet SDK sets up a network parser. A network is defined by multiple layers and organized as a directed acyclic graph. The SDK code does not need to be modified regardless the network architecture changes.</p><p>Matrix Multiplication Accelerating.</p><p>The single instruction multiple data (SIMD) instructions is used in VIPLFaceNet SDK to accelerate the matrix multiplication operation in both convolutional layer and fully-connected layer.</p><p>Meanwhile, VIPLFaceNet also supports the third-party linear algebra library, e.g. Armadillo [59].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Analysis</head><p>In this section, we compare the proposed VIPLFaceNet with the existing methods on the real-world face recognition benchmark LFW. Then, we discuss how VIPLFaceNet can be extended to other face recognition scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and Evaluation Protocol</head><p>The training set of our open-source VIPLFaceNet is the CASIA-Web dataset <ref type="bibr" target="#b22">[24]</ref>.</p><p>All the training data are pre-processed as illustrated in section 3.3. Totally, we have 479,777 detected and normalized facial images of 10,575 identities in the training set. The VIPLFaceNet is learned from scratch, and the weights of both the convolutional kernels and the fully-connected layers are initialized using the MSRA filler, i.e. Var[w] = 2/n , where n is the number of input neurons <ref type="bibr" target="#b45">[47]</ref>. The mean of training images are subtracted firstly. During training, face patches equal to crop_size ? crop_size pixel are randomly sampled from the input images and the images are also randomly flipped with 50% probability.</p><p>In all the experiments, the default crop_size is set as 227.</p><p>The evaluation dataset is Labeled Faces in the Wild dataset (LFW) <ref type="bibr" target="#b32">[34]</ref>, which contains 13,233 images of 5,749 identities.</p><p>For the standard 10-fold face verification experiment on LFW, we follow the unrestricted setting using external labeled data. Each fold of the test set consists of 300 inter-class and 300 intra-class face pairs. We also conducted an additional experiment following the face <ref type="table">Table 3</ref>: The performance of our VIPLFaceNet and stateof-the-art methods on LFW under the identification protocol <ref type="bibr" target="#b36">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Rank-1 DIR @ FAR = 1% COTS-s1 <ref type="bibr" target="#b36">[38]</ref> 56.70% 25.00% COTS-s1+s4 <ref type="bibr" target="#b36">[38]</ref> 66.50% 35.00% DeepFace <ref type="bibr" target="#b13">[15]</ref> 64.90% 44.50% WSTFusion <ref type="bibr" target="#b57">[60]</ref> 82.50% 61.90% AlexNet+FNL <ref type="bibr" target="#b18">[20]</ref> 89.26% 58.72% VIPLFaceNetFull+FNL 92.79% 68.13% VIPLFaceNet+FNL 91.95% 63.26%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results on LFW</head><p>In this part, we report the accuracy of VIPLFaceNet on LFW under both verification protocol and identification protocol. In all the experiments, we take the 2,048 outputs of the FC2 fully-connected layer as the representation of each face images and exploit the cosine function as the similarity metric between features.</p><p>Comparisons with the state-of-the-art methods. In <ref type="table">Table 2</ref>, we compare VIPLFaceNet with the state-of-the-art methods on LFW View 2 in terms of 10-fold mean accuracy. Using only single 10-layer deep convolutional neural network, our VIPLFaceNet achieves accuracy comparable to that of VGGFace <ref type="bibr" target="#b42">[44]</ref> with a 16-layer deep network and DeepID2+ with 25 deep networks, even though VIPLFaceNet is trained with less training data than DeepFace, VGGFace and FaceNet. VIPLFaceNet also reduces 40% computation cost with slight performance degradation. Compared with AlexNet, VIPLFaceNet cuts down 40% error rate on LFW (1.4% vs. 2.3%). The superiority of our method comes from the careful design of network architecture and simplification.</p><p>We further evaluate the VIPLFaceNet in the close-set and open-set face identification tasks, following the protocol in <ref type="bibr" target="#b36">[38]</ref>. The close-set identification protocol reports the Rank-1 identification accuracy and the open-set identification reports the detection and identification rate (DIR) at False Alarm Rate (FAR) equal to 1%. The comparisons with state-of-the-art methods are shown in <ref type="table">Table 3</ref>.</p><p>The proposed VIPLFaceNet also achieves state-of-the-art performance in the face identification tasks.</p><p>Comparisons of the crop size. The crop size is related to both the performance and computational cost of deep networks. Due to the random data argumentation, the smaller the crop size, the bigger randomness in training. In <ref type="table">Table 2</ref>: The performance of our VIPLFaceNet and state-of-the-art methods on LFW View2 under the verification protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy # of Network # of Training Images High-dim LBP <ref type="bibr" target="#b3">[4]</ref> 95.17% --Fisher Vector Face <ref type="bibr" target="#b20">[22]</ref> 93.03% --DeepFace <ref type="bibr" target="#b13">[15]</ref> 97.35% 3 4M DeepID <ref type="bibr" target="#b14">[16]</ref> 97.45% 25 200K DeepID2 <ref type="bibr" target="#b14">[16]</ref> 99.15% 25 200K Gaussian Face <ref type="bibr" target="#b41">[43]</ref> 98.52% --DeepID2+ <ref type="bibr" target="#b16">[18]</ref> 99.47% 25 290K DeepID2+(Single) <ref type="bibr" target="#b16">[18]</ref> 98.70% 1 290K WSTFusion <ref type="bibr" target="#b57">[60]</ref> 98.37% -10M VGGFace <ref type="bibr" target="#b42">[44]</ref> 98.95% 1 2.6M FaceNet <ref type="bibr" target="#b17">[19]</ref> 99.63% 1 200M AlexNet + FNL <ref type="bibr" target="#b18">[20]</ref> 97.70% 1 500K VIPLFaceNetFull + FNL 98.62% 1 500K VIPLFaceNet + FNL 98.60% 1 500K Evaluation of the Fast Normalization Layer. In <ref type="figure">Figure  4</ref>, we evaluate the effectiveness of the fast normalization layer (FNL) on LFW. As can be seen, adding FNL significantly improves the performance. Besides improving the generalization, FNL also significantly improves the convergence speed. By adding FNL, we can set the base learning rate as 0.04 and set the total echo as 15, while the baseline network needs 80 echoes for a good convergence. In <ref type="table" target="#tab_3">Table 5</ref>  <ref type="figure">Fig. 4</ref>: The mean accuracy of our VIPLFaceNet on LFW View2 with or without FNL. In the future, we would advocate the whole community to improve the SDK, e.g. supporting more language interface such as Python or PHP. Besides, we intend to build a VIPLFaceNet-based active face recognition development community and support more related application scenarios such as ID photo vs. real-word image verification, facial attribute analysis and age estimation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Evolution of the face recognition techniques on LFW from 2009 to 2015. In the figure, triangles with different color refer to what type of outside training data is used in the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 1 : 2 3: 4 :</head><label>1124</label><figDesc>Fast Normalization Layer (FNL) Input: DCNN Network and mini-batch B x Output: Normalized output for each sample in B x Calculate the batch mean: ? = 1 Calculate the normalized value:? i = x i ?? ? ? Update the global mean: ? x = ? * ? x + (1 ? ?) * ? 5: Update the global variance: ? x = ? * ? x + (1 ? ?) * ?. 6: return? i , i = 1, 2,..., N.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Example of face normalization using five points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>The VIPLFaceNet SDK Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Xin</head><label></label><figDesc>Liu recieved the B.S. degree at the Chongqing University, Chongqing, China, in June 2011. Currently, he is a Ph.D candidate at the Institute of Computing Technology, Chinese Academy of Sciences. His research interests include face recognition, image retrieval and deep learning. Meina Kan is an Associate Professor with the Institute of Computing Technology (ICT), Chinese Academy of Sciences (CAS). She received the Ph.D. degree from the University of Chinese Academy of Sciences (CAS), Beijing, China. Her research mainly focuses on Computer Vision especially face recognition, transfer learning, and deep learning. Wanglong Wu recieved the B.S. degree at the Beijing Jiaotong University, Beijing, China, in June 2014. Currently, he is a Ph.D candidate at the Institute of Computing Technology, Chinese Academy of Sciences. His research interests include face recognition and deep learning. Shiguang Shan received M.S. degree in computer science from the Harbin Institute of Technology, Harbin, China, in 1999, and Ph.D. degree in computer science from the Institute of Computing Technology (ICT), Chinese Academy of Sciences (CAS), Beijing, China, in 2004. He joined ICT, CAS in 2002 and has been a Professor since 2010. He is now the Deputy Director of the Key Lab of Intelligent Information Processing of CAS. His research interests cover computer vision, pattern recognition, and machine learning. He especially focuses on face recognition related research topics. He has published more than 200 papers in refereed journals and proceedings. Xilin Chen received the B.S., M.S., and Ph.D. degrees in computer science from the Harbin Institute of Technology, Harbin, China, in 1988, 1991, and 1994, respectively. He is a Professor with the Institute of Computing Technology, Chinese Academy of Sciences (CAS). He has authored one book and over 200 papers in refereed journals and proceedings in the areas of computer vision, pattern recognition, image processing, and multimodal interfaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of AlexNet<ref type="bibr" target="#b18">[20]</ref>, VIPLFaceNetFull and VIPLFaceNet. In the table, S denotes stride, G denotes group and Pad denotes padding. The ReLU layers are not shown in this table for the efficiency of presentation.</figDesc><table><row><cell>AlexNet</cell><cell></cell><cell>VIPLFaceNetFull</cell><cell>VIPLFaceNet</cell></row><row><cell cols="2">Conv1: 96x11x11, S:4, Pad:0</cell><cell>Conv1: 96x9x9, S:4, Pad:0</cell><cell>Conv1: 48x9x9, S:4, Pad:0</cell></row><row><cell>LRN</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>Pool1: 3x3,S:2</cell><cell></cell><cell>Pool1: 3x3,S:2</cell><cell>Pool1: 3x3,S:2</cell></row><row><cell cols="3">Conv2: 256x5x5, G:2, S:1, Pad:2 Conv2: 192x3x3,S:1, Pad:1</cell><cell>Conv2: 128x3x3,S:1, Pad:1</cell></row><row><cell>LRN</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>-</cell><cell></cell><cell>Conv3: 192x3x3,S:1, Pad:1</cell><cell>Conv3: 128x3x3,S:1, Pad:1</cell></row><row><cell>Pool2: 3x3,S:2</cell><cell></cell><cell>Pool2: 3x3,S:2</cell><cell>Pool2: 3x3,S:2</cell></row><row><cell cols="2">Conv3: 384x3x3, S:1, Pad:1</cell><cell>Conv4: 384x3x3,S:1, Pad:1</cell><cell>Conv4: 256x3x3,S:1, Pad:1</cell></row><row><cell cols="3">Conv4: 384x3x3, G:2, S:1, Pad:1 Conv5: 256x3x3,S:1, Pad:1</cell><cell>Conv5: 192x3x3,S:1, Pad:1</cell></row><row><cell>-</cell><cell></cell><cell>Conv6: 256x3x3,S:1, Pad:1</cell><cell>Conv6: 192x3x3,S:1, Pad:1</cell></row><row><cell cols="3">Conv5: 256x3x3, G:2, S:1, Pad:1 Conv7: 192x3x3,S:1, Pad:1</cell><cell>Conv7: 128x3x3,S:1, Pad:1</cell></row><row><cell>Pool3: 3x3,S:2</cell><cell></cell><cell>Pool3: 3x3,S:2</cell><cell>Pool3: 3x3,S:2</cell></row><row><cell>FC1, 4,096</cell><cell></cell><cell>FC1, 4,096</cell><cell>FC1, 4,096</cell></row><row><cell cols="2">Dropout1: dropout_ratio:0.5</cell><cell cols="2">Dropout1: dropout_ratio:0.5 Dropout1: dropout_ratio:0.5</cell></row><row><cell>FC2, 4,096</cell><cell></cell><cell>FC2, 2,048</cell><cell>FC2, 2,048</cell></row><row><cell cols="2">Dropout2: dropout_ratio:0.5</cell><cell cols="2">Dropout2: dropout_ratio:0.5 Dropout2: dropout_ratio:0.5</cell></row><row><cell>FC3, 10,575</cell><cell></cell><cell>FC3, 10,575</cell><cell>FC3, 10,575</cell></row><row><cell>improvement [4, 14, 22, 40].</cell><cell cols="2">Finally, deep learning</cell></row><row><cell cols="3">approaches reach the best results on LFW [15-19, 44].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 :</head><label>4</label><figDesc>The performance of our VIPLFaceNet with different crop size on LFW View2 under the verification protocol.</figDesc><table><row><cell cols="3">Network Architecture Crop Size Accuracy</cell></row><row><cell>VIPLFaceNet</cell><cell>256</cell><cell>98.12%</cell></row><row><cell>VIPLFaceNet</cell><cell>248</cell><cell>98.53%</cell></row><row><cell>VIPLFaceNet</cell><cell>227</cell><cell>98.60%</cell></row><row><cell>VIPLFaceNet</cell><cell>200</cell><cell>98.57%</cell></row><row><cell>VIPLFaceNet</cell><cell>180</cell><cell>98.21%</cell></row><row><cell cols="3">under the LFW verification protocol. It can be concluded</cell></row><row><cell cols="3">that a moderate crop size 227 yields the best performance.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, we compare the training time of VIPLFaceNet and VIPLFaceNet with FNL on the CASIA-Web training set. With the FNL, it only consumes 20% training time and only slightly increases the online test cost.Extension of the VIPLFaceNet. We can easily extend the VIPLFaceNet for more applications. For example, finetuning the VIPLFaceNet for other-domain applications such</figDesc><table><row><cell>100%</cell><cell></cell></row><row><cell>99%</cell><cell></cell></row><row><cell>98%</cell><cell></cell></row><row><cell>97%</cell><cell></cell></row><row><cell>96%</cell><cell></cell></row><row><cell>95%</cell><cell>Baseline</cell></row><row><cell>94%</cell><cell>With FNL</cell></row><row><cell>93%</cell><cell></cell></row><row><cell>92%</cell><cell></cell></row><row><cell>91%</cell><cell></cell></row><row><cell>90%</cell><cell></cell></row><row><cell>AlexNet</cell><cell>VIPLFaceNetFull VIPLFaceNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>The time cost of our VIPLFaceNet with or without FNL. The training time of VIPLFaceNet with fast normalization layer is reduced by 80%. By sticking a fast normalization layer to the ReLU layer, the training time is improved. On the real-world face recognition benchmark LFW, VIPLFaceNet achieves a mean accuracy of 98.60%, which is comparable to the state-of-the-art. A fully C++ implementation of the VIPLFaceNet SDK is released as an open source SDK under the BSD license. VIPLFaceNet can serve as a good start point for both academic research and industrial applications under various real-world face recognition scenarios.</figDesc><table><row><cell>Method</cell><cell cols="2">Training Time Test speed on CPU</cell></row><row><cell>AlexNet</cell><cell>67 hours</cell><cell>250ms / per image</cell></row><row><cell>VIPLFaceNetFull</cell><cell>60 hours</cell><cell>235ms / per image</cell></row><row><cell>VIPLFaceNet</cell><cell>40 hours</cell><cell>145ms / per image</cell></row><row><cell cols="2">VIPLFaceNetFull + FNL 12 hours</cell><cell>245ms / per image</cell></row><row><cell>VIPLFaceNet + FNL</cell><cell>8 hours</cell><cell>150ms / per image</cell></row><row><cell>6 Conclusions</cell><cell></cell><cell></cell></row></table><note>In this paper, we propose and release an open-source deep face recognition SDK with carefully designed network architecture and simplification.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work is partially supported by 973 Program under contract No. 2015CB351802, Natural Science Foundation of China under contracts Nos. 61402443, 61390511, 61379083, 61222211.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition: A literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="476" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blessing of dimensionality: Highdimensional feature and its efficient compression for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3025" to="3032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face recognition using hog-ebgm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Albiol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Monzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sastre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Albiol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1537" to="1543" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhanced patterns of oriented edge magnitudes for face recognition and image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N S</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caplier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face recognition using local quantized patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S U</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Napol?on</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machive Vision Conference</title>
		<meeting>British Machive Vision Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the use of sift features for face authentication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bicego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lagorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tistarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="35" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminant image filter learning for face recognition with local binary pattern like representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B C</forename><surname>Vemuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2512" to="2517" />
		</imprint>
	</monogr>
	<note>Trainable convolution filters and their application to face recognition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learned local gabor patterns for face representation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2333" to="2344" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face recognition with learningbased descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X O</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2707" to="2714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fusing robust face region descriptors via multiple metric learning for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3554" to="3561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tom-vs-pete classifiers and identitypreserving alignment for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolf</forename><forename type="middle">L</forename><surname>Deepface</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">O</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1265</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03832</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Everything is in the face? represent faces with object bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Asian Conference on Computer Vision Workshops</title>
		<meeting>Asian Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="180" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fisher vector faces in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machive Vision Conference</title>
		<meeting>British Machive Vision Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="365" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian face revisited: A joint formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="566" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Parameterisation of a stochastic model for human face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F S</forename><surname>Samaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Workshop on Applications of Computer Vision</title>
		<meeting>IEEE Workshop on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="138" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The ar face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVC Technical Report</title>
		<imprint>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The feret evaluation methodology for face-recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S A</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1090" to="1104" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The cmu pose, illumination, and expression (pie) database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bsat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="46" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Acquiring linear subspaces for face recognition under variable lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="684" to="698" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The cas-peal large-scale chinese face database and baseline evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics Part A System and Humans</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">149</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-pie. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="807" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><forename type="middle">M</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-age reference coding for ageinvariant face recognition and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="768" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Weakly labeled face databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y W D</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K Z J</forename><surname>Wlfdb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Finding celebrities in billions of web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="995" to="1007" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unconstrained face recognition: Identifying a person of interest from a media collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Best-Rowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on nformation Forensics and Security</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Is that you? metric learning approaches for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="498" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiple one-shots for utilizing class label information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machive Vision Conference</title>
		<meeting>British Machive Vision Conference</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An associate-predict model for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X O</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A practical transfer learning algorithm for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G Q</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3208" to="3215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Surpassing human-level face verification performance on lfw with gaussianface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.3840</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep face recognition. Proceedings of the British Machine Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Going deeper with convolutions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Q</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1710</idno>
		<title level="m">Convolutional neural networks at constrained time cost</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Asynchronous stochastic gradient descent for dnn training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6660" to="6663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficient</surname></persName>
		</author>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="9" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Internatioal Conference on Machine Learning</title>
		<meeting>Internatioal Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Visual information processing and learning (vipl) group in institute of computing technology (ict) chinese academy of sciences (cas)</title>
		<ptr target="53.www.vipl.ict.ac.cn" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Locally assembled binary (lab) feature with feature-centric cascade for fast and accurate face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Coarse-to-fine auto-encoder networks (cfan) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M N</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y Q</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A novel features ranking metric with application to scalable visual and bioinformatics data classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J C</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="346" to="354" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">LibD3C: ensemble classifiers with a clustering and dynamic selection strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="424" to="435" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Web-scale training for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2746" to="2754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deeply learned regressor and classifier for robust apparent age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M N</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agenet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision Workshops</title>
		<meeting>IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="258" to="266" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
