<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">X-Distill: Improving Self-Supervised Monocular Depth via Cross-Task Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cai</surname></persName>
							<email>hongcai@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm AI Research San Diego</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janarbek</forename><surname>Matai</surname></persName>
							<email>jmatai@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm AI Research San Diego</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhankar</forename><surname>Borse</surname></persName>
							<email>sborse@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm AI Research San Diego</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
							<email>yizhe.zhang.cs@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ansari</surname></persName>
							<email>amina@qti.qualcomm.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Qualcomm Technologies, Inc. San Diego</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
							<email>fporikli@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm AI Research San Diego</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">X-Distill: Improving Self-Supervised Monocular Depth via Cross-Task Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>CAI ET AL.: X-DISTILL 1 Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. Second and third authors contributed equally to this work Work done at Qualcomm AI Research</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel method, X-Distill, to improve the self-supervised training of monocular depth via cross-task knowledge distillation from semantic segmentation to depth estimation. More specifically, during training, we utilize a pretrained semantic segmentation teacher network and transfer its semantic knowledge to the depth network. In order to enable such knowledge distillation across two different visual tasks, we introduce a small, trainable network that translates the predicted depth map to a semantic segmentation map, which can then be supervised by the teacher network. In this way, this small network enables the backpropagation from the semantic segmentation teacher's supervision to the depth network during training. In addition, since the commonly used object classes in semantic segmentation are not directly transferable to depth, we study the visual and geometric characteristics of the objects and design a new way of grouping them that can be shared by both tasks. It is noteworthy that our approach only modifies the training process and does not incur additional computation during inference. We extensively evaluate the efficacy of our proposed approach on the standard KITTI benchmark and compare it with the latest state of the art. We further test the generalizability of our approach on Make3D. Overall, the results show that our approach significantly improves the depth estimation accuracy and outperforms the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate monocular depth estimation plays a critical role in 3D visual scene understanding and is of great importance for a variety of application domains, such as self-driving, AR/VR, and robotics. Thanks to the advancement of deep learning algorithms, recent years have seen considerable progress in this area <ref type="bibr" target="#b24">[25]</ref>. However, training accurate deep learning models in a supervised manner requires high-quality (e.g., dense and correctly aligned) ground-truth depth maps, which are difficult and costly to obtain.</p><p>In order to overcome this challenge, self-supervision has emerged as a new paradigm for training monocular depth estimation models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b44">45]</ref>. Since the inception of such self-supervised training, researchers have looked at various directions in order to further improve the depth estimation accuracy, such as designing more complex architectures <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>, improving the photometric matching <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref>, handling dynamic objects <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref>, utilizing edge information <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref>, multi-task learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref>, and exploiting temporal information <ref type="bibr" target="#b26">[27]</ref>.</p><p>Given the importance of visual scene understanding for depth estimation, researchers have recently started to study how to utilize semantic segmentation information to improve accuracy. In <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b43">44]</ref>, the authors use pretrained or jointly trained semantic segmentation networks to assist the depth network during both training and test. While such approaches can considerably improve accuracy, they incur significant extra computation during inference as they require running a separate and usually heavy-weight segmentation network. Another route is to incorporate the semantic information into the loss function, which only requires the extra computation of semantic information during training. One possible way is to include semantic segmentation as an auxiliary task, by co-training a semantic network and a depth network that share a set of layers <ref type="bibr" target="#b34">[35]</ref>. Other papers compare the semantic segmentations on both the warped and actual versions of a frame, and enforce a consistency regularization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39]</ref>. However, this requires running the segmentation network in every training iteration, which still incurs considerable overhead. In <ref type="bibr" target="#b45">[46]</ref>, the authors use the segmentation masks to explicitly regularize the edges on the depth map, but their approach requires semantic labels on the same dataset and introduces many additional hyper-parameters.</p><p>In this paper, we propose a novel cross-task knowledge distillation approach, X-Distill, to utilize semantic information to improve self-supervised monocular depth estimation. Given a pretrained semantic segmentation teacher network, our goal is to transfer the semantic knowledge from this teacher network to the depth network during training, in order to enhance the depth network's visual scene understanding capability. Note that our setting is different from the conventional knowledge distillation where the teacher and student networks share the same visual task. In our case, the outputs of the depth network and the semantic segmentation network are not directly comparable. In order to enable such crosstask distillation, we utilize a small neural network to connect segmentation and depth, by generating semantic segmentation based on the predicted depth. The resulting depth-based semantic segmentation is then supervised by the teacher network. The small network is trained together with the depth network and as such, allows backpropagation from the semantic segmentation teacher's supervision to the depth network.</p><p>In addition to enabling gradient flow across the two tasks, it is necessary to redesign the semantic classes such that they are compatible with the visual information in the depth map. In particular, the classes commonly used in semantic segmentation are usually too fine-grained for depth. For instance, road and sidewalk are typically treated as two separate classes in semantic segmentation. However, it is not necessary to treat them differently on the depth map since both of them are on the ground surface and have highly similar depth variation patterns in the field of view. As such, we regroup the objects based on their visual and geometric characteristics. This allows the depth network to distill the key depth-relevant semantic information, without introducing unnecessary difficulties to the learning process.</p><p>We next summarize our main contributions as follows:</p><p>? We propose a novel method, X-Distill, to exploit semantic information to improve selfsupervised monocular depth estimation. X-Distill enables the depth network to distill semantic knowledge in a cross-task manner from a segmentation teacher network dur-ing training. At inference time, the depth network then runs in a standalone manner, without requiring extra computation to process/generate semantic information. ? In order to make the semantic segmentation knowledge compatible with the visual information in depth, we regroup the semantic classes based on the visual and geometric characteristics of the objects. This allows the depth network to distill the key semantic knowledge while removing the unnecessary complexities in the learning. ? We evaluate our proposed approach on KITTI and Make3D, and compare it with the state of the art. We further conduct extensive ablation studies on our method. Overall, our proposed approach achieves considerably more accurate depth estimation, e.g., outperforming <ref type="bibr" target="#b8">[9]</ref> by 14% on KITTI (in terms of squared relative error).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Self-Supervised Monocular Depth Estimation: Due to the difficulty of collecting dense, high-quality ground-truth depth maps, researchers have proposed self-supervised training to obtain monocular depth estimation models. Such self-supervision leverages the geometric relationship among neighboring video frames <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b44">45]</ref> or between the left and right cameras in a stereo setting <ref type="bibr" target="#b9">[10]</ref>. While these methods provide a new way to train a depth network without labels, factors such as moving objects, occlusion, poor lighting, and low texture can considerably degrade their performance. Utilizing Semantic Information for Depth Estimation: Given the high correlation between semantic and depth information, researchers have studied how to incorporate semantic information to improve depth accuracy. One way is to run an additional (sub)network to generate semantic information at inference time, which can be fed to the depth network <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b43">44]</ref>. While this approach can considerably improve the depth estimation performance, it incurs significantly more computation. Other works include new loss functions during training, either via multi-task training <ref type="bibr" target="#b34">[35]</ref> or by enforcing segmentation consistency between the warped and real images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39]</ref>. These methods do not require extra semantic computation during test, but require running a semantic network at every training iteration, which still generates a considerable overhead. Knowledge Distillation: Knowledge Distillation is usually used to transfer the knowledge from a more complex model to a smaller model, where both of them are designed for the same visual task <ref type="bibr" target="#b11">[12]</ref>. Few papers have looked at knowledge distillation across two different visual tasks, e.g., classification tasks with non-overlapping classes <ref type="bibr" target="#b39">[40]</ref>, classification and text-to-image synthesis <ref type="bibr" target="#b41">[42]</ref>, RGB-based depth estimation and depth super resolution <ref type="bibr" target="#b33">[34]</ref>. None of the existing works has studied cross-task distillation from semantic segmentation to depth and we show how to enable it in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we present our novel take, X-Distill, on utilizing semantic segmentation to improve self-supervised monocular depth estimation, through cross-task distillation. In order to transfer the relevant knowledge from a semantic segmentation teacher network to the depth network during training, we use a small network to translate depth to segmentation, thus enabling gradient flow across the two visual tasks. In addition, we redesign the semantic classes to make them compatible with the visual information contained in depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Self-Supervised Monocular Depth Estimation</head><p>We utilize self-supervision to train a monocular depth estimation model, based on singleview video sequences <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b44">45]</ref>. training of a monocular depth network based on single-view videos. The blue block illustrates our proposed cross-task semantics-to-depth distillation. By utilizing a trainable depth-to-segmentation network to translate predicted depth to segmentation, we enable cross-task knowledge transfer from the pretrained segmentation teacher network to the depth network during training. In addition, we regroup the semantic classes such that they become compatible with the visual information in depth.</p><p>Geometric Modeling: Consider two neighboring video frames, I t and I s . Suppose that pixel p t ? I t and pixel p s ? I s are two different views of the same point of an object, then p t and p s are related geometrically as follows:</p><formula xml:id="formula_0">d(p s )h(p s ) = K(R t?s K ?1 d(p t )h(p t ) + t t?s ),<label>(1)</label></formula><p>where h(p) = [h, w, 1] denotes the homogeneous coordinates of a pixel p with h and w being its vertical and horizontal positions on the image, d(p) is the depth at p, K ? R 3?3 is the camera intrinsic matrix, and T t?s = [R t?s |t t?s ] ? R 3?4 is the 6DoF relative camera motion/pose from t to s, with R t?s ? R 3?3 and t t?s ? R 3?1 being the rotation matrix and translation vector. Given the depth map of I t , denoted by D t , and the relative camera pose from I t to I s , we can synthesize I t from I s based on Eq. 1, assuming that the 3D points captured in I t are also present in I s . We denote the synthesized/warped version of I t as I t . Self-Supervised Training: Suppose that the depth map and the relative camera pose are provided by a depth network and a pose network, respectively. By minimizing the difference between the warped and actual versions of I t , we can train these two networks. A common photometric loss function for comparing I t and I t is given by</p><formula xml:id="formula_1">L PH (I t , I t ) = ? I t ? I t 1 + (1 ? ?) 1 ? SSIM(I t , I t ) 2 ,<label>(2)</label></formula><p>where ? 1 denotes the L 1 norm and SSIM is the Structural Similarity Index Measure <ref type="bibr" target="#b37">[38]</ref>. Note that L PH is computed in a per-pixel manner.</p><p>It is common to further include a smoothness regularization to prevent drastic variations in the predicted depth map. Furthermore, in practice, not all the 3D points in I t can be found in I s , due to occlusion and objects (partially) moving out of the frame. Some objects can also be moving (e.g., cars), which is not considered in the geometric model of Eq. 1. In order to correctly measure the photometric loss and train the networks, it is a common practice to mask out the pixel points that violate the geometric model (see <ref type="bibr" target="#b8">[9]</ref> for more details on the masking techniques). <ref type="figure" target="#fig_0">Fig. 1 (gray block)</ref> illustrates the self-supervised training scheme of a monocular depth network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-Task Distillation from Semantics to Depth</head><p>Consider a depth network, f D , and a pretrained semantic segmentation network, f S . Our goal is to transfer the knowledge of the teacher network, f S , to the depth network, f D . However, unlike conventional knowledge distillation where teacher and student networks are used for the same visual task, f D and f S are used for two different tasks, and their outputs are not directly comparable. In other words, given an input, we cannot directly measure the difference between the outputs of f D and f S in order to generate a loss to train f D .</p><p>As such, we utilize a Depth-to-Segmentation (D2S) neural network, h D2S , to translate depth to semantic segmentation. Given the segmentation map generated from the predicted depth map, we are now able to construct a segmentation loss to distill semantic knowledge from f S to f D . More formally, the new loss term is given as follows:</p><formula xml:id="formula_2">L D2S (S D t , S t ) = H ? i=1 W ? j=1 L CE (S D t (i, j), S t (i, j)) HW ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">S D t = h D2S ( f D (I t ))</formula><p>is the semantic segmentation map generated by h D2S based on the predicted depth map D t = f D (I t ), S t is the semantic segmentation output generated by the semantic segmentation teacher network, L CE denotes the cross-entropy loss, and H and W are the height and width of the input image. <ref type="bibr" target="#b0">1</ref> The total loss is then given by</p><formula xml:id="formula_4">L Total = N s ? k=1 L PH, k + N s ? k=1 ? SM, k L SM, k + ? D2S L D2S ,<label>(4)</label></formula><p>where the self-supervised depth loss is computed over N s scales, L PH, k is the photometric loss at the k th scale, ? SM, k and L SM, k are the weight and loss for the smoothness regularization at the k th scale, and ? D2S is the weight of the cross-task distillation loss, L D2S . It can be seen that during training, h D2S is jointly trained with the depth network. This makes it possible for the pretrained teacher network to provide semantic supervision to the depth network, by backpropagating through h D2S . Our proposed approach is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, with the semantics-to-depth distillation module highlighted in the blue block.</p><p>For the depth-to-segmentation network, h D2S , we adopt a small architecture. More specifically, h D2S consists of two 3?3 convolutional layers, each followed by a BatchNorm layer and a ReLu layer, as well as a pointwise convolutional layer at the end which outputs the segmentation. Note that the h D2S should not be too complex, since a deeper network would take over too much of the learning load and weaken the knowledge flow to the depth network. As we shall see in our experiments in Sec. 4, while using a deeper h D2S can still increase the accuracy of the depth network, the improvement is not as significant as that by using our proposed smaller h D2S .</p><p>Once the training is finished, the depth network can then run in a standalone manner, without requiring any extra computation of semantic information during inference. Furthermore, our proposed distillation approach only adds a small amount of computation to training. More specifically, the segmentation maps from the teacher network only need to be computed once and the additional forward/backward passes are cheap since h D2S is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Depth-Compatible Grouping of Semantic Classes</head><p>Semantic segmentation usually contains much more fine-grained visual recognition information that is not present in the depth map. For instance, road and sidewalk are typically treated  as two different semantic classes. However, the depth map does not contain such classification information as both road and sidewalk are on the ground plane and have similar depth variations. As a result, it is not necessary to differentiate them on the depth map. On the other hand, the depth map does contain the information for differentiating certain classes. For instance, a road participant (e.g., pedestrian, vehicle) can be easily separated from the background (e.g., road, building) given the different patterns of their depth values. As such, is it necessary to reconsider the grouping of semantic classes, such that the key semantic information is preserved while the unnecessary complexity is removed from the distillation. <ref type="table" target="#tab_1">Table 1</ref> summarizes our new grouping, which results in four groups. <ref type="bibr" target="#b1">2</ref> In the first two groups, we have objects in the foreground. The respective foreground objects in these two groups are then further differentiated based on their shapes, where the first group contains thin structures, e.g., traffic lights/signs (including the poles), and the second group consists of people and vehicles which are of more general shapes. The third and fourth groups then contain the background objects, such as buildings, vegetation, road, and sidewalk. We further separate the ground plane (e.g., road and sidewalk) from the other background objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section (and also in the supplementary file), we present a comprehensive performance analysis on our proposed X-Distill approach and compare with the current state of the art. We furthermore conduct in-depth ablation studies on various aspects of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Datasets: We evaluate depth estimation on KITTI <ref type="bibr" target="#b7">[8]</ref> using the standard Eigen split <ref type="bibr" target="#b6">[7]</ref>, with two input resolutions, 640?192 and 1024?320. Following <ref type="bibr" target="#b44">[45]</ref>, we remove the static frames in the training set. There are 39,810, 4,424, and 697 samples for training, validation, and test. <ref type="bibr" target="#b1">2</ref> We focus on outdoor scenes in this paper and will consider an extension to indoor scenes as part of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Depth estimation on three sample images. The second row shows the estimated depth maps by Monodepth2 <ref type="bibr" target="#b8">[9]</ref> and the third row shows the depth maps by our proposed X-Distill approach. It can be seen that our method provides more accurate depth estimation. The green boxes indicate sample regions where our method considerably improves the estimation quality.</p><p>We use Cityscapes <ref type="bibr" target="#b4">[5]</ref> training and validation sets to train the segmentation teacher network. We further use Make3D <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> to evaluate the generalizability of our KITTI-trained model. Grouping of Semantic Classes: We group the Cityscapes classes according to our proposed scheme in <ref type="table" target="#tab_1">Table 1</ref>, such that they are compatible with the depth information. More specifically, we have 1) thin objects: poles and traffic lights/signs, 2) people and vehicles: persons, riders, cars, trucks, buses, motorcycles, bicycles, and trains, 3) background objects: buildings, walls, fences, vegetation, terrain, and sky, and 4) ground: road and sidewalk. Networks: For the depth network and the pose network, we use the ResNet-50 (RN50)based models in <ref type="bibr" target="#b8">[9]</ref>. The semantic segmentation teacher network is an HRNet <ref type="bibr" target="#b36">[37]</ref> with OCR <ref type="bibr" target="#b42">[43]</ref> and InverseForm <ref type="bibr" target="#b0">[1]</ref>. It has an mIoU of 85.6% on Cityscapes test set. During the self-supervised training of the depth network, this segmentation network is frozen. Hyperparameters: For the self-supervised part, we follow the hyperparameter setting in <ref type="bibr" target="#b8">[9]</ref>. For the semantics-to-depth distillation loss, L D2S , we linearly increase its weight from 0 to 0.005 during training. As we shall see in the ablation studies, this linear schedule can improve the training as compared to using a constant weight. Evaluation Metrics: We use the commonly used error metrics to evaluate the depth estimation performance, including the Absolute Relative Error (Abs Rel), Squared Relative Error (Sq Rel), Root Mean Squared Error (RMSE), and the RMSE of the log of the depth values. In addition, we use the classification metrics, ? 1 , ? 2 , and ? 3 , which measure whether the ratio between the ground-truth and predicted depth values is within a certain interval around 1. Mathematical definitions of these metrics can be found in the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We extensively compare our proposed approach with the latest state of the art (SOTA) on KITTI, including methods that 1) use more complex architectures <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>, 2) require additional computation of semantic information during inference <ref type="bibr" target="#b13">[14]</ref>, 3) utilize semantic information during training and do not incur extra computation during test <ref type="bibr" target="#b17">[18]</ref>, 4) propose better photometric matching <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref>, 5) utilize multiple frames <ref type="bibr" target="#b26">[27]</ref>, and 6) perform multitask learning <ref type="bibr" target="#b34">[35]</ref>. Note that we do not consider pretraining/online finetuning of the depth network or applying post-processing on the predicted depth maps. We also analyze both the depth estimation accuracy and computation efficiency of the methods. Furthermore, we test our KITTI-trained model on Make3D and compare it with the related SOTA to evaluate generalizability. Finally, we perform extensive ablation studies on our proposed approach. <ref type="table">Table 2</ref> shows the evaluation results on KITTI and comparison with the latest SOTA methods. It can be seen that our proposed X-Distill approach performs the best for most of the metrics. When our approach does not achieve the top-1 result, it is very close to the best number. <ref type="figure">Fig. 2</ref> shows sample prediction results of our proposed approach as compared to those by Monodepth2. It can be seen that Monodepth2 can predict inconsistent depth values on an object, which visually appear as missing parts on the depth map (e.g., see the missing upper part of the car in <ref type="figure">Fig. 2 (middle)</ref>). On the other hand, our approach provides more accurate and semantically more structured depth maps, thanks to its ability to better understand the semantics of the scene. For instance, it generates more structurally complete depth estimations for the biker in <ref type="figure">Fig. 2 (left)</ref> and for the cars in the middle and right figures (as indicated by the green boxes). Moreover, our approach is also able to capture the thin structures better. For instance, in <ref type="figure">Fig. 2 (right)</ref>, our model is able to generate a more clear depth estimation over the lamp post. This is because the thin objects are grouped as a  <ref type="table">Table 2</ref>: Performance evaluation on KITTI Eigen split. For methods that report performance for multiple models, we use the encoder to differentiate them (e.g., RN18 vs. RN50). Note that two architectures can be very different even if they use the same encoder (e.g., Monodepth2 <ref type="bibr" target="#b8">[9]</ref> vs. Johnston et al. <ref type="bibr" target="#b15">[16]</ref>). For each metric, the best (second best) results are in bold (underlined). We use ? to indicate methods that utilize semantic information during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Performance Evaluation Evaluation on KITTI:</head><p>class in the semantics-to-depth distillation, which encourages the depth network to learn to recognize these structures. Accuracy vs. Computation Efficiency: <ref type="figure" target="#fig_1">Fig. 3</ref> shows the accuracy, in terms of squared relative error, and the efficiency, in terms of GMAC (Multiply-Accumulate Operations in 10 9 ), of our proposed approach and the SOTA methods on KITTI. <ref type="bibr" target="#b2">3</ref> It can be seen that our trained model is able to achieve smaller depth estimation errors while using the same or less computation. We further show the performance of applying our cross-task distillation to an RN18-based model from <ref type="bibr" target="#b8">[9]</ref>. It can be seen that our method allows this smaller network to achieve an accuracy similar to PackNet (which uses 20? more computation). Depth Estimation on Center and Surrounding Areas: Since KITTI images are acquired with a wide-angle lens, we further evaluate the depth estimation performance on center and surrounding areas in the image. Specifically, we horizontally divide each image into 3 equal sections. The middle part is considered the center area and the left and right parts are surrounding areas. It can be seen in <ref type="table">Table 3</ref> that the depth estimation is much more accurate in the center area, for both Monodepth2 and X-Distill, since surrounding areas suffer from lens distortion/rectification artifacts. We note that for both areas, our proposed X-Distill consistently provides more accurate depth estimation as compared to Monodepth2.  <ref type="table">Table 3</ref>: Performance evaluation on center and surrounding image areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalizability on Make3D:</head><p>We evaluate the generalizability of our KITTI-trained model on Make3D (following the test setup in <ref type="bibr" target="#b8">[9]</ref>). It can be seen in <ref type="table" target="#tab_5">Table 4</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ablation Studies</head><p>Grouping Semantic Classes: In addition to our proposed grouping shown in <ref type="table" target="#tab_1">Table 1</ref>, we further test the baseline of using the original 19 Cityscapes classes without regrouping, as well as a more aggressive grouping method that only considers foreground and background objects. As shown in <ref type="table" target="#tab_7">Table 5</ref>, while the other two grouping baselines can also improve the depth estimation, the improvements are not as large as compared to our proposed method.  Complexity of Depth-to-Segmentation Network: As discussed in Sec. 3.2, the D2S network should be of a proper complexity such that it does not take away the learning from the depth network. As shown in <ref type="table" target="#tab_9">Table 6</ref>, by using a more complex D2S network (about 2? larger), the depth network gains a smaller improvement. We further test a baseline using a simple D2S network with one-layer pointwise convolution. This baseline does not perform well as the corresponding D2S network is too simple to translate depth to segmentation. Weighting Segmentation Loss: In our proposed approach, we adopt a linear weighting schedule to combine the segmentation distillation loss with the self-supervised depth loss. It can be seen in <ref type="table" target="#tab_10">Table 7</ref>, the linearly scheduled weight allows the depth network to achieve a higher depth estimation accuracy as compared to using a constant weight. We further vary   Applying X-Distill to Different Architectures: We apply our proposed approach to different depth networks, e.g., Monodepth2 <ref type="bibr" target="#b8">[9]</ref> with different encoders and HR-Depth <ref type="bibr" target="#b22">[23]</ref>. Specifically, for the encoder of Monodepth2, in addition to RN18 and RN50 that are used in the original paper, we also employ a recent backbone, DONNA, which is optimized for mobile processors via neural architecture search <ref type="bibr" target="#b25">[26]</ref>. This will demonstrate the efficacy of our method for practical mobile use cases. As can be seen in <ref type="table" target="#tab_12">Table 8</ref>, our proposed X-Distill considerably improves the depth estimation accuracy for all these different depth networks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we presented a novel cross-task distillation approach, X-Distill, to improve the self-supervised training of monocular depth by transferring semantic knowledge from a teacher segmentation network to the depth network. In order to enable such cross-task distillation, we utilized a small, trainable network that translates the predicted depth map to a semantic segmentation map, which the semantic teacher network can then supervise. This enables the backpropagation from the semantic teacher's supervision to the depth network during training. We further studied the visual and geometric characteristics of the objects and designed a new way of grouping them that can be shared by both tasks. We evaluated our proposed approach on KITTI and Make3D, and conducted extensive ablation studies. The results show that by training with the proposed cross-task distillation, we can significantly improve the depth estimation accuracy and outperform the state of the art without incurring additional computation during inference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our proposed X-Distill approach. The gray block describes the self-supervised</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Accuracy (in squared relative error) vs. computation efficiency (in GMAC log-scale).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Depth-compatible semantic class grouping for outdoor scenes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>that our model significantly outperforms other SOTA self-supervised methods on this dataset.</figDesc><table><row><cell>Method</cell><cell>Supervision</cell><cell>Abs Rel</cell><cell cols="2">Lower is Better Sq Rel RMSE</cell><cell>RMSE Log</cell></row><row><cell>Karsch [17]</cell><cell>GT</cell><cell>0.428</cell><cell>5.079</cell><cell>8.389</cell><cell>0.149</cell></row><row><cell>Liu [21]</cell><cell>GT</cell><cell>0.475</cell><cell>6.562</cell><cell>10.05</cell><cell>0.165</cell></row><row><cell>Laina [20]</cell><cell>GT</cell><cell>0.204</cell><cell>1.840</cell><cell>5.683</cell><cell>0.084</cell></row><row><cell>Monodepth [10]</cell><cell>S</cell><cell>0.544</cell><cell>10.94</cell><cell>11.760</cell><cell>0.193</cell></row><row><cell>Zhou et al. [45]</cell><cell>M</cell><cell>0.383</cell><cell>5.321</cell><cell>10.470</cell><cell>0.478</cell></row><row><cell>DDVO [36]</cell><cell>M</cell><cell>0.387</cell><cell>4.720</cell><cell>8.090</cell><cell>0.204</cell></row><row><cell>Monodepth2 [9] (RN18)</cell><cell>M</cell><cell>0.322</cell><cell>3.589</cell><cell>7.417</cell><cell>0.163</cell></row><row><cell>X-Distill (ours)  ?</cell><cell>M</cell><cell>0.308</cell><cell>3.122</cell><cell>7.015</cell><cell>0.158</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance evaluation on Make3D. GT indicates that the method is trained with groundtruth Make3D depth annotations, S indicates self-supervised training using KITTI stereo data, and M indicates self-supervised training using KITTI single-view videos. We use ? to indicate methods that utilize semantic information during training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Performance of different ways of grouping the semantic classes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Depth-to-Segmentation network. the final weight by ?20% and the results show that our proposed method is not very sensitive to the exact value of the weight.</figDesc><table><row><cell>Weighting of</cell><cell></cell><cell cols="2">Lower is Better</cell><cell></cell><cell cols="3">Higher is Better</cell></row><row><cell>Distillation Loss</cell><cell>Abs Rel</cell><cell>Sq Rel</cell><cell>RMSE</cell><cell>RMSE Log</cell><cell>? 1</cell><cell>? 2</cell><cell>? 3</cell></row><row><cell>Monodepth2 (RN50)</cell><cell>0.110</cell><cell>0.831</cell><cell>4.642</cell><cell>0.187</cell><cell>0.883</cell><cell>0.962</cell><cell>0.982</cell></row><row><cell>Constant: 0.0050</cell><cell>0.109</cell><cell>0.810</cell><cell>4.637</cell><cell>0.185</cell><cell>0.887</cell><cell>0.963</cell><cell>0.983</cell></row><row><cell>Linear: 0 -0.0040</cell><cell>0.107</cell><cell>0.779</cell><cell>4.632</cell><cell>0.185</cell><cell>0.887</cell><cell>0.962</cell><cell>0.982</cell></row><row><cell>Linear: 0 -0.0045</cell><cell>0.107</cell><cell>0.751</cell><cell>4.553</cell><cell>0.185</cell><cell>0.884</cell><cell>0.963</cell><cell>0.983</cell></row><row><cell>Linear: 0 -0.0050</cell><cell>0.106</cell><cell>0.777</cell><cell>4.580</cell><cell>0.184</cell><cell>0.888</cell><cell>0.963</cell><cell>0.982</cell></row><row><cell>Linear: 0 -0.0055</cell><cell>0.108</cell><cell>0.795</cell><cell>4.606</cell><cell>0.183</cell><cell>0.887</cell><cell>0.963</cell><cell>0.983</cell></row><row><cell>Linear: 0 -0.0060</cell><cell>0.107</cell><cell>0.775</cell><cell>4.580</cell><cell>0.184</cell><cell>0.888</cell><cell>0.963</cell><cell>0.983</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Weighting of segmentation loss.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Applying our semantics-to-depth distillation to different depth networks. For each model, improved numbers by using X-Distill are highlighted in bold.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2021. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:2110.12516v1 [cs.CV] 24 Oct 2021</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that we can include a "background" class for the pretrained segmentation model (which is a common practice). This will allow us to ignore pixels that are not of interest when computing the distillation loss of Eq. 3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For Johnston et al.<ref type="bibr" target="#b15">[16]</ref>, the GMAC shown inFig. 3is a lower bound which only includes the RN101 encoder's computation since their self-attention and discrete disparity volume implementation is not publicly available.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inverseform: A loss function for structured boundary-aware segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhankar</forename><surname>Borse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards scene understanding: Unsupervised monocular depth estimation with semantic-aware representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2624" to="2632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7063" to="7072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-supervised object motion and depth estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishakh</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1004" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8977" to="8986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge distillation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D packing for selfsupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantically-guided representation learning for self-supervised monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dipe: Deeper into photometric errors for unsupervised learning of depth and ego-motion from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hualie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laiyan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenglong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10061" to="10067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sing Bing</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2144" to="2158" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-A</forename><surname>Term?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SynDistNet: Self-supervised monocular fisheye camera distance estimation synergized with semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Varun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Fingscheidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision</title>
		<meeting>the International Conference on 3D Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Every pixel counts++: Joint learning of geometry and motion with 3D holistic understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2624" to="2641" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HR-Depth: High resolution self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Signet: Semantic instance aided unsupervised 3D geometry perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Sunarjo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Bharadia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning for monocular depth estimation: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyang</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distilling optimal neural networks: Rapid search in diverse spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Moons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parham</forename><surname>Noorzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrii</forename><surname>Skliar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Don&apos;t forget the past: Recurrent depth estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishakh</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="6813" to="6820" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geometry meets semantics for semi-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pierluigi Zama Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="298" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12240" to="12249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Boosting monocular depth with panoptic segmentation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faraz</forename><surname>Saeedan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3853" to="3862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Make3D: Learning 3D scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature-metric loss for self-supervised learning of depth and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning scene structure guidance via cross-task knowledge transfer for single depth super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoli</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12955</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distilled semantics for comprehensive scene understanding from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pierluigi Zama Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">Miguel</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2022" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Segstereo: Exploiting semantic information for disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="636" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distilling cross-task knowledge via relationship matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Jia</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-Chuan</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12396" to="12405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ckd: Cross-task knowledge distillation for text-toimage synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1955" to="1968" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Computer Vision Conference</title>
		<meeting>the European Computer Vision Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semi-supervised monocular depth estimation based on semantic supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyuan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongqiao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent &amp; Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="455" to="463" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The edge of depth: Explicit constraints between segmentation and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13116" to="13125" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
