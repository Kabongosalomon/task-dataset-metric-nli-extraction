<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rapid Adaptation with Conditionally Shifted Neurons</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
						</author>
						<title level="a" type="main">Rapid Adaptation with Conditionally Shifted Neurons</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe a mechanism by which artificial neural networks can learn rapid adaptation -the ability to adapt on the fly, with little data, to new tasks -that we call conditionally shifted neurons. We apply this mechanism in the framework of metalearning, where the aim is to replicate some of the flexibility of human learning in machines. Conditionally shifted neurons modify their activation values with task-specific shifts retrieved from a memory module, which is populated rapidly based on limited task experience. On metalearning benchmarks from the vision and language domains, models augmented with conditionally shifted neurons achieve state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ability to adapt our behavior rapidly in response to external or internal feedback is a primary ingredient of human intelligence. This cognitive flexibility is commonly ascribed to prefrontal cortex (PFC) and working memory in the brain. Neuroscientific evidence suggests that these areas use incoming information to support task-specific temporal adaptation and planning <ref type="bibr" target="#b33">(Stokes et al., 2013;</ref><ref type="bibr" target="#b31">Siegel et al., 2015;</ref><ref type="bibr" target="#b4">Brincat &amp; Miller, 2016)</ref>. This occurs on the fly, within only a few hundred milliseconds, and supports a wide variety of task-specific behaviors <ref type="bibr" target="#b22">(Monsell, 2003;</ref><ref type="bibr" target="#b27">Sakai, 2008)</ref>.</p><p>On the other hand, most existing machine learning systems are designed for a single task. They are trained through one optimization phase after which learning ceases. Systems built in such a train-and-then-test manner do not scale to complex, realistic environments: they require gluts of single-task data and are prone to issues related to distributional shifts, such as catastrophic forgetting <ref type="bibr" target="#b32">(Srivastava et al., 2013;</ref><ref type="bibr" target="#b7">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b14">Kirkpatrick et al., 2016)</ref> and adversarial data points <ref type="bibr" target="#b34">(Szegedy et al., 2013)</ref>.</p><p>There is growing interest and progress in building flexible, adaptive models, particularly within the framework of metalearning (learning to learn) <ref type="bibr" target="#b21">(Mitchell et al., 1993;</ref><ref type="bibr" target="#b0">Andrychowicz et al., 2016;</ref><ref type="bibr" target="#b37">Vinyals et al., 2016;</ref><ref type="bibr" target="#b1">Bachman et al., 2017)</ref>. The goal of metalearning algorithms is the ability to learn new tasks efficiently, given little training data for each individual task. Metalearning models learn this ability (to learn) by training on a distribution of related tasks.</p><p>In this work we develop a neural mechanism for metalearning via rapid adaptation that we call conditionally shifted neurons. Conditionally shifted neurons (CSNs), like standard artificial neurons, produce activation values based on input from connected neurons modulated by the connection weights. Additionally, they have the capacity to shift their activation values on the fly based on auxiliary conditioning information. These conditional shifts adapt model behavior to the task at hand.</p><p>A model with CSNs operates in two phases: a description phase and a prediction phase. Assume, for each task ? ? p(? ), that we have access to a description D ? . In the simplest case, this is a set of example datapoints and their corresponding labels: D ? = {(x i , y i )} n i=1 . 1,2 In the description phase, the model processes D ? and extracts conditioning information as a function of its performance on D ? . Based on this information, it generates activation shifts to adapt itself to the task and stores them in a key-value memory. In the prediction phase, the model acts on unseen datapoints x j ? ? , from the same task, to predict their labels y j . To improve these predictions, the model retrieves shifts from memory and applies them to the activations of individual neurons. During training, the model learns the meta procedure of how to extract conditioning information in the description phase and generate useful conditional shifts for the prediction phase. At test time, it uses this procedure to adapt itself to new tasks from p(? ).</p><p>We define and investigate two forms of conditioning information in this work ( ?2.3). The first uses the gradient of the model's prediction loss with respect to network preactivations, computed on the description data; the second replaces the loss gradient with direct feedback alignment <ref type="bibr" target="#b24">N?kland, 2016)</ref>. The direct feedback information is computationally cheaper and leads to competitive or superior performance in our experiments. Note that other sources of conditioning are possible.</p><p>Our proposed neuron-level adaptation has several advantages over previous methods for metalearning that adapt the connections between neurons, for instance via fast weights <ref type="bibr" target="#b23">(Munkhdalai &amp; Yu, 2017)</ref> or an optimizer <ref type="bibr" target="#b6">(Finn et al., 2017;</ref><ref type="bibr" target="#b26">Ravi &amp; Larochelle, 2017)</ref>. First, it is more efficient computationally, since the number of neurons is generally much less than the number of weight parameters (e.g., the number of weights scales quadratically in the number of neurons per layer for a fully connected network). Second, conditionally shifted neurons can be incorporated into various neural architectures, including convolutional and recurrent networks, without special modifications to suit the structure of such models.</p><p>After describing the details of our framework, we demonstrate experimentally that ResNet <ref type="bibr" target="#b9">(He et al., 2016)</ref> and deep LSTM <ref type="bibr" target="#b11">(Hochreiter &amp; Schmidhuber, 1997</ref>) models equipped with CSNs achieve 56.88% and 71.94% accuracy on the standard Mini-ImageNet 1-and 5-shot benchmarks, and 41.25%, 52.1%, and 57.8% accuracy on Penn Treebank 1-, 2-, and 3-shot language modeling tasks. These results mark a significant improvement over the previous state of the art.</p><p>Our primary contributions in this paper are as follows: (i) we propose a generic neural mechanism, conditionally shifted neurons, by which learning systems can adapt on the fly; (ii) we introduce direct feedback as a computationally inexpensive metalearning signal; and (iii) we implement and evaluate conditionally shifted neurons in several widelyused neural network architectures. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Conditionally Shifted Neurons</head><p>The core idea of conditionally shifted neurons is to modify a network's activation values on the fly, by shifting them as a function of auxiliary conditioning information. A layer with CSNs takes the following form:</p><formula xml:id="formula_0">h t = ?(a t ) + ?(? t ) t = T softmax(a t + ? t ) t = T<label>(1)</label></formula><p>for hidden layer t or output layer T (which represents a probability distribution). The pre-activation vector a t ? R Lt , for a layer with L t neurons, can take various forms depending on the network architecture (fully connected, convolutional, etc.). The nonlinear function ? computes an element-wise 3 Code and data will be available at https://aka.ms/csns activation. ? t ? R Lt is the layer-wise conditional shift vector, determined from layer-wise conditioning information I t (defined in ?2.3).</p><p>To implement a model with CSNs, we must define functions that extract and transform the conditioning information I t into the shifts ? t . For this we build on the MetaNet architecture of <ref type="bibr" target="#b23">Munkhdalai &amp; Yu (2017)</ref>. MetaNet consists of a base learner plus a shared meta learner with working memory. For each task ? , MetaNet processes the task description D ? = {(x i , y i )} n i=1 and stores relevant "meta information" in a key-value memory. To classify unseen examples x j from the described task, the model queries its working memory with an attention mechanism to generate a set of fast weights; these modify the base learner, which in turn predicts labels y j .</p><p>To begin, we describe model details for a fully connected feed-forward network (FFN) with CSNs. The architecture is depicted in <ref type="figure">Figure 1</ref>. As shown, the model factors into a base learner, which makes predictions on inputs, and a meta learner. The meta learner extracts conditioning information from the base learner and uses a key-value memory to store and retrieve activation shifts. After walking through the model details, we define error gradient and direct feedback variants of the conditioning information I t ( ?2.3). In ?2.4 and ?2.5 we describe how CSNs can be added to ResNet and LSTM architectures, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feed-Forward Networks with Conditionally Shifted Neurons</head><p>Our model operates in two phases: a description phase, wherein it processes the task description D ? = {(x i , y i )} n i=1 , and a prediction phase, wherein it acts on unseen datapoints x j to predict their labels y j . In an episode of training or test, we sample a task from p(? ). The model then ingests the task description and uses what it learns therefrom, via the conditioning information, to make predictions on unseen task data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">BASE LEARNER</head><p>The base learner maps an input datapoint to its label prediction through layers described by equation 1, where in the FFN case, the pre-activation vector a t is given by a t = W t h t?1 + b t . Weight matrix W t and bias vector b t are learned parameters.</p><p>The base learner operates similarly in both phases. During the description phase, the base learner's input is a datapoint x i from D ? . Its softmax output is an estimate? i for the label y i . The conditional shifts ? t in eq. 1 are set to 0 in this phase.</p><p>During the prediction phase, the base learner operates on inputs x j . It receives conditional shifts ? t from the meta learner and applies them layer-wise according to eq. 1. Con-</p><formula xml:id="formula_1">Memory Attention Keys Values t = 1 t t = T V 1,1 = g(I 1,1 ) V 1,2 = g(I 1,2 ) V 1,n = g(I 1,n )</formula><p>. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Learner</head><p>x j ? j Description Phase Prediction Phase </p><formula xml:id="formula_2">? 1 ? 1 ? 1 ? L ? 2 ? 2 ? 2 ? L ? ? ? n ? L . . . Description ? t V t,1 = g(I t,1 ) V t,2 = g(I t,2 ) V t,n = g(I t,n )</formula><p>. . .</p><formula xml:id="formula_3">V T, 1 = g(I T, 1 ) V T,2 = g(I T,2 ) V T,n = g(I T,n ) . . . k' 1 = f(x' 1 ) k' 2 = f(x' 2 ) k' n = f(x' n )</formula><p>. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Learner</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Learner</head><formula xml:id="formula_4">k j = f(x j ) Meta Learner ? t = 0 Figure 1.</formula><p>Schematic illustration of our model with conditionally shifted neurons. In the description phase, the meta learner populates working memory with keys and values, based on the base learner's performance on the task description; in the prediction phase, the meta learner retrieves task-specific shifts from memory through key-based attention and feeds them to the base learner to adapt it to the task. ditioned on these shifts, the base learner computes an estimate? j for the label y j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">META LEARNER</head><p>The meta learner's operation is more complicated and differs more significantly from phase to phase.</p><p>During the description phase, as the base learner processes x i ? D ? , the meta learner extracts layer-wise conditioning information for this example, I t,i , according to eq. 6 or 7. The meta learner uses the conditioning information to generate memory values. These act as "template" conditional shifts for the task, and are computed via the memory function g:</p><formula xml:id="formula_5">V t,i = g(I t,i ),<label>(2)</label></formula><p>where V t,i ? IR Lt encodes the shift template at layer t for input x i . There are n of these; we arrange them into matrix V t ? IR n?Lt over the full task description.</p><p>For parsimony, we desire a single memory function g for all layers of the base learner, which may have different sizes L t . Therefore, we parameterize g as a multi-layer perceptron (MLP) that operates independently on the vector of conditioning information for each neuron (defined in ?2.3). More sophisticated L t -agnostic parameterizations for g are possible, such as recurrent networks.</p><p>In parallel during the description phase, the meta learner constructs an embedded representation of the input that it uses to key the memory. This is the objective of the key function, f , which we parameterize here as an MLP with a linear output layer. The key function generates, for each description input, a d-dimensional key vector</p><formula xml:id="formula_6">k i = f (x i ).</formula><p>At prediction time, the meta learner generates a memory query k j from input x j using the key function. It uses k j to recall layer-wise shifts ? t from memory via soft attention:</p><formula xml:id="formula_7">? = softmax i (cos(k j , k i )),<label>(3)</label></formula><formula xml:id="formula_8">? t = ? V t .<label>(4)</label></formula><p>Note that keys correspond to inputs, not base-learner layers. The meta learner finally feeds the layer-wise shifts ? t to the base learner to condition the computation of? j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Training and Test</head><p>We train and test the model in episodes. For each episode: we sample a training or test task from p(? ), process its description D ? , and then feed its unseen data forward to obtain their label predictions. Training and test tasks are both drawn from the same distribution, but crucially, we partition the data such that the classes seen at training time do not overlap with those seen at test time.</p><p>Over a collection of training episodes, we optimize the model parameters end-to-end via stochastic gradient descent (SGD). Gradients are taken with respect to the (crossentropy) task losses, L ? = j L CE (? j , y j ). In this scheme, the model's computational graph contains the operations for processing the description, like the transformation of conditioning information and the generation of memory keys and values. Parameters of these operations are also optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Conditioning Information</head><p>Error gradient information Inspired by the success of MetaNets, we first consider gradients of the base learner's loss on the task description as the conditioning information.</p><p>To compute these error gradients we apply the chain rule and the standard backpropagation algorithm to the base learner. Given a true label y i from the task description and the model's corresponding label prediction? i , we obtain loss gradients for base-learner neurons at layer t as</p><formula xml:id="formula_9">? t,i = ?L(? i , y i ) ?a t ,<label>(5)</label></formula><p>where a t is the L t -dimensional vector of pre-activations at layer t, ? t,i has the same size, and we denote with L(?) a loss function (such as the cross entropy loss on the labels). Note that L here is not the target of optimization via SGD.</p><p>We obtain the conditioning information I t,i, for each neuron (indexed by ) using the gradient preprocessing formula of <ref type="bibr" target="#b0">Andrychowicz et al. (2016)</ref>:</p><formula xml:id="formula_10">I t,i, = log(|? t,i, |) p , sgn(? t,i, ) if |? t,i, | ? e ?p (?1, e p ? t,i, ) otherwise (6)</formula><p>where sgn is the signum function and we set p = 7. We use this preprocessing to smooth variation in I t,i, , since gradients with respect to different base-learner activations can have very different magnitudes. By eq. 6, each neuron obtains a 2-dimensional vector of conditioning information. In this case, we can interpret eq. 1 as a one-step, transformed gradient update on the neuron activations via ? t . "Raw" gradients are transformed through preprocessing, the memory read and write operations, and the nonlinearity ?.</p><p>Because backpropagation is inherently sequential, this information is expensive to compute. It becomes increasingly costly for deeper networks, such as RNNs processing long sequences.</p><p>Direct feedback information Direct feedback (DF) information is inspired by feedback alignment methods <ref type="bibr" target="#b24">N?kland, 2016)</ref> and biologically plausible deep learning <ref type="bibr" target="#b3">(Bengio et al., 2015)</ref>. We obtain the DF information for base-learner neurons at layer t as</p><formula xml:id="formula_11">I t,i, = ? (a t, ) ? (? i ? y i ),<label>(7)</label></formula><p>where ? (?) represents the derivative of the nonlinear activation function ? and (? i ? y i ) is the derivative of the cross entropy loss with respect to the softmax input. Thus, the DF conditioning information for each neuron is the derivative of the loss function scaled by the derivative of the activation function. In the DF case, each neuron obtains a C-dimensional vector of information, with C the number of output classes. We can compute this conditioning information for all neurons in a network simultaneously, with a single multiplication. This is more efficient than sequentially locked backpropagation-based error gradients. Furthermore, to obtain DF information, it is sufficient that only the loss and neuron activation functions are differentiable. This is more relaxed than for backpropagation methods. We demonstrate the effectiveness of both conditioning variants in ?4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Deep Residual Networks with CSNs</head><p>For ResNets <ref type="bibr" target="#b9">(He et al., 2016)</ref> we incorporate conditionally shifted neurons into the output of a residual block. Let us denote the residual block as ResBlock, which is defined as follows:</p><formula xml:id="formula_12">h 1 = ReLU(conv(x)) h 2 = ReLU(conv(h 1 )) h 3 = conv(h 2 ) h 4 = conv(x) a t = h 3 + h 4</formula><p>where x and a t are the inputs to the block and the output pre-activations, respectively. Function conv denotes a convolutional layer, which may optionally be followed by a batch normalization <ref type="bibr" target="#b13">(Ioffe &amp; Szegedy, 2015)</ref> layer. The activations h t of the CSNs for the ResBlock are computed as:</p><formula xml:id="formula_13">h t = ?(a t ) + ?(? t )</formula><p>where ? t is the task-specific shift retrieved from the memory, constructed based on the activation values a t analogously to the FFN case; i.e., the conditioning information is computed for neurons at the output of each residual block. We stack several residual blocks with CSNs to construct a deep adaptive ResNet model. We use the ReLU function as the nonlinearity ? in this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Long Short-Term Memory Networks with CSNs</head><p>Given the current input x t , the previous hidden state h t?1 , and the previous memory cell state c t?1 , an LSTM model with CSNs computes its gates, new memory cell states, and hidden states at time step t with the following update rules:</p><formula xml:id="formula_14">i t = Sigmoid(W i [x t ; h t?1 ] + b i ) f t = Sigmoid(W f [x t ; h t?1 ] + b f ) o t = Sigmoid(W o [x t ; h t?1 ] + b o ) c t = ?(W v [x t ; h t?1 ] + b v ) i t + c t?1 f t h t = (?(c t ) + ?(? t )) o t where represents element-wise multiplication, [.;</formula><p>.] is concatenation, and ? t is the task-specific shift from the memory.</p><p>In the LSTM case, the memory is constructed by processing conditioning information extracted from the memory cell c t . By stacking such layers together we build a deep LSTM model that adapts across both depth and time. We use the tanh function as the nonlinearity ? in this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Among the many problems in supervised, reinforcement, and unsupervised learning that can be framed as metalearning, few-shot learning has emerged as a natural and popular test bed. Few-shot supervised learning refers to a scenario where a learner is introduced to a sequence of tasks, where each task entails multi-class classification given a single or very few labeled examples per class. A key challenge in this setting is that the classes or concepts vary across the tasks; thus, models require a capacity for rapid adaptation in order to recognize new concepts on the fly.</p><p>Few-shot learning problems were previously addressed using metric learning methods <ref type="bibr" target="#b15">(Koch, 2015)</ref>. Recently, there has been a shift towards building flexible models for these problems within the learning-to-learn paradigm <ref type="bibr" target="#b20">(Mishra et al., 2017;</ref><ref type="bibr" target="#b28">Santoro et al., 2016)</ref>. <ref type="bibr" target="#b37">Vinyals et al. (2016)</ref> unified the training and testing of a one-shot learner under the same procedure and developed an end-to-end, differentiable nearest-neighbor method for one-shot learning. More recently, one-shot optimizers were proposed by <ref type="bibr" target="#b26">Ravi &amp; Larochelle (2017);</ref><ref type="bibr" target="#b6">Finn et al. (2017)</ref>. The MAML framework <ref type="bibr" target="#b6">(Finn et al., 2017</ref>) learns a parameter initialization from which a model can be adapted rapidly to a given task using only a few steps of gradient updates. To learn this initialization it makes use of more sophisticated second-order gradient information. Here we harness only first-order gradient information, or the simpler direct feedback information.</p><p>As highlighted, the architecture of our model with conditionally shifted neurons is closely related to Meta Networks <ref type="bibr" target="#b23">(Munkhdalai &amp; Yu, 2017)</ref>. The MetaNet modifies synaptic connections (weights) between neurons using fast weights <ref type="bibr" target="#b29">(Schmidhuber, 1987;</ref><ref type="bibr" target="#b10">Hinton &amp; Plaut, 1987)</ref> to implement rapid adaptation. While MetaNet's fast weights enable flexibility, it is very expensive to modify these weights when the connections are dense. Neuron-level adaptation as proposed in this work is significantly more efficient while lending itself to a range of network architectures, including ResNet and LSTM. Other previous work on metalearning has also formulated the problem as two-level learning: specifically, "slow" learning of a meta model across several tasks, and "fast" learning of a base model that acts within each task <ref type="bibr" target="#b29">(Schmidhuber, 1987;</ref><ref type="bibr" target="#b2">Bengio et al., 1990;</ref><ref type="bibr" target="#b12">Hochreiter et al., 2001;</ref><ref type="bibr" target="#b21">Mitchell et al., 1993;</ref><ref type="bibr" target="#b36">Vilalta &amp; Drissi, 2002;</ref><ref type="bibr" target="#b20">Mishra et al., 2017)</ref>. <ref type="bibr" target="#b30">Schmidhuber (1993)</ref> discussed the use of network weight matrices themselves for continuous adaptation in dynamic environments.</p><p>Viewed as a form of feature-wise transformation, CSNs are closely related to conditional normalization techniques (Lei <ref type="bibr" target="#b17">Ba et al., 2015;</ref><ref type="bibr">?;</ref><ref type="bibr">?;</ref><ref type="bibr" target="#b5">De Vries et al., 2017;</ref><ref type="bibr" target="#b25">Perez et al., 2017)</ref>. FiLM <ref type="bibr" target="#b25">(Perez et al., 2017)</ref>, the most similar such approach which was inspired by ? and ?, modulates CNN feature maps using global scale and shift operations conditioned on an auxiliary input modality. In contrast, CSNs apply shifts to individual neurons' activations, locally, and this modification is based on the model's behavior on the task description rather than the input itself.</p><p>In the case of gradient-based conditioning information, our approach can be viewed as a synthesis of a conditional normalization model (in the style of FiLM) with a learned optimizer (in the style of Andrychowicz et al. <ref type="formula" target="#formula_0">(2016)</ref>). Specifically, the learned memory and key functions, g and f , transform error gradients into the conditioning shifts ? t , which are then applied like a one-step update to the activation values. A CSN model uses this learned optimizer on the fly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>We evaluate the proposed CSNs on tasks from the vision and language domains. Below we describe the datasets we evaluate on and the according preprocessing steps, followed by test results and an ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Few-shot Image Classification</head><p>In the vision domain, we used two widely adopted fewshot classification benchmarks: the Omniglot and Mini-ImageNet datasets.</p><p>Omniglot consists of images from 1623 classes from 50 different alphabets, with only 20 images per class <ref type="bibr" target="#b16">(Lake et al., 2015)</ref>. As in previous studies, we randomly selected 1200 classes for training and 423 for testing and augmented the training set with 90, 180 and 270 degree rotations. We resized the images to 28 ? 28 pixels for computational efficiency.</p><p>For the Omniglot benchmark we performed 5-and 20-way classification tests, each with one or five labeled examples from each class as the description D ? . We use a convolutional network (CNN) with 64 filters as the base learner.  <ref type="bibr" target="#b28">(Santoro et al., 2016)</ref> 82.8 94.9 --Matching Nets <ref type="bibr" target="#b37">(Vinyals et al., 2016)</ref> 98.1 98.9 93.8 98.5 MAML <ref type="bibr" target="#b6">(Finn et al., 2017)</ref> 98.7 ? 0.4 99.9 ? 0.3 95.8 ? 0.3 98.9 ? 0.2 MetaNet <ref type="bibr" target="#b23">(Munkhdalai &amp; Yu, 2017)</ref> 98.95 -97.0 -TCML <ref type="bibr" target="#b20">(Mishra et al., 2017)</ref> 98  <ref type="bibr" target="#b26">(Ravi &amp; Larochelle, 2017)</ref> 43.4 ? 0.77 60.2 ? 0.71 MAML <ref type="bibr" target="#b6">(Finn et al., 2017)</ref> 48.7 ? 1.84 63.1 ? 0.92 MetaNet <ref type="bibr" target="#b23">(Munkhdalai &amp; Yu, 2017)</ref> 49. This network has 5 convolutional layers, each of which uses 3 ? 3 convolutions followed by the ReLU nonlinearity and a 2 ? 2 max-pooling layer. Convolutional layers are followed by a fully connected (FC) layer with softmax output. Another CNN with the same architecture is used for the key function f . We use CSNs in the last four layers of the CNN components, referring to this model as "adaCNN." Full implementation details can be found in Appendix A.  <ref type="bibr" target="#b20">(Mishra et al., 2017)</ref> with two exceptions due to memory constraints. Instead of two 1 ? 1 convolutional lay-ers with 2048 and 512 filters we use only a single such layer with 1024 filters, and the ReLU nonlinearity instead of its leaky variant. We incorporate CSNs into the last two residual blocks as well as the two fully connected output layers. Full implementation details can be found in Appendix A.</p><p>For every 400 training tasks, we tested the model for another 400 tasks sampled from the validation set. If the model performance exceeded the previous best validation result, we applied it to the test set. Following previous approaches that we compare with in <ref type="table">Table 2</ref>, we sampled another 400 tasks randomly from the test set to report model accuracy.</p><p>Unlike Omniglot, there remains significant room for improvement on Mini-ImageNet. As shown in <ref type="table">Table 2</ref>, on this more challenging task, CNN-based models with conditionally shifted neurons achieve performance just below that of the best CNN-based approaches like MAML and MetaNet (recall that these modify weight parameters rather than activation values). The more sophisticated adaResNet model, on the other hand, achieves state-of-the-art results. The best-performing adaResNet (DF) yields almost 10% improvement over the corresponding adaCNN model and improves over the previous best result of TCML by 1.16% and 3.06% on the one and five shot 5-way classification tasks, respectively. Note that TCML likewise uses a ResNet architecture. The best accuracy among five different seed runs of the adaResNet with DF conditioning was 72.91% on the five-shot task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Few-shot Language Modeling</head><p>To evaluate the effectiveness of recurrent models with conditionally shifted neurons, we ran experiments on the few-shot Penn Treebank (PTB) language modeling task introduced by <ref type="bibr" target="#b37">Vinyals et al. (2016)</ref>.</p><p>In this task, a model is given a query sentence with one missing word and a support set (i.e., description) of onehot-labeled sentences that also have one missing word each. One of the missing words in the description set is identical to that missing from the query sentence. The model must select the label of this corresponding sentence.</p><p>Following <ref type="bibr" target="#b37">Vinyals et al. (2016)</ref>, we split the PTB sentences into training and test such that, for the test set, target words for prediction and the sentences in which they appear are unseen during training. Concretely, we removed the test target words as well as sentences containing those words from the training data. This process necessarily reduces the training data and increases out-of-vocabulary (OOV) test words. We used the same 1000 target words for testing as provided by <ref type="bibr" target="#b37">Vinyals et al. (2016)</ref>.</p><p>We evaluated two models with conditionally shifted neurons on 1-, 2-, and 3-shot language modelling (LM) tasks. In both cases, we represent words with randomly initialized dense embeddings. For the first model we stacked a 3-layer feed-forward net with CSNs (adaFFN) on top of an LSTM network (LSTM+adaFFN) at each prediction timestep. In this model, only the adaFFN can adapt to the task while it processes the hidden state of the underlying LSTM. The LSTM encoder builds up the context for each word and provides a generic (non-task-specific) representation to the adaFFN. Both components are trained jointly.</p><p>The second model we propose for this task is more flexible, an LSTM with conditionally shifted neurons in the recurrence (adaLSTM). This entire model is adapted with task-specific shifts at every time step. For few-shot classi-fication output, a softmax layer with CSNs is stacked on top of the adaLSTM. Comparing LSTM+adaFFN and adaL-STM, the former is much faster since we only adapt the activations of the three feedforward layers, but it lacks full flexibility since the LSTM is unaware of the current task information. We also evaluated deep (2-layer) versions of both LSTM+adaFFN and adaLSTM models. Full implementation details can be found in Appendix A.</p><p>We used two different methods to form test tasks for evaluation. First, we randomly sampled 400 tasks from the test data and report the average accuracy. Second, we make sure to include all test words in the task formulation. We randomly partition the 1000 target words into 200 groups and solve each group as a task. In the random approach there is a chance that a word could be missed or included multiple times in different tasks. However, the random approach also enables formulation of an exponential number of test tasks.  <ref type="bibr" target="#b37">Vinyals et al. (2016)</ref> is 72.8%. Our best accuracy -around 58% on the 3-shot task -comes using a 2-layer adaLSTM, and improves over the Matching Nets results by 11.1%, 16.0% and 19.6% for 1-, 2-, and 3-shot tasks, respectively. Comparing model variants, adaLSTM consistently outperforms the standard LSTM augmented with a conditionally shifted output FFN, and deeper models yield higher accuracy. Providing more sentences for the target word increases performance, as expected. These results indicate that our model's few-shot language modelling capabilities far exceed those of Matching Networks <ref type="bibr" target="#b37">(Vinyals et al., 2016)</ref>. Some of this improvement surely arises from adaLSTM's recurrent structure, which is known to apply well to sequence-based tasks and in the language domain. However, it is one of the strengths of conditionally shifted neurons that they can be ported easily to various neural architectures.</p><p>Comparing direct feedback information to the gradientbased variant across the full suite of experiments, we observe overall that DF information performs competitively well. Even more positively, DF information speeds up the the model runtime considerably. For example, the 2-layer adaLSTM processed 400 test episodes in 200 seconds using gradient information vs. 160 seconds for DF information, representing a speedup of about 25%. In Appendix B, we compare runtimes for the adaCNN variants and MetaNet model on the Mini-ImageNet task. Even for the shallow adaCNN model, we observe 2-3 ms/task speed-up with the DF conditioning information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>To better understand our model, we performed an ablation study on adaCNN trained on Mini-ImageNet and the 1-layer adaLSTM trained on PTB. Results are shown in <ref type="figure" target="#fig_0">Figure 2</ref> and <ref type="figure">Figure 3</ref>, respectively.  Error Gradient Direct Feedback <ref type="figure">Figure 3</ref>. Model ablation on the one-shot language modeling task, for a single layer adaLSTM. We report average accuracy on 400 random test tasks. Yellow: g as a scalar multiplier of ?t,i (gradient case) or perceptron (DF case); Violet: ? without normalization; Grey: baseline model.</p><p>Our first ablation was to determine the effect of normalizing the task shifts ? t through the nonlinear activation function ?. Ablating the activation function from eq. 1 and simply adding ? t resulted in a slight performance drop on the Mini-ImageNet task and a significant decrease (around 7%) on the one-shot LM task, for both variants of conditioning information. We similarly tried adding ? t directly to the pre-activations a t inside the nonlinearity ?. On Mini-ImageNet, this variant of adaCNN achieved 47.8% and 48.09% accuracy with gradient and DF conditioning information, respectively, which is competitive with the baseline. However, adaLSTM performance decreased more significantly to 30.7/31.8% and 33.25/34.05% on the LM task. We conclude that squashing the conditional shift ? t to the same range as the neuron's standard activation value is beneficial.</p><p>Our second ablation evaluates variations on the function g for transforming the conditioning information I t,i into the memory values V t,i . In the case of gradient-based conditioning information, we remove the preprocessing of eqn. 6 and replace the learned MLP with a simple learned scalar, ? ? R, that multiplies the gradient vector ? t,i . In this case we can more clearly interpret the conditional shift as a one-step gradient update on the activation values (although this update is still modulated by the memory read procedure and the function ?). As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the adaCNN model with learned scaling loses about 3 percentage points of accuracy on the image classification task. However, as per <ref type="figure">Figure 3</ref>, adaLSTM performance plummets with learned scaling, dropping to 20% test accuracy (random chance).</p><p>For direct feedback conditioning information, we cannot use a scalar parameter because we have a C-dimensional information vector for each neuron (recall ?2.3). We therefore parameterize g as a one-layer perceptron in this case rather than a deep MLP. Using a simple perceptron to process the direct feedback information decreased test accuracy significantly on the Mini-ImageNet and LM tasks (drops of over 10%). This highlights that a deeper mapping function is crucial for processing DF conditioning information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced conditionally shifted neurons, a mechanism for rapid adaptation in neural networks. Conditionally shifted neurons are generic and easily incorporated into various neural architectures. They are also computationally efficient compared to alternative metalearning methods that adapt synaptic connections between neurons. We proposed two variants of conditioning information for use with CSNs, one based on error gradients and another based on feedback alignment methods. The latter is more efficient because it does not require a sequential backpropagation procedure, and achieves competitive performance with the former. We demonstrated empirically that models with conditionally shifted neurons improve the state of the art on metalearning benchmarks from the vision and language domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Implementation Details</head><p>The hyperparameters for our models are listed in <ref type="table" target="#tab_8">Tables 4  and 5</ref>. Key size d was 64 throughout all experiments. A dropout rate of 0.2 was applied to each layer of adaFFN. For the other adaptive models, the input dropout rate was set to 0.2 or 0.0. The dropout for the last two layers were varied as shown in <ref type="table" target="#tab_8">Table 4</ref> and 5. Due to memory constraints, we used adaLSTM with a smaller number of hidden units (i.e., 200 vs 300) for deep models when applying to 3-shot tasks.</p><p>The neural network weights were initialized using <ref type="bibr" target="#b8">He et al. (2015)</ref>'s method. We set the hard gradient clipping threshold for adaCNN model to 10. No gradient clipping was performed for the other models. We listed the setup for optimizers in <ref type="table" target="#tab_8">Table 4</ref> and 5. For Adam optimizer, the rest of the hyperparameters were set to their default values (i.e., ? 1 = 0.9, ? 2 = 0.999, and = 10 ?8 ).</p><p>Although different parameterizations for the meta learner function g may improve the performance, for simplicity we used a 3-layer MLP with ReLU activation with 20 or 40 units per layer. This MLP acts coordinate-wise and processes conditioning information for each neuron independently.</p><p>Empirically, we found that selecting the vector from V t corresponding to the key k i with maximum cosine similarity to the query k j (hard attention) gave similar performance to soft attention.</p><p>We occasionally observed difficulty in optimizing the LSTM+adaFFN models, often seeing no improvement in the training loss from certain initializations. Decreasing the learning rate and in case of DF information applying dropouts to adaFFN layers helped training this model.</p><p>Models were implemented using the Chainer <ref type="bibr" target="#b35">(Tokui et al., 2015)</ref> framework 4 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Running Time Comparison with MetaNet</head><p>We compared the speed of our adaCNN model variants with MetaNet model on Mini-ImageNet task. We implemented all models in the Chainer framework <ref type="bibr" target="#b35">(Tokui et al., 2015)</ref> and tested on an Nvidia Titan X GPU. In <ref type="figure" target="#fig_2">Figure 4</ref> we see that adaCNN variants are significantly faster than MetaNet while being conceptually simpler and easier to implement.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Model ablation for adaCNN tested on the Mini-ImageNet one-shot task. Blue: g as a scalar multiplier of ?t,i (gradient case) or perceptron (DF case); Green: ? without normalization; Red: baseline model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Training (top)  and inference (bottom) speeds of MetaNet, adaCNN variants are compared on the 1-shot, 5-way Mini-ImageNet task. The y-axis shows wall-clock time (ms/task) in log scale. Training time includes the feedforward computations and the parameter updates. Inference time includes computations for the description and prediction phases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Omniglot few-shot classification test accuracy for error gradient (?) and direct feedback (DF) conditioning information.</figDesc><table><row><cell>5-way</cell><cell>20-way</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc>shows that our adaCNN model achieves competitive, though not state-of-the-art, results on the Omniglot tasks. There is an obvious ceiling effect among the best performing models as accuracy saturates near 100%.</figDesc><table><row><cell>Mini-ImageNet features 84 ? 84-pixel color images from</cell></row><row><cell>100 classes (64/16/20 for training/validation/test splits) and</cell></row><row><cell>each class has 600 exemplar images. We ran our experi-</cell></row><row><cell>ments on the class subset released by Ravi &amp; Larochelle</cell></row><row><cell>(2017). Compared to Omniglot, Mini-ImageNet has fewer</cell></row><row><cell>classes (100 vs 1623) with more labeled examples provided</cell></row><row><cell>of each class (600 vs 20). Given this larger number of</cell></row><row><cell>examples, we evaluated a similar adaCNN model with 32</cell></row><row><cell>filters as well as a model with more sophisticated ResNet</cell></row><row><cell>components ("adaResNet") on the Mini-ImageNet 5-way</cell></row><row><cell>classification tasks. The ResNet architecture follows that</cell></row><row><cell>of TCML</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Penn Treebank few-shot classification test accuracy for error gradient (?) and direct feedback (DF) conditioning information.</figDesc><table><row><cell>5-way (400 random/all-inclusive)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc>summarizes our results. The approximate upper bound achieved by the oracle LSTM-LM of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>Hyperparameters for few-shot image classification tasks</figDesc><table><row><cell>Model</cell><cell>Layers</cell><cell></cell><cell>Filters</cell><cell>Dropout rate</cell><cell>Optimizer</cell></row><row><cell>adaCNN (?)</cell><cell>5</cell><cell></cell><cell>32/64</cell><cell>0.0, 0.3, 0.3</cell><cell>Adam (?=0.001)</cell></row><row><cell>adaCNN (DF)</cell><cell>5</cell><cell></cell><cell>32/64</cell><cell>0.2, 0.3, 0.3</cell><cell>Adam (?=0.001)</cell></row><row><cell>adaResNet (?)</cell><cell>4</cell><cell cols="2">64, 96, 128, 256</cell><cell>0.2, 0.5, 0.5</cell><cell>SGD with momentum (lr=0.01, m=0.9)</cell></row><row><cell>adaResNet (DF)</cell><cell>4</cell><cell cols="2">64, 96, 128, 256</cell><cell>0.2, 0.5, 0.5</cell><cell>SGD with momentum (lr=0.01, m=0.9)</cell></row><row><cell cols="5">Table 5. Hyperparameters for few-shot language modelling tasks</cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell cols="3">Hidden unit size Dropout rate</cell><cell>Optimizer</cell></row><row><cell cols="2">2-layer LSTM + adaFFN (?)</cell><cell></cell><cell>300</cell><cell>-</cell><cell>Adam (?=0.0003)</cell></row><row><cell cols="3">2-layer LSTM + adaFFN (DF)</cell><cell>300</cell><cell cols="2">0.2, 0.2, 0.2</cell><cell>Adam (?=0.0003)</cell></row><row><cell cols="2">1-layer adaLSTM (?)</cell><cell></cell><cell>300</cell><cell>-</cell><cell>Adam (?=0.001)</cell></row><row><cell cols="2">1-layer adaLSTM (DF)</cell><cell></cell><cell>300</cell><cell>-</cell><cell>Adam (?=0.001)</cell></row><row><cell cols="2">2-layer adaLSTM (?)</cell><cell></cell><cell>300, 200</cell><cell>-</cell><cell>Adam (?=0.001)</cell></row><row><cell cols="2">2-layer adaLSTM (DF)</cell><cell></cell><cell>300, 200</cell><cell>-</cell><cell>Adam (?=0.001)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">More abstractly, a task description could be given by a set of instructions or demonstrations of expert behavior.2 In a C-way, k-shot classification task, n = k ? C.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://chainer.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Misha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning algorithms for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/bachman17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, Doina and Teh, Yee Whye</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning a synaptic learning rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Cloutier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
		<respStmt>
			<orgName>Universit? de Montr?al, D?partement d&apos;informatique et de recherche op?rationnelle</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong-Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bornschein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jorg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04156</idno>
		<title level="m">Towards biologically plausible deep learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prefrontal cortex networks shift from external to internal modes during learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">L</forename><surname>Brincat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Earl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">37</biblScope>
			<biblScope unit="page" from="9739" to="9754" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>J?r?mie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6597" to="6607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/finn17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, Doina and Teh, Yee Whye</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An empirical investigation of catastrophic forgetting in gradient-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using fast weights to deblur old memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">C</forename><surname>Plaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth annual conference of the Cognitive Science Society</title>
		<meeting>the ninth annual conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Younger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conwell</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3045118.3045167" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kieran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agnieszka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00796</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimmy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4247" to="4255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Random synaptic feedback weights support error backpropagation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cownden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tweed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akerman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Working memory capacity: Limits on the bandwidth of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Earl</forename><forename type="middle">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>Buschman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Daedalus</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="112" to="122" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03141</idno>
		<title level="m">Meta-learning with temporal convolutions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Explanationbased neural network learning for robot control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sebastian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="287" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Task switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Monsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="134" to="140" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Networks</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/munkhdalai17a.html" />
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, Doina and Teh, Yee Whye</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Direct feedback alignment provides learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arild</forename><surname>N?kland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1037" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Film</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07871</idno>
		<title level="m">Visual reasoning with a general conditioning layer</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Task set and prefrontal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuyuki</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="219" to="245" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Evolutionary principles in selfreferential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>Technical University of Munich</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A &apos;self-referential&apos; weight matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PROCEEDINGS OF THE INTERNATIONAL CON-FERENCE ON ARTIFICIAL NEURAL NETWORKS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="446" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cortical information flow during flexible sensorimotor decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buschman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Earl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="issue">6241</biblScope>
			<biblScope unit="page" from="1352" to="1355" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Compete to compute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kazerounian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohrob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2310" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic coding for cognitive control in prefrontal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">G</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kusunoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Makoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Natasha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gaffan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="364" to="375" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Rob. Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kenta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Clayton</surname></persName>
		</author>
		<ptr target="http://learningsys.org/papers/LearningSys_2015_paper_33.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A perspective view and survey of meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Vilalta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Drissi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="95" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
