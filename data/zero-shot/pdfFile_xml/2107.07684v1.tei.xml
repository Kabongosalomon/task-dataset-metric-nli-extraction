<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CutDepth: Edge-aware Data Augmentation in Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasunori</forename><surname>Ishii</surname></persName>
							<email>ishii.yasunori@jp.panasonic.com</email>
							<affiliation key="aff0">
								<address>
									<postCode>1006</postCode>
									<settlement>Panasonic, Kadoma, Kadoma City, Osaka</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayoshi</forename><surname>Yamashita</surname></persName>
							<email>yamashita@isc.chubu.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Chubu University</orgName>
								<address>
									<addrLine>1200 Matsumotocho</addrLine>
									<settlement>Kasugai</settlement>
									<region>Aichi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CutDepth: Edge-aware Data Augmentation in Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>(a) Input image (b) Depth (c) Proposed method (d) CutOut (e) RE (f) CutMix Figure 1. Examples of data augmentation</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is difficult to collect data on a large scale in a monocular depth estimation because the task requires the simultaneous acquisition of RGB images and depths. Data augmentation is thus important to this task. However, there has been little research on data augmentation for tasks such as monocular depth estimation, where the transformation is performed pixel by pixel. In this paper, we propose a data augmentation method, called CutDepth. In CutDepth, part of the depth is pasted onto an input image during training. The method extends variations data without destroying edge features. Experiments objectively and subjectively show that the proposed method outperforms conventional methods of data augmentation. The estimation accuracy is improved with CutDepth even though there are few training data at long distances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>It is difficult to collect data on a large scale in a monocular depth estimation because the task requires the simultaneous acquisition of RGB images and depths. Data augmentation is thus important to this task. However, there has been little research on data augmentation for tasks such as monocular depth estimation, where the transformation is performed pixel by pixel. In this paper, we propose a data augmentation method, called CutDepth. In CutDepth, part of the depth is pasted onto an input image during training. The method extends variations data without destroying edge features. Experiments objectively and subjectively show that the proposed method outperforms conventional methods of data augmentation. The estimation accuracy is improved with CutDepth even though there are few training data at long distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Data augmentation is a practical way of improving the recognition performance without increasing the computational cost of inference. Various data augmentation methods have been proposed in the field of computer vision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. These data augmentation methods have been studied for higher-order tasks (e.g., object recogni-tion). However, there has been little research on lower-order tasks, such as pixel-by-pixel transformations (e.g., monocular depth estimation).</p><p>The monocular depth estimation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> estimates depth from a single-view RGB image. This task requires the simultaneous acquisition of RGB images and depths. Data augmentation is required because it is difficult to collect a large amount of data. Image flipping, random cropping, and color and luminance transformation are often used for data augmentation. However, few studies have examined data augmentation that changes the geometry because such augmentation reduces the estimation accuracy.</p><p>Yoo et al. <ref type="bibr" target="#b11">[12]</ref> proposed CutBlur, a data augmentation that adopts super-resolution. CutBlur improves the estimation accuracy by pasting a partial region of a high-resolution image onto the same position of a low-resolution image. The adoption of super-resolution generally has the problem of over-sharpening the estimated image. CutBlur, however, has a regularization effect, which prevents excessive sharpening and reduces the estimation error. Ghiasi et al. <ref type="bibr" target="#b1">[2]</ref> proposed a data augmentation for segmentation in which images are cut and pasted in units of the instance.</p><p>We propose CutDepth, which is a data augmentation that pastes the area cut from the teacher data (depth) at the same position on the input image (RGB image). By replacing a part of the input image with depth, the change in appearance is greater than that in conventional data augmentations. Meanwhile, the change is small at the lower feature level because the edge positions of the depth and the RGB image are similar. CutDepth regularizes the image by depth because depth information is given to the input image. Therefore, the distance between the RGB image and the depth decreases in the latent space, and it becomes easier to estimate the depth.</p><p>The contributions of our work are as follows.</p><p>? We propose a new data augmentation method that both improves visual diversity and suppresses excessive geometric changes in the scene.</p><p>? We show the quality of the data distribution after data augmentation in terms of diversity and affinity.</p><p>? We show that the depth estimation performance is improved subjectively and objectively for a real image using the proposed data augmentation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Data augmentation</head><p>Optical transformations and geometric transformations can be conducted for data augmentation <ref type="bibr" target="#b7">[8]</ref>. The former transformations include changing luminance and colors whereas the latter transformations include image flipping, translation, affine transformation, and random clipping.</p><p>There are methods of making changes optically and geometrically by replacing a partial area of the image with other information <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>  <ref type="figure" target="#fig_0">(Figure 1</ref>). CutOut <ref type="bibr" target="#b0">[1]</ref> and Random Erasing (RE) <ref type="bibr" target="#b13">[14]</ref> replace a portion of the image with the average value of the image or a random number. CutMix <ref type="bibr" target="#b12">[13]</ref> replaces a portion of an image with another image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Monocular depth estimation</head><p>Monocular depth estimation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> is the task of estimating the depth from a single-viewpoint image. In monocular depth estimation, the input is an RGB image and the teacher data are of depth. BTS <ref type="bibr" target="#b6">[7]</ref> has a structure that implicitly estimates the normal in a decoder. Monocular depth estimation has difficulty in estimating object contours. However, the BTS structure improves the accuracy of the contour estimation. Laplacian Depth <ref type="bibr" target="#b9">[10]</ref> uses a Laplacian pyramid structure in a decoder. The structure uses the residuals obtained from the input image for each resolution. It effectively estimates both local details and the global layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data augmentation for depth estimation</head><p>We propose a data augmentation called CutDepth <ref type="figure">(Figure 2)</ref>. Let x s ? R W ?H?Cs be the input (RGB) image and x t ? R W ?H?Ct be the teacher (depth) data. W and H are, respectively, the width and height of the image and C s and C t are, respectively, the numbers of channels in the input image and the teacher data. x s with data augmentation on x s is obtained as</p><formula xml:id="formula_0">x s = M * x s + (1 ? M ) * x t .<label>(1)</label></formula><p>If C s and C t are different, they are combined in the channel direction so that they are the same number in advance. M is a matrix (M ? {0, 1}) that indicates the region where x s is replaced by x t . The position (l, u) and size (w, h) of the . <ref type="figure">Figure 2</ref>. Data augmentation using CutDepth replacement region are obtained as</p><formula xml:id="formula_1">(l, u) = (a ? W, b ? H) (2) (w, h) = (min((W ? a ? W ) ? c ? p, 1), (3) min((H ? b ? H) ? d ? p, 1)))<label>(4)</label></formula><p>where a, b, c, and d are U(0, 1). p is a hyperparameter that determines the maximum values of w and h for W and H, and it is set at a value of (0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setting</head><p>We use BTS <ref type="bibr" target="#b6">[7]</ref> and Laplacian Depth <ref type="bibr" target="#b9">[10]</ref> to evaluate the performance of the depth estimation. The optimizers are Adam and AdamW respectively. The learning rate is 10 ?4 for both, and the scheduling is performed with a polynomial decay (power of 0.9 and 0.5, respectively). The encoders are DenseNet161 <ref type="bibr" target="#b5">[6]</ref> and ResNext-101 <ref type="bibr" target="#b10">[11]</ref>, pre-trained on Im-ageNet. The baseline data augmentation method uses horizontal flipping, color transformations, and image rotation randomly.</p><p>We use the NYU Depth V2 Dataset <ref type="bibr" target="#b8">[9]</ref>. The image size used for training is 416 ? 544 and that used for testing is 480 ? 640, as in BTS <ref type="bibr" target="#b6">[7]</ref>. As in BTS, 24,231 RGB image/depth pairs are used for training and 654 images are used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Depth estimation results</head><p>In addition to the baseline method, we use CutOut <ref type="bibr" target="#b0">[1]</ref>, RE <ref type="bibr" target="#b13">[14]</ref>, and CutMix <ref type="bibr" target="#b12">[13]</ref>. <ref type="table" target="#tab_1">Table 3</ref> gives the evaluation results. The proposed method outperforms the conventional methods on all metrics. The depth estimation performance of the proposed method tended to be higher when p was 0.5 or 0.75. The performances of the conventional methods decreased as p increased. The performance of the proposed method did not degrade with increasing p owing to the small change in the lower-order feature levels. We next compare the depth estimation performances when the number of data are 25 %, 50 %, and 75 % of the original number and p is set at 0.75. We randomly sample the data from the original dataset. <ref type="table" target="#tab_1">Table 3</ref> gives the evaluation results for different sizes of data. Even if the data size was small, the proposed method outperformed the other methods on various measures. <ref type="figure" target="#fig_1">Figure 3</ref> shows example results of depth estimation. Near distances are represented in blue and far distances in red. The proposed method outperformed the conventional methods in terms of the accuracy of the estimation for far distances and object contours. The conventional training model readily overfit some data because there were few data at a distance. However, the proposed method had better accuracy of the data at a distance because overfitting was reduced by regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of the effect of regularization</head><p>The distance between the RGB image and depth in the latent space becomes small if the image is regularized by depth. Therefore, to verify the regularization effect, we compare the distances in the latent space, which are the output of the BTS encoder, when the RGB image and depth are input to the BTS model. The root-mean-square error (RMSE), mean absolute error (MAE), and cosine distance are used as distance measures. <ref type="table" target="#tab_1">Table 3</ref> gives the comparison results. In terms of the RMSE and MAE, the distances of the proposed method and CutMix are comparable. However, the cosine distance is small for the proposed method. It is difficult to see the difference between the RMSE and MAE because of the small scale of the feature map. However, the difference becomes clear for the cosine distance where the scale is normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation of the properties of data augmentation</head><p>We examine the properties of data augmentation. Gontijo-Lopes et al. <ref type="bibr" target="#b3">[4]</ref> proposed the measurement of the properties of data augmentation in terms of diversity and affinity. A larger value of diversity corresponds to a greater spread of the data distribution due to data augmentation. A larger value of affinity corresponds to a smaller deviation from the original data distribution. <ref type="figure" target="#fig_2">Figure 4</ref> plots the diversity and affinity of each method. Pink circles show the results of the proposed method and are encircled by a red dotted ellipse for clarity. Both the diversity and affinity of the proposed method are greater than those of the baseline method, and the distribution is thus more spread out and the deviation is smaller than for the original data. The proposed method has lower diversity than the various conventional methods, and the effect of suppressing excessive changes in edge features is thus confirmed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a data augmentation method, called Cut-Depth, for depth estimation. CutDepth is a method of past- ing part of the depth to an input image, which increases the variation of the input image. We confirmed that the estimation accuracy of the proposed method is better than that of conventional methods. In the proposed method, the edge features are similar before and after the data augmentation. We therefore found that the proposed method does not expand the data distribution excessively compared with the conventional methods. In future work, we will test the effectiveness of the proposed method in tasks other than depth estimation.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples of data augmentation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Depth estimation results obtained using different data augmentation methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of diversity and affinity between different data augmentation methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the depth estimation performances when using different data augmentation methods. Lower Abs Rel, log10, RMSE and RMSE log indicate higher performance whereas higher d1, d2 and d3 indicate higher performance. The best performances are presented in bold text.Abs Rel ? log10 ? RMSE ? RMSE log ? d1 ? d2 ? d3 ? Abs Rel ? log10 ? RMSE ? RMSE log ? d1 ?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BTS</cell><cell></cell><cell></cell><cell></cell><cell>Laplacian Depth</cell></row><row><cell cols="2">Method</cell><cell>p</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>d2 ?</cell><cell>d3 ?</cell></row><row><cell cols="2">Baseline</cell><cell></cell><cell></cell><cell>0.1122</cell><cell></cell><cell>0.048</cell><cell>0.406</cell><cell>0.145</cell><cell cols="2">0.878 0.979 0.995</cell><cell>0.11</cell><cell>0.047</cell><cell>0.39</cell><cell>0.139</cell><cell>0.884 0.983 0.996</cell></row><row><cell></cell><cell></cell><cell cols="2">0.25</cell><cell>0.1122</cell><cell></cell><cell>0.048</cell><cell>0.405</cell><cell>0.144</cell><cell cols="2">0.878 0.98 0.996</cell><cell>0.106</cell><cell>0.046</cell><cell>0.384</cell><cell>0.136</cell><cell>0.891 0.984 0.996</cell></row><row><cell cols="2">CutOut</cell><cell cols="2">0.50 0.75</cell><cell>0.1118 0.1146</cell><cell></cell><cell>0.048 0.05</cell><cell>0.402 0.414</cell><cell>0.144 0.148</cell><cell cols="2">0.879 0.981 0.996 0.871 0.979 0.996</cell><cell>0.109 0.106</cell><cell>0.046 0.045</cell><cell>0.382 0.382</cell><cell>0.137 0.135</cell><cell>0.889 0.983 0.997 0.893 0.985 0.997</cell></row><row><cell></cell><cell></cell><cell cols="2">1.00</cell><cell>0.1194</cell><cell></cell><cell>0.051</cell><cell>0.427</cell><cell>0.152</cell><cell cols="2">0.864 0.977 0.996</cell><cell>0.11</cell><cell>0.047</cell><cell>0.394</cell><cell>0.14</cell><cell>0.884 0.984 0.997</cell></row><row><cell cols="2">Random Erasing</cell><cell cols="2">0.25 0.50 0.75</cell><cell>0.1106 0.1116 0.1132</cell><cell></cell><cell>0.048 0.048 0.049</cell><cell>0.4 0.4 0.415</cell><cell>0.143 0.143 0.147</cell><cell cols="2">0.88 0.981 0.996 0.881 0.981 0.996 0.871 0.979 0.996</cell><cell>0.109 0.106 0.106</cell><cell>0.046 0.045 0.045</cell><cell>0.384 0.378 0.379</cell><cell>0.137 0.134 0.134</cell><cell>0.89 0.982 0.996 0.892 0.985 0.997 0.893 0.985 0.997</cell></row><row><cell></cell><cell></cell><cell cols="2">1.00</cell><cell>0.1186</cell><cell></cell><cell>0.051</cell><cell>0.429</cell><cell>0.152</cell><cell cols="2">0.863 0.977 0.996</cell><cell>0.111</cell><cell>0.047</cell><cell>0.394</cell><cell>0.14</cell><cell>0.884 0.983 0.997</cell></row><row><cell></cell><cell></cell><cell cols="2">0.25</cell><cell>0.1105</cell><cell></cell><cell>0.047</cell><cell>0.397</cell><cell>0.142</cell><cell cols="2">0.882 0.981 0.996</cell><cell>0.107</cell><cell>0.046</cell><cell>0.388</cell><cell>0.137</cell><cell>0.889 0.983 0.996</cell></row><row><cell cols="2">CutMix</cell><cell cols="2">0.50 0.75</cell><cell>0.1132 0.1231</cell><cell></cell><cell>0.049 0.054</cell><cell>0.406 0.438</cell><cell>0.146 0.158</cell><cell cols="2">0.874 0.979 0.996 0.848 0.976 0.996</cell><cell>0.107 0.107</cell><cell>0.046 0.046</cell><cell>0.386 0.386</cell><cell>0.136 0.136</cell><cell>0.891 0.983 0.996 0.891 0.983 0.996</cell></row><row><cell></cell><cell></cell><cell cols="2">1.00</cell><cell>0.1851</cell><cell></cell><cell>0.086</cell><cell>0.674</cell><cell>0.241</cell><cell cols="2">0.659 0.918 0.982</cell><cell>0.11</cell><cell>0.047</cell><cell>0.391</cell><cell>0.139</cell><cell>0.886 0.982 0.996</cell></row><row><cell></cell><cell></cell><cell cols="2">0.25</cell><cell>0.1083</cell><cell></cell><cell>0.047</cell><cell>0.398</cell><cell>0.141</cell><cell cols="2">0.884 0.981 0.996</cell><cell>0.106</cell><cell>0.045</cell><cell>0.38</cell><cell>0.135</cell><cell>0.895 0.984 0.996</cell></row><row><cell cols="2">Proposed</cell><cell cols="2">0.50 0.75</cell><cell>0.1077 0.1074</cell><cell></cell><cell>0.046 0.047</cell><cell>0.391 0.392</cell><cell>0.14 0.14</cell><cell cols="2">0.884 0.982 0.997 0.885 0.982 0.996</cell><cell>0.104 0.106</cell><cell>0.044 0.045</cell><cell>0.375 0.379</cell><cell>0.132 0.135</cell><cell>0.899 0.985 0.997 0.894 0.984 0.997</cell></row><row><cell></cell><cell></cell><cell cols="2">1.00</cell><cell>0.1127</cell><cell></cell><cell>0.047</cell><cell>0.392</cell><cell>0.142</cell><cell cols="2">0.88 0.981 0.996</cell><cell>0.104</cell><cell>0.045</cell><cell>0.376</cell><cell>0.132</cell><cell>0.898 0.985 0.996</cell></row><row><cell cols="11">Table 2. Comparison of the depth estimation performances when</cell><cell></cell></row><row><cell cols="11">using different numbers of data (p = 0.75). Lower Abs Rel, log10,</cell><cell></cell></row><row><cell cols="11">RMSE and RMSE log indicate higher performance whereas higher</cell><cell></cell></row><row><cell cols="8">d1, d2 and d3 indicate higher performance.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Scale</cell><cell cols="2">Method</cell><cell cols="6">Abs Rel ? log10 ? RMSE ? RMSE log ? d1 ?</cell><cell>d2 ?</cell><cell>d3 ?</cell><cell></cell></row><row><cell></cell><cell cols="2">Baseline</cell><cell></cell><cell>0.1226</cell><cell>0.052</cell><cell>0.428</cell><cell>0.154</cell><cell cols="3">0.859 0.977 0.995</cell><cell></cell></row><row><cell></cell><cell cols="2">CutOut</cell><cell></cell><cell>0.1242</cell><cell>0.053</cell><cell>0.432</cell><cell>0.156</cell><cell cols="3">0.854 0.976 0.996</cell><cell></cell></row><row><cell>25%</cell><cell>RE</cell><cell></cell><cell></cell><cell>0.1268</cell><cell>0.054</cell><cell>0.440</cell><cell>0.158</cell><cell cols="3">0.848 0.976 0.995</cell><cell></cell></row><row><cell></cell><cell cols="2">CutMix</cell><cell></cell><cell>0.1467</cell><cell>0.064</cell><cell>0.520</cell><cell>0.188</cell><cell cols="3">0.782 0.956 0.993</cell><cell></cell></row><row><cell></cell><cell cols="2">Proposed</cell><cell></cell><cell>0.1225</cell><cell>0.052</cell><cell>0.424</cell><cell>0.153</cell><cell cols="3">0.858 0.978 0.995</cell><cell></cell></row><row><cell></cell><cell cols="2">Baseline</cell><cell></cell><cell>0.1174</cell><cell>0.050</cell><cell>0.414</cell><cell>0.150</cell><cell cols="3">0.867 0.978 0.995</cell><cell></cell></row><row><cell></cell><cell cols="2">CutOut</cell><cell></cell><cell>0.1168</cell><cell>0.050</cell><cell>0.418</cell><cell>0.150</cell><cell cols="3">0.867 0.979 0.996</cell><cell></cell></row><row><cell>50%</cell><cell>RE</cell><cell></cell><cell></cell><cell>0.1184</cell><cell>0.051</cell><cell>0.422</cell><cell>0.151</cell><cell cols="3">0.862 0.978 0.996</cell><cell></cell></row><row><cell></cell><cell cols="2">CutMix</cell><cell></cell><cell>0.1307</cell><cell>0.056</cell><cell>0.460</cell><cell>0.168</cell><cell cols="3">0.832 0.970 0.994</cell><cell></cell></row><row><cell></cell><cell cols="2">Proposed</cell><cell></cell><cell>0.1155</cell><cell>0.049</cell><cell>0.411</cell><cell>0.148</cell><cell cols="3">0.870 0.981 0.996</cell><cell></cell></row><row><cell></cell><cell cols="2">Baseline</cell><cell></cell><cell>0.1154</cell><cell>0.049</cell><cell>0.410</cell><cell>0.147</cell><cell cols="3">0.871 0.979 0.996</cell><cell></cell></row><row><cell></cell><cell cols="2">CutOut</cell><cell></cell><cell>0.1148</cell><cell>0.050</cell><cell>0.413</cell><cell>0.147</cell><cell cols="3">0.870 0.980 0.997</cell><cell></cell></row><row><cell>75%</cell><cell>RE</cell><cell></cell><cell></cell><cell>0.1179</cell><cell>0.051</cell><cell>0.424</cell><cell>0.151</cell><cell cols="3">0.863 0.977 0.996</cell><cell></cell></row><row><cell></cell><cell cols="2">CutMix</cell><cell></cell><cell>0.1353</cell><cell>0.058</cell><cell>0.465</cell><cell>0.172</cell><cell cols="3">0.826 0.967 0.993</cell><cell></cell></row><row><cell></cell><cell cols="2">Proposed</cell><cell></cell><cell>0.1142</cell><cell>0.048</cell><cell>0.401</cell><cell>0.144</cell><cell cols="3">0.876 0.981 0.996</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparison of the distances between the RGB image and depth in the latent spacep RMSE ? MAE ? Cosine ?</figDesc><table><row><cell>Baseline</cell><cell></cell><cell>1.094</cell><cell>0.49</cell><cell>0.24</cell></row><row><cell></cell><cell>0.25</cell><cell>1.12</cell><cell>0.50</cell><cell>0.21</cell></row><row><cell>CutOut</cell><cell>0.50 0.75</cell><cell>1.16 1.20</cell><cell>0.52 0.52</cell><cell>0.17 0.17</cell></row><row><cell></cell><cell>1.00</cell><cell>1.39</cell><cell>0.61</cell><cell>0.15</cell></row><row><cell>Random Erasing</cell><cell>0.25 0.50 0.75</cell><cell>1.05 1.09 1.13</cell><cell>0.48 0.49 0.50</cell><cell>0.22 0.20 0.17</cell></row><row><cell></cell><cell>1.00</cell><cell>1.17</cell><cell>0.52</cell><cell>0.17</cell></row><row><cell></cell><cell>0.25</cell><cell>1.03</cell><cell>0.47</cell><cell>0.28</cell></row><row><cell>CutMix</cell><cell>0.50 0.75</cell><cell>0.92 0.95</cell><cell>0.41 0.43</cell><cell>0.22 0.20</cell></row><row><cell></cell><cell>1.00</cell><cell>1.35</cell><cell>0.50</cell><cell>0.12</cell></row><row><cell></cell><cell>0.25</cell><cell>0.92</cell><cell>0.42</cell><cell>0.37</cell></row><row><cell>Proposed</cell><cell>0.50 0.75</cell><cell>1.06 0.96</cell><cell>0.48 0.44</cell><cell>0.37 0.35</cell></row><row><cell></cell><cell>1.00</cell><cell>1.07</cell><cell>0.48</cell><cell>0.33</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07177</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Keepaugment: A simple informationpreserving data augmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11778,2020.1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Affinity and diversity: Quantifying mechanisms of data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Gontijo-Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><forename type="middle">J</forename><surname>Smullin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08973,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Kyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Wook</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Hong</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using laplacian pyramid-based depth residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokjae</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking data augmentation for image super-resolution: A comprehensive analysis and a new strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhyuk</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Ah</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8375" to="8384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
