<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PONet: Robust 3D Human Pose Estimation via Learning Orientations Only</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20211">AUGUST 2021 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Class</forename><surname>Journal Of L A T E Xe</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">PONet: Robust 3D Human Pose Estimation via Learning Orientations Only</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20211">AUGUST 2021 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D Human Pose</term>
					<term>keypoint</term>
					<term>Robust Pose</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional 3D human pose estimation relies on first detecting 2D body keypoints and then solving the 2D to 3D correspondence problem. Despite the promising results, this learning paradigm is highly dependent on the quality of the 2D keypoint detector, which is inevitably fragile to occlusions and out-of-image absences. In this paper, we propose a novel Pose Orientation Net (PONet) that is able to robustly estimate 3D pose by learning orientations only, hence bypassing the errorprone keypoint detector in the absence of image evidence. For images with partially invisible limbs, PONet estimates the 3D orientation of these limbs by taking advantage of the local image evidence to recover the 3D pose. Moreover, PONet is competent to infer full 3D poses even from images with completely invisible limbs, by exploiting the orientation correlation between visible limbs to complement the estimated poses, further improving the robustness of 3D pose estimation. We evaluate our method on multiple datasets, including Human3.6M, MPII, MPI-INF-3DHP, and 3DPW. Our method achieves results on par with state-ofthe-art techniques in ideal settings, yet significantly eliminates the dependency on keypoint detectors and the corresponding computation burden. In highly challenging scenarios, such as truncation and erasing, our method performs very robustly and yields much superior results as compared to state of the art, demonstrating its potential for real-world applications. Our code will be made publicly available. Please see our video results in the supplementary materials for pose estimation in extreme cases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H Uman pose estimation aims at recovering the coordinates of a human body from one or multiple images, and therefore plays a vital role in an exceptionally broad spectrum of applications. State-of-the-art 3D human pose estimation methods <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b15">[14]</ref>, <ref type="bibr" target="#b20">[19]</ref>, <ref type="bibr" target="#b21">[20]</ref>, <ref type="bibr" target="#b25">[24]</ref>, <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b54">[53]</ref> rely on first detecting several 2D keypoints, like the body joints, from the image, followed by mapping the 2D keypoint locations back to the 3D world. The advantage of building 3D pose estimation on 2D keypoint detection lies in that the former can inherit the good generalization capacity from the latter. Despite its popularity, this scheme still suffers from several flaws. Firstly, methods built on 2D keypoint detection are usually sensitive to the 2D detection errors, since the 2D-to-3D mapping is a highly ill-posed and -conditioned problem. Minor errors in the locations of the 2D keypoints may lead to major drifts in the 3D results. Secondly, 2D keypoint detection requires all the body joints to be visible in the image. Such prerequisite is justifiable for monitored lab environments, but unfortunately * Corresponding authors. Our PONet is very robust to images with invisible limbs. More visual results on challenging scenarios can be found in our supplement.</p><p>too strong for practical application scenarios, where out-ofimage absences of body joints and heavy occlusions frequently occur and thus deteriorate the 3D estimation results. Some recent endeavors have thus focused on bypassing 2D keypoint detection through recovering human mesh. For instance, Kanazawa et allet@tokeneonedot <ref type="bibr" target="#b14">[13]</ref> propose a method that directly estimates the parameters of SMPL <ref type="bibr" target="#b23">[22]</ref> and then infer the 3D pose from the 3D body shape. These methods provide richer 3D information about the body, like 3D human skin, while satisfying the anthropometric constrains. Although this method is capable to handle keypoint-absent cases, it would fail if a whole limb 1 is visually absent.</p><p>In this paper, we attempt to study 3D human pose estimation in challenging scenarios, where image evidences are incomplete. To this end, we first categorize such evidenceincomplete images into two levels of difficulties: images with only missing joints, i.elet@tokeneonedoteach limb is at least partially visible, and images with missing limbs, i.elet@tokeneonedotat least one limb is completely invisible. The latter case is unarguably more demanding than the former, since more visual cues are absent. However, state-of-the-art methods and even benchmarks have been focused on complete images or the former case only, yet have largely overlooked the latter, which, unfortunately, better reflects in-the-wild data in real-world applications.</p><p>We propose here a robust 3D pose estimation termed as Pose Orientation Net (PONet), which allows us to effortlessly to handle both scenarios, as demonstrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. At the heart of our approach is region-based 3D orientation learning and pose complementation. Specifically, our method estimates limb confidence maps, 2D and 3D limb orientation maps directly from images using a three-branch multi-stage fully convolutional neural network (FCNN). The limb confidence map represents the probability of each pixel within the corresponding limb region, while the 2D and 3D orientation maps represent the limb orientations in the pixel space and 3D space, respectively. The 3D pose estimation is produced by integrating the estimated 3D limb orientations with an embedded skeleton-shaped human model, which consists of free joints and fixed-length limbs. This region-based orientation learning makes our method robust to the cases where keypoints are missing. The motivation behind such a design lies in that, 3D limb orientations enable 3D human pose recovery by utilizing the estimated directional vector oriented from a neighboring body part, even if the body part of interest is visually incomplete.</p><p>The proposed method also includes a pose complementation sub-network, which aims to estimate a full 3D pose when some limbs are missing. The idea is to infer the 3D configuration of missing body parts from the visible parts, using the prediction confidence as an indicator. Intuitively, the correlation between 3D limb orientations is stronger than that between keypoints, making it more dependable to infer a full pose based on orientation relations between visible limbs. We thereby infer the 3D orientations of invisible limbs from initial 3D orientation predictions of visible limbs, by taking 3D orientation correlation and limb confidence into account. The pose complementation sub-network further improves the robustness of 3D pose estimation, especially on images with inaccurate bounding boxes and large translation.</p><p>Unlike prior orientation-based methods that usually combine orientation learning with 2D keypoint detection, all the three maps adopted in our method are limb-region-based representation. Such a design not only allows us to handily carry out knowledge transfer to images without 3D annotations by using limb region estimation and 2D orientation learning, which can be trained with both 2D and 3D data, as a bridge, but also enable differentiable post-processing and end-to-end training. Besides, we take the average 3D orientation vector, weighted by the region confidence of each pixel, as the final estimated 3D limb orientation. This pixel-wise voting scheme reduces the effect of noise and outliers in the estimated 3D orientation maps, making the 3D orientation estimation and pose estimation more robust and stable.</p><p>We evaluate our method on several benchmarks, including Human3.6M, MPII, MPI-INF-3DHP, and 3DPW. Our proposed method achieves performance on par with stateof-the-art techniques in standard settings, but shows much stronger generalization capacity to unseen in-the-wild images. Moreover, unlike prior methods that usually rely on external keypoint detectors <ref type="bibr" target="#b9">[8]</ref>, <ref type="bibr" target="#b20">[19]</ref>, <ref type="bibr" target="#b21">[20]</ref>, <ref type="bibr" target="#b25">[24]</ref>, <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b35">[34]</ref> or complex human models <ref type="bibr" target="#b4">[3]</ref>, <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b32">[31]</ref>, <ref type="bibr" target="#b49">[48]</ref>, our method stands on its own and does not require any other third-party pre-trained model. Most importantly, our method significantly outperforms state-of-the-art techniques on robustness testing including translation, occlusion, and various erasing, demonstrating its potential for real-world applications.</p><p>Our main contributions are summarized as follows.</p><p>? We propose PONet for robust 3D human pose estimation, which is capable to infer the complete 3D pose even when the input image is incomplete. Our method significantly outperforms state-of-the-art techniques in challenging scenarios such as heavy object-occlusion, large translation and various erasing. ? We propose a region-based orientation learning, which allows us to handily carry out knowledge transfer to in-the-wild images without 3D annotations. Our method yields very promising results on MPI-INF-3DHP and 3DPW without training on them, validating its gratifying generalization capacity. ? We propose a differentiable voting scheme to extract orientations from predicted orientation maps, allowing us to end-to-end train the network and making the orientation estimation more robust and stable.</p><p>II. RELATED WORK In this section, we briefly review monocular 3D human pose estimation approaches by dividing them into three categories that may overlap each other, i.elet@tokeneonedot2D keypoint detection based, human mesh recovery based and orientationbased approaches.</p><p>2D keypoint detection based. 2D human pose estimation has witnessed unprecedented progress in recent years and lots of 3D pose estimation methods are built on this technique. Early efforts on 3D pose estimation used dictionary learning, with the assumption that a 3D pose can be represented by a sparse linear combination of a set of basis poses <ref type="bibr" target="#b44">[43]</ref>, <ref type="bibr" target="#b51">[50]</ref>, <ref type="bibr" target="#b53">[52]</ref>- <ref type="bibr" target="#b55">[54]</ref>. Recently, more and more researchers start to use deep neural networks for 3D pose regression <ref type="bibr" target="#b3">[2]</ref>, <ref type="bibr" target="#b11">[10]</ref>, <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b20">[19]</ref>, <ref type="bibr" target="#b21">[20]</ref>, <ref type="bibr" target="#b25">[24]</ref>, <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b39">[38]</ref>, <ref type="bibr" target="#b40">[39]</ref>. For instance, Martinez et allet@tokeneonedot <ref type="bibr" target="#b25">[24]</ref> propose a light weighted fully connected network with residual connections and achieved impressing results. Lee et allet@tokeneonedot <ref type="bibr" target="#b20">[19]</ref> propose a long short-term memory (LSTM) architecture to reconstruct 3D depth from the centroid to edge joints through learning the joint inter-dependencies. Li et allet@tokeneonedot <ref type="bibr" target="#b21">[20]</ref> improves 2D-to-3D regression by synthesizing new 2D-3D pairs with evolution algorithm. Cheng et allet@tokeneonedot <ref type="bibr" target="#b8">[7]</ref> use explicit occlusion augmentation to improve the robustness to keypoint detection and 3D pose estimation in image sequences. The major problem of keypoint-detection-based approaches is that 2D-to-3D lifting is ill-posed and ill-conditioned. Minor errors in the locations of 2D keypoints can have large consequences in the 3D results. In addition, these methods are prone to ambiguities in the 2D to 3D correspondence step.</p><p>Human mesh recovery based. Human mesh recovery is a highly related topic with 3D human pose estimation. Several recent works deal with both tasks. These methods <ref type="bibr" target="#b3">[2]</ref>, <ref type="bibr" target="#b4">[3]</ref>, <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b15">[14]</ref>, <ref type="bibr" target="#b19">[18]</ref>, <ref type="bibr" target="#b34">[33]</ref>, <ref type="bibr" target="#b41">[40]</ref>, <ref type="bibr" target="#b42">[41]</ref> recover the 3D human body shape based on the generative body model SMPL <ref type="bibr" target="#b23">[22]</ref>. Bogo et allet@tokeneonedot <ref type="bibr" target="#b4">[3]</ref> propose SM-PLify, an optimization-based method to recover SMPL parameters from detected 2D joints that leverages multiple priors. Lassner et allet@tokeneonedot <ref type="bibr" target="#b19">[18]</ref> take curated results from SMPLify to train 91 keypoint detectors, some of which correspond to the traditional body joints and others correspond to locations on the surface of the body. Kanazawa et allet@tokeneonedot <ref type="bibr" target="#b14">[13]</ref> directly infer SMPL parameters from images without relying on detected 2D keypoints, allowing shape and pose estimation on images with missing joints. However, these methods would fail on images with missing limbs.</p><p>Orientation learning based There are a handful of approaches try to learn orientations for 3D pose estimation. Zhou et allet@tokeneonedot <ref type="bibr" target="#b52">[51]</ref> propose a kinematic human model that adds different constrains to different joints, to simulate real human structure. They use CNN to learn rotation angles for the adjustable joints. Cao et allet@tokeneonedot <ref type="bibr" target="#b6">[5]</ref> propose part affinity field (PAF) to help linking the keypoints on a person in the multi-person 2D pose estimation problem. Xiang et allet@tokeneonedot <ref type="bibr" target="#b46">[45]</ref> use PAFs to fit a deformable mesh model to recover 3D shape and pose. Luo et allet@tokeneonedot <ref type="bibr" target="#b24">[23]</ref> introduce OriNet that predicts 2D keypoint heatmaps and 3D PAFs for 3D pose estimation. In Liu et allet@tokeneonedot's work <ref type="bibr" target="#b22">[21]</ref>, 3D orientations are used as additional image evidence to improve the 2D-to-3D regression. Shi et allet@tokeneonedot <ref type="bibr" target="#b37">[36]</ref> predict 3D pose by estimates 3D orientations and limb lengths from 2D keypoints. Again, these approaches rely heavily on 2D keypoints. As a result, they are fragile to keypoint absences as well, let alone cases with limb absence.</p><p>Our approach. The proposed approach estimates 3D pose by only learning limb orientations from images without detecting 2D keypoints. Our region-based orientation learning allows 3D pose estimation on images where the limbs are partially visible. In addition, the proposed PONet can infer a full body pose from images with completely invisible limbs, by exploiting the orientation correlation between visible limbs. Compared to traditional keypoint-detection-based and SMPLbased 3D pose estimation approaches, our method eliminates the dependency on 2D keypoint detectors, and is much more robust to images with visually absent joints or limbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>The inference pipeline of the propose PONet is illustrated in <ref type="figure" target="#fig_4">Fig. 2</ref>. Our method takes as input a single color image ( <ref type="figure" target="#fig_4">Fig. 2a</ref>), which can be incomplete and have visually absent joints or limbs. Firstly, we use a three-branch multistage fully convolutional neural network ( <ref type="figure" target="#fig_4">Fig. 2b</ref>) to simultaneously predict three sets of maps, i.elet@tokeneonedotthe limb confidence mapsM C , 2D limb orientation mapsM 2D and 3D orientation mapsM 3D . Then we use a differentiable voting scheme to extract 3D limb orientations from the predicted maps. An initial 3D pose estimation ( <ref type="figure" target="#fig_4">Fig. 2c</ref>) is produced by integrating the predicted 3D limb orientations into a skeleton-shaped human model with fixed limb lengths.</p><p>Since the input image may be incomplete, we introduce a pose complementation sub-network ( <ref type="figure" target="#fig_4">Fig. 2e</ref>) to infer the 3D orientations of the missing limbs from that of the visible ones, so that the network can estimate a complete final 3D pose ( <ref type="figure" target="#fig_4">Fig.2f</ref>) even if the input image is incomplete. In the following sections, we explain these steps in detail.</p><p>A. Collaborative orientation learning 1) Limb orientation representation: Following <ref type="bibr" target="#b6">[5]</ref>, we use orientation maps to represent the limb orientations. Specifically, given an image of size w ? h, a 3D orientation map is a w ? h three-channel map, where each pixel within the limb region is a directional vector representing the 3D orientation of the limb in 3D space. The pixels outside the limb region are set to zero vectors. The 2D orientation maps are defined similarly as 3D orientation maps. The limb confidence map is a 1-channel map, where each pixel is a value between 0 and 1, representing the probability of the pixel within the limb region.</p><p>In human pose estimation, the exact human limb region is unavailable. For simplicity, we take the rectangular region defined by the two end points of the limb and a fixed width d as the limb region. When the 2D length of the limb is smaller than d, we take the square region centred at the mid-point of the limb as the limb region, to avoid a too small region while keeping the 2D orientation information. In this work, the 3D pose consists of 17 joints and thus 16 limbs. The FCNN predicts these three sets of maps simultaneously, that is 16 ? (1 + 2 + 3) = 96 channels in total.</p><p>2) Weakly-supervised 3D orientation learning: Prior methods usually require specially designed losses <ref type="bibr" target="#b10">[9]</ref>, <ref type="bibr" target="#b50">[49]</ref>, additional annotations <ref type="bibr" target="#b33">[32]</ref> or adversarial training <ref type="bibr" target="#b48">[47]</ref> to achieve knowledge transfer to in-the-wild images. By contrast, our PONet can handily generalize to in-the-wild images without extra bells and whistles but achieving better generalization capacity. Specifically, we use a three-branch multi-stage FCNN to predict the limb confidence maps, 2D and 3D orientation maps, respectively. Estimation of 2D orientation map is an auxiliary task, which is only used in the training phase to help learning orientation collaboratively and improve generalization. During training, each mini-batch is randomly sampled from two datasets, an in-the-wild 2D pose dataset MPII <ref type="bibr" target="#b2">[1]</ref> and an indoor 3D pose dataset Human3.6M <ref type="bibr" target="#b13">[12]</ref>, with equal probability. Since both datasets have 2D annotations, we can fully-supervised train the limb confidence map prediction branch and 2D orientation map prediction branch. For the 3D orientation branch, on average only a half of the training examples, i.elet@tokeneonedotthose sampled from the 3D dataset, have 3D orientation map supervisions. The other half of the training data are left unsupervised in this branch. Our experiments demonstrate that, the proposed PONet can easily generalize to in-the-wild images using this simple weaklysupervised training strategy and achieve much better generalization capacity than state-of-the-art methods.</p><p>In this paper, the confidence map estimation is taken to be a pixel-wise binary classification problem. We use the Binary    Cross Entropy (BCE) loss for this task:</p><formula xml:id="formula_0">V g Q j U K J v m 0 1 M 0 M T y k b 0 Q H v W K q o X R N M 5 h d P y Z l V + i R K t C 2 F Z K 7 + n p j Q 2 J h x H N r O m O L Q L H s z 8 T + v k 2 F 0 E 0 y E S j P k i i 0 W R Z k k m J D Z + 6 Q v N G c o x 5 Z Q p o W 9 l b A h 1 Z S h D a l k Q / C X X 1 4 l z Q v X v</formula><formula xml:id="formula_1">Z O E + x E d K h E K R t F K j 4 1 b 7 J f K b s W d g 6 w S L y d l y F H v l 7 5 6 g 5 i l E V f I J D W m 6 7 k J + h n V K J j k 0 2 I v N T y h b E y H v G u p o h E 3 f j Y / d U r O r T I g Y a x t K S R z 9 f d E R i N j J l F g O y O K I 7 P s z c T / v G 6 K 4 Y 2 f C Z W k y B V b L A p T S T A m s 7 / J Q G j O U E 4 s o U w L e y t h I 6 o p Q 5 t O 0 Y b g L b + 8 S l q X F</formula><formula xml:id="formula_2">Z O E + x E d K h E K R t F K j 4 1 b 7 J f K b s W d g 6 w S L y d l y F H v l 7 5 6 g 5 i l E V f I J D W m 6 7 k J + h n V K J j k 0 2 I v N T y h b E y H v G u p o h E 3 f j Y / d U r O r T I g Y a x t K S R z 9 f d E R i N j J l F g O y O K I 7 P s z c T / v G 6 K 4 Y 2 f C Z W k y B V b L A p T S T A m s 7 / J Q G j O U E 4 s o U w L e y t h I 6 o p Q 5 t O 0 Y b g L b + 8 S l q X F</formula><formula xml:id="formula_3">L t CM = BCE(M t C , M t C ),<label>(1)</label></formula><p>whereM t C and M t C represent the predicted and ground truth limb confidence maps at stage t, respectively. The 2D and 3D orientation map estimation is taken to be a regression problem and we use Mean Squared Error (MSE) as the loss function:</p><formula xml:id="formula_4">L t OM2D = MSE(M t 2D , M t 2D ),<label>(2)</label></formula><formula xml:id="formula_5">L t OM3D = MSE(M t 3D , M t 3D ),<label>(3)</label></formula><p>whereM t 2D/3D and M t 2D/3D denote the predicted and ground truth 2D/3D orientation maps at stage t, respectively.</p><p>3) Differentiable voting scheme for orientation extraction: Prior orientation based methods <ref type="bibr" target="#b6">[5]</ref>, <ref type="bibr" target="#b24">[23]</ref> extract the directional vectors from orientation maps by computing the line integral over the corresponding orientation maps along the line segment connecting the detected part locations. Many other candidate directional vectors that not on the line segment, which also contain important orientation information, however, are discarded. Besides, this procedure depends on the detected 2D keypoint locations, making it non-differentiable and thus disables end-to-end training.</p><p>To address the above problems, we present a differentiable voting scheme to extract orientation vectors from the predicted orientation maps. As described in Section III-A1, our method predicts the limb confidence maps and 3D orientation maps at the same time. For each limb i , we extract its 3D limb orientation by taking average of all the pixels in the pixelwise product of the predicted limb confidence mapM Ci and 3D orientation mapM 3Di of the last stage:</p><formula xml:id="formula_6">v i = 1 w h w h M Ci M 3Di ,<label>(4)</label></formula><p>where denotes pixel-wise product. Obviously, Eq. 4 is differentiable, which makes end-to-end training possible. Besides, this confidence weighted voting scheme can leverage all the predicted orientation information, making our orientation estimation more accurate and stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Integration 3D orientations with human model:</head><p>Our method generates 3D pose estimation by integrating the estimated 3D orientations with a skeleton-shaped human model with fixed-length limbs. This process works like twisting the limbs of the human model to fit the predicted 3D limb orientations. Like many prior methods, we treat the pelvis as the root node and fix it to the origin. For each child node i, its location Y i is determined by its parent node's location X i , the estimated 3D orientation v i and the 3D limb length L i as follows:</p><formula xml:id="formula_7">Y i = X i + L i v i ,<label>(5)</label></formula><p>where X 0 = 0. The 3D pose is generated from root node to leaf nodes by recursively applying Eq. 5 until all the leaf nodes are determined. 5) End-to-end training with 3D pose loss: In Section III-A2, each limb orientation is separately optimized. Although limb orientation errors do not accumulate, 3D joint location errors could propagate along the skeleton tree and possibly accumulate into large errors for joints at the leaf node.</p><p>To this end, long-term objectives should be considered so that the 3D orientations are jointly optimized. In our method, since all the steps are differentiable, we can directly use the 3D pose loss as the long-term objective and train the model end-to-end. This works like slightly adjusting the limb orientation so that the joint locations fit the ground truth better. In experiment, we find that end-to-end training can speed up the convergence, and improve the inference accuracy as well.</p><p>Here we use L1 loss for 3D pose following many prior works:</p><formula xml:id="formula_8">L P3D = |? ? S|,<label>(6)</label></formula><p>where? and S represent the predicted and ground truth 3D pose at the last stage, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Human pose complementation</head><p>The orientation learning network can produce a complete pose estimation unless all limbs are at least partially visible in the image. If a limb is completely visually absent, the estimated 3D orientation of the missing limb could be a zero vector or noisy values, resulting in an incomplete or wrong estimation. To this end, we propose a pose complementation (PC) network (see <ref type="figure" target="#fig_4">Fig. 2e</ref>) to infer the 3D orientations of the invisible limbs from visible ones. To achieve this, there are two key problems to answer.</p><p>(a) Which limbs should be complemented? Different testing images may have different missing limbs and different number of missing limbs. How to let the network know which limbs need to be complemented? Our solution is straightforward, i.elet@tokeneonedotusing the predicted limb confidence score as an indicator. Generally, the 3D orientation estimations of invisible limbs tend to have low confidence scores while visible ones have high ones. In other words, limbs with lower confidence scores are probably invisible and the corresponding 3D orientations may need to be re-inferred. So we explicitly provide the PC sub-network information about which limbs need complementation by feeding the limb confidence scores into it.</p><p>(b) How to complement? <ref type="figure" target="#fig_5">Fig. 3</ref> shows an example of the proposed pose complementation, which is aimed to infer the configuration of missing limbs and provide an overall reasonable pose estimation, while keeping estimation of the visible parts consistent with the input image. To achieve this, we feed into the PC sub-network with three components, the predicted limb confidence score, the initial 3D limb orientation estimation and the upper triangular part of the 3D orientation correlation matrix masked by the confidence correlation matrix. The dimension of the input is 16+16?3+16(16+1)/2 = 200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence Correlation Matrix Orientation Correlation Matrix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Pose Before Complementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Orientation Correlation Matrix</head><p>After Complementation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Pose After Complementation</head><p>Limb order in the correlation matrix:  The architecture of the PC sub-network is quite simple, which only consists of two linear layers of size 512 with residual connection. In our experiment, we find that the network learns better pose complementation by predicting modification vectors ?v i than directly predicting v c i :</p><formula xml:id="formula_9">v c i = v i + ?v i ||v i + ?v i || 2 .<label>(7)</label></formula><p>The complemented 3D orientations are then converted into 3D pose? c and optimized with the L1 loss as well:</p><formula xml:id="formula_10">L CP3D = |? c ? S|.<label>(8)</label></formula><p>In all, for a T stages network, our overall objective is the combination of the five objectives: <ref type="bibr" target="#b10">(9)</ref> where ? 1 , ? 2 and ? 3 control the relative importance of each objective, 1 is an indicator function that is 1 if ground truth 3D is available for an image and 0 otherwise. We set ? 1 = 0.1, ? 2 = 1 and ? 3 = 1 in our experiment.</p><formula xml:id="formula_11">L = T t=1 (?1L t CM +?2L t OM2D +1?3L t OM3D )+1(L P3D +L CP3D ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We provide here details on our experiments, including datasets and implementation details, results under regular settings, results on images with missing joints/limbs, crossdataset evaluations to test the generalization capacity and qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Human3.6M <ref type="bibr" target="#b13">[12]</ref> is a large-scale indoor 3D human pose dataset that comprises 3.6 million images and the corresponding 2D pose and 3D pose annotations. It features 7 subjects performing 15 everyday activities. Following the standard protocols, we use S1, S5, S6, S7, S8 for training, and S9, S11 for testing. We report results in two metrics, i.elet@tokeneonedotMean Per Joint Position Error (MPJPE) and MPJPE after Procrustes Alignment (PA-MPJPE). The original videos are down-sampled from 50fps to 10fps to reduce redundancy.</p><p>MPI-INF-3DHP <ref type="bibr" target="#b26">[25]</ref> test set consists of 2929 indoor and outdoor images from six subjects performing seven actions. We only use the test set of this dataset to evaluate our method's generalization quantitatively.</p><p>MPII <ref type="bibr" target="#b2">[1]</ref> is a widely used benchmark for 2D human pose estimation. It contains 5K in-the-wild images covering a wide range of activities. It is annotated with 2D keypoints but no 3D ground truth. We use it for weakly supervised training to achieve better generalization.</p><p>3DPW <ref type="bibr" target="#b43">[42]</ref> is a recently proposed 3D poses dataset in the wild. It contains 60 video sequences and 3D annotations captured via IMUs. For a fair comparison, the evaluation is performed following the same protocol as <ref type="bibr" target="#b49">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>Our method is implemented with PyTorch <ref type="bibr" target="#b31">[30]</ref>. We train our model in two steps from scratch. First, we train the whole network in regular settings. Augmentations of random scaling (1 ? 0.25), random translation (?0.2), random rotation (?30 ? ), random horizontal flipping (p = 0.5) and random color jitter (1 ? 0.2), are used for both MPII and Human3.6M. The network is trained for 100 epochs with a batch size of 12. The learning rate is initially set to 2 ? 10 ?4 and decays at 60th epoch by a factor of 0.1.</p><p>In the first training step, the images are almost always complete, because the translation augmentation is limited at a very small range. To this end, we need to create incomplete images to train the PC sub-network. In our experiments, we use random translation up to ?50% of the image size and randomly render objects on the 3D training images, to simulate joint and limb absence caused by truncation and occlusion. Then we fine-tune network on these synthetic incomplete images for 50 epochs with the same batch size. The learning rate is initially set to 1 ? 10 ?4 and decays at the 30th epoch by a factor of 0.1. We use the RMSProp as the optimizer for all the steps. In both training steps, the PC sub-network is detached from the FCNN. In other words, the gradient in the PC sub-network does not backpropagate to the FCNN, to avoid the deterioration of orientation learning. It takes about 35 hours on two Tesla V100 GPUs with 16 GB memory on each to train a 4-stage model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Rules to generate incomplete images</head><p>In Section IV-D of the main manuscript, we provided quantitative results on various incomplete images. Here we explain the rules to generate these incomplete images.</p><p>Translation. For an image of the size a ? a, we randomly translate the image center by (x, y)?a, where x, y are random numbers drawn from uniform distribution between [??, ? ]. We set ? = 0.25, 0.4 respectively in our experiments.</p><p>Synthetic occlusions. For each image, we add a random number of VOC objects at random locations with random sizes. We directly use the code provided by <ref type="bibr" target="#b36">[35]</ref> to generate occlusions.</p><p>Rectangular erasing. Given an image of size a ? a, we first select two arbitrary points within the image as the two mid-points of the widths of the rectangle, and then draw a value between [0, a] at random as the width of the rectangle.</p><p>Circle erasing. Given an image of size a?a, we first select an arbitrary point within the image as the center of the circle, and then draw a value between [a/5, 2a/5] at random as its radius.</p><p>Edge erasing. Given an image of size a ? a, we first select an arbitrary edge from left, right, top, bottom, and then draw a value between [0, a/2] at random as the width of edge to erase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Quantitative results</head><p>In this section, we first report results on Human3.6M and compare with state-of-the-art methods in regular settings. Then we evaluate our method on incomplete images to test the robustness of 3D pose estimation. Some recent works estimate 3D pose by recovering 3D human mesh based on SMPL model <ref type="bibr" target="#b23">[22]</ref>. We compare with these model-based methods on Human3.6M and 3DPW. Finally, we quantitatively evaluate our method on MPI-INF-3DHP without training on it, to evaluate the generalization of our PONet.</p><p>1) Results on Human3.6M in regular settings: We first compare the proposed method with state-of-the-art 3D pose estimation methods in regular settings, i.elet@tokeneonedot, all testing images are precisely cropped using ground truth bounding boxes so that the subjects always appear right in the center of testing images with all the joints within the images.</p><p>The  for ours, regress the 3D joint locations so that they can fit the data by distorting the body. By contrast, our method uses a skeleton-shaped human model with fixed-length limbs so that our pose estimation will always satisfy anthropometric constraints, at the cost of some fitting accuracy. Even so, our results are on par with these methods.</p><p>2) Results on images with noisy bounding boxes : We carry out experiments on Human3.6M by adding Gaussian noises to the centers C and/or the sizes S of ground truth bounding boxes,     We can see that: (a) When ? c &lt; 0.1 and ? s &lt; 0.1, the PCK and AUC of our approach drop less than 1%, while MPJPE drops about 1.2%, proving that our performance is very stable on images with small noise (25.6 pixels) added to the bounding boxes. (b) When ? c = 0.2 and ? s = 0.2, the PCK only decreases 4.7% and AUC decreases 6.8%, which shows that our PONet can work rather well even when both the size and the center of bounding boxes have an average error of 51.2 pixels. The experiments demonstrate that our method does not rely on accurate bound boxes. This implies that in practical applications, we can use a person detector to obtain a rough bounding box to crop the image, without losing much accuracy in the performance of 3D human pose estimation.</p><formula xml:id="formula_12">C ? C + S ? N ((0, 0), (? 2 c , ? 2 c )), S ? S + S ? N (0, ? 2 s ),</formula><p>3) Results on Human3.6M on incomplete images: Hu-man3.6M is captured in monitored laboratory environment with precise bounding boxes and little noise. In practical applications, occlusion and truncation frequently occur in testing images. To find out the performance of 3D pose estimation in these challenging cases, it's not enough to only evaluate in ideal settings. To this end, we introduce three kinds of synthetic disturbance, including random object occlusion, random translation and random erasing to the testing images and evaluate the robustness of the proposed method on these images and compare it with several state-of-the-art keypointdetection-based methods. To reduce the impact of random number generator, we independently repeat each experiment for 5 times, and report the average results in Tab. III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method (Authors)</head><p>None Occl.  All the methods across Tab. III show to have certain robustness to synthetic object occlusion <ref type="bibr" target="#b36">[35]</ref>, but our method proves to be more robust in this testing case. In terms of translation, our method turn out to have much better robustness than the other ones. While other methods already more than double their errors under 25% translation, our MPJPE only increases 7.5%. Under 40% translation, the other methods' MPJPEs sharply increase to more than 240mm. By contrast, our result after pose complementation remains smaller than 69mm, demonstrating the outstanding robustness of our PONet on truncated images. We also randomly erasing a rectangle, circle or a certain edge on the input image to simulate keypoint and limb absent cases. Again our method outperforms the other ones, although we did not use data augmentation of erasing in training. In summary, although our method does not achieve the best score in ideal settings, it performs much better on incomplete images including occlusion, translation and erasing.</p><p>There are four reasons why our method performs significantly better on images with missing joints or limbs. Firstly, our method does not detect the only intermittently visible body keypoints, but learn to estimate the limb orientation using a region-based representation. This pure orientationbased design allows us to estimate the 3D pose even when some limbs are partially invisible. Secondly, through pose complementation, our method can output a full-body pose estimation even if some limbs are completely out-of-image. Thirdly, we embed a fixed-length skeleton-shaped human model into the network so that the pose estimation will always satisfy anthropometric constrains such as limb ratios. Lastly, the 3D limb orientations are voted by all the pixels in the estimated orientation maps weighted with the corresponding confidence. This voting scheme can suppress the noise and outliers in the estimated 3D orientation maps, making the 3D orientation estimation and thus 3D pose estimation more robust and stable. 4) Comparison with SMPL-based methods: We compare with these SMPL-based methods because they also introduce anthropometric constraints to the estimated 3D poses. First, we compare our method with them on Human3.6M in regular settings. The second column of Tab. IV shows the PA-MPJPE of our method and some recent ones Our method outperforms all the other approaches.  We also compare our method on datasets with occlusion. The third column of Tab. IV reports results on Human3.6M with synthetic occlusions. Our method, again, outperforms all the other methods including a very recent work <ref type="bibr" target="#b49">[48]</ref> that specially designed for pose estimation on object-occluded images. We show the performance on 3DPW in the last column of Tab. IV. Our method achieves the second best performance on this dataset without training on it.</p><p>In summary, our PONet performs better in both the original and occluded datasets, demonstrating the superiority of the proposed method over the others in terms of the accuracy of 3D joint location prediction and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Evaluation of cross-dataset generalization:</head><p>To quantitatively show the generalization on unseen images, we compare with other methods on MPI-INF-3DHP without using any data from this dataset for training. In addition to MPJPE, we report Percentage of Correct Keypoints (PCK) thresholded at 150mm and Area Under the Curve (AUC) over a range of PCK thresholds.</p><p>The results are shown in Tab. V. Our method outperforms state-of-the-art methods by a large margin without training on  <ref type="bibr" target="#b25">[24]</ref> and <ref type="bibr" target="#b21">[20]</ref>. PC denotes pose complementation. Our method is robust to joint absence (first row), limb absence (second row) and occlusion (last row).  this dataset, demonstrating our method's excellent generalization to unseen images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative results</head><p>In this section, we qualitatively compare our method with state-of-the-art methods on MPII and 3DPW. In <ref type="figure" target="#fig_9">Fig. 5</ref>, we show six examples with invisible parts and compare with two approaches that based on 2D keypoint detection, SimpleBaseline <ref type="bibr" target="#b25">[24]</ref> and a very recent work EvoSkeleton <ref type="bibr" target="#b21">[20]</ref>. Simple-Baseline uses StackedHourglass <ref type="bibr" target="#b29">[28]</ref> to detect 2D poses, while EveSkeleton uses a better 2D pose estimator HRNet <ref type="bibr" target="#b38">[37]</ref>, and achieved state-of-the-art results on Human3.6M in regular settings.</p><p>The first row of <ref type="figure" target="#fig_9">Fig. 5</ref> shows two cases from MPII with only a single out-of-image joint. Neither of SimpleBaseline and EvoSkeleton can correctly predict the 3D location of the missing joint, while our method provides visually appealing results even without pose complementation. The second row of <ref type="figure" target="#fig_9">Fig. 5</ref> shows two cases with invisible limbs. The performance of the two keypoint-detection-based methods drops drastically, compared to single joint absent cases. By contrast, our method can correctly estimate the configuration of the visible parts, and provide a reasonable full-body pose estimation after pose complementation. The third row of <ref type="figure" target="#fig_9">Fig. 5</ref> shows two occluded images. Again, our method performs better in these cases as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Failure cases</head><p>In <ref type="figure">Fig. 6</ref>, we provide 4 typical failure cases. The first two rows of <ref type="figure">Fig. 6</ref> show two failure cases in limb region estimation in multi-person scenarios. The third row of <ref type="figure">Fig. 6</ref> is a false positive case on clothes. The last row of <ref type="figure">Fig. 6</ref> shows an example that the right half of the body is truncated so that the network gets confused about the left and right side of the body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Ablation study</head><p>To analyze the effectiveness of each component in our method, we conduct ablation study on Human3.6M in both regular setting and translation up to 40% of the image size. In Tab. VI, Baseline refers to model trained with L CM , L OM 2D and L OM 3D . Aug. refers to using the data augmentation of synthetic occlusion and 40% random translation. PC refers to pose complementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Regular Translation  the other methods in Tab. III, proving the superiority of our orientation-based method over keypoint-detection-based ones in improving the robustness. End-to-end training can improve the performance on both complete and incomplete images, demonstrating the importance of our differentiable orientation extraction. Data augmentation of occlusion and large translation can further improve the performance. At last, the performance gain brought by pose complementation in regular setting is limited, but it can improve the performance on translation of 40% by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose PONet, a robust and effectual 3D human pose estimation network based on learning orientation only. Our method bypasses 2D keypoint detection, which is prone to errors in the absence of body parts, by learning three sets of maps that encode the limb confidence, 2D and 3D orientations of each limb. This design allows us to predict the 3D limb orientations on images with absent joints. In the more challenging scenarios where limbs are completely occluded or out-of-image, PONet can provide a complete 3D pose estimation by inferring the 3D orientations of the invisible limbs from the visible ones using pose complementation. We evaluate our method on multiple datasets including Human3.6M, MPII, MPI-INF-3DHP and 3DPW. Extensive experiments demonstrate the gratifying robustness of the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Example results on Human3.6M with vertical translation from ?40% to 40%. PC denotes pose complementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>16 &lt;</head><label>16</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " z 4 d l J l W t g x 3 3 T x n / t l O 0 v U f H P q Y = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 h E q 8 e C F 4 8 V 7 A e 2 o W y 2 m 3 b p Z h N 2 J 0 I p / R d e P C j i 1 X / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 P O + n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m i T T j D d Y I h P d D q n h U i j e Q I G S t 1 P N a R x K 3 g p H t z O / 9 c S 1 E Y l 6 w H H K g 5 g O l I g E o 2 i l R 7 / a R R F z 4 1 d 7 5 Y r n e n O Q V e L n p A I 5 6 r 3 y V 7 e f s C z m C p m k x n R 8 L 8 V g Q j U K J v m 0 1 M 0 M T y k b 0 Q H v W K q o X R N M 5 h d P y Z l V + i R K t C 2 F Z K 7 + n p j Q 2 J h x H N r O m O L Q L H s z 8 T + v k 2 F 0 E 0 y E S j P k i i 0 W R Z k k m J D Z + 6 Q v N G c o x 5 Z Q p o W 9 l b A h 1 Z S h D a l k Q / C X X 1 4 l z Q v X v 3 S v 7 i 8 r N T e P o w g n c A r n 4 M M 1 1 O A O 6 t A A B g q e 4 R X e H O O 8 O O / O x 6 K 1 4 O Q z x / A H z u c P h 2 + Q H g = = &lt; / l a t e x i t &gt; 16 ? 16 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z 4 d l J l W t g x 3 3 T x n / t l O 0 v U f H P q Y = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 h E q 8 e C F 4 8 V 7 A e 2 o W y 2 m 3 b p Z h N 2 J 0 I p / R d e P C j i 1 X / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 P O + n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m i T T j D d Y I h P d D q n h U i j e Q I G S t 1 P N a R x K 3 g p H t z O / 9 c S 1 E Y l 6 w H H K g 5 g O l I g E o 2 i l R 7 / a R R F z 4 1 d 7 5 Y r n e n O Q V e L n p A I 5 6 r 3 y V 7 e f s C z m C p m k x n R 8 L 8 V g Q j U K J v m 0 1 M 0 M T y k b 0 Q H v W K q o X R N M 5 h d P y Z l V + i R K t C 2 F Z K 7 + n p j Q 2 J h x H N r O m O L Q L H s z 8 T + v k 2 F 0 E 0 y E S j P k i i 0 W R Z k k m J D Z + 6 Q v N G c o x 5 Z Q p o W 9 l b A h 1 Z S h D a l k Q / C X X 1 4 l z Q v X v 3 S v 7 i 8 r N T e P o w g n c A r n 4 M M 1 1 O A O 6 t A A B g q e 4 R X e H O O 8 O O / O x 6 K 1 4 O Q z x / A H z u c P h 2 + Q H g = = &lt; / l a t e x i t &gt; 16 ? 16 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z 4 d l J l W t g x 3 3 T x n / t l O 0 v U f H P q Y = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 h E q 8 e C F 4 8 V 7 A e 2 o W y 2 m 3 b p Z h N 2 J 0 I p / R d e P C j i 1 X / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 P O + n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m i T T j D d Y I h P d D q n h U i j e Q I G S t 1 P N a R x K 3 g p H t z O / 9 c S 1 E Y l 6 w H H K g 5 g O l I g E o 2 i l R 7 / a R R F z 4 1 d 7 5 Y r n e n O Q V e L n p A I 5 6 r 3 y V 7 e f s C z m C p m k x n R 8 L 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3 S v 7 i 8 r N T e P o w g n c A r n 4 M M 1 1 O A O 6 t A A B g q e 4 R X e H O O 8 O O / O x 6 K 1 4 O Q z x / A H z u c P h 2 + Q H g = = &lt; / l a t e x i t &gt; Stage T = t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l + h S b h e e 1 X o Z B 7 f 8 E Q / 3 l 8 o O B / 8 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o l U 9 C I U v H i s 2 C 9 o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 2 v r G 5 t b 2 4 W d 4 u 7 e / s F h 6 e i 4 Z e J U M 9 5 k s Y x 1 J 6 C G S 6 F 4 E w V K 3 k k 0 p 1 E g e T s Y 3 8 3 8 9 h P X R s S q g</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>a 9 a u X q o l m v V P I 4 C n M I Z X I A H 1 1 C D e 6 h D E x g M 4 R l e 4 c 2 R z o v z 7 n w s W t e c f O Y E / s D 5 / A E F H I 2 X &lt; / l a t e x i t &gt; T = t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l + h S b h e e 1 X o Z B 7 f 8 E Q / 3 l 8 o O B / 8 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o l U 9 C I U v H i s 2 C 9 o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 2 v r G 5 t b 2 4 W d 4 u 7 e / s F h 6 e i 4 Z e J U M 9 5 k s Y x 1 J 6 C G S 6 F 4 E w V K 3 k k 0 p 1 E g e T s Y 3 8 3 8 9 h P X R s S q g</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 :</head><label>2</label><figDesc>a 9 a u X q o l m v V P I 4 C n M I Z X I A H 1 1 C D e 6 h D E x g M 4 R l e 4 c 2 R z o v z 7 n w s W t e c f O Y E / s D 5 / A E F H I 2 X &lt; / l a t e x i t &gt; M t C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 X 8 D w x m y a R s S 5 N T U v L 0 h e o Y H E 5 A = " &gt; A A A B 9 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o s d A L l 6 E C O Y B y R p m J 7 P J k N m H M 7 2 B s O x 3 e P G g i F c / x p t / 4 y T Z g y Y W N B R V 3 X R 3 e b E U G m 3 7 2 y q s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 4 S x X i T R T J S H Y 9 q L k X I m y h Q 8 k 6 s O A 0 8 y d v e u D 7 z 2 x O u t I j C B 5 z G 3 A 3 o M B S + Y B S N 5 P Z G F N O 7 r J / W s 0 f s l y t 2 1 Z 6 D r B I n J x X I 0 e i X v 3 q D i C U B D 5 F J q n X X s W N 0 U 6 p Q M M m z U i / R P K Z s T I e 8 a 2 h I A 6 7 d d H 5 0 R s 6 M M i B + p E y F S O b q 7 4 m U B l p P A 8 9 0 B h R H e t m b i f 9 5 3 Q T 9 G z c V Y Z w g D 9 l i k Z 9 I g h G Z J U A G Q n G G c m o I Z U q Y W w k b U U U Z m p x K J g R n + e V V 0 r q o O p f V q / v L S s 3 O 4 y j C C Z z C O T h w D T W 4 h Q Y 0 g c E T P M M r v F k T 6 8 V 6 t z 4 W r Q U r n z m G P 7 A + f w A M 6 J I 8 &lt; / l a t e x i t &gt;M t 2D &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m 0 S X z 7 3 L p d + m + A W T 1 9 0 a u L j s 7 S k = " &gt; A A A B 9 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S S l o s e C H r w I F e w H t G n Z b L f t 0 s 0 m 7 E 6 U E v I / v H h Q x K v / x Z v / x m 2 b g 7 Y + G H i 8 N 8 P M P D 8 S X K P j f F u 5 t f W N z a 3 8 d m F n d 2 / / o H h 4 1 N R h r C h r 0 F C E q u 0 T z Q S X r I E c B W t H i p H A F 6 z l T 6 5 n f u u R K c 1 D + Y D T i H k B G U k + 5 J S g k X r d M c H k L u 0 n l Z u 0 h / 1 i y S k 7 c 9 i r x M 1 I C T L U + 8 W v 7 i C k c c A k U k G 0 7 r h O h F 5 C F H I q W F r o x p p F h E 7 I i H U M l S R g 2 k v m V 6 f 2 m V E G 9 j B U p i T a c / X 3 R E I C r a e B b z o D g m O 9 7 M 3 E / 7 x O j M M r L + E y i p F J u l g 0 j I W N o T 2 L w B 5 w x S i K q S G E K m 5 u t e m Y K E L R B F U w I b j L L 6 + S Z q X s V s s X 9 9 V S z c n i y M M J n M I 5 u H A J N b i F O j S A g o J n e I U 3 6 8 l 6 s d 6 t j 0 V r z s p m j u E P r M 8 f g 7 O S e Q = = &lt; / l a t e x i t &gt;M t 3D &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m n 0 b U t V d h W k t V G R 6 i s h Q Q f d w l U 0 = " &gt; A A A B 9 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x q R I 8 B P X g R I p g H J J s w O 5 k k Q 2 Y f z P Q q Y d n / 8 O J B E a / + i z f / x k m y B 0 0 s a C i q u u n u 8 i I p N N r 2 t 5 V b W V 1 b 3 8 h v F r a 2 d 3 b 3 i v s H D R 3 G i v E 6 C 2 W o W h 7 V X I q A 1 1 G g 5 K 1 I c e p 7 k j e 9 8 f X U b z 5 y p U U Y P O A k 4 q 5 P h 4 E Y C E b R S N 3 O i G J y l / a S 8 5 u 0 i 7 1 i y S 7 b M 5 B l 4 m S k B B l q v e J X p x + y 2 O c B M k m 1 b j t 2 h G 5 C F Q o m e V r o x J p H l I 3 p k L c N D a j P t Z v M r k 7 J i V H 6 Z B A q U w G S m f p 7 I q G + 1 h P f M 5 0 + x Z F e 9 K b i f 1 4 7 x s G V m 4 g g i p E H b L 5 o E E u C I Z l G Q P p C c Y Z y Y g h l S p h b C R t R R R m a o A o m B G f x 5 W X S O C s 7 l f L F f a V U t b M 4 8 n A E x 3 A K D l x C F W 6 h B n V g o O A Z X u H N e r J e r H f r Y 9 6 a s 7 K Z Q / g D 6 / M H h T u S e g= = &lt; / l a t e x i t &gt; The inference pipeline of PONet. The system takes as input a color image, which can be incomplete. First, the threebranch multi-stage FCNN simultaneously estimates limb confidence mapsM c and 2D/3D limb orientation mapsM 2D and M 3D . Then we extract 3D limb orientations from the predicted maps using a differentiable voting scheme to produce an initial 3D pose estimation. Finally, we use a pose complementation (PC) sub-network, to infer a complete and reasonable final 3D pose estimation from initial estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 :</head><label>3</label><figDesc>An illustration of the proposed pose complementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>where ? c and ? s control the level of noise relative to the GT size of bounding box. The results are shown in Tab. II and Fig. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4 :</head><label>4</label><figDesc>(a) PCK vs.&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u v e q 5 1 X s k e Z / B m B b y b / D E z k G 8 y U = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q Y 9 B L x 4 j m A c k S 5 i d z C Z j 5 r H M z A p h y T 9 4 8 a C I V / / H m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N a J M o r n Q n w o Z y J m n T M s t p J 9 E U i 4 j T d j S + n f n t J 6 o N U / L B T h I a C j y U L G Y E W y e 1 e o Y N B e 6 X K 3 7 V n w O t k i A n F c j R 6 J e / e g N F U k G l J R w b 0 w 3 8 x I Y Z 1 p Y R T q e l X m p o g s k Y D 2 n X U Y k F N W E 2 v 3 a K z p w y Q L H S r q R F c / X 3 R I a F M R M R u U 6 B 7 c g s e z P x P 6 + b 2 v g 6 z J h M U k s l W S y K U 4 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q C U X Q r D 8 8 i p p 1 a r B R b V 2 f 1 m p 3 + R x F O E E T u E c A r i C O t x B A 5 p A 4 B G e 4 R X e P O W 9 e O / e x 6 K 1 4 O U z x / A H 3 u c P n a 2 P J w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u v e q 5 1 X s k e Z / B m B b y b / D E z k G 8 y U = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q Y 9 B L x 4 j m A c k S 5 i d z C Z j 5 r H M z A p h y T 9 4 8 a C I V / / H m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N a J M o r n Q n w o Z y J m n T M s t p J 9 E U i 4 j T d j S + n f n t J 6 o N U / L B T h I a C j y U L G Y E W y e 1 e o Y N B e 6 X K 3 7 V n w O t k i A n F c j R 6 J e / e g N F U k G l J R w b 0 w 3 8 x I Y Z 1 p Y R T q e l X m p o g s k Y D 2 n X U Y k F N W E 2 v 3 a K z p w y Q L H S r q R F c / X 3 R I a F M R M R u U 6 B 7 c g s e z P x P 6 + b 2 v g 6 z J h M U k s l W S y K U 4 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q C U X Q r D 8 8 i p p 1 a r B R b V 2 f 1 m p 3 + R x F O E E T u E c A r i C O t x B A 5 p A 4 B G e 4 R X e P O W 9 e O / e x 6 K 1 4 O U z x / A H 3 u c P n a 2 P J w = = &lt; / l a t e x i t &gt; (b) AUC vs. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u v e q 5 1 X s k e Z / B m B b y b / D E z k G 8 y U = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q Y 9 B L x 4 j m A c k S 5 i d z C Z j 5 r H M z A p h y T 9 4 8 a C I V / / H m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N a J M o r n Q n w o Z y J m n T M s t p J 9 E U i 4 j T d j S + n f n t J 6 o N U / L B T h I a C j y U L G Y E W y e 1 e o Y N B e 6 X K 3 7 V n w O t k i A n F c j R 6 J e / e g N F U k G l J R w b 0 w 3 8 x I Y Z 1 p Y R T q e l X m p o g s k Y D 2 n X U Y k F N W E 2 v 3 a K z p w y Q L H S r q R F c / X 3 R I a F M R M R u U 6 B 7 c g s e z P x P 6 + b 2 v g 6 z J h M U k s l W S y K U 4 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q C U X Q r D 8 8 i p p 1 a r B R b V 2 f 1 m p 3 + R x F O E E T u E c A r i C O t x B A 5 p A 4 B G e 4 R X e P O W 9 e O / e x 6 K 1 4 O U z x / A H 3 u c P n a 2 P J w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u v e q 5 1 X s k e Z / B m B b y b / D E z k G 8 y U = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q Y 9 B L x 4 j m A c k S 5 i d z C Z j 5 r H M z A p h y T 9 4 8 a C I V / / H m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N a J M o r n Q n w o Z y J m n T M s t p J 9 E U i 4 j T d j S + n f n t J 6 o N U / L B T h I a C j y U L G Y E W y e 1 e o Y N B e 6 X K 3 7 V n w O t k i A n F c j R 6 J e / e g N F U k G l J R w b 0 w 3 8 x I Y Z 1 p Y R T q e l X m p o g s k Y D 2 n X U Y k F N W E 2 v 3 a K z p w y Q L H S r q R F c / X 3 R I a F M R M R u U 6 B 7 c g s e z P x P 6 + b 2 v g 6 z J h M U k s l W S y K U 4 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q C U X Q r D 8 8 i p p 1 a r B R b V 2 f 1 m p 3 + R x F O E E T u E c A r i C O t x B A 5 p A 4 B G e 4 R X e P O W 9 e O / e x 6 K 1 4 O U z x / A H 3 u c P n a 2 P J w = = &lt; / l a t e x i t &gt; (c) MPJPE vs. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u v e q 5 1 X s k e Z / B m B b y b / D E z k G 8 y U = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q Y 9 B L x 4 j m A c k S 5 i d z C Z j 5 r H M z A p h y T 9 4 8 a C I V / / H m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N a J M o r n Q n w o Z y J m n T M s t p J 9 E U i 4 j T d j S + n f n t J 6 o N U / L B T h I a C j y U L G Y E W y e 1 e o Y N B e 6 X K 3 7 V n w O t k i A n F c j R 6 J e / e g N F U k G l J R w b 0 w 3 8 x I Y Z 1 p Y R T q e l X m p o g s k Y D 2 n X U Y k F N W E 2 v 3 a K z p w y Q L H S r q R F c / X 3 R I a F M R M R u U 6 B 7 c g s e z P x P 6 + b 2 v g 6 z J h M U k s l W S y K U 4 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q C U X Q r D 8 8 i p p 1 a r B R b V 2 f 1 m p 3 + R x F O E E T u E c A r i C O t x B A 5 p A 4 B G e 4 R X e P O W 9 e O / e x 6 K 1 4 O U z x / A H 3 u c P n a 2 P J w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u v e q 5 1 X s k e Z / B m B b y b / D E z k G 8 y U = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q Y 9 B L x 4 j m A c k S 5 i d z C Z j 5 r H M z A p h y T 9 4 8 a C I V / / H m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N a J M o r n Q n w o Z y J m n T M s t p J 9 E U i 4 j T d j S + n f n t J 6 o N U / L B T h I a C j y U L G Y E W y e 1 e o Y N B e 6 X K 3 7 V n w O t k i A n F c j R 6 J e / e g N F U k G l J R w b 0 w 3 8 x I Y Z 1 p Y R T q e l X m p o g s k Y D 2 n X U Y k F N W E 2 v 3 a K z p w y Q L H S r q R F c / X 3 R I a F M R M R u U 6 B 7 c g s e z P x P 6 + b 2 v g 6 z J h M U k s l W S y K U 4 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q C U X Q r D 8 8 i p p 1 a r B R b V 2 f 1 m p 3 + R x F O E E T u E c A r i C O t x B A 5 p A 4 B G e 4 R X e P O W 9 e O / e x 6 K 1 4 O U z x / A H 3 u c P n a 2 P J w = = &lt; / l a t e x i t &gt; Results on Human3.6M with Gaussian noises N (0, ? 2 ) added to the centers or/and sizes of GT bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative results on MPII (first two rows) and 3DPW (last row) and comparison with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Quantitative results on Human3.6M in regular settings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Performance of our method on Human3.6M with different levels of Gaussian noises in the bounding boxes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III :</head><label>III</label><figDesc>Comparison with state-of-the-art keypointdetection-based methods on images from Human3.6M with missing joints or limbs in terms of MPJPE. PC denotes pose complementation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison with state-of-the-art SMPL-based methods on Human3.6M and 3DPW in terms of PA-MPJPE. The best results are marked in bold and second best ones are underlined. No data from 3DPW are used to train our model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE V :</head><label>V</label><figDesc>Results on MPI-INF-3DHP dataset. For PCK and AUC, higher is better; For MPJPE, lower is better. MPJPE is evaluated without Procrustes alignment. CE denotes crossdataset evaluation without training on this dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VI :</head><label>VI</label><figDesc>Ablation study on Human3.6M in terms of MPJPE.From Tab. VI we can see that, the baseline yields MPJPE of 92.2 on images with 40% translation, outperforms all</figDesc><table><row><cell>Input</cell><cell>Confidence</cell><cell>3D Orientation</cell><cell>Pose Estimation</cell><cell>Pose Estimation</cell></row><row><cell>Image</cell><cell>Maps</cell><cell>Maps</cell><cell>Before PC</cell><cell>After PC</cell></row><row><cell></cell><cell></cell><cell cols="2">Fig. 6: Failure Cases.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b2">1</ref> <p>We denote a limb to be the body region between two adjacent joints. 0000-0000/00$00.00 ? 2021 IEEE</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Ours (Before PC) Ours (After PC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simplebaseline</surname></persName>
		</author>
		<idno>24] EvoSkeleton[20</idno>
		<title level="m">Ours (Before PC) Ours (After PC) Input Image</title>
		<imprint/>
	</monogr>
	<note>Input Image SimpleBaseline[24] EvoSkeleton[20</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d human pose estimation using spatio-temporal networks with explicit occlusion training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wending</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoshu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via modelfitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6050" to="6059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungoh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inwoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cascaded deep monocular 3d human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving 3d human pose estimation via 3d part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1004" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Smpl: a skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Ponsmoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Orinet: A fully convolutional network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Bruce Xiaohan Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">How robust is 3d human pose estimation to occlusion?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istv?n</forename><surname>S?r?ndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timm</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09316</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Motionet: 3d human motion reconstruction from monocular video with skeleton consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Aristidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep highresolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5236" to="5246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3d human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Not all parts are created equal: 3d pose estimation by modeling bi-directional dependencies of body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Object-occluded human shape and pose estimation from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weaklysupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">3d shape estimation from 2d landmarks: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="186" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">Spyridon Leonardos, and Kostas Daniilidis. Sparse representation for 3d shape estimation: A convex relaxation approach. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1648" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="901" to="914" />
		</imprint>
	</monogr>
	<note>If you will not include a photo</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">John Doe Use \begin{IEEEbiographynophoto} and the author name as the argument followed by the biography text</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
