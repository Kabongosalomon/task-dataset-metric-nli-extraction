<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CoDesc: A Large Code-Description Parallel Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masum</forename><surname>Hasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bangladesh University of Engineering and Technology (BUET</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanveer</forename><surname>Muttaqueen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bangladesh University of Engineering and Technology (BUET</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Al Ishtiaq</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bangladesh University of Engineering and Technology (BUET</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi</forename><forename type="middle">Sajeed</forename><surname>Mehrab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bangladesh University of Engineering and Technology (BUET</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Mahim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjum</forename><surname>Haque</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bangladesh University of Engineering and Technology (BUET</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahmid</forename><surname>Hasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bangladesh University of Engineering and Technology (BUET</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasi</forename><forename type="middle">Uddin</forename><surname>Ahmad</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anindya</forename><surname>Iqbal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bangladesh University of Engineering and Technology (BUET</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rifat</forename><surname>Shahriyar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bangladesh University of Engineering and Technology (BUET</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CoDesc: A Large Code-Description Parallel Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Translation between natural language and source code can help software development by enabling developers to comprehend, ideate, search, and write computer programs in natural language. Despite growing interest from the industry and the research community, this task is often difficult due to the lack of large standard datasets suitable for training deep neural models, standard noise removal methods, and evaluation benchmarks. This leaves researchers to collect new small-scale datasets, resulting in inconsistencies across published works. In this study, we present CoDesc -a large parallel dataset composed of 4.2 million Java methods and natural language descriptions. With extensive analysis, we identify and remove prevailing noise patterns from the dataset. We demonstrate the proficiency of CoDesc in two complementary tasks for code-description pairs: code summarization and code search. We show that the dataset helps improve code search by up to 22% and achieves the new state-of-the-art in code summarization. Furthermore, we show CoDesc's effectiveness in pre-training-finetuning setup, opening possibilities in building pretrained language models for Java. To facilitate future research, we release the dataset, a data processing tool, and a benchmark at https://github.com/csebuetnlp/CoDesc.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural models for natural language processing have benefited from large datasets and standard evaluation benchmarks <ref type="bibr">(Wang et al., 2019b,a;</ref><ref type="bibr" target="#b7">Rajpurkar et al., 2016;</ref><ref type="bibr">Hermann et al., 2015;</ref><ref type="bibr">Com-monCrawl)</ref>. However, the programming language counterpart is lagging behind due to a lack in such large datasets and benchmarks. To put this into perspective, the original Transformer network * Equal contribution. <ref type="bibr" target="#b12">(Vaswani et al., 2017)</ref> was trained on WMT'14 English-German and English-French datasets <ref type="bibr">(Bojar et al., 2014)</ref> containing 4.5 million and 36 million parallel sentences, respectively, whereas a similar network that achieved state-of-the-art results in source code summarization has been trained on only 69 thousand code-description pairs <ref type="bibr" target="#b0">(Ahmad et al., 2020)</ref>. We argue that the existing models used for programming language tasks in the literature have a significant scope of improvement given a large, good-quality dataset, and such a dataset is the missing link for effectively applying deep learning methods on programming languages.</p><p>In this work, we collect and release a large (4.2 million) Java source code -natural language (NL) parallel dataset along with denoising methods and baseline results. We apply our dataset to established works in both training from scratch and pretraining-fine-tuning setting and we demonstrate a notable performance gain in both settings. We gain 10% to 22% improvement over baseline code search models using CoDesc, and attain performances comparable to models having 8? more parameters. We achieve a new state-of-the-art BLEU score of 45.89 in code summarization by pretraining a Transformer network with our dataset for two epochs. With extensive empirical analysis, we propose a set of noise removal techniques for the source code and the NL descriptions in our dataset.</p><p>Our work brings together several datasets and multiple tasks on the intersection of Natural Language Processing (NLP) and Software Engineering (SE), such as code summarization, code search and code synthesis, and allows researchers to compare their methods on the same benchmark. It also opens the door for building large pretrained models to jointly learn code and NL representations that can be leveraged in downstream tasks that do not have adequate data, such as, code refactoring, clone detection, etc. as done by <ref type="bibr">Feng et al. (2020)</ref>.  <ref type="bibr" target="#b15">Wei et al. (2019)</ref> proposed a dual learning method that simultaneously trained code summarization and code generation and improved both of them using 60k parallel data. In the same dataset, <ref type="bibr" target="#b0">Ahmad et al. (2020)</ref> achieved state-of-theart results in source code summarization using a Transformer network <ref type="bibr" target="#b12">(Vaswani et al., 2017)</ref>. Along with the mentioned dataset, <ref type="bibr">Clement et al. (2020)</ref> presented PyMT5, a text to text transformer that notably improved method generation and code summarization. <ref type="bibr">Ahmad et al. (2021)</ref> collected more than 300 GB monolingual code and NL data, and trained PLBART, a pretrained seq2seq model for both program understanding and comprehension.</p><p>3 CoDesc Dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Sources</head><p>We collect our data from several sources and formulate rules for data cleaning. 5 of the authors spent 45-50 man-hours manually going over the dataset to identify patterns of noises in different data sources. Upon group discussion, common patterns were identified and a noise removal method was established. Details about these noise patterns are provided in Appendix A.</p><p>One of the datasets used in CoDesc is CODE-SEARCHNET (CSN) 1 (Husain et al., 2019) -a parallel method-description dataset for code search. Furthermore, other datasets used are DeepCom 2 (Hu et al., 2018a), CONCODE 3 (Iyer et al., 2018), FunCom 4 (LeClair and McMillan, 2019) -datasets created for code summarization. The CODE-SEARCHNET dataset originally contained 6 programming languages, from which the Java methods are directly used in CoDesc, however, the Python methods are used after being automatically translated to Java. We combine all aforementioned datasets to create CoDesc. Appendix B shows a sample code-description parallel data from each of these datasets. <ref type="table">Table 1</ref> describes our data sources and their characteristics in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSN Python to Java Translation</head><p>To utilize maximum possible data from the CSN CORPUS, we translate the Python methods to Java using TransCoder <ref type="bibr" target="#b1">(Lachaux et al., 2020)</ref>, a state-of-theart, neural source-to-source compiler. We modified and re-released the open-source implementation of TransCoder 5 , enabling it to translate data in batches instead of one at a time, and resulting in a 16X faster translation. Upon empirical inspection, we found that the converted Java codes are human-readable and bear a strong resemblance to the original Python code intent. The converted codes seem correct to the human eye and their syntax matches with Java syntax. However, a few cases the transcompiler suffers are -converting to Java library methods, and converting from Python coding conventions that does not have a Java equivalent (e.g. use of SELF). These conversion errors, however, were not severe enough to affect our model to learn the NL-source code mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Cleaning and Noise Removal</head><p>We created an easy-to-use, parameterized data processing tool for removing the different types of noise that we observed in our dataset. From the natural language descriptions, we remove symbols and characters that do not carry a meaning in a natural language description, such as, comment tags (e.g., //, / * , * /), stray code characters (e.g., @, #, {, }, etc.), HTML and XML tags, non-ASCII and escape characters, and some patterns of autogenerated tags (e.g., @param, @return, @throws, etc.). From source code, we remove comments and the non-ASCII and escape characters. In previous studies, many meaningful data are discarded due to having some noisy patterns/symbols either in the code or description <ref type="bibr">(Husain et al., 2019;</ref><ref type="bibr">Iyer et al., 2018;</ref><ref type="bibr" target="#b2">LeClair and McMillan, 2019)</ref>. We identify and remove the noisy part of the data points without excluding them from the dataset to reduce data loss during preprocessing.</p><p>For both source code and NL description, we split CamelCase and snake case code tokens into subtokens (e.g., Camel Case, snake case) and separate linked alphabets and numbers (e.g., var0 to var 0) <ref type="bibr" target="#b0">(Ahmad et al., 2020;</ref><ref type="bibr">LeClair and McMillan, 5</ref> https://github.com/csebuetnlp/TransCoder 2019). After the aforementioned processing, we remove the data points where the source code is less than 3 tokens, or the description contains less than 2 alphabets <ref type="bibr">(Husain et al., 2019)</ref>. We lowercase the natural language as the case is not necessary for describing codes. We release our data processing tool along with the CoDesc dataset for applying the dataset to diverse tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset Characteristics</head><p>After the previous steps, we are left with nearly 4.2 million Java method and description parallel data. <ref type="table">Table 1</ref> presents the statistics characteristics of our dataset. The combined CoDesc dataset consists of more than one million unique tokens, which is significantly larger than natural language vocabulary <ref type="bibr" target="#b4">(Chen et al., 2019)</ref>. This can be partially attributed to inseparable multi-words (e.g. 'updateproductvariationlocalizeddeltaprice') in our dataset. Hence, we perform BPE <ref type="bibr" target="#b10">(Sennrich et al., 2016)</ref> tokenization in our preprocessing pipeline. We also see that although the average token length of Java source codes vary in the different dataset sources, the natural language descriptions have a relatively uniform length. We create a balanced, deduplicated, and representative train-valid-test dataset by splitting individual source-dataset in 8:1:1 ratio <ref type="table">(Table 1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our code-description corpus in two well-known complementary tasks: source code summarization and natural language code search. In this section, we demonstrate that models trained on CoDesc bring about a noticeable improvement over two established baselines in code search and code summarization. Each benchmarking follows a standard cleaning, preprocessing, and train-test de-duplication process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Natural Language Code Search</head><p>We use the code search models used by Husain et al. (2019) that jointly trains a source code and an NL encoder networks to minimize their encoded vector distance ( <ref type="figure">Figure. 1)</ref>. We apply our dataset on the CODESEARCHNET (CSN) (Husain et al., 2019) -a well-studied benchmark in the semantic code search literature. We train 5 different encoder networks <ref type="table">(Table.</ref> 2) with the CSN Java dataset, and CoDesc respectively. We compare our results with CodeBERT and RoBERTa (code) <ref type="bibr">(Feng et al., 2020)</ref>, two pretrained models achieving state-of-  Results We use Mean Reciprocal Rank (MRR) -the commonly used evaluation metric for code search <ref type="bibr">(Husain et al., 2019;</ref><ref type="bibr" target="#b8">Sachdev et al., 2018;</ref><ref type="bibr">Cambronero et al., 2019)</ref> as the evaluation criteria for code search. <ref type="table" target="#tab_2">Table 2</ref> shows our results, along with state-of-the-art models <ref type="bibr" target="#b4">(Liu et al., 2019;</ref><ref type="bibr">Feng et al., 2020)</ref> that have nearly 8-10 times more parameters than the baseline networks and a more complex training objective. We achieve remarkably close performance with the state-of-the-art models with much simpler and smaller networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Source Code Summarization</head><p>For this task, we follow the methodology proposed by <ref type="bibr" target="#b0">Ahmad et al. (2020)</ref>. They used a seq2seq Transformer <ref type="bibr" target="#b12">(Vaswani et al., 2017)</ref> network with 77M parameters with relative positional encoding (Shaw  Training We train a Transformer model proposed by <ref type="bibr" target="#b0">Ahmad et al. (2020)</ref> with CoDesc-train dataset. We use Adam optimizer with an initial learning rate of 10 ?4 , mini-batch size of 32, and dropout rate 0.2, vocabulary size 50k for code and 30k for NL. However, we use maximum input length of 200 token instead of 150 based on our observation of CoDesc dataset from <ref type="table">Table 1</ref>.</p><p>Each epoch of the model took nearly 8 hours in an NVIDIA V100 16GB GPU. In comparison, the train-small dataset took 8.5 minutes only. For limitation of computational resource, we saved the network weights after training it with the large dataset for two epochs, and to be consistent with the original implementation, trained them further with the train-small dataset for a maximum of 198 more epochs. We perform an early stop if the validation performance does not improve for consecutive 20 epoch. The pretraining provides the network parameters a more favorable initialization than random, helping the network find better local minima.</p><p>Results <ref type="table" target="#tab_4">Table 3</ref> shows that our two epoch pretraining with CoDesc significantly improves the state-of-the-art code summarization methods in all three evaluation metrics -BLEU <ref type="bibr" target="#b6">(Papineni et al., 2002)</ref>, METEOR (Banerjee and Lavie, 2005), and ROUGE-L <ref type="bibr" target="#b3">(Lin, 2004)</ref>. We observe that the pretrained model often generates more descriptive summary even when it achieves lower BLEU score <ref type="figure">(Fig. 2)</ref>. We believe the model has more room for  improvement with further pretraining and we wish to validate this in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation &amp; Analysis</head><p>To quantify the effect of individual data sources and our noise removal methodology, we train each dataset before and after applying our data cleaning method using an NBOW model and test them in the CSN benchmark using their released test set. Although our collected data was already cleaned by the respective authors, <ref type="table" target="#tab_6">Table 4</ref> shows that the performance of every dataset improves drastically after our noise removal. Interestingly, without our extra layer of data cleaning, CoDesc dataset performs worse than training with only CSN data although being significantly larger. This shows the importance of a standard cleaning and processing method. Moreover, CSN (Java) have the highest accuracy, which can be attributed to the fact that it came from the same distribution of data as the evaluation and test sets, and hence contains similar tokens and patterns <ref type="bibr">(Husain et al., 2019)</ref>. We can see from <ref type="table" target="#tab_6">Table 4</ref> that the model trained with CSN (Python2Java) achieves an MRR score of 0.5548. Although this score is lower than other datasets, it is still a good indication that the translated data is helping the model is to learn NL-code association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New Benchmark Results in Code Search</head><p>We provide a new set of benchmark results for CoDesc dataset in natural language code search. We train, validate, and test an NBOW, an RNN, and a Selfattn code search network with the balanced train, validation, and test data shown in <ref type="table">Table 1</ref>. The three models achieve MRR score of 0.812, 0.766, and 0.839 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>In this work, we have accumulated CoDesc -a large code-description parallel dataset and established baseline results. CoDesc brings a noteworthy improvement in two tasks: code search and code summarization. We believe CoDesc will serve as a base for future studies on code-description joint tasks. We also show that automatically translated source code from a source-to-source compiler can be applied in a code-NL parallel task, suggesting that, translating our Java dataset to other programming languages can also be helpful.</p><p>The most striking finding of our study is that, by training with 2X larger parallel data, we achieve equivalent performance to models having 8X parameters <ref type="bibr">(Feng et al., 2020)</ref> in code search. This raises an interesting question: are we fully utilizing the model capacities in code-description studies? From our pretraining results in code summarization, it can be reasonably assumed that pretraining with our large dataset the larger models will also improve further. In future works, we wish to apply new techniques for code search, code summarization, along with exploring our dataset for generalpurpose code synthesis, where the best models are still struggling in accuracy <ref type="bibr" target="#b15">(Wei et al., 2019;</ref><ref type="bibr" target="#b17">Yin and Neubig, 2017</ref>   <ref type="bibr">et al., 2019)</ref>. <ref type="bibr">6</ref> Despite the authors' effort for data cleaning, in our observation, CSN CORPUS is one of the noisiest. The dataset contains duplicate descriptions, inseparable multi-words (e.g., updateproductvariationlocalizeddeltaprice, updatelocationinventory), XML tags (e.g., &lt;tt&gt;, &lt;soup&gt;, &lt;sub&gt;), non-English documentation, non-ASCII and escape characters, unwanted symbols (e.g., @, #, {, }), deprecated methods and descriptions, comments inside code, annotations (e.g., @link, @code, @inheritdoc) in description, etc. Datapoints truncated by TransCoder during Python to Java translation (total 27,471) are marked with a special flag in our released corpus.</p><p>DeepCom Hu et al. (2018a) released a dataset of 588,108 Java method and documentation pairs collected from 9,714 GitHub projects for code summarization 7 . Similar to CODESEARCHNET (Husain et al., 2019), they considered the first sentence of a documentation as the summary of the method as it typically describes the functionalities of Java methods. They filter out empty and single world descriptions and the setter, getter, constructors, and test methods, since they are easy for a model to summarize. In our manual analysis, we found HTML tags (e.g. &lt;tt&gt;, &lt;p&gt;, &lt;p class = ...&gt; , &lt;li&gt;, &lt;ul&gt;), comment tags, annotations, escape characters inside descriptions, empty parentheses as descriptions, repetitive and nonmeaningful descriptions, comments inside source code, etc. Despite the authors' claim, we found numerous test methods in the dataset, which were mostly meaningful data.</p><p>CONCODE Iyer et al. (2018) released a dataset named CONCODE, collected by mining nearly 33,00 GitHub repositories 8 . In their preprocessed dataset, they replaced the names of the identifier and method names with generic terms, (e.g., arg0, loc0, function, etc.) and replaced all string literals with constants. This created a discrepancy with the other datasets, hence, we opted for their</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Baseline models trained with default dataset and CoDesc, along with, comparison with SoTA pre- trained models in CSN Java test set. Training on CoDesc outperforms training on CSN-Java only, and it is comparable to SOTA with 8x fewer parameters.the-art results in CSN Benchmark. They are trained with a Masked Language Modeling (MLM) (De- vlin et al., 2019) objective on 2.1 million bimodal code-NL data, and 6.4 million unimodal data re- leased with CODESEARCHNET.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Code summarization with<ref type="bibr" target="#b0">Ahmad et al. (2020)</ref> proposed Transformer network with and without pretraining with CoDesc.</figDesc><table><row><cell>et al., 2018) and copying mechanism (See et al.,</cell></row><row><cell>2017) and achieved state-of-the-art results.</cell></row><row><cell>Data preparation Ahmad et al. (2020) used a</cell></row><row><cell>Java dataset released by Hu et al. (2018b) and pre-</cell></row><row><cell>processed by Wei et al. (2019) consisting of train-</cell></row><row><cell>ing, validation, and test datasets of size 69,708,</cell></row><row><cell>8,714, and 8,714 respectively. We refer to this</cell></row><row><cell>training data as train-small. We create a new</cell></row><row><cell>dataset CoDesc-train by combining train-small</cell></row><row><cell>with CoDesc. We replace all literals as Wei et al.</cell></row><row><cell>(2019) and tokenize the dataset using Character</cell></row><row><cell>BPE Tokenization (Sennrich et al., 2016) to create</cell></row><row><cell>the same size vocabulary as the previous works.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>BLEU: 0.12): if there are any object in the list then the object is not immutableFigure 2: CoDesc pretrained model generates more descriptive summary, even in cases it achieves lower score.</figDesc><table><row><cell cols="3">public void makeImmutable(){</cell></row><row><cell>if(mutable){</cell><cell></cell><cell></cell></row><row><cell cols="3">if(results ! = null){</cell></row><row><cell cols="4">int length = results.size();</cell></row><row><cell cols="4">for(int i = NUM; i &lt; length; i + +){</cell></row><row><cell cols="4">Result result = (Result)results.get(i); result.makeImmutable();</cell></row><row><cell cols="4">} results = Collections.unmodifiableList(results);</cell></row><row><cell cols="3">} mutable = BOOL; } }</cell></row><row><cell cols="3">Human written: makes the object immutable</cell></row><row><cell cols="4">Transformer prediction (BLEU: 1.0): makes the object immutable</cell></row><row><cell cols="2">CoDesc pretrained model prediction (Dataset Raw data</cell><cell>Clean data</cell><cell>Inc. (%)</cell></row><row><cell>CSN (Java)</cell><cell cols="2">0.5870 0.6427</cell><cell>5.57</cell></row><row><cell>DeepCom</cell><cell cols="2">0.4677 0.6069</cell><cell>13.92</cell></row><row><cell>FunCom</cell><cell cols="2">0.5379 0.6366</cell><cell>9.87</cell></row><row><cell>CONCODE</cell><cell cols="2">0.5444 0.6234</cell><cell>7.90</cell></row><row><cell cols="3">CSN (Python2Java) 0.5081 0.5546</cell><cell>4.65</cell></row><row><cell>CoDesc (All)</cell><cell cols="2">0.5852 0.6826</cell><cell>9.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: MRR of individual datasets (Section 3.1) be-</cell></row><row><cell>fore and after noise removal.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>).</figDesc><table><row><cell></cell><cell cols="3">9052-9065, Online. Association for Computational</cell></row><row><cell></cell><cell>Linguistics.</cell><cell></cell></row><row><cell></cell><cell>CommonCrawl.</cell><cell>Common crawl.</cell><cell>https://</cell></row><row><cell></cell><cell cols="3">commoncrawl.org/. Accessed: 2021-01-31.</cell></row><row><cell></cell><cell cols="3">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and</cell></row><row><cell></cell><cell cols="3">Kristina Toutanova. 2019. BERT: Pre-training of</cell></row><row><cell></cell><cell cols="3">deep bidirectional transformers for language under-</cell></row><row><cell>Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi</cell><cell cols="3">standing. In Proceedings of the 2019 Conference</cell></row><row><cell>Ray, and Kai-Wei Chang. 2021. Unified pre-training</cell><cell cols="3">of the North American Chapter of the Association</cell></row><row><cell>for program understanding and generation. In Pro-</cell><cell cols="3">for Computational Linguistics: Human Language</cell></row><row><cell>ceedings of the 2021 Conference of the North Amer-</cell><cell cols="3">Technologies, Volume 1 (Long and Short Papers),</cell></row><row><cell>ican Chapter of the Association for Computational</cell><cell cols="3">pages 4171-4186, Minneapolis, Minnesota. Associ-</cell></row><row><cell>Linguistics.</cell><cell cols="2">ation for Computational Linguistics.</cell></row><row><cell>Satanjeev Banerjee and Alon Lavie. 2005. METEOR:</cell><cell cols="3">Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-</cell></row><row><cell>An automatic metric for MT evaluation with im-</cell><cell cols="3">aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,</cell></row><row><cell>proved correlation with human judgments. In Pro-</cell><cell cols="3">Ting Liu, Daxin Jiang, and Ming Zhou. 2020. Code-</cell></row><row><cell>ceedings of the ACL Workshop on Intrinsic and Ex-</cell><cell cols="3">BERT: A pre-trained model for programming and</cell></row><row><cell>trinsic Evaluation Measures for Machine Transla-</cell><cell cols="3">natural languages. In Findings of the Association</cell></row><row><cell>tion and/or Summarization, pages 65-72, Ann Ar-</cell><cell cols="3">for Computational Linguistics: EMNLP 2020, pages</cell></row><row><cell>bor, Michigan. Association for Computational Lin-</cell><cell cols="3">1536-1547, Online. Association for Computational</cell></row><row><cell>guistics.</cell><cell>Linguistics.</cell><cell></cell></row><row><cell>Ond?ej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale? Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation,</cell><cell cols="3">Karl Moritz Hermann, Tom?? Ko?isk?, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Proceedings of the 28th Inter-national Conference on Neural Information Process-ing Systems (NIPS 2015), pages 1693-1701, Mon-treal, Canada.</cell></row><row><cell>pages 12-58, Baltimore, Maryland, USA. Associa-tion for Computational Linguistics.</cell><cell cols="3">Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018a. Deep code comment generation. In Proceedings</cell></row><row><cell>Jose Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra. 2019. When deep learn-</cell><cell cols="3">of the 26th Conference on Program Comprehension, ICPC '18, page 200-210.</cell></row><row><cell>ing met code search. In Proceedings of the 2019 27th ACM Joint Meeting on European Software En-gineering Conference and Symposium on the Foun-dations of Software Engineering, ESEC/FSE 2019, pages 964-974.</cell><cell cols="3">Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018b. Summarizing source code with trans-ferred api knowledge. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, pages 2269-2275. Interna-</cell></row><row><cell>Wenhu Chen, Yu Su, Yilin Shen, Zhiyu Chen, Xifeng Yan, and William Yang Wang. 2019. How large a</cell><cell cols="3">tional Joint Conferences on Artificial Intelligence Organization.</cell></row><row><cell>vocabulary does text classification need? a varia-tional approach to vocabulary selection. In Proceed-ings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-guistics: Human Language Technologies, Volume 1</cell><cell cols="3">Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Code-searchnet challenge: Evaluating the state of seman-tic code search. arXiv preprint arXiv:1909.09436.</cell></row><row><cell>(Long and Short Papers), pages 3487-3497, Min-</cell><cell cols="3">Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and</cell></row><row><cell>neapolis, Minnesota. Association for Computational</cell><cell cols="3">Luke Zettlemoyer. 2016. Summarizing source code</cell></row><row><cell>Linguistics.</cell><cell cols="3">using a neural attention model. In Proceedings</cell></row><row><cell></cell><cell cols="3">of the 54th Annual Meeting of the Association for</cell></row><row><cell>Kevin Clark, Minh-Thang Luong, Quoc V. Le, and</cell><cell cols="3">Computational Linguistics (Volume 1: Long Papers),</cell></row><row><cell>Christopher D. Manning. 2020. ELECTRA: Pre-</cell><cell cols="3">pages 2073-2083, Berlin, Germany. Association for</cell></row><row><cell>training text encoders as discriminators rather than</cell><cell cols="2">Computational Linguistics.</cell></row><row><cell>generators. In ICLR.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and</cell></row><row><cell>Colin Clement, Dawn Drain, Jonathan Timcheck,</cell><cell cols="3">Luke Zettlemoyer. 2018. Mapping language to code</cell></row><row><cell>Alexey Svyatkovskiy, and Neel Sundaresan. 2020.</cell><cell cols="3">in programmatic context. In Proceedings of the</cell></row><row><cell>PyMT5: multi-mode translation of natural language</cell><cell cols="3">2018 Conference on Empirical Methods in Natu-</cell></row><row><cell>and python code with transformers. In Proceed-</cell><cell cols="3">ral Language Processing, pages 1643-1652, Brus-</cell></row><row><cell>ings of the 2020 Conference on Empirical Methods</cell><cell cols="3">sels, Belgium. Association for Computational Lin-</cell></row><row><cell>in Natural Language Processing (EMNLP), pages</cell><cell>guistics.</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/github/CodeSearchNet 2 https://github.com/xing-hu/DeepCom 3 https://github.com/sriniiyer/concode 4 http://leclair.tech/data/funcom/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/github/CodeSearchNet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank the ICT Division, Bangladesh for funding the project and Intelligent Machines Limited for providing the cloud support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>7 https://github.com/xing-hu/DeepCom 8 https://github.com/sriniiyer/concode unprocessed dataset rather than the processed version. The unprocessed dataset released with CON-CODE contained approximately 2.1 million Java methods and lowercased Javadoc-style document pairs. Upon duplicate removal, we were left with 733,878 datapoints.</p><p>Although some noises were present in this dataset, we found this data to be least noisy in manual observation. We find that because of lower casing the documentations, some CamelCase tokens became inseparable. The dataset also contained non-English comments with English alphabets (mostly Italian). We found these documents hard to identify and remove.</p><p>FunCom LeClair and McMillan (2019) released a dataset of over 2.1 million pairs of Java methods and one-sentence method descriptions from over 28k Java projects 9 . They collected this dataset by filtering over 51 million Java methods from UCI Source Code datasets <ref type="bibr" target="#b5">(Lopes et al., 2010)</ref>. In their preprocessing step, <ref type="bibr" target="#b2">LeClair and McMillan (2019)</ref> removed all datapoints where the method is more than 100 tokens long, or the method description is over 13 tokens or below 3 tokens.</p><p>In our observation of this dataset, we found method descriptions containing HTML tokens (e.g. &lt;tt&gt;, annotations (e.g., @link, @param), comment tokens, unwanted symbols, solely nonalphabetic characters, etc. It also contained comments inside methods, and a large portion of the data were getter, setter, tester, and toString methods. Description: sorts the elements in the list acording to their comparator. there are two reasons why lights should be resorted. first, if the lights have moved, that means their distance to the spatial changed. second, if the spatial itself moved, it means the distance from it to the individual lights might have changed. <ref type="bibr">(FunCom)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Sample Data</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A transformer-based approach for source code summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasi</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.449</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4998" to="5007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised translation of programming languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baptiste</forename><surname>Roziere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lowik</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recommendations for datasets for source code summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Leclair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><surname>Mcmillan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1394</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3931" to="3937" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">UCI source code data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bajracharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ossher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Retrieval on source code: a neural code search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saksham</forename><surname>Sachdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seohyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satish</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="DOI">10.1145/3211346.3211353</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</title>
		<meeting>the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="31" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS 2017)</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems (NIPS 2017)<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SuperGLUE: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3266" to="3280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Code generation as a dual task of code summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6563" to="6573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to mine aligned code and natural language pairs from stack overflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1145/3196398.3196408</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Mining Software Repositories, MSR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="476" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1041</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
