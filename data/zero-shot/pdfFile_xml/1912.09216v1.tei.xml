<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">B. CHATTERJEE, C. POULLIS: SEMANTIC SEGMENTATION AND LATENT LEARNING FOR CLASSIFICATION OF AUXILIARY TASKS 1 Semantic Segmentation from Remote Sensor Data and the Exploitation of Latent Learning for Classification of Auxiliary Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Bodhiswatta</forename><surname>Chatterjee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Charalambos</forename><surname>Poullis</surname></persName>
						</author>
						<title level="a" type="main">B. CHATTERJEE, C. POULLIS: SEMANTIC SEGMENTATION AND LATENT LEARNING FOR CLASSIFICATION OF AUXILIARY TASKS 1 Semantic Segmentation from Remote Sensor Data and the Exploitation of Latent Learning for Classification of Auxiliary Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-latent learning</term>
					<term>sub-classification of negative label</term>
					<term>interpretability of semantic segmentation networks</term>
					<term>building classification</term>
					<term>building reconstruction</term>
					<term>classification accuracy</term>
					<term>reconstruction accuracy</term>
					<term>relationship between classification and reconstruction accuracy</term>
					<term>!</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we address three different aspects of semantic segmentation from remote sensor data using deep neural networks. Firstly, we focus on the semantic segmentation of buildings from remote sensor data and propose ICT-Net: a novel network with the underlying architecture of a fully convolutional network, infused with feature re-calibrated Dense blocks at each layer. Uniquely, the proposed network combines the localization accuracy and use of context of the U-Net network architecture, the compact internal representations and reduced feature redundancy of the Dense blocks, and the dynamic channel-wise feature re-weighting of the Squeeze-and-Excitation(SE) blocks. The proposed network has been tested on the INRIA and AIRS benchmark datasets and is shown to outperform all other state of the art by more than 1.5% and 1.8% on the Jaccard index, respectively. Secondly, as the building classification is typically the first step of the reconstruction process, we investigate the relationship of the classification accuracy to the reconstruction accuracy. A comparative quantitative analysis of reconstruction accuracies corresponding to different classification accuracies confirms the strong correlation between the two. We present the results which show a consistent and considerable reduction in the reconstruction accuracy. Finally, we present the simple yet compelling concept of latent learning and the implications it carries within the context of deep learning. We posit that a network trained on a primary task (i.e. building classification) is unintentionally learning about auxiliary tasks (e.g. the classification of road, tree, etc) which are complementary to the primary task. Although embedded in a trained network, this latent knowledge relating to the auxiliary tasks is never externalized or immediately expressed but instead only knowledge relating to the primary task is ever output by the network. We experimentally prove this occurrence of incidental learning on the pre-trained ICT-Net and show how sub-classification of the negative label is possible without further training/fine-tuning. We present the results of our experiments and explain how knowledge about auxiliary and complementary tasks -for which the network was never trained -can be retrieved and utilized for further classification. We extensively tested the proposed technique on the ISPRS benchmark dataset which contains multi-label ground truth, and report an average classification accuracy (F1 score) of 54.29% (SD=17.03) for roads, 10.15% (SD=2.54) for cars, 24.11% (SD=5.25) for trees, 42.74% (SD=6.62) for low vegetation, and 18.30% (SD=16.08) for clutter. The source code and supplemental material is publicly available at http://www.theICTlab.org/lp/2019ICT-Net/.</p><p>Index Terms-latent learning; sub-classification of negative label; interpretability of semantic segmentation networks; building classification; building reconstruction; classification accuracy; reconstruction accuracy; relationship between classification and reconstruction accuracy; ! ? B. Chatterjee is a researcher at the Immersive and Creative Technologies lab and an Applied Scientist at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I N recent years there have been tremendous advances in the area of computer vision due to the advent of deep learning. Human-like performance -and in some cases superhuman as in <ref type="bibr" target="#b45">[45]</ref> -has already been achieved for the cognitive task of visual and spatial processing. However, this high performance is often limited to test data which exhibits similar characteristics to the training dataset e.g. has the same feature space with the Manuscript received <ref type="bibr">April 19, 2005</ref>; revised August <ref type="bibr">26, 2015.</ref> same distribution. Thus, in order for a network to achieve high accuracy it is imperative that it is trained on a large dataset having large variability.</p><p>To address this need, many online competitions are offering large benchmark datasets for training, validation, and testing. The creation of a large training dataset involving images or video requires an extensive number of people to manually label the data in order to ensure the correctness and completeness of the ground truth. This is exacerbated by the fact that each dataset typically serves a single classification task. This is especially true for remote sensor data. For example, in our work we use two datasets provided by INRIA <ref type="bibr" target="#b26">[27]</ref> and AIRS <ref type="bibr" target="#b5">[6]</ref> which include only positive labels for buildings, therefore can only be used for this specific binary classification task.</p><p>Despite the fact that to create such datasets is a tedious, time consuming, and often very expensive process, researchers arXiv:1912.09216v1 [cs.CV] <ref type="bibr" target="#b18">19</ref> Dec 2019 are relying on large datasets for training classifiers to assist in the solution of more difficult problems, such as reconstruction. More specifically, reconstructing large-scale urban areas is an inherently complex problem which involves a number of vision tasks. Typically, the first step is classification where the objective is to label each pixel into an urban feature type e.g., building, road, tree, car, ground, vegetation, etc. Next, the pixellevel labels are used to cluster the pixels into contiguous groups corresponding to instances of the urban features they represent. Finally, the reconstruction is performed on each cluster. A reconstruction algorithm is applied on each cluster according to the urban feature type the cluster corresponds to. In the case of clusters corresponding to buildings, a boundary refinement process is typically performed prior to extruding the building facades.</p><p>Hence, as it is evident from the pipeline described above, classification is an imperative component of the process and if the objective is to achieve a complete urban-area reconstruction one has to first address the problems relating to the classification. Thus, in this paper our contributions relate to the following three aspects of classification -more specifically semantic segmentation -from remote sensor data using deep neural networks:</p><p>? Firstly, we address the problem of the classification of buildings in remote sensor imagery. We investigate a number of state of the art deep neural network architectures and present a comparative study of the results along with a reasoned justification on the design decisions for the proposed network named ICT-Net: a novel network with the underlying architecture of a fully convolutional network infused with Dense feature re-calibrated blocks at each layer. We demonstrate that this combination of components leads to superior performance. The proposed network is ranked first 1 at two international benchmark competitions organized by INRIA and AIRS, with higher performance by 1.5% and 1.8% respectively from the second best networks. ? Secondly, we study the relationship between the classification accuracy and reconstruction accuracy. We perform a comparative quantitative analysis on the reconstructions corresponding to classifications of different accuracies and report the results. Due to the lack of depth information, reconstructing 3D models is not feasible therefore the accuracy of the border localization is used as proxy for the evaluation since it is tightly coupled to the reconstruction accuracy i.e. buildings are extruded using their boundaries. As anticipated there is a strong correlation between the classification accuracy and the accuracy of the reconstruction however the analysis has shown that there is a consistent and considerable decrease in the reconstruction accuracy in terms of the per-pixel and per-building Jaccard indices. To the best of our knowledge this is the first time a quantitative analysis is performed in order to establish how the classification accuracy relates to the accuracy of the reconstruction as determined by the accuracy of the border localization. ? Thirdly, we introduce the concept of latent learning in networks trained with datasets tailored for a binary classification <ref type="bibr" target="#b0">1</ref>. As of February 2019 task. In psychology, latent learning (or incidental learning) is a form of learning which occurs unintentionally <ref type="bibr" target="#b47">[47]</ref>. It stems from our ability as humans to learn without making any conscious attempt and thus retain information not directly relevant to the primary task we are performing, without any reinforcement or motivation i.e. no reward or penalty. This concept can be transferred to the context of learning in deep neural networks. It would therefore mean that at the same time a deep neural network is trained to perform a primary task for which labeled data is required, latent learning is also occurring for auxiliary tasks for which labeled data is not required. In other words, supervised learning is performed on a primary task while unsupervised learning is concurrently occurring on auxiliary tasks. We experimentally prove that latent learning is indeed occurring in the training of deep neural networks, and demonstrate how the latent knowledge can be externalized on the pretrained binary classification convolutional neural network ICT-Net. Although it has been reported in literature that clustering of features relating to labeled classes occurs throughout the network for simple architectures e.g. AlexNet <ref type="bibr" target="#b21">[22]</ref>, VGG <ref type="bibr" target="#b21">[22]</ref>, to the best of our knowledge this is the first time this is shown for semantic segmentation networks with very large depth, for classes not contained in the training labels.</p><p>Our experiments indicate that deep neural networks encode and cluster information that is not directly relevant to the primary task thus making it possible to re-purpose a pretrained network for sub-classification of auxiliary tasks without further training or fine-tuning. The proposed technique was evaluated on a third benchmark dataset (ISPRS) which contained multi-label ground truth for 6 classes: buildings, roads, cars, trees, low vegetation, and clutter. Using the proposed technique and without further training, we are able to achieve accuracies of 54.29% (SD=17.03), 10.15% (SD=2.54), 24.11% (SD=5.25), 42.74% (SD=6.62), 18.30% (SD=16.08) for auxiliary tasks relating to the classification of roads, cars, trees, low vegetation, and clutter respectively. Paper organization: The paper is organized as follows: Section 2 presents an overview of state of the art in the area of classification of urban features in satellite images using deep neural networks, and in the area of interpretability of neural networks. Section 3 provides an overview of the work. The proposed neural network ICT-Net trained on the binary classification task of building/non-building is explained in Section 4 including a reasoned justification of the design decisions, and details on the training and testing of the network. Section 5 presents a quantitative analysis of the reconstruction accuracies resulting from different classification accuracies. Section 6 describes the hypothesis and experimental proof of latent learning and explains how a pre-trained network can be re-purposed for classification of auxiliary tasks by sub-classification of the negative label. Finally, Section 7 concludes the work and discusses future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Below we provide an overview of the most relevant work relating to (a) the classification of urban features from remote sensor data using deep learning, and (b) interpretability in deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Classification</head><p>Over the years object recognition has become one of the most addressed vision challenges and as a result a plethora of work has already been proposed. Initially the goal was on image classification where the entire image was classified according to the single object it contained; with some of the most important work in this area being <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b46">[46]</ref>. More recently, the objective has shifted towards the semantic object segmentation or semantic labeling where multiple objects contained in a single image are labeled according to their class at the pixel level. Some of the most important work in this area is the work in <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b36">[36]</ref>. Recent techniques using deep neural networks have demonstrated excellent results. Below we provide a brief overview of the state of the art related to the area of semantic labeling with a particular focus on remote sensor imagery. A comprehensive review of neural network architectures for semantic segmentation can be found in <ref type="bibr" target="#b9">[10]</ref>.</p><p>Typical semantic segmentation architectures comprise of a down-sampling path responsible for feature extraction and an up-sampling path to restore the resolution of the semantic labels. Skip connections between the two paths help to have a smooth gradient back-propagation and fast training of the network. The U-Net <ref type="bibr" target="#b36">[36]</ref> architecture was able to achieve end-to-end semantic labeling with high accuracy in the field of medical image segmentation. Since then the U-Net <ref type="bibr" target="#b36">[36]</ref> architecture has been extensively used and adapted to many other domains especially labeling of buildings from aerial imagery as in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p><p>At the same time, deeper networks <ref type="bibr" target="#b46">[46]</ref> have demonstrated the capacity to extract better features from images. Skip connections have been shown <ref type="bibr" target="#b12">[13]</ref> to play a critical role in the training of very deep networks as they facilitate very good gradient propagation. There has been a lot of work on the pattern of skip connections with a very promising pattern known as Dense blocks proposed in <ref type="bibr" target="#b15">[16]</ref> for the problem of image classification. In a Dense block every layer is connected to every other layer in a feed-forward fashion. This provides implicit deep supervision and feature reuse which in turn improves the feature extraction power without making it difficult to train the network. The Tiramisu network architecture proposed in <ref type="bibr" target="#b19">[20]</ref> extended the use of Dense blocks for semantic segmentation and was able to outperform state of the art on two benchmark data sets: Gatech and CamVid.</p><p>Most deep neural networks for object recognition consider all extracted features at each layer to be of equal importance. This was until the method proposed in <ref type="bibr" target="#b13">[14]</ref> showed that feature re-calibration i.e. weighing of the features, can be used effectively to model inter-dependencies between channels and produce even better performance with little computational overhead. Along a similar direction, in <ref type="bibr" target="#b38">[38]</ref> the authors have shown that feature re-calibration combined with well known FCN networks perform well for medical image segmentation.</p><p>With respect to urban reconstruction, the extraction of urban geospatial features such as buildings from remote sensor imagery has also been an area of research interest for a very long time <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Automatic reconstruction of 3D models from the extracted features is extremely useful for many applications ranging from urban and community planning, development and architectural design, training of emergency response personnel, military personnel, etc. In <ref type="bibr" target="#b34">[34]</ref> the authors propose a novel, robust, automatic segmentation technique based on the statistical analysis of the geometric properties of the data as well as an efficient and automatic modeling pipeline for the reconstruction of large-scale areas containing several thousands of buildings. With the recent advances in deep neural network architectures the pipeline has been upgraded to feature extraction using a semantic labeling CNN followed by clustering the points based on their label, and specialized processing for each of the labels of geospatial objects as proposed in <ref type="bibr" target="#b8">[9]</ref>.</p><p>Recently there has been a lot of interest for semantic labeling of buildings <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref> fueled by the release of very large datasets such as the INRIA Aerial Image Labeling dataset <ref type="bibr" target="#b26">[27]</ref>, and SpaceNet where a corpus of commercial satellite imagery with labeled training data was made publicly available for use in machine learning research. In <ref type="bibr" target="#b16">[17]</ref> the authors use a variant of the aforementioned U-Net network architectures replacing the VGG11 <ref type="bibr" target="#b42">[42]</ref> encoder with a more powerful activated Batch Normalized <ref type="bibr" target="#b3">[4]</ref> WideResnet-38 <ref type="bibr" target="#b50">[50]</ref> in the context of instance segmentation of buildings for DeepGlobe-CVPR 2018 building detection sub-challenge, and were able to get very good results.</p><p>In this work, we propose ICT-Net: a novel network architecture that combines the strengths of deep neural network architectures (UNet) and building blocks (DenseNet block, SE block) which when applied to the problem of semantic labeling of buildings is proven to achieve better classification accuracy than state of the art on the INRIA Aerial Image Labeling dataset. As of writing this manuscript the proposed network is top ranked since February 2019 on 2 benchmark competitions' leaderboard with more than 1.5% and 1.8% difference from the second best entry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Interpretability</head><p>As soon as the deep neural networks started performing well there has been a huge emphasis on understanding the internal workings of deep neural networks. Zeiler et al. <ref type="bibr" target="#b51">[51]</ref> proposed deconvolutional networks to visualize and improve the performance of Convolutional Neural network like AlexNet <ref type="bibr" target="#b21">[22]</ref>. After that there has been a lot of effort to interpret the internal workings of deep neural networks. All currently known techniques focus on interpretibility of image classification networks. These techniques can be roughly categorised into 4 different groups (i) Saliency based techniques like <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b44">[44]</ref> investigate change of pixels which lead to increase or decrease of prediction probability for some specific class of objects, (ii) Decomposition based techniques like <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b52">[52]</ref>, <ref type="bibr" target="#b29">[30]</ref> decompose the prediction of the network output into the relevance scores and propagate back to the previous layer, ultimately leading back to the input image to find which input pixels are responsible for a prediction, (iii) Deconvolutional approaches like <ref type="bibr" target="#b51">[51]</ref>, <ref type="bibr" target="#b43">[43]</ref> try to map lower layer activations to higher layers and ultimately back to the input image to have <ref type="figure">Fig. 1</ref>: The diagram summarizes the work presented in this paper. Firstly, we focus on the building classification and propose a novel network architecture which outperforms state-of-the-art on benchmark datasets and is currently top-ranking. Secondly, we investigate the relation between the classification accuracy and the reconstruction accuracy and conduct a comparative quantitative analysis which shows a strong correlation but also a consistent and considerable decrease of the reconstruction accuracy when compared to the classification accuracy. a better understanding of what kind of patches provide some specific layer activations, (iv) Another important line of work proposed in <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b55">[55]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref> is understanding the predictions of the model by analyzing the individual units of the network and trying to come up with an explanation for the most common patches that produce the specific activation.</p><p>The discussed techniques provide a very brief overview of the huge amount of work done in the recent past on interpreting deep neural networks. All the currently known techniques focus on interpreting classification networks where there is only one label for the complete image. There are other tasks in computer vision like semantic segmentation where neural networks based architectures are performing very well but there has been very little focus on interpreting the network prediction. The currently known techniques for interpreting image classification networks can not be directly applied to image segmentation networks as they have an encoder-decoder architecture as compared to an only encoded architecture of classification networks. In this work we interpret the predictions of our proposed ICT-Net architecture by analysing the last few layers of the network, which to the best of our knowledge is the first time to be done in the context of semantic segmentation.</p><p>Perhaps the closest concept to what we are proposing is that of auxiliary learning where in order to improve the ability of a primary task to generalize to new unseen data, an auxiliary task is trained in unison. This, of course, has the requirement that labels for the auxiliary task are also available for supervised learning as in <ref type="bibr" target="#b22">[23]</ref>. Hence, an alternative would be unsupervised auxiliary learning similar to the work presented in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b53">[53]</ref>. Although these methods eliminate the need for labeled data, they typically involve a secondary label generation network which is tightly coupled to the network trained on the primary task as in <ref type="bibr" target="#b24">[25]</ref>. In contrast to the above work and definition of auxiliary learning, our work refers to the unintentional learning of auxiliary tasks resulting from the learning of a primary task for which labeled data is available and easily obtainable.</p><p>In summary, previous work has only dealt with the related yet different concepts of interpretability of neural networks, and auxiliary tasks. In contrast, our technical contribution is on exploiting latent learning in order to re-purpose a pre-trained classification network on auxiliary tasks for which labeled data is not available. To the best of our knowledge, this is the first time the concept of latent learning is introduced within the context of deep neural networks. <ref type="figure">Figure 1</ref> gives an overview of our first two contributions. Firstly, an orthorectified RGB image is fed forward into the neural network to produce a binary (building/non-building) classification map. Next, the binary classification map becomes the input to the reconstruction process. Due to the fact that it is extremely difficult to acquire building blueprints or CAD models for such large areas and depth/3D information is not available for the images of the benchmark, we posit that the building boundaries extracted from the binary classification map and subsequently refined, can serve as a proxy to the accuracy of the reconstruction. This is justified since the extracted boundaries are extruded in order to create the 3D models for the buildings. Therefore, the building boundaries are extracted, refined, and are used in the comparative analysis and evaluation of the accuracy of the reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SYSTEM OVERVIEW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BUILDING CLASSIFICATION</head><p>In this section we describe the details of the proposed neural network architecture including information about the datasets, training/validation, and testing, as well as the justification for all design decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We have chosen the two datasets described below over other available options because they uniquely offer two significant advantages. Firstly, the training and testing datasets are from completely different cities with no overlap. Secondly, the datasets cover dissimilar urban settlements e.g., European, American, etc, with large variability in building density, architecture, and overall characteristics e.g., red shingles, flat roofs, etc. For these reasons, we have chosen the following benchmark datasets because they are ideal for assessing the generalization capacity of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">INRIA Aerial Image labeling dataset</head><p>The training of the network is performed using the INRIA Aerial Image labeling dataset <ref type="bibr" target="#b26">[27]</ref> which consists of pixelwise labeled aerial imagery for building classification. The dataset covers 810km 2 area across 10 different cities with ground sampling density resolution of 30cm, and is split into two equal sets (405Km 2 each) for training and testing. The dataset consists of 3-band orthorectified RGB images and the training labels consist of ground truth data for two semantic classes: building and non-building. The training data covers parts of the cities of Austin, Chicago, Kitsap county, western Tyrol, and Vienna. The test data covers parts of the cities of Bellingham, Bloomington, Innsbruck, San Francisco and Eastern Tyrol. There are 36 tiles with resolution of 5000?5000 pixels for each city, each tile covering 1500?1500m 2 area on the ground. The training data is further divided into two sets: (1) the validation set which comprises of the first 5 tiles of each city, and (2) the training set which consists of the rest of the tiles as suggested in <ref type="bibr" target="#b26">[27]</ref>. An example image from the dataset can be seen in <ref type="figure">Figure  1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">AIRS dataset</head><p>AIRS (Aerial Imagery for Roof Segmentation) is a public dataset that aims at benchmarking the algorithms of roof segmentation from very high-resolution aerial imagery. AIRS dataset covers almost the full area of Christchurch, the largest city in the South Island of New Zealand. Although the aerial imagery is from one city, there is huge variety of settlement types. The imagery contains 3-band orthorectified RGB images at 7.5cm ground sampling density. It has a coverage of 457Km 2 aerial images with approximately over 220,000 buildings and refined ground truths that strictly align with roof outlines.There are 1046 tiles with resolution of 10000 ? 10000 pixels which are already divided into train, validation and test sets of sizes 857, 94, and 95. The ground truth for buildings is carefully refined to align with their roofs and the segmentation task for AIRS contains two semantic classes: roof and nonroof. We have chosen the AIRS benchmark dataset as it covers a completely different geographic location with different urban settlements and the aerial imagery is very high resolution so we are able to further validate the generalization capacity of the trained neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Network Architecture</head><p>A vast number of networks has been proposed for image classification and semantic labeling. State of the art performance is generally achieved with deep networks however these are difficult to train due to vanishing or exploding gradients. Many networks <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b19">[20]</ref> have shown skip connections play an important role in having good gradient propagation through the network. In our work, as part of the network design process, we first identified the requirements for the particular task at hand i.e. semantic segmentation of buildings from remote sensor images, and then decisions were made to address these:</p><p>? Requirement 1: An important aspect of semantic segmentation of buildings is to have high localization accuracy and take into account as much context information as possible. This is necessary in order to address the wide variability in buildings typically relating to their function e.g., shape, size, color and/or region they appear in e.g., density in urban/rural, etc. Decision: To that end, the U-Net architecture <ref type="bibr" target="#b36">[36]</ref> takes into account spatial information and combines it with contextual information via the direct downsamplingupsampling links. ? Requirement 2: In order to be able to process large chunks of data at a time it is imperative that the network contains as few parameters as possible.</p><p>Decision: Dense blocks connect every layer to every other layer in a feed-forward fashion. Along with good gradient propagation they also encourage feature reuse and reduce the number of parameters substantially as there is no need to relearn the redundant feature maps. At the end of every Dense block all the extracted features accumulate, creating a very diverse set of features. As a result of this feature redundancy there is a substantial reduction in the network parameters leading to faster training times. This allows the processing of larger patch (and batch) sizes (which also addresses Requirement 1) therefore allowing additional contextual information during each feed-forward pass. ? Requirement 3: The contribution of the feature maps at each layer to the output must depend on their importance. Decision: Using the Squeeze-and-Excitation (SE) blocks the dynamic channel-wise feature re-weighting mechanism provides a way to upweigh important feature maps and downweigh the rest. In <ref type="bibr" target="#b13">[14]</ref> authors show adaptive re-calibration of channel-wise feature responses by explicitly modelling inter-dependencies between channels using squeeze and excitation block on existing architectures <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b49">[49]</ref> results in improved performance. The proposed network architecture is distinct and combines the strengths of the U-Net architecture, Dense blocks, and Squeeze-and-Excitation (SE) blocks. This results in improved prediction accuracy and it has been shown to outperform other state of the art network architectures such as the ones proposed in <ref type="bibr" target="#b14">[15]</ref> which have a much higher number of learning parameters on the INRIA benchmark dataset. <ref type="figure" target="#fig_0">Figure 2</ref> shows a diagram of the proposed feature recalibrated Dense block with 4 convolutional layers and a growth rate ? = 16 used by the ICT-Net. The proposed network has 11 feature recalibrated dense blocks with <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref> number of convolutional layers in each dense block respectively.</p><p>Perhaps the closest architecture to the one proposed was discussed in <ref type="bibr" target="#b19">[20]</ref> which uses 103 convolutional layers. If SE blocks are introduced at the output of every layer this will cause a vast increase in the number of parameters which will hinder the training. In contrast, in our work we have chosen to include an SE block only at the end of every Dense block in order to re-calibrate the accumulated feature-maps of all preceding layers. Thus, the variations in the information learned at each layer -in the form of the features maps -are weighted by the SE block according to their importance as determined by the loss function. Discussion: To verify the validity of the above design decisions we performed a comparative study involving a number of state of the art architectures and blocks. Following the same training procedure for all architectures reported, and without any data augmentation the ICT-Net was compared with U-Net <ref type="bibr" target="#b36">[36]</ref> and Tiramisu-103 <ref type="bibr" target="#b19">[20]</ref>. The results on the validation dataset are shown in <ref type="table" target="#tab_0">Table 1</ref> where it is evident that the proposed architecture outperforms both U-Net and Tiramisu-103.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paper Method</head><p>Overall IoU (%) Overall Accuracy (%) <ref type="bibr" target="#b36">[36]</ref> UNet 70.86 95.51 <ref type="bibr" target="#b19">[20]</ref> Tiramisu-103 73.91 95.71 Ours ICT-Net 75.5 96.05 Limited by GPU memory we had to choose a small batch size of 4 to have a comparatively larger patch size of 256 ? 256 as we observed context is very important for semantic labeling of buildings. Implementation details: The network was trained using cross-entropy loss with RMSProp Optimizer with an initial learning rate of 0.001 and decay of 0.995 for the first 50 epochs. After the 50 th epoch the learning rate was reduced to 0.0001 and trained for another 50 epochs. Instead of using dropout as a regularization technique we applied a large number of data augmentations in order to restrict the network from overfitting to the training dataset.</p><p>Data input: Our network takes in patches of 256 ? 256 out of the entire tile with 50% overlap. The patches are selected sequentially for every odd epoch and the same number of patches is selected randomly for every even epoch during the training. We use the alternating patch generation strategy <ref type="figure">Fig. 3</ref>: Empirical study to determine the optimal thresholding value for converting the grayscale classification map produced by the network to a binary map. The models shown correspond to the same network ICT-Net at different training snapshots for which the classification accuracy (i.e. IoU in the graph) was calculated after the thresholding at every 0.05 intervals as shown. The optimal threshold value is ? = 0.4.</p><p>to restrict the network from overfitting while still having the opportunity to learn all the features from every tile. At testing the input patch size in increased to 768 ? 768 (the maximum that could fit in the GPU memory) so that we are able to increase the context for large buildings in every patch. During testing, the patches are selected using 50% overlap similar to what is done during training.</p><p>Network output: The output produced by the network is a 1-channel gray-scale image of the same size as the input image where each pixel has a probability score of being a building in the range [0, 1]. We convert the probability map into a binary mask by thresholding. We conducted an empirical study on the validation dataset and have chosen ? = 0.4 as the optimal threshold value for converting the gray-scale image to a binary map as shown in <ref type="figure">Figure 3</ref>. The output patches are then assembled into tiles of size 5000 ? 5000 by weighted average and overlapping areas near the edges are down-weighted. During the testing, the standard test time augmentations are applied to each tile and they are merged back using an average of the probability scores.</p><p>Data augmentations: Based on the validation results we used the pre-trained weights and trained our network with the following data augmentations with a probability of 70% to be applied to every patch: random rotations in the range [0 ? , 360 ? ] using reflection padding, random flip, random selection of a patch in the range of [0.75, 1.25] of the image patch size and re-size it to original patch size of 256. Data augmentations significantly improved the performance of the network in terms of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training and Validation on AIRS dataset:</head><p>Initially we use the pre-trained ICT-Net trained on INRIA dataset to test the generalization ability of the network on AIRS validation dataset of 94 tiles. Then the network is trained on 857 tiles of AIRS training dataset where each tile is of resolution 10000 ? 10000 with their corresponding ground truth. The training is performed for 5 epochs on a single nVidiaGTX 1080Ti. Due to the large size of the dataset it requires approximately 30 hours to complete 1 epoch of training. Every epoch was divided into sub-epochs where each sub-epoch consists of 5 tiles from the training dataset. We use the same batch and patch size of 4 and 256 ? 256 respectively as we used for INRIA dataset. The optimization hyper-parameters remain the same and the learning rate used was 0.0001 for all 5 epochs. The output patches produced by the network are then assembled into tiles of size 10000?10000 by weighted average and overlapping areas near the edges are down-weighted. On the AIRS dataset we use the same set of data augmentation techniques that were used while training ICT-Net on INRIA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation -INRIA Test dataset</head><p>The INRIA dataset uses two main performance measures: Intersection over Union (Jaccard index) and Accuracy. Intersection over Union (IoU) is defined as the number of pixels labeled as buildings in both the prediction and the reference, divided by the number of pixels labeled as buildings in the prediction or the reference. Accuracy is defined as the percentage of correctly classified pixels.</p><p>The measures are calculated by the organizers of the competition and involve the classification of 5 cities for which no images have been used for training and validation, and for which no ground truth is available to the participants. As of writing this manuscript the proposed architecture is ranked as the top performing since February 2019 in terms of both IoU (80.32%) and accuracy (97.14%) on the competition's leaderboard 2 . <ref type="figure" target="#fig_1">Figure 4</ref> shows an example of a result for a small area of an image from the test dataset (top left). The probability image produced by the network is shown as a heat map (bottom right) overlaid on top of the RGB image (bottom left). The binary map resulting after the thresholding is shown in the top right image.</p><p>As previously mentioned, the proposed network is currently ranked as the top performing network with the second best having more than 1.5% difference in terms of the IoU. The competition organizers in <ref type="bibr" target="#b14">[15]</ref> provide details of the next 4 top performing techniques on the INRIA aerial image labeling benchmark dataset. All 4 methods are Convolutional Neural Networks(CNNs), among which 3 of them are based on U-Net architecture. <ref type="table" target="#tab_2">Table 2</ref> shows a quantitative comparison between the proposed network ICT-Net and these other techniques on the test dataset as reported by the competition organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Evaluation -AIRS dataset</head><p>The AIRS dataset uses the following performance measures: Intersection over Union (Jaccard index), F1-Score, Precision and Recall. The evaluation metrics of intersection over union (IoU) and F1-score are used to reflect the overall performance of the baseline methods. On the other hand, precision and recall indicate the correctness and completeness of the roof segmentation results respectively. The evaluation provided by 2. https://project.inria.fr/aerialimagelabeling/leaderboard/ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paper Method</head><p>Overall IoU Overall Accuracy <ref type="bibr" target="#b14">[15]</ref> Raisa 69.57 95.30 <ref type="bibr" target="#b14">[15]</ref> ONERA   in terms of F1-score. The authors in <ref type="bibr" target="#b5">[6]</ref> provide details of the top performing techniques on the AIRS (Aerial Imagery for Roof Segmentation) benchmark dataset. All of the methods are Convolutional Neural Networks based approaches(CNNs). <ref type="table" target="#tab_3">Table 3</ref> shows a quantitative comparison between the proposed ICT-Net and these other techniques on the test dataset as reported by the competition organizers.</p><p>Paper Method IoU F1-score Precision Recall <ref type="bibr" target="#b23">[24]</ref> FPN 0.882 0.937 0.963 0.913 <ref type="bibr" target="#b5">[6]</ref> FPN+MSFF 0.888 0.941 0.958 0.924 <ref type="bibr" target="#b54">[54]</ref> PSP 0.899 0.947 0.961 0.933 <ref type="bibr" target="#b4">[5]</ref> ICT-Net 0.917 0.957 0.955 0.959 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">COMPARATIVE QUANTITATIVE ANALYSIS OF RECONSTRUCTION ACCURACIES</head><p>As previously stated the objectives and contributions of our work are three-fold. In Section 4 we proposed a novel, top ranking architecture for classifying buildings from remote sensor imagery. This binary classification map is typically used as a first step to the reconstruction process since it allows the application of specialized reconstruction algorithms according to the classified type of the pixels. In this section we focus on the equally important aspect of the relation between the classification accuracy and the accuracy of the reconstruction.</p><p>Since it is extremely difficult to acquire building blueprints or CAD models for such large areas, and no 3D/depth information is available as part of the benchmark dataset we posit that the building boundaries extracted from the classification binary map can serve as a proxy to the quality of the reconstruction since the boundaries are typically extruded in order to create the 3D models corresponding to the buildings. More specifically, the procedure for quantitatively evaluating the accuracy of the reconstruction is as follows:</p><p>? Building boundaries B g are extracted from the ground truth provided as part of the training dataset. ? The RGB image corresponding to the ground truth above is used as input to the ICT-Net. The binary classification map C b resulting from feeding forward the RGB image classifies pixels into buildings and non-buildings.</p><formula xml:id="formula_0">? The binary classification map C b is refined C ref ined b</formula><p>using an MRF-based technique where an energy function is minimized via graph-cut optimization for finding an optimal labeling f p for every pixel p such that f p ? l, where l is the new label. The data term of the energy function of a pixel p with label l pi is defined as,</p><formula xml:id="formula_1">E d = 10, if f (p i ) = l pi 0, otherwise<label>(1)</label></formula><p>The smoothness term of the energy function of two neighbouring pixels p 1 and p 2 with labels l p1 and l p2 respectively is defined as,</p><formula xml:id="formula_2">E s = 20, if l p1 == l p2 and f (p 1 ) = f (p 2 ) 0, otherwise<label>(2)</label></formula><p>The values of 10 and 20 in the equations were selected such that smoothness is favored over the observed data.   </p><note type="other">IoU Tyrol-W1 IoU Vienna1 IoU Kitsap1 IoU Chicago1 IoU Average IoU per</note><formula xml:id="formula_3">? = 0.5</formula><p>, is applied to the boundaries. This simplification process is a step applied to the building boundaries prior to extruding the 3D model if 3D/depth information is available <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref>. ? The simplified boundaries B approx b are finally converted back to a binary classification map and quantitatively compared to the ground truth B g . This comparison involves IoU metrics on (i) a per-pixel and (ii) a per-building bases. In the case of the per-building IoU metric, a true positive is considered only if a building has at least 75% of its pixels overlap the pixels of the same building in the ground truth.</p><p>The procedure described above is followed for all input images with no changes to the values and thresholds used; the only varying condition is the classification accuracy. In our experiments, the input images are processed by the proposed ICT-Net at different training snapshots having different classification accuracies. Thus, multiple binary classification maps were produced each with a different classification accuracy. <ref type="table" target="#tab_5">Table 4</ref> shows the quantitative results of the comparison. A total of 5 cities were processed using the aforementioned procedure. <ref type="figure" target="#fig_2">Figures 5a and 5b</ref> show the relation between the reconstruction accuracy with respect to the classification accuracy. We have used increasing classification accuracies based on the same architecture (ICT-Net) at different snapshots during the training. Using the binary classification maps we have followed the procedure which is typical to the reconstruction process. Two metrics have been used to assess the reconstruction accuracy, namely per-pixel IoU and per-buildng IoU (with 75% threshold for being considered a true positive).</p><p>As expected, the graph shows a strong correlation between the classification accuracy and the reconstruction accuracy. However the reconstruction accuracy is consistently lower than the classification accuracy by an average of 4.43% ? 1.65% (confidence level 95%) on the per-pixel IoU and an average of 21.7%? 4.21% (confidence level 95%) on the per-building IoU. This discrepancy can be attributed to the fact that the ground truth images used for training the network may contain errors and are in most cases manually created which results in much higher classification accuracy than the reconstruction accuracy. Moreover, the high discrepancy on the per-building IoU can be attributed to the fact that a threshold must be used i.e. 75%, when calculating the true positives.</p><p>The results of this analysis clearly indicate that high classification accuracy does not translate into high reconstruction accuracy. More importantly though, the results of the analysis clearly indicate that the reconstruction accuracy must be taken into account as part of the loss function along with the classification accuracy during the training of the network. <ref type="figure">Fig. 6</ref>: A fully automated result without any post-processing. Downtown Montreal for which no training images were used and no ground truth is available. Classification by ICT-Net and reconstruction by extruding the extracted boundaries of the buildings using the LiDAR pointcloud corresponding to the same area. The elevation of all non-building points is set to zero. All buildings have been manually verified that they are correctly classified. The accuracy of the classification can also be visually verified since there is no "bleeding" between the buildings and any other urban features e.g., roads, trees, cars, etc. The orthophoto RGB image is courtesy of Defence Research and Development Canada and Thales Canada. <ref type="figure">Figure 6</ref> shows an example of the downtown Montreal. The building classification is generated with the proposed ICT-Net network and refined as explained above. In this example, LiDAR information was available which after resampling at the same resolution as the orthorectified image was used to extrude the 3D buildings from the extracted boundaries. The result shown is fully automated and no post-processing was performed. It should be noted that no images of the city of Montreal have been used in the training. We have manually evaluated the result by counting the number of buildings and confirming that all of them have been classified correctly by the network and therefore reconstructed. The accuracy of the classification is also evident from the fact that there is no texture "bleeding" between the buildings and any other urban features e.g., roads, trees, cars, etc in the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LATENT LEARNING IN DEEP NEURAL NET-WORKS</head><p>In this section we explain the observations which motivated the hypothesis and how this hypothesis can be proven experimentally. In the following subsections, we use the definition and notation from transfer learning as defined in <ref type="bibr" target="#b31">[31]</ref> and subsequent literature, on domains and tasks.</p><p>A domain ? is defined as ? = {? ? , P ? (x)} on feature space ? ? and a marginal probability distribution P ? (x) over that feature space. x = x 1 , x 2 , . . . , x n ? ? ? are samples from feature space ? ? of the domain ?. A task ? consists of a label space ? from which sample labels y = y 1 , y 2 , . . . , y n ? ? are drawn.</p><p>In the special case of a binary classification problem the task ? b is to classify an object as an instance of a class, or otherwise; hence there are only two labels {0, 1} ? y. In this case only the positive label (i.e. 1) carries important information and 0 is defined in terms of 1, and can be simply interpreted as a negative label i.e. ?1 or "not an instance of the class 1". This resembles the one-vs-all learning paradigm used for multi-class classification problems which forms the basis for our hypothesis for sub-classification of the negative label 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Hypothesis</head><p>A neural network trained on domain ? with pairs of training samples (x i , y i ), where x i ? x and y i ? y, learns the conditional probability distribution P ? (y|x). Hence the network can be thought of as approximating an injetive surjective mapping function f : x i ?? y i from feature space ? to label space ?. We hypothesize that a neural network trained to perform a task ? A with labels y ? A , also learns about additional complementary</p><formula xml:id="formula_4">tasks ? B , ? C , . . . where {? B ? ? C ? . . .} ? ? A = ?, with labels y ? B , y ? C , . . . ? ? y ? A .</formula><p>Intuitively, we posit that in order for a network to learn task ? with labels ?, the network has to learn auxiliary tasks with associated complementary labels ??. Thus, one can exploit the latent learning occurring for further sub-classification of the negative label. The above hypothesis is based on the following observations:</p><p>1) A trained neural network approximates a mapping function from the feature space of a domain to the label space of a task from which the sample labels are drawn. 2) If a pixel is not mapped to a positive label in y then it is assigned the negative label 0 which can be interpreted as ?y. The negative label comprises of a set of subclasses e.g. in the context of urban feature classification the negative label non-building can correspond to roads, cars, trees, low vegetation, clutter, etc. <ref type="figure">Fig. 8</ref>: Sub-classification of the negative label. The pre-trained ICT-Net is used to infer the activation values at each feature map of the penultimate layer. A classification image C k is generated for each feature map k using maximum likelihood classification (MLC). Finally, the classification aggregator combines all the K classification images to produce the final classification labels.</p><p>Given the fact that the deeper convolutional layers contain information about complex objects, we proceed with the experimental proof of the hypothesis by analyzing the penultimate layer before the final logit (softmax) layer which outputs the binary classification of building/non-building.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Proof</head><p>In order to prove the above hypothesis, ground truth for the subclasses of the negative label is required. In our experiments we use four orthophoto images from the ISPRS benchmark dataset <ref type="bibr" target="#b37">[37]</ref>. For each image I i , 1 ? i ? 4, a labeled image I i label is also provided as ground-truth showing the manually assigned per-pixel classification into six classes L: (a) buildings (blue), (b) roads (white), (c) trees (green), (d) red (clutter), (e) low vegetation/natural ground (cyan), (f) cars (yellow). The 'clutter' class contains areas for which a class could not be assigned e.g. water, vertical walls, etc. The 'low vegetation/natural ground' class contains areas on the ground covered by vegetation other than trees such as low bushes, grass, etc.</p><p>Using the four pairs of images, we first estimate a probabilistic model for each sub-class as shown in <ref type="figure" target="#fig_4">Figure 7</ref>. Next, using these models as probability priors, we make subclassification predictions at each point by estimating the maximum likelihoods as shown in <ref type="figure">Figure 8</ref>. The process is explained in more detail in the sections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Probabilistic Model Estimation</head><p>We use the pre-trained ICT-Net for inference on the images</p><formula xml:id="formula_5">I i , 1 ? i ? 4.</formula><p>After each inference, we gather all activation values corresponding to each class C at each feature map of the penultimate layer. To achieve this we use the indices of all the pixels classified as l ? L in the labeled image I i label to gather all the activation values for that class. The activation values for each class l ? L are aggregated across the four images I i .</p><p>An analysis of the histograms of the activation values at each feature map showed that the values resemble a uniform distribution. Thus, a Gaussian function N ?,? C is used to model the probability distribution function (PDF) at penultimate layer ? n?1 and feature map ?. As an example, <ref type="figure">Figure 9</ref> shows the histograms of a randomly selected feature map of the penultimate layer ? n?1 , ? = 131, and their corresponding PDFs. <ref type="figure">Fig. 9</ref>: Example histograms of activation values for each subclass of the negative label. Below each histogram is the PDF of each label modeled with a Gaussian function. Shown are the results for the penultimate layer ? n?1 before the final logit (softmax) layer ? n which outputs the binary classification of building/non-building.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Sub-classification of the Negative label</head><p>In this step, Maximum-Likelihood Classification (MLC) is used for pixel-wise sub-classification. Thus, every pixel is assigned to the label to which it has the highest probability of being part of.</p><p>The sub-classification is performed at each feature map of the penultimate layer ? n?1 . In the case of ICT-Net this results in a total of K = 256 classification images C k , 0 ? k ? K. Each classification image encodes information at the pixellevel about (i) the label that maximizes its probability, and (ii) the probability value. For example, i, P(C (x,y) k , N (? i , ? i )) k indicates that the activation value of pixel (x, y) at feature map C k gives the highest probability P(C (x,y) k , N (? i , ? i )) for label i with PDF N (? i , ? i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Classification Aggregator</head><p>Next, the K label, probability pairs are weighted using the squeeze-excitation weights corresponding to each feature map. The weighted pairs for each pixel are converted into one-hot 6-dimensional vectors (e.g. corresponding to the 6 labels) containing only the value of the highest probability P(C (x,y) k , N (? i , ? i )) at the index of label i. Thus, continuing with the previous example, vec is the one-hot 6-dimensional vector corresponding to pixel (x, y) in C k where the only nonzero value is at vec[i] = P(I (x,y) , N (? i , ? i )). Finally, for each pixel the K one-hot vectors are aggregated and the label for which the total probability is maximal is assigned to the pixel. <ref type="figure" target="#fig_5">Figure 10</ref> summarizes the process of the classification aggregator.</p><p>A final classification result using the activation values of each feature map of the penultimate layer is shown in <ref type="figure" target="#fig_6">Figure  11a</ref>. The ground truth is shown in <ref type="figure" target="#fig_6">Figure 11b</ref>. <ref type="figure" target="#fig_6">Figure 11d</ref> shows the result after refinement on 11a using the technique described in Section 5 after updating it to use 6 labels. Since the focus is on sub-classification of the negative label, we overlaid the binary classification result of ICT-Net on buidings on top of the sub-classification output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experimental Results</head><p>We have extensively tested the proposed technique on images from the ISPRS benchmark dataset for which multilabel ground truth was available. The results for a number of randomly chosen images are shown in <ref type="table" target="#tab_7">Table 5</ref>. First, the building predictions were generated using the pre-trained ICT-Net (third column). As previously mentioned, the training and validation of the ICT-Net did not include any images from the ISPRS dataset. Next, the sub-classification of the negative label (i.e. non-building) was generated using the proposed technique (fourth column). Since the focus is on sub-classification of the negative label the binary building predictions are overlaid with the blue color on the sub-classifications (fifth column). <ref type="table" target="#tab_7">Table 5</ref> fifth column shows the final composited result which is evaluated using the multi-label ground truth (second column) and the F1-score (sixth column).</p><p>As it can be seen from the results, using the proposed technique one is able to externalize information about the subclasses of the negative label for which the network did not train on. In particular, for the road (R) and low vegetation (LV) classes the proposed technique achieves much higher performance than one would get by pure chance (i.e. <ref type="bibr" target="#b15">16</ref>.7%) which is quantitatively confirmed by the average of the F1 scores on a set of 10 randomly chosen images from the ISPRS benchmark dataset, i.e. R: 54.29% (SD=17.03), C: 10.15% (SD=2.54), T: 24.11% (SD=5.25), LV: 42.74% (SD=6.62), Cl: 18.30% (SD=16.08). Thus, a network which was trained on a binary classification task using only binary examples of building/nonbuilding, is indeed performing clustering of the sub-classes of the negative label based solely on their similarities and without knowledge on what each sub-class represents. It is worth noting that the F1 scores for the sub-classification of the road (R) and low vegetation (LV) classes are the highest when compared to the scores for the rest of the classes (e.g. cars, trees, clutter). This can be primarily attributed to the fact that roads and low vegetation are integral parts in the building classification process since they are mutually exclusive: a building can never be part of the road nor low vegetation, and vice-versa. This means</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Multi-label Ground-truth Building prediction  that misclassifications in these two classes (R, LV) negatively affects the performance on the primary task (building/nonbuilding). As the results clearly show, the network forces the clusters of the activation values corresponding to these two classes to be distinct and separable from the building class. This is further strengthened by the fact that the two classes (R, LV) combined, occupy the majority area within any satellite image compared to the other classes. This observation is also supported by the fact that cars (C), which are very small in size (i.e. number of pixels) and number (i.e. occurrences in an image), have the lowest reported F1 scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Ablation Study</head><p>As part of the ablation study we have performed the following experiments. Instead of using MLC, we explored the use of MAP estimation for MRF as a classifier. Procedure: The estimated probabilistic models of each class are used as priors in a maximum-a-posterior (MAP) estimation. MAP inference is performed using Graph-cut energy minimization for optimizing the cost function given by,</p><formula xml:id="formula_6">E(f ) = E unary (f ) + w ? E pairwise (f )<label>(3)</label></formula><p>where f : I p ? L is an optimal labeling that assigns a label l ? L to each pixel p ? I, and w is the weight indicating the importance of one term with respect to the other. The unary term E unary provides a per-pixel estimate of how appropriate a label l ? L is, for a pixel p ? I in the observed data. The probabilistic models of each class at each layer/feature map are used as priors for calculating this term and is given by,</p><formula xml:id="formula_7">E unary (f ) = p?I 1 1 + N l (?,?) (? p (?,?) |? (?,?) , ? (?,?) )<label>(4)</label></formula><p>where ? p (?,?) is the activation value of pixel p at layer ? and feature map ?, N l (?,?) (? (?,?) , ? (?,?) ) is the probability prior for label l.</p><p>The pairwise term E pairwise provides an estimate of the similarity of the labels of two neighbouring pixels p, q ? I with labels l p , l q ? L respectively. Let f (p) and f (q) be the new labeling under f for pixels p, q ? I, respectively. E pairwise (f ) at layer ? and feature map ? is then defined as,</p><formula xml:id="formula_8">E pairwise (f ) = {p,q}?N 40, if l p = l q ?(f p , f q ), otherwise<label>(5)</label></formula><p>where N is the local 4-neighbourhood, and ?(f p , f q ) is defined as the difference of the probability priors of f p , f q given by, <ref type="table" target="#tab_9">Table 6</ref> shows quantivative and qualitative comparisons of the results between MLC and MAP-MRF for different weights w from equation 3. With respect to the sub-classes of the negative labels (e.g. roads, cars, trees, low vegetation, and clutter) the MLC outperforms MAP-MRF F1 scores in all subclasses except cars. At the same time, the computational time increases by an average of 00h:30m:38s (experiments were ran on a quad-core Intel Core i7 64-bit machine with 16GB of RAM).</p><formula xml:id="formula_9">?(f p , f q ) = N f (p) (?,?) (? p (?,?) |? (?,?) , ? (?,?) )? N f (q) (?,?) (? q (?,?) |? (?,?) , ? (?,?) )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Discussion</head><p>Internal validity. The fact that the activation values of each label are separable and that using a single Gaussian function can accurately model the PDF of every label at each layer ? and feature map ? is a clear indication that the network is performing multi-label clustering. In the case of binary classification such as ours, we showed that this clustering involves labels corresponding to objects of similar characteristics (e.g. shape, color, etc) which even at the penultimate layer are distinct and separable. At the last layer this multi-label information is squashed into a single negative label i.e. non-building. This can be confirmed by comparing the sub-classification results using the PDFs at the penultimate layer shown in <ref type="figure" target="#fig_6">Figure 11a</ref>, and the final logit layer shown in <ref type="figure" target="#fig_6">Figure 11c</ref>. As it can be seen, the pre-activated convolutions of the final layer have transformed the activation values of the sub-classes of the negative label such that separability of their clusters cannot be easily achieved; leading to the majority of the non-building pixels to be classified with the same label e.g. clutter (red). Outliers. The pre-activated convolutions of ICT-Net include as part of it the activation function ReLU which maps all negative activation values to zero. This can lead to a large number of zero values at each layer/feature map which should not be taken into account when estimating the probabilistic model. Thus, outlier removal is applied at each layer/feature map to remove these zero-values from all gathered activation values per-label prior to fitting a Gaussian distribution. Ground Sampling Density (GSD). ICT-Net is trained on orthophotos with a GSD of 30cm. Our experiments in <ref type="table" target="#tab_11">Table  7</ref> show that some variations in GSD can be tolerated by ICT-Net without greatly impacting the performance. However, in the case of the ISPRS benchmark dataset the GSD is 5cm which is significantly smaller. Therefore, for the experiments on latent learning we first convert the four orthophotos and ground truth images to GSD 30cm by down-sampling. SE weights. -While analyzing the activations at different layers of the network we extracted the activations of dense blocks and the corresponding excitation values produced by the Squeeze and Excitation block. It is interesting to observe that the excitation values produced by the network for all but last dense block's activations were binary in nature, which leads to a lot of feature maps having zero values and not contributing to the subsequent inference. It is for this reason that the excitation values are used as importance weights to modulate the &lt; label, probability &gt; pairs by the classification aggregator as shown in <ref type="figure" target="#fig_5">Figure 10</ref>. Zero-shot learning. The proposed technique is related tobut different from -zero-shot learning. In contrast to zero-shot learning where the objective is to classify unseen classes based on embedding-vectors, our objective is to classify seen classes which are unlabeled but appear in the training examples and for which no further information is available.  Few-shot learning. Our technique has more similarities with the objective of few-shot learning. In few-shot learning a small number of images is available and labeled. Meta-learning is used for adapting a pre-trained network to a new task. The main difference with our proposed technique is that in our case there is no additional training occurring. A minimal labeled dataset is used for estimating the probabilistic models. The pre-trained network remains unchanged and is used only for inference without additional training for adaptation to the new tasks. Furthermore, the "seen" unlabeled classes coappear in the images with the labeled class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We have addressed three aspects of semantic segmentation from remote sensor data. We have presented a novel network which combines the strengths of state of the art techniques like Dense blocks in fully convolutional networks and feature recalibration using SE blocks. We have identified the requirements for the particular task and based our decisions on the actual characteristics and observations. We have shown that the proposed architecture outperforms other state of the art including ensemble techniques.</p><p>Furthermore, we investigated the relation between the classification accuracy and the reconstruction accuracy. Due to the extreme difficulty of acquiring blueprints for such large areas and the unavailability of 3D information we have used the building boundaries as a proxy to the reconstruction accuracy. The proposed ICT-Net at different training snapshots was used to generate binary maps of different classification accuracies which were then used for extracting the boundaries.  We presented a comparative quantitative analysis which shows a strong correlation between the two but also a consistent and considerable decrease of the reconstruction accuracy when compared to the classification accuracy. Finally, we introduce the concept of latent learning in the context of deep neural network. We experimentally showed that a pre-trained network on binary classification task unintentionally learns about auxiliary tasks and presented a technique for externalizing this knowledge by performing sub-classification of the negative label. Using the proposed technique we demonstrated that sub-classification is possible with acceptable accuracies. This technique is ideal for cases where labels are not easily obtainable but labels for a complementary task are available.</p><p>With respect to the future work, we plan on extending the work (i) by designing a loss function that incorporates the reconstruction accuracy in addition to the classification accuracy during training, and (ii) by applying the concept of latent learning on deep neural networks of different architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Proposed feature recalibrated Dense block with 4 convolutional layers and a growth rate ? = 12 used by the ICT-Net. c stands for concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Building Classification of Bellingham city, tile 17. Result for an image from the test dataset. Top left: A closeup of a small area of an input image. Bottom left: The probability image overlaid on top of the RGB image. Bottom right: The probability image shown as a heat map. Top right: The binary map resulting after the thresholding of the probability image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>(a) Reconstruction vs Classification accuracy. The classification accuracy ranges from [0.6451, 0.8441] as calculated on the validation test. The different classification accuracies correspond to the same architecture (ICT-Net) but at different snapshots during training. The binary classification map produced at each snapshot: (i) is refined using conditional random fields (CRF), (ii) building boundaries are extracted, (iii) building boundaries are refined using the Douglas-Pecker algorithm, (iv) converted back to a binary classification map, and (v) compared with the ground truth. Two metrics are used: per-pixel IoU and per-building IoU (with a threshold of 75% overlap for true positive). There is an average decrease of 4.43% ? 1.65% (confidence level 95%) in per-pixel IoU (a) of the reconstruction accuracy; and an average decrease of 21.7%? 4.21% (confidence level 95%) in per-building IoU (b) of the reconstruction accuracy. The reported averages are calculated across the accuracy levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>Building boundaries B b are extracted from the refined classification map C ref ined b . A simplification process i.e. Douglas-Pecker approximation with a tolerance of Classification Accuracy Austin1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Probabilistic model estimation. We infer the activation values for each feature map at the penultimate layer for four images of size 640 ? 640. Multi-label ground-truth corresponding to the four input images is used for aggregate the activation values based on their label. Finally, for each label and for each feature map of the penultimate layer, a probabilistic model N (?, ? is estimated. Analysis of the histograms of the activation values shows that it resembles a Gaussian distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 :</head><label>10</label><figDesc>Classification aggregator. There is one classification image per feature map. Each classification image contains pairs of &lt; label, probability &gt; for each pixel. The pairs are converted to one-hot vectors (e.g. vec[label] = probability) and aggregated together. The label with the highest probability is assigned to each pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 :</head><label>11</label><figDesc>(a) Sub-classification using MLC at penultimate layer (i.e. one before the final logit layer). ICT-Net was not trained nor fine-tuned using any of the images from the ISPRS dataset; yet latent learning is occurring and can be exploited for the sub-classification of the negative label i.e. non-building. (b) Ground-truth. (c) Sub-classification using MLC at the final layer before output. Sub-classes of the negative label are squashed into a single negative label i.e. non-building and separability of their clusters cannot be easily achieved. (d) Refinement of (a) using the technique described in Section 5 after updating it to use 6 labels. Since the focus is on subclassification of the negative label the building predictions from ICT-Net are overlaid on top.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>The network is trained on 155 tiles each with resolution 5000? 5000 from the available training data with their corresponding ground truth. The training is performed for 100 epochs on a single nVidiaGTX 1080Ti. We used Tensorflow API for the development and training/testing of the network. Due to the large size of the dataset it requires approximately 6 hours to complete 1 epoch of training. Every epoch was divided into 31 sub-epochs each consisting of 5 tiles (1 from each city).</figDesc><table /><note>Performance evaluation of SOTA architectures (U- Net [36] and Tiramisu-103 [20]) on the validation dataset 4.3 Training and Validation on INRIA dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Performance evaluation of the best performing networks on the test dataset. First 4 entries are defined as State of the art by INRIA<ref type="bibr" target="#b14">[15]</ref>. Next 4 entries are other top performances from the leaderboard<ref type="bibr" target="#b17">[18]</ref>. ICT-Net outperforms all others with more than 1.5% difference in terms of the IoU. the organizers of the competition involves the segmentation of roofs for 95 tiles of imagery for which no images have been used for training and validation, and no ground truth is available to the participants. As of November 2019 the proposed architecture is ranked as the top performing in terms of</figDesc><table /><note>both IoU (91.70%) and F1-score (95.70%) on the competition's leaderboard 3 . The proposed network achieves a 1.8% improvement over the second best network in terms of the IoU and 1% difference3. https://www.airs-dataset.com/leaderboard/</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc></figDesc><table /><note>Performance evaluation of the top performing net- works on the AIRS test dataset. ICT-Net outperforms all others with 1.8% difference in terms of the IoU and 1% in terms of F1-Score.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 :</head><label>4</label><figDesc>The ICT-Net at different training snapshots having different classification accuracy vs the reconstruction accuracy measured using two metrics: per-pixel IoU, and per-building IoU (with a threshold of 75% overlap for true positives)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 :</head><label>5</label><figDesc>Results on the randomly selected images (first column) from the ISPRS benchmark dataset (Pottsdam). The second column shows the multi-label ground-truth. The third column shows the building predictions from the pretrained ICT-Net. Note that the ISPRS dataset was not used during its training/validation. The fourth column is the sub-classification of the negative label (i.e. non-buildings) using the proposed technique. The fifth column shows the result of overlaying the building predictions</figDesc><table /><note>(third column) on the sub-classification (fourth column). The sixth column shows the F1 scores for Building, Road, Car, Tree, Low Vegetation, and Clutter.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>R (%) C (%) T (%) LV (%) Cl (%) Compute time</figDesc><table><row><cell>Sub-classification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>of negative label</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with building</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ablations B (%) Model: prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sub-classification:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLC</cell><cell>85.93</cell><cell>62.49</cell><cell>7.32</cell><cell>34.49</cell><cell>38.85</cell><cell>55.47</cell><cell>01h:03m:02s</cell></row><row><cell>Model:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sub-classification:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAP-MRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(w = 0.0005)</cell><cell>90.30</cell><cell>59.96</cell><cell>5.28</cell><cell>35.93</cell><cell>31.68</cell><cell>54.39</cell><cell>01h:32m:47s</cell></row><row><cell>Model:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sub-classification:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAP-MRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(w = 0.005)</cell><cell>89.97</cell><cell>56.47</cell><cell>18.42</cell><cell>35.35</cell><cell>7.79</cell><cell>48.17</cell><cell>01h:28m:19s</cell></row><row><cell>Model:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sub-classification:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAP-MRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(w = 0.05)</cell><cell>89.44</cell><cell>60.42</cell><cell>15.17</cell><cell>20.12</cell><cell>9.02</cell><cell>46.87</cell><cell>01h:24m:31s</cell></row><row><cell>Model:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sub-classification:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAP-MRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(w = 0.1)</cell><cell>90.62</cell><cell>37.74</cell><cell>76.98</cell><cell>0.39</cell><cell>0.0</cell><cell>37.99</cell><cell>01h:35m:55s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 :</head><label>6</label><figDesc>Ablations. F1 scores for Building, Road, Car, Tree, Low Vegetation, and Clutter. The bottom four rows show the results of using an alternative method for sub-classification i.e. MAP-MRF, each with a different weight w in equation 3.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 7 :</head><label>7</label><figDesc>Performance evaluation of ICT-Net (pretrained on 30cm INRIA dataset) on multiple ground sampling density of ISPRS potsdam dataset created by down-sampling the orginal orthophoto RGB imagery. Original ground sampling density of ISPRS potsdam dataset was 5cm. The evaluation is performed after converting the multi-labels ground truth into building/nonbuilding per-pixel. IoU refers to IoU of the building class, and Accuracy is the per-pixel accuracy.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Charalambos is a senior member of the Institute of Electrical and Electronics Engineers (IEEE) Computer Society and a member of the Association for Computing Machinery(ACM); Marie Curie Alumni Association (MCAA); British Machine Vision Association (BMVA). He has been serving as a regular reviewer in numerous premier conferences and journals since 2003.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<idno>abs/1712.02616</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On building classification from remote sensor imagery using deep neural networks and the relation between classification and reconstruction accuracy using border localization as proxy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poullis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 16th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Aerial imagery for roof segmentation: A large-scale dataset towards automatic mapping of buildings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<idno>abs/1807.09532</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep autoencoders with aggregated residual transformations for urban reconstruction from remote sensing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poullis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A review on deep learning techniques applied to semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Villena-Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Rodr?guez</surname></persName>
		</author>
		<idno>abs/1704.06857</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building footprint extraction and 3-d reconstruction from lidar data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Haithcoat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Hipple</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote Sensing and Data Fusion over Urban Areas, IEEE/ISPRS Joint Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="74" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building detection from satellite imagery using ensemble of size-specific detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hikosaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale semantic classification: outcome of the first year of inria aerial image labeling benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khalel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bradbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Geoscience and Remote Sensing Symposium-IGARSS 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1608.06993</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ternausnetv2: Fully convolutional network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Seferbekov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shvets</surname></persName>
		</author>
		<idno>abs/1806.00844</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<ptr target="https://bit.ly/2GC88nr" />
		<title level="m">Inria Aerial Image Labeling Benchmark. Inria Aerial Image Labeling Benchmark LeaderBoard</title>
		<imprint>
			<date type="published" when="2019-02-18" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>last accessed</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1611.09326</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automatic pixelwise object labeling for aerial imagery using stacked u-nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khalel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
		<idno>abs/1803.04953</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>NIPS&apos;12</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?rner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06334</idno>
		<title level="m">Auxiliary tasks in multi-task learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Self-supervised generalisation with meta auxiliary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08933</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation. CoRR, abs/1411</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4038</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Maggiori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Geoscience and Remote Sensing Symposium (IGARSS). IEEE</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Machine learning for aerial image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto (Canada</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Explaining nonlinear classification decisions with deep taylor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A framework for automatic modeling from pointcloud data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poullis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large-scale urban reconstruction with tensor clustering and global boundary refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poullis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic reconstruction of cities from remote sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poullis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003-06" />
			<biblScope unit="page" from="2775" to="2782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic creation of massive virtual cities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poullis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Virtual Reality</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="199" to="202" />
			<date type="published" when="2009" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>abs/1505.04597</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The isprs benchmark on urban object classification and 3d building reconstruction. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Benitez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Breitkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Concurrent spatial and channel squeeze &amp; excitation in fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
		<idno>abs/1803.02579</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automated 2-d building footprint extraction from high-resolution satellite multispectral imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Shackelford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1996" to="1999" />
		</imprint>
	</monogr>
	<note>IGARSS&apos;04. Proceedings</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Introduction and removal of reward, and maze performance in rats. University of California publications in psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Tolman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Honzik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A bayesian approach to building footprint extraction from aerial lidar data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Lodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Helmbold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DPVT, Third International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1611.05431</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Wide residual networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1311.2901</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fine-grained visual categorization using meta-learning optimization with sample selection of auxiliary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="233" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
