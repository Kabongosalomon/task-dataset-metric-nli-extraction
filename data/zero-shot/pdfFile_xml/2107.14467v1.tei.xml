<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DPT: Deformable Patch-based Transformer for Visual Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event</publisher>
				<availability status="unknown"><p>Copyright Virtual Event</p>
				</availability>
				<date>October 20-24, 2021. October 20-24, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Chen</surname></persName>
							<email>zhiyang.chen@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousong</forename><surname>Zhu</surname></persName>
							<email>yousong.zhu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Zhao</surname></persName>
							<email>chaoyang.chao@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
							<email>huguosheng100@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">AnyVision</orgName>
								<address>
									<settlement>Belfast</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zeng</surname></persName>
							<email>weizeng@pku.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
							<email>jqwang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
							<email>tangm@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousong</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">AnyVision</orgName>
								<address>
									<settlement>Belfast</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zeng</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DPT: Deformable Patch-based Transformer for Visual Recognition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th ACM Inter-national Conference on Multimedia (MM &apos;21)</title>
						<meeting>the 29th ACM Inter-national Conference on Multimedia (MM &apos;21) <address><addrLine>China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Virtual Event</publisher>
							<date type="published">October 20-24, 2021. October 20-24, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475467</idno>
					<note>Event, China. ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Computer vision</term>
					<term>Image rep- resentations</term>
					<term>Object recognition</term>
					<term>Object detection KEYWORDS vision transformer</term>
					<term>deformable patch</term>
					<term>image classification</term>
					<term>object detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer has achieved great success in computer vision, while how to split patches in an image remains a problem. Existing methods usually use a fixed-size patch embedding which might destroy the semantics of objects. To address this problem, we propose a new Deformable Patch (DePatch) module which learns to adaptively split the images into patches with different positions and scales in a data-driven way rather than using predefined fixed patches. In this way, our method can well preserve the semantics in patches. The DePatch module can work as a plug-and-play module, which can easily be incorporated into different transformers to achieve an end-to-end training. We term this DePatch-embedded transformer as Deformable Patch-based Transformer (DPT) and conduct extensive evaluations of DPT on image classification and object detection. Results show DPT can achieve 81.9% top-1 accuracy on ImageNet classification, and 43.7% box mAP with RetinaNet, 44.3% with Mask R-CNN on MSCOCO object detection. Code has been made available at: https://github.com/CASIA-IVA-Lab/DPT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recently, transformer <ref type="bibr" target="#b24">[25]</ref> has made significant progress in natual language process (NLP) and speech recognition. It has gradually become the prevailing method for sequence modeling tasks. Inspired by this, some studies have successfully applied transformer in computer vision, and achieved promising performance on image classification <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>, object detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref>, and semantic segmentation <ref type="bibr" target="#b32">[33]</ref>. Similar to NLP, transformer usually divides the input image into a sequence of fixed-size patches (e.g. <ref type="bibr">16 ? 16)</ref>  <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>, and models the context relationships between different patches through multi-head self-attention. Compared to convolution neural networks (CNNs), transformer can effectively capture long-range dependency inside the sequence, and the features extracted contain more semantic information.</p><p>Although transformer has tasted sweetness in vision tasks, there are still many aspects to be improved. DeiT <ref type="bibr" target="#b23">[24]</ref> exploits data augmentation and knowledge distillation to learn visual transformer in a data-efficient manner. T2T-ViT <ref type="bibr" target="#b29">[30]</ref> decomposes the patch embedding module by recursively aggregating neighboring tokens for better local representation. TNT <ref type="bibr" target="#b9">[10]</ref> maintains a fine-grained branch to better model local details inside a patch. PVT <ref type="bibr" target="#b25">[26]</ref> transforms the architecture into four stages, and generates feature pyramid for dense prediction. These works use a fixed-size patch embedding under an implicit assumption that the fixed image split design is suitable for all images. However, such 'hard' patch split may bring two problems: (1) Collapse of local structures in an image. As shown in <ref type="figure" target="#fig_2">Figure 1(a)</ref>, a regular patch <ref type="bibr">(16 ? 16)</ref> is always hard to capture the complete object-related local structure, since objects are with various scales in different images. <ref type="bibr" target="#b1">(2)</ref> Semantic inconsistency across images. The same object in different images might have different geometric variations (scale, rotation, etc). The fixed way of splitting images will potentially capture inconsistent information for one object in different images. As discussed, these fixed patches can potentially destroy the semantic information, leading to degraded performance.</p><p>To address the aforementioned problems, in this paper, we propose a new module, called DePatch, which divides images in a deformable way. In this way, we can well preserve the semantics in one patch, reducing the semantics destruction caused by image splitting. To achieve this, we learn the offset and scale of each patch in feature map space. The offset and scale are learned based on the input feature map and are generated for each patch as illustrated in <ref type="figure" target="#fig_2">Figure 1</ref>(b). The proposed module is lightweight and introduces a very small number of parameters and computations. More importantly, it can work as a plug-and-play module which can easily be incorporated into other transformer architectures. A transformer with DePatch module is named Deformable Patch-based Transformer, DPT. In this work, we integrate DePatch module to Pyramid Vision Transformer (PVT) <ref type="bibr" target="#b25">[26]</ref> to verify its efficacy since PVT achieves state-of-the-art performance in pixel-level prediction tasks like object detection and semantic segmentation. With deformable patch adjustment, DPT generates complete, robust and discriminative features for each patch based on local contextual structures. Therefore it can not only achieve high performance on classification tasks, but also outperform other methods on tasks which highly depend on local features, e.g. object detection. Our method achieves 2.3% improvements on ImageNet classification, and improves box mAP by 2.8%/3.5% with RetinaNet and Mask R-CNN framework for MSCOCO object detection compared to its conterpart, PVT-Tiny.</p><p>Our main contributions can be summarized as: </p><formula xml:id="formula_0">?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Vision Transformer</head><p>Transformer <ref type="bibr" target="#b24">[25]</ref> has been the mainstream approach for NLP tasks. It uses self-attention to capture long-range dependence within the whole sequence, and achieves state-of-the-art performance. This idea is applied into computer vision firstly by Non-Local block and its variants <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>. Recently, there appear a large amount of works building pure vision transformers without convolution layers. ViT <ref type="bibr" target="#b6">[7]</ref> is as far as we know the first work in this trend. It achieves comparable results with traditional CNN architectures with the help of large training data. DeiT <ref type="bibr" target="#b23">[24]</ref> uses complex training schedules and knowledge distillation to improve performance trained on ImageNet only. Current works focus on combining the advantages of transformer and CNN in order to capture better local information. This object is obtained by combining convolution blocks and self-attention layers together <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref>, maintaining high-resolution feature maps <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>, adding parameters biased for locality <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref> or redesigning the brute-force patch-embedding module <ref type="bibr" target="#b29">[30]</ref>. Though large improvements achieved, most architectures split the input image with a fixed pattern, without awareness of the input content and geometric variations. Our DPT can modify the position and scale of each patch in an adaptive way. To the best of our knowledge, our model is the first vision transformer that do patch splitting in a data-specific way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deformable-Related Work</head><p>Modifying fixed pattern into an adaptive way is a common idea to improve performance. There have been a great number of works help models focus on important features and adopt geometric variations in computer vision. All related works fall into two categories, attention-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref> and offset-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. We mainly review offset-based ones.</p><p>Offset-based methods predict offsets to explicitly direct important locations. This idea bears some similarity with region proposal network in object detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>. Unlike our task, region proposal network uses supervision of bounding box annotations. In image classification task, there are also some works explicitly learning positions of the important regions for better performance <ref type="bibr" target="#b7">[8]</ref> or faster inference <ref type="bibr" target="#b27">[28]</ref>. The learning process are merely guided by cross-entropy loss and final accuracy. Deformable convolution <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref> is the work most similar to ours. It predicts an offset for each pixel of the convolution kernel, while the predicted regions in our method are more regular ones. Irregular patches are not compatible in vision transformers. Deformable-DETR <ref type="bibr" target="#b34">[35]</ref> applies deformable operations in the self-attention layers and cross-attention layers of DETR. However, its main purpose is to accelerate training, and Deformable-DETR still relies on feature maps extracted from CNNs. As far as we know, our work is the first to apply deformable operations in a pure vision transformer architecture. We focus on adjusting the position and scale of each patch, therefore extracting features better maintaining local structures. Our module can work as a plug-and-play module, and is compatible for various vision transformer architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD 3.1 Preliminaries: Vision Transformer</head><p>Vision transformer is composed of three parts, a patch embedding module, multi-head self-attention blocks and feed-forward multilayer perceptrons (MLP). The network starts with the patch embedding module which transforms the input image into a sequence of tokens, and then alternately stacks multi-head self-attention blocks and MLPs to obtain the final representation. We mainly elaborate on the patch embedding module in this section, and then have a quick review over the multi-head self-attention.</p><p>Patch embedding module divides images into patches with fixed size and positions, and embeds each of them with a linear layer. We denote the input image or feature map as ? R ? ? . For simplicity, we assume = . Previous works split into a sequence of patches with size ?</p><formula xml:id="formula_1">( = ? / ? ?). The sequence is denoted as { ( ) } 1? ? .</formula><p>To better interpret the patch splitting process, we reformulate the patch embedding module. Each patch ( ) can be seen as a representation for a rectangle region of the input image. We denote its center coordinate as ( , ( , ) ). All coordinates ? ( , ) are integers themselves. The features at these pixels are denoted as {?( , ) } 1? ? . These features are then flattened and processed by a linear layer to get representation for the new patch, as shown in Eq. (1).</p><formula xml:id="formula_2">( ) = ? ? {?( ,1) , ...,?( , ? ) } + ?<label>(1)</label></formula><p>Multi-head self-attention module aggregates relative information over the whole input sequence, giving each token a global view. This module learns three groups of representative features for each head, query ( ? ? R ? ), key ( ? ? R ? ) and value ( ? ? R ? ). ? and ? are multiplied to obtain the attention map ? , which represents similarity between different patches. The attention map is used as the weights to sum up ? . Independent results are calculated for different heads to get more variant features. Results from all heads are then concatenated and transformed to become new representations ? .</p><formula xml:id="formula_3">? = ( ? ? / ? ) (2) ? = { 1 1 , ..., } + (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DePatch Module</head><p>The patch embedding process described in 3.1 is fixed and inflexible. Positions ( ( ) , ( ) ) and size are fixed, therefore the rectangle region is unchangable for each patch. The feature for each patch is directly represented with its inside pixels. In order to better locate important structures and handle geometric deformation, we loosen these constraints to develop our deformable patch embedding module, DePatch. Firstly, we turn the location and the scale of each patch into predicted parameters based on input contents. As for the location, we predict an offset ( , ) allowing it to shift around the original center. As for the scale, we simply replace the fixed patch size with predictable ? and . In this way, we can determine a new rectangle region, and denote its left-top corner as ( 1 , 1 ) and rightbottom corner as ( 2 , 2 ). For clarity, we omit the superscript ( ). We emphasize that ( , , , ? ) can be different even for patches in a single image.</p><formula xml:id="formula_4">1 = + ? 2 , 1 = + ? ? 2 2 = + + 2 , 2 = + + ? 2<label>(4)</label></formula><p>As shown in <ref type="figure" target="#fig_4">Figure 2</ref>, we add a new branch to predict these parameters. Based on the input feature map, we predict ( , , , ? ) densely for all patches first, and then embed them with predicted regions. Offsets and scales are predicted with Eq. (5) and <ref type="bibr" target="#b5">(6)</ref>. (?) can be any feature extractor, and here we use a single linear layer. After that, and are followed to predict offsets and scales. At the beginning of training, these weights are initialized to zero.</p><p>is initialized to make sure each patch focuses on exactly the same rectangle region as the original model.</p><formula xml:id="formula_5">, = ?( ? ( ))<label>(5)</label></formula><formula xml:id="formula_6">, ? = ( ?( ? ( ) + ))<label>(6)</label></formula><p>After the rectangle region is determined, we extract the feature for each patch. The main problem is that regions are with different sizes, and the predicted coordinates are usually fractional. We solve this problem in a sampling-and-interpolation manner. Given the rectangle coordinates ( 1 , 1 ) and ( 2 , 2 ), we sample ? points uniformly inside the region, is a super-parameter for our method, which will be ablated in 4.3. Each sampling location is denoted as  sampled points {?( ) } 1? ? ? are then flattened and processed in a linear layer to generate patch embedding, as in Eq. <ref type="bibr" target="#b6">(7)</ref>.</p><formula xml:id="formula_7">? ( ) = ( ( ) ,<label>( ) )</label></formula><formula xml:id="formula_8">( ) = ? {?( 1) , ...,?( ? ) } +<label>(7)</label></formula><p>The index of sampled points is mostly fractional. Assume that we intend to extract feature at point ( , ). Its corresponding feature is obtained via bilinear interpolation as</p><formula xml:id="formula_9">( , ) = ?? , ( , ; , ) ? ( , )<label>(8)</label></formula><formula xml:id="formula_10">( , ; , ) = (0, 1? | ? |) ? (0, 1? | ? |) (9) In Eq. (8), (?)</formula><p>is the bilinear interpolation kernel all over the integral spatial locations. It only becomes non-zero at the four locations close to ( , ). Therefore, it can be computed quickly with few multiply-adds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overall Architecture</head><p>DePatch is a self-adaptive module to change positions and scales of patches. As DePatch can work as a plug-and-play module, we can easily incorporate DePatch into various vision transformers. Because of the superiority and generality, we choose PVT as our base model. PVT has four stages with feature maps of decreasing scales. It utilizes spatial-reduction attention to reduce cost on high resolution feature maps. For detailed information please refer to <ref type="bibr" target="#b25">[26]</ref>. Our model is denoted as DPT. It is built by replacing the patch embedding modules at the beginning of stage 2, 3 and 4 with DePatch, while keeping other configurations unchanged. The overall architecture is shown in <ref type="figure" target="#fig_5">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct image classification experiments on ImageNet <ref type="bibr" target="#b20">[21]</ref> and object detection experiments on COCO <ref type="bibr" target="#b15">[16]</ref>. After that, some ablation studies are provided then for further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Classification</head><p>Experiment Settings We use ImageNet <ref type="bibr" target="#b20">[21]</ref> for image classification experiments. The ImageNet dataset consists of 1.28M images for training and 50K for validation. These images belong to 1000 categories. We report top-1 error on validation set for comparison. The images are resized into 256 ? 256 and randomly cropped into 224 ? 224 for training. Advanced data augmentation methods including Mixup <ref type="bibr" target="#b31">[32]</ref>, CutMix <ref type="bibr" target="#b30">[31]</ref>, label smoothing <ref type="bibr" target="#b22">[23]</ref> and Rand-Augment <ref type="bibr" target="#b3">[4]</ref> are utilized. Our models are trained with the batch size of 1024 for 300 epochs and optimized by AdamW <ref type="bibr" target="#b18">[19]</ref> with initial learning rate of 1 ? 10 ?3 and cosine schedule <ref type="bibr" target="#b17">[18]</ref>. Weight decay is set to 0.05 for non-bias parameters. All these settings keep the same with original PVT <ref type="bibr" target="#b25">[26]</ref> for fair comparison.</p><p>Results As shown in <ref type="table" target="#tab_2">Table 1</ref>, our smallest DPT-Tiny achieves 77.4% top-1 accuracy, which is 2.3% higher than the corresponding baseline PVT model. The best result is achieved by our medium one. It achieves 81.9% top-1 accuracy, and even outperforms models with much larger costs like PVT-Large and catch up with DeiT-Base. As for CNN-based models, DPT-Small outperforms the popular ResNet50 by 4.9%. Our models achieve better results than both CNN-based and transformer-based models, and outperform them by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Object Detection</head><p>Experiment Settings Our experiments for object detection are conducted on COCO <ref type="bibr" target="#b15">[16]</ref>, a large-scale detection benchmark. We set train2017 split with 118K images as our training set, and val2017 split with 5K images for validation. Mean Average Precision (mAP) is used as our evaluation metric. Following <ref type="bibr" target="#b25">[26]</ref>, we evaluate our DPT backbones on three prevailing frameworks, RetinaNet <ref type="bibr" target="#b14">[15]</ref>, Mask R-CNN <ref type="bibr" target="#b10">[11]</ref> and DETR <ref type="bibr" target="#b1">[2]</ref>. We load ImageNet pretrained weights to initialize the backbone. Our models are trained with the batch size of 16 and optimized by AdamW <ref type="bibr" target="#b18">[19]</ref> with initial learning rate 1 ? 10 ?4 . As to RetinaNet and Mask R-CNN, we report results with both 1x and multi-scale 3x train schedules (12 and 36 epochs). For 1x schedule, images are resized so that the shorter edge has 800 pixels and the longer edge does not exceed 1333 pixels. For multi-scale training, the shorter edge is resized within the range of [640, 800]. As for DETR, the model is trained for 50 epochs with random flip and random scale.</p><p>Results We compare DPT to PVT <ref type="bibr" target="#b25">[26]</ref> and standard ResNe(X)t <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref>. The comparison is shown in <ref type="table" target="#tab_3">Table 2</ref>. As for RetinaNet, DPT-Small significantly outperforms PVT-Small by 2.1% and Resnet50 DETR is a latest framework for object detection. It requires a long trained schedule (e.g. 500 epochs). We only validate our model with a shorter schedule (50 epochs). According to <ref type="table" target="#tab_5">Table 4</ref>, our DPT-Small achieves 37.7% box mAP, outperforming PVT-Small by 3.0% and ResNet50 by 5.4%. Therefore we conclude that DPT is also compatible with transformer-based detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>Effect of module position There are four patch embedding modules in PVT. The first directly operates on the input image, and the rest are inserted at the beginning of the following stages. We perform detailed experiments to illustrate where we should add DePatch.</p><p>Since raw images contain little semantic information, it is difficult for the first module to predict offsets and scales beyond its own region. Therefore, we only attempt to replace the rest three patch embedding modules. The results are shown in <ref type="table" target="#tab_6">Table 5</ref>. The improvements obtained by stage 2, 3 and 4 are 0.3%, 1.0%, and 1.5%. The more  patch embedding modules we replace, larger improvement it brings. According to the results, replacing all patch embedding modules in stage 2, 3 and 4 achieves the best performance. It outperforms baseline PVT model by 1.5%, with only 0.86M parameters increased. In the following experiments, it will be our default configuration to replace all three patch embedding modules.</p><p>Effect of number of sampling points We experiment to show how many points we should sample in one predicted region. Sampling more points slightly increases FLOPs, but also has a stronger learning ability to capture features from a larger region. The results are shown in <ref type="table" target="#tab_7">Table 6</ref>. Increasing sampling points from 2 ? 2 to 3 ? 3 provides another 0.8% improvement, while further increasing it to 4 ? 4 only improves by 0.2%. Since sampling 4 ? 4 points only gets marginal improvement. We take the = 3 as the default configuration in following experiments.</p><p>We claim that sampling more points will benefits DPT with stronger ability to extract features from larger area. We show how the distributions of predicted scales change during training with different number of sampling points in <ref type="figure" target="#fig_7">Figure 4</ref>. Although we initialize the region scales strictly the same as that in PVT ( ?_ = 2), DePatch can learn to expand on its own. This phenomenon accords to the common sense in CNN, that enlarging the receptive field        benefits the model. The distributions of scales for = 2 converge early with low variance. Sampling 2?2 points is unable to represent any larger regions, hence it limits the capacity of the our module for understanding images with heavy geometric deformation. The statistics do not differ much for = 3 and 4 except in stage 2. We assume that sampling more points will achieve even better performance, while it is not worth additional cost. Designing a more sophisticated spatial pyramid cound be another way to improve our method. We leave it as our future work. Decouple offsets and scales DePatch learns both the offset and scale for each patch. Offsets are predicted to shift the patches towards more important regions, and scales are for better maintaining local structures. They both facilitates the performance of our model. We decouple these two factors in <ref type="table" target="#tab_8">Table 7</ref> in order to see how each single one influences our model. When scales are not predicted, the shape of all rectangle regions is fixed the same as patches in original PVT. Only predicting offsets can improve 1.5% above baseline, and another 0.8% is obtained by predicting scales. We claim that both offsets and scales are important for our self-adaptive patch embedding module.</p><p>Analysis for fast convergence DePatch module is able to adjust the patches to a proper shape for each image. Adequate patches maintain important local structures, and features are learned more efficiently. Therefore the whole network can learn at a faster speed. We draw the training curve for both our DPT-Tiny and PVT-Tiny in <ref type="figure" target="#fig_8">Figure 5</ref>. The training loss and test accuracy do converge faster in first few epochs.</p><p>Based on this phenomenon, we expect that our module can alleviate the requirement of long training schedule. We prove it by simply reducing training epochs by half. As shown in <ref type="table" target="#tab_9">Table 8</ref>, DPT-Tiny trained with only 150 epochs outperforms a fully-trained PVT-Tiny by 1.1%, and the performance degradation caused by a shorter schedule is only 1.2%, which is much smaller than original PVT-Tiny. This indicates that our DePatch module can significantly accelerate training for vision transformers, which would benefit further research. Effect of parameter initialization As stated in 3.2, we initialize and to zero. We shown in <ref type="table" target="#tab_10">Table 9</ref> that initialization methods have little impact on final performance. We take zero initialization in the all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization</head><p>We illustrate offsets learned by DePatch in <ref type="figure" target="#fig_9">Figure 6</ref>. The visualization shows that patches predicted by DePatch are well-located to capture important features. DePatch has more obvious impacts at the edge of foreground objects. It encourages the patches outside to shift a bit towards the object, thus covering more critical areas than normal patches. When there are more than one object appearing in the image, patches would adjust their positions to the closest one (the two whales in <ref type="figure" target="#fig_9">Figure 6(b)</ref>). This attribute would be more crucial for object detection, since different patches can be more representative for different objects. Therefore, the detector can better locate and classify all the objects with more related features. The predicted scales are also influenced by richness of local context, such as edges or corners. It becomes small when it needs to focus on subtle details (the beak of the bird in <ref type="figure" target="#fig_9">Figure 6(d)</ref>), and large if more context is needed (homogenous area of the dog's stomach <ref type="figure" target="#fig_9">Figure 6</ref>(h)). The high variance of offsets and scales indicates strong self-adaptability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we introduce DePatch, a deformable module to split patches. It encourages the network to extract patch information from object-related regions and make our model insensitive to geometric deformation. This module can work as a plug-and-play module and improve various vision transformers. We also build a transformer with DePatch module, named Deformable Patch-based Transformer, DPT. Extensive experiments on image classification and object detection indicate that DPT can extract better features and outperform CNN-based models and other vision transformers. DePatch can be utilized in other vision transformers as well as other downstream tasks to improve their performance. Our model is the first work to modify vision transformer in a data-dependant way. We hope our idea could serve as a good starting point for future studies. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Original patch embedding module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>An example of vanilla patch splitting and our deformable way. (a) Original patch embedding module divides the image in a fixed way. It sometimes destroys the semantics of objects. (b) Our DePatch splits the image into patches in a deformable way with learnable positions and scales. (Better viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Since patch size is fixed, the left-top corner and right-bottom corner are determined as ( There are ? pixels inside this region, their coordinates are represented by ? ( , ) = ( ( , )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>DePatch module ( = 3) DePatch module instruction. Offsets and scales are predicted with local features, and new embeddings are obtained by bilinear interpolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Left: Original PVT architecture. Right: DPT, Equipped with our DePatch module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Scale distribution at stage 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Region scale learned by DPT-Tiny with different number of sampling points ? . We illustrate statistics in stage 2, 3 and 4. Scale is measured by the edge size of the region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Training curve for both DPT-Tiny and PVT-Tiny.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of learned patches and their offsets with our DPT-Small at stage 4. Our method can adaptively adjust the position and scale for each patch based on the input content.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>accuracy by 2.3% on ImageNet classification, and also gains 2.8%/3.5% improvements for both RetinaNet and Mask R-CNN detectors under the tiny configuration.</figDesc><table><row><cell>We introduce a new adaptive patch embedding module, De-</cell></row><row><cell>Patch. DePatch can adjust the position and scale of each</cell></row><row><cell>patch based on the input image, and effectively preserve se-</cell></row><row><cell>mantics in one patch, reducing semantics destruction caused</cell></row><row><cell>by image splitting.</cell></row><row><cell>? Our DePatch is lightweight and can work as a plug-and-play</cell></row><row><cell>module integrated into different transformers, leading to a</cell></row><row><cell>Deformable Patch-based Transformer (DPT). In this work,</cell></row><row><cell>we incorporate DePatch into PVT to verify the efficacy of</cell></row><row><cell>DPT.</cell></row></table><note>? We conduct extensive experiments on image classification and object detection. For example, our module improves top-1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on ImageNet Classification. 1% box mAP and 39.9% mask mAP under 1? schedule, outperforming PVT-Small by 2.7% and 2.1%. With 3? training schedule and multi-scale training, Mask R-CNN+DPT-Small achieves the best result with 44.4% box mAP and 41.0% mask mAP.</figDesc><table><row><cell>Method</cell><cell cols="3">#Param (M) FLOPs (G) Top-1 Acc(%)</cell></row><row><cell>ResNet18 [12]</cell><cell>11.7</cell><cell>1.8</cell><cell>69.8</cell></row><row><cell>DeiT-Tiny [24]</cell><cell>5.7</cell><cell>1.3</cell><cell>72.2</cell></row><row><cell>PVT-Tiny [26]</cell><cell>13.2</cell><cell>1.9</cell><cell>75.1</cell></row><row><cell>DPT-Tiny (ours)</cell><cell>15.2</cell><cell>2.1</cell><cell>77.4</cell></row><row><cell>ResNet50 [12]</cell><cell>25.6</cell><cell>4.1</cell><cell>76.1</cell></row><row><cell>DeiT-Small [24]</cell><cell>22.1</cell><cell>4.6</cell><cell>79.9</cell></row><row><cell>T2T-ViT-14 [30]</cell><cell>21.4</cell><cell>5.2</cell><cell>80.6</cell></row><row><cell>PVT-Small [26]</cell><cell>24.5</cell><cell>3.8</cell><cell>79.8</cell></row><row><cell>DPT-Small (ours)</cell><cell>26.4</cell><cell>4.0</cell><cell>81.0</cell></row><row><cell>ResNet101 [12]</cell><cell>44.7</cell><cell>7.9</cell><cell>77.4</cell></row><row><cell>X101-32x4d [29]</cell><cell>44.2</cell><cell>8.0</cell><cell>78.8</cell></row><row><cell>X101-64x4d [29]</cell><cell>83.5</cell><cell>15.6</cell><cell>79.6</cell></row><row><cell>ViT-Base [7]</cell><cell>86.6</cell><cell>17.6</cell><cell>77.9</cell></row><row><cell>DeiT-Base [24]</cell><cell>86.6</cell><cell>17.6</cell><cell>81.8</cell></row><row><cell>T2T-ViT-19 [30]</cell><cell>39.0</cell><cell>8.0</cell><cell>81.2</cell></row><row><cell>PVT-Medium [26]</cell><cell>44.2</cell><cell>6.7</cell><cell>81.2</cell></row><row><cell>PVT-Large [26]</cell><cell>61.4</cell><cell>9.8</cell><cell>81.7</cell></row><row><cell>DPT-Medium (ours)</cell><cell>46.1</cell><cell>6.9</cell><cell>81.9</cell></row><row><cell cols="4">by 6.2% mAP at a comparable computational cost, which indicates</cell></row><row><cell cols="4">that DPT provides more discriminative features for target objects</cell></row><row><cell cols="4">in images. With our DePatch module, each patch is aware of its</cell></row><row><cell cols="4">neighboring content, and extracts crucial information needed for</cell></row><row><cell cols="4">different locations. Moreover, with 3? training schedule and multi-</cell></row><row><cell cols="4">scale training, RetinaNet+DPT-Medium achieves 43.7% mAP. It</cell></row><row><cell cols="4">outperforms PVT-Medium and ResNet101 by a large margin, and</cell></row><row><cell cols="4">even achieves better performance than PVT-Large model, but with</cell></row><row><cell cols="2">more than 20% cost reduced.</cell><cell></cell><cell></cell></row><row><cell cols="4">Results for Mask R-CNN are similar. Our DPT-Small model</cell></row><row><cell>achieves 43.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Object detection performance on MS COCO (RetinaNet)</figDesc><table><row><cell>RetinaNet 1?</cell><cell>RetinaNet 3? + MS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="8">: Object detection performance on MS COCO (Mask R-CNN)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mask R-CNN 1?</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mask R-CNN 3? + MS</cell></row><row><cell>Backbone</cell><cell>#Param(M)</cell><cell></cell><cell>50</cell><cell>75</cell><cell></cell><cell>50</cell><cell>75</cell><cell></cell><cell>50</cell><cell>75</cell><cell>50</cell><cell>75</cell></row><row><cell>ResNet18 [12]</cell><cell>31.2</cell><cell>34.0</cell><cell cols="2">54.0 36.7</cell><cell>31.2</cell><cell cols="2">51.0 32.7</cell><cell>36.9</cell><cell cols="2">57.1 40.0</cell><cell>33.6</cell><cell>53.9 35.7</cell></row><row><cell>PVT-Tiny [26]</cell><cell>32.9</cell><cell>36.7</cell><cell cols="2">59.2 39.3</cell><cell>35.1</cell><cell cols="2">56.7 37.3</cell><cell>39.8</cell><cell cols="2">62.2 43.0</cell><cell>37.4</cell><cell>59.3 39.9</cell></row><row><cell>DPT-Tiny (ours)</cell><cell>34.8</cell><cell>40.2</cell><cell cols="2">62.8 43.8</cell><cell>37.7</cell><cell cols="2">59.8 40.4</cell><cell>42.2</cell><cell cols="2">64.4 46.1</cell><cell>39.4</cell><cell>61.5 42.3</cell></row><row><cell>ResNet50 [12]</cell><cell>44.2</cell><cell>38.0</cell><cell cols="2">58.6 41.4</cell><cell>34.4</cell><cell cols="2">55.1 36.7</cell><cell>41.0</cell><cell cols="2">61.7 44.9</cell><cell>37.1</cell><cell>58.4 40.1</cell></row><row><cell>PVT-Small [26]</cell><cell>44.1</cell><cell>40.4</cell><cell cols="2">62.9 43.8</cell><cell>37.8</cell><cell cols="2">60.1 40.3</cell><cell>43.0</cell><cell cols="2">65.3 46.9</cell><cell>39.9</cell><cell>62.5 42.8</cell></row><row><cell>DPT-Small (ours)</cell><cell>46.1</cell><cell>43.1</cell><cell cols="2">65.7 47.2</cell><cell>39.9</cell><cell cols="2">62.9 43.0</cell><cell>44.4</cell><cell cols="2">66.5 48.9</cell><cell>41.0</cell><cell>63.6 44.2</cell></row><row><cell>ResNet101 [12]</cell><cell>63.2</cell><cell>40.4</cell><cell cols="2">61.1 44.2</cell><cell>36.4</cell><cell cols="2">57.7 38.8</cell><cell>42.8</cell><cell cols="2">63.2 47.1</cell><cell>38.5</cell><cell>60.1 41.3</cell></row><row><cell>ResNeXt101-32x4d [29]</cell><cell>62.8</cell><cell>41.9</cell><cell cols="2">62.5 45.9</cell><cell>37.5</cell><cell cols="2">59.4 40.2</cell><cell>44.0</cell><cell cols="2">64.4 48.0</cell><cell>39.2</cell><cell>61.4 41.9</cell></row><row><cell>ResNeXt101-64x4d [29]</cell><cell>101.9</cell><cell>42.8</cell><cell cols="2">63.8 47.3</cell><cell>38.4</cell><cell cols="2">60.6 41.3</cell><cell>44.4</cell><cell cols="2">64.9 48.8</cell><cell>39.7</cell><cell>61.9 42.6</cell></row><row><cell>PVT-Medium [26]</cell><cell>63.9</cell><cell>42.0</cell><cell cols="2">64.4 45.6</cell><cell>39.0</cell><cell cols="2">61.6 42.1</cell><cell>44.2</cell><cell cols="2">66.0 48.2</cell><cell>40.5</cell><cell>63.1 43.5</cell></row><row><cell>PVT-Large [26]</cell><cell>81.0</cell><cell>42.9</cell><cell cols="2">65.0 46.6</cell><cell>39.5</cell><cell cols="2">61.9 42.5</cell><cell>44.5</cell><cell cols="2">66.0 48.3</cell><cell>40.7</cell><cell>63.4 43.7</cell></row><row><cell>DPT-Medium (ours)</cell><cell>65.8</cell><cell>43.8</cell><cell cols="2">66.2 48.3</cell><cell>40.3</cell><cell cols="2">63.1 43.4</cell><cell>44.3</cell><cell cols="2">65.6 48.8</cell><cell>40.7</cell><cell>63.1 44.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">Object detection performance on MS COCO (DETR</cell></row><row><cell>with 50 epochs)</cell><cell></cell><cell></cell></row><row><cell>Backbone</cell><cell>50</cell><cell>75</cell></row><row><cell>ResNet50 [12]</cell><cell cols="2">32.3 53.9 32.3 10.7 33.8 53.0</cell></row><row><cell>PVT-Small [26]</cell><cell cols="2">34.7 55.7 35.4 12.0 36.4 56.7</cell></row><row><cell cols="3">DPT-Small (ours) 37.7 59.2 38.8 15.0 40.3 58.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Effect of module position ( = 2)</figDesc><table><row><cell cols="5">Stage 2 Stage 3 Stage 4 #Params(M) Top-1 Acc(%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>13.23</cell><cell>75.1</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>13.26</cell><cell>75.4</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell>13.43</cell><cell>76.1</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>14.09</cell><cell>76.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Effect of number of sampling points</figDesc><table><row><cell cols="4">Sampling points #Params(M) Flops(G) Top-1 Acc(%)</cell></row><row><cell>Baseline</cell><cell>13.23</cell><cell>1.94</cell><cell>75.1</cell></row><row><cell>2 ? 2</cell><cell>14.09</cell><cell>2.03</cell><cell>76.6</cell></row><row><cell>3 ? 3</cell><cell>15.15</cell><cell>2.14</cell><cell>77.4</cell></row><row><cell>4 ? 4</cell><cell>16.64</cell><cell>2.30</cell><cell>77.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Decouple of predicting offsets and scales</figDesc><table><row><cell cols="3">Offsets Scales Top-1 Acc(%)</cell></row><row><cell></cell><cell></cell><cell>75.1</cell></row><row><cell>?</cell><cell></cell><cell>76.6</cell></row><row><cell>?</cell><cell>?</cell><cell>77.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Top-1 accuracy (%) with short training schedule</figDesc><table><row><cell>Method</cell><cell cols="2">150 epochs 300 epochs</cell></row><row><cell>PVT-Tiny [26]</cell><cell>73.1</cell><cell>75.1</cell></row><row><cell>DPT-Tiny (Ours)</cell><cell>76.2</cell><cell>77.4</cell></row><row><cell>(a)</cell><cell></cell><cell>(b)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Effect of initialization for and</figDesc><table><row><cell>Initialization</cell><cell>Top-1 Acc(%)</cell></row><row><cell>Truncated normal</cell><cell>77.36</cell></row><row><cell>Zero init</cell><cell>77.39</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gcnet: Nonlocal networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Do We Really Need Explicit Position Encodings for Vision Transformers?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697</idno>
		<title level="m">ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast convergence of detr with spatially modulated co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07448</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112</idno>
		<title level="m">Transformer in transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. 2021. Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<title level="m">Alexandre Sablayrolles, and Herv? J?gou. 2020. Training data-efficient image transformers &amp; distillation through attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
	<note>Non-local neural networks</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Glance and focus: a dynamic approach to reducing spatial redundancy in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangchen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05300</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<title level="m">Deformable DETR: Deformable Transformers for End-to-End Object Detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
