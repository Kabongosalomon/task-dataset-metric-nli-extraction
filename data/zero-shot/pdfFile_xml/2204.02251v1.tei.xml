<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RBGNet: Ray-based Grouping for 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wang</surname></persName>
							<email>wanghaiyang@stu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongyao</forename><surname>Fang</surname></persName>
							<email>rongyaofang@link</email>
							<affiliation key="aff3">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">MOE</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Alibaba Group 6 Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hkqi.qian@alibaba-inc.com</email>
							<affiliation key="aff3">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<email>schiele@mpi-inf.mpg.dezeyang@cs.toronto.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<email>wanglw@cis.pku.edu.cnsshi</email>
							<affiliation key="aff5">
								<orgName type="department">International Center for Machine Learning Research</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RBGNet: Ray-based Grouping for 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As a fundamental problem in computer vision, 3D object detection is experiencing rapid growth. To extract the point-wise features from the irregularly and sparsely distributed points, previous methods usually take a feature grouping module to aggregate the point features to an object candidate. However, these methods have not yet leveraged the surface geometry of foreground objects to enhance grouping and 3D box generation. In this paper, we propose the RBGNet framework, a voting-based 3D detector for accurate 3D object detection from point clouds. In order to learn better representations of object shape to enhance cluster features for predicting 3D boxes, we propose a ray-based feature grouping module, which aggregates the point-wise features on object surfaces using a group of determined rays uniformly emitted from cluster centers. Considering the fact that foreground points are more meaningful for box estimation, we design a novel foreground biased sampling strategy in downsample process to sample more points on object surfaces and further boost the detection performance. Our model achieves state-of-the-art 3D detection performance on ScanNet V2 and SUN RGB-D with remarkable performance gains. Code will be available at https://github.com/Haiyang-W/RBGNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D object detection is becoming an active research topic in computer vision, which aims to estimate oriented 3D bounding boxes and semantic labels of objects in 3D scenes. As a fundamental technique for 3D scene understanding, it plays a critical role in many applications, such as autonomous driving <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39]</ref>, augmented reality <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> and domestic robots <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b39">40]</ref>. Unlike the scenarios in the well-* Corresponding author: Shaoshuai Shi.  studied 2D image problems, 3D scenes are generally represented by point clouds, a set of unordered, sparse and irregular points captured by depth sensors (e.g., RGB-D cameras, LiDAR sensors), which makes it significantly different from traditional regular input data like images and videos.</p><p>Previous 3D detection approaches can be coarsely classified into two lines in terms of point representations, i.e., the grid-based methods and the point-based methods. The grid-based methods generally convert the irregular points to regular data structure such as 3D voxels <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b8">9]</ref> or 2D bird's eye view maps <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>. Thanks to the great success of PointNet series <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, the point-based methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7]</ref> directly extract the point-wise features from the irregular and points. These point-wise features are generally enhanced by various feature grouping modules for predicting the 3D bounding boxes. However, these feature grouping strategies have not well explored the fine-grained surface geometry to help improve the performance of 3D box generation.</p><p>We argue that feature grouping module plays an important role in point-based 3D detectors, and how to better incorporate the foreground object geometry features to en-  <ref type="table">Table 1</ref>: Results of VoteNet <ref type="bibr" target="#b28">[29]</ref> variants on ScannetV2 <ref type="bibr" target="#b7">[8]</ref>. GT-Features: Aggregate the features of ground-truth surface points for the 3D box generation of this object, where the "explicit GTcenter" / "implicit GT-center" indicate that the above features are grouped to the ground-truth center / predicted vote centers, respectively. FgSamp: FPS is only conducted on foreground points. hance the quality of point-wise features is the key to predict better 3D bounding boxes. As shown in <ref type="table">Table 1</ref>, for the popular VoteNet <ref type="bibr" target="#b28">[29]</ref> point-wise 3D detector, by simply grouping the features of accurate object surface points to the features of their correct vote centers, the performance can be improved dramatically with a gain of 13.31 on mAP@0. <ref type="bibr" target="#b24">25</ref> for explicit usage of ground truth labels (2 nd row of Table 1), and a gain of 8.79 for implicit usage of ground truth labels (3 rd row of <ref type="table">Table 1</ref>). Here the "explicit usage" indicates that the ground truth labels are not only utilized for grouping the object surface points but also for replacing the vote centers with ground truth centers, while the "implicate usage" means the ground truth labels are only used for grouping the object surface points. These facts inspire us to explore on designing a better feature representation for the surface geometry of foreground objects, to help the prediction of 3D bounding boxes.</p><p>Hence, we present a new 3D detection framework, RBGNet, which is a one-stage 3D detector for 3D object detection from raw point clouds. Our RBGNet is built on top of VoteNet <ref type="bibr" target="#b28">[29]</ref>, and we propose two novel strategies to boost the performance of 3D object detection by implicitly learning from foreground object features.</p><p>Firstly, we propose the ray-based feature grouping that could learn better feature representation of the surface geometry of foreground objects. The learned features are utilized to augment the cluster features for 3D boxes estimation. Specifically, we formulate a ray-based mechanism to capture the object surface points, where a number of rays are uniformly emitted from the cluster center with the determined angles (see <ref type="figure" target="#fig_1">Fig. 1</ref>). The far bounds of the rays are based on our predicted object scale of this cluster. Then a number of anchor points is densely sampled on each ray, where the aggregated local features of each anchor point are utilized to predict whether they are on the object surface to learn the geometry shape. Moreover, a coarse-tofine strategy is proposed to generate different number of anchor points based on the sparsity of different regions. The learned features from all the anchor points will be finally aggregated to boost the features of cluster centers for predicting 3D bounding boxes. The experiments <ref type="table" target="#tab_5">(Table 4)</ref> show that our ray-based feature grouping strategy can effectively encode the surface geometry of foreground objects and sig-nificantly improves 3D detection performance.</p><p>Secondly, we propose the foreground biased sampling strategy to allocate more foreground object points for predicting 3D boxes. We observe that the points on object surfaces are more useful than those on the background for 3D box estimation (similar observations are also mentioned by <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b32">33]</ref>), and 4 th row of <ref type="table">Table 1</ref> shows that by conducting farthest point sampling only on the ground truth foreground points, the performance of VoteNet <ref type="bibr" target="#b28">[29]</ref> could be boosted from 62.90 to 71.27 in terms of mAP@0.25. Therefore, we propose a simple but effective strategy to sample points biased towards object surface while still keeping the coverage rate of the whole scene. Specifically, we append a segmentation head to the point-wise features before each farthest point sampling, where the head will predict the confidence of each point being a foreground point. According to the ranking of their foreground scores, the input points are separated into foreground set and background set. And these two sets will apply farthest point sampling separately, where we sample most target points (i.e., 87.5% in our case) from the foreground set and a small number (i.e., 12.5%) from the background set to keep the coverage rate of the whole scene. Our foreground biased sampling can produce a more informative sampling of points over foreground objects surface for feature extraction, and the performance gains <ref type="table" target="#tab_5">(Table 4</ref>) demonstrate its effectiveness.</p><p>In a nutshell, our contributions are three-fold: 1) We propose a novel ray-based feature grouping module to encode object surface points with determined rays, which can learn better surface geometry features of objects to boost the performance of point-based 3D object detectors. 2) We present foreground biased sampling module to focus feature learning of the network on foreground surface points while also keeping the coverage rate for the whole scene, which can incorporate more object points to benefit point-based 3D box generation. 3) Equipped with the above two modules, our proposed RBGNet framework outperforms state-of-the-art methods with remarkable margins both on ScanNetV2 <ref type="bibr" target="#b7">[8]</ref> and SUN RGB-D <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Object Detection is challenging due to the irregular, sparse and orderless characteristics of 3D points. Most existing works could be classified into two categories in terms of point cloud representations, i.e., grid-based and point-based. Grid-based approaches transform point clouds to regular data, such as 2D grids <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b46">47]</ref> or 3D voxels <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b33">34]</ref>. 2D grid methods project point clouds to a bird view before proceeding to the rest of the pipeline. Voxel-based methods convert the point clouds into 3D voxels to be processed by 3D CNN or efficient 3D sparse convolution <ref type="bibr" target="#b11">[12]</ref>, which greatly facilitate 3D object detection. Popularized by PointNet <ref type="bibr" target="#b30">[31]</ref> and  its variants <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b51">52]</ref>, point-based methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b20">21]</ref> have become extensively employed on estimating bounding box directly from raw points. Most of existing methods can be considered as a bottom-up manner, which requires point grouping step to obtain object features. Point R-CNN <ref type="bibr" target="#b34">[35]</ref> groups point features within the 3D proposals by point cloud region pooling. VoteNet <ref type="bibr" target="#b28">[29]</ref> applies Hough Voting to group the points that vote to the similar center region. Group-free <ref type="bibr" target="#b20">[21]</ref> implicitly groups point features by an attention module. Although these methods have explored various feature grouping strategies, they have not leveraged the surface geometry of foreground objects. We propose a novel ray-based feature grouping module to encode object shape distribution with determined rays, and the learned features are used to further boost 3D detection performance. 2D Shape Representation. Shape representation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref> is of particular interest due to the ability to explicitly describe the 2D object shape with points. Polar Mask <ref type="bibr" target="#b40">[41]</ref> and ESE-SEG <ref type="bibr" target="#b43">[44]</ref> both use polar representation to model the object boundary and then regress the object locations as well as the length of rays emitting uniformly from the object centroids. However, these shape representations may fail to model 3D object surface, because of the limited expressive ability on concave shape, the difficulty of inner center definition. We design a ray-based 3D shape representation to effectively model object surface geometry. Point Cloud Sampling. Sampling <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10]</ref> aims to represent the original point cloud in a sparse way, plays a key role in point cloud analysis. Farthest point sampling (FPS) has been widely used as a pooling operation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b28">29]</ref>, since it can uniformly sample distributed points. However, FPS is agnostic to downstream tasks by a predefined rule, foreground instances with few interior points may lose all points after sampling. 3DSSD <ref type="bibr" target="#b47">[48]</ref> applies a fused FPS based on feature and euclidean distance, but still does not focus on foreground points explicitly. To deal with the dilemma, we design a simple but effective strategy, foreground biased sampling, to sample more points on object surface while still keeping the coverage rate of the whole scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This section describes the technical details of the proposed RBGNet detector. ?3.1 briefly presents the overview of our approach. Next, ?3.2 to ?3.4 elaborate on the network design and the learning objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>RBGNet is a one-stage 3D object detection framework aiming at more accurate bounding box estimation from irregular point clouds. As illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>, RBGNet consists of three major components: i) a backbone network with foreground biased sampling to extract feature representation from point clouds, ii) a ray-based feature grouping module to effectively capture the points on object surface and learn from the shape distribution to augment cluster feature and iii) a proposal and classification module followed by 3D non-maximum-suppression (NMS). Our paper mainly focuses on the sampling and grouping modules, so we follow the same proposal and classification strategy as in VoteNet <ref type="bibr" target="#b28">[29]</ref> to estimate final bounding boxes. We will describe the technical details in the following parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ray-based Feature Grouping</head><p>VoteNet <ref type="bibr" target="#b28">[29]</ref> has shown tremendous success for 3D object detection. After getting the seed points from the backbone (PointNet++ <ref type="bibr" target="#b31">[32]</ref>), it reformulates traditional Hough voting, and generates object candidates by grouping the seed points whose votes are within the same cluster. The aggregated feature is then used to estimate the 3D bounding boxes and associated semantic labels. However, the quality of the grouping principally determines the reliability of proposal features and detector performance. Some followup works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b42">43]</ref> are actually trying to solve this problem, but they have not well explored on the fine-grained surface geometry of foreground objects. To address this limitation, we propose the ray-based feature grouping module, which can effectively encode the shape distribution and learn better object features to enhance 3D detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Ray Point Representation.</head><p>We first illustrate the process of our proposed ray point representation, where two types of anchor points are generated on each ray to encode the object geometry around the cluster centers. These anchor points of all rays will be utilized for the final feature enhancement in ?3.2.2. Formulation of determined rays. We generate a set of vote cluster centers {c i } M i=1 based on the vote sampling and grouping defined in VoteNet <ref type="bibr" target="#b28">[29]</ref>, where</p><formula xml:id="formula_0">c i = [v i , f i ] indi- cating the vote center v i ? R 3 and its features f i ? R C ,</formula><p>M is the number of vote clusters. For each vote cluster, N rays are emitted uniformly from the cluster center with the determined angles and far bounds. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, the rules for generating rays are based on the spherical coordinate system, which are formulated as follows:</p><p>? The polar angle ? ? [0, ?] is split into P bins, and each bin corresponds a round surface that is perpendicular to the Z-axis. The angle of p th bin is:</p><formula xml:id="formula_1">?p = ?p (P -1) , p ? {0, ..., P -1}.<label>(1)</label></formula><p>? The number of rays (denoted as A p ) terminated on the p th round surface is calculated as follows:</p><formula xml:id="formula_2">Ap = ? ? ? ? ? ? ? ? ? ? ? 1, if p = 0 or P -1, 4 ? p, if 0 &lt; p ? P -1 2 , 4 ? (P -p -1), if P -1 &gt; p &gt; P -1 2 ,<label>(2)</label></formula><p>where 4 is a hyper-parameter to indicate the factor for the number of sampled rays in each round surface. ? With A p and ? p , a ray could be determined. Its azimuth angle ? p,a ? [0, 2?] and polar angle ? p,a ? [0, ?] could be formulated as follows:</p><formula xml:id="formula_3">?p,a = 2?a Ap , ?p,a = ?p, a ? {0, ...Ap -1}.<label>(3)</label></formula><p>Our adopted strategy could generate more uniformly distributed rays to better cover the surrounding region of clusters. Given the polar-bin number P , the number of rays is N = P ?1 p=0 A p (i.e. P = 9 ? N = 66 in our case). Note that more rays will be generated when the polar angle is closer to ? 2 . As for the far bounds of the rays of each cluster, all the rays are of the same length as the object scale l i , which is predicted based on the cluster features f i . Here, we explicitly supervise the object scale l i by regression loss</p><formula xml:id="formula_4">L scale-reg = 1 I i ||l i ? l * i || ? I[i th is positive],<label>(4)</label></formula><p>where l * i is the half diagonal side of the assigned GT box and I[i th is positive] is the indicator function to indicate whether the vote center c i is around a GT object center (within a radius of 0.3m). I is the number of positive vote centers. ? means smooth-1 norm. Coarse-to-fine anchor point generation. After generating determined rays, RBGNet samples a number of anchor points along each ray. However, it is inefficient to directly sampling points: 1) the less important free space and background region are still sampled, 2) the number of sampled points in a ball query operation is limited, and thus a lot of object points in dense areas are not captured. To address these limitations, we propose a coarse-to-fine anchor point generation strategy. It increases the sample efficiency by querying more points for dense areas. Inspired by the success of <ref type="bibr" target="#b21">[22]</ref>, we adopt a hybrid sampling strategy, which contains two sampling processes: one "coarse" and one "fine". Before generating anchor points, we first up-sample the seed points back to 2048 points by trilinear interpolation to obtain more meaningful points, especially on object surface. The target point positions for upsampling are the same as the 1 st SA layers of PointNet++ <ref type="bibr" target="#b31">[32]</ref> backbone. For the i th cluster center with cluster features f (we remove the subscript i of f i for simplicity), we conduct the coarse-tofine anchor point generation in the following process.</p><p>Firstly, in coarse stage, as for the n th ray, we sample a set of anchor points as</p><formula xml:id="formula_5">Q (c) n = {q (c) n,k = (x (c) n,k , y (c) n,k , z (c) n,k )}, k ? {1, ? ? ? , K c }, (5)</formula><p>where K c is the number of anchor points sampled on each ray, and the anchor points are generated by stratified sampling to evenly partition the ray into K c bins.</p><p>To extract local feature of each anchor point, we apply set abstraction <ref type="bibr" target="#b31">[32]</ref> to aggregate the features of the seed points around each anchor point. The aggregated local features of anchor point q </p><formula xml:id="formula_6">m (c) n,k = F (c) mask (? (c) n,k , f ),<label>(6)</label></formula><p>where the ground-truth of positive masks are calculated by applying ball query operation <ref type="bibr" target="#b31">[32]</ref> for each anchor point. We assign positive label to an anchor point if some surface points of its assigned GT object are within its ball query region, or the anchor point will be assigned with a negative label. Hence this point mask module could predict whether each anchor point belongs to its corresponding object or not. Secondly, in fine stage, different from <ref type="bibr" target="#b21">[22]</ref> which computes sample probability from point density, our fine anchor points are biased towards the dense part of its corresponding object. To achieve this goal, we apply inverse transform sampling to uniformly generate some K f anchor points set Q</p><formula xml:id="formula_7">(f ) n = {q (f ) n,k } K f</formula><p>k=1 on positive regions (predicted by the point mask module of coarse stage) of each ray. As adopted in the coarse branch, we also extract the local features and predict the positive mask for each fine anchor point.</p><p>Repeat this coarse-to-fine process on all rays, we obtain the coarse and fine local point feature set</p><formula xml:id="formula_8">P (c) = {? (c) n,k } Kc,N k=1,n=1 , P (f ) = {? (f ) n,k } K f ,N k=1,n=1 , point mask set M (c) = {m (c) n,k } Kc,N k=1,n=1 , M (f ) = {m (f ) n,k } K f ,N k=1,n=1</formula><p>and their corresponding positions of anchor points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Feature Enhancement by Determined Rays</head><p>As discussed in ?1, the fined-grained surface geometry of foreground objects plays a crucial role in generating accurate object proposals. The process of our proposed coarseto-fine anchor point generation already encodes such a surface geometry features, since our predicted surface masks and learned local features could implicitly describe the object geometry. Here we propose to aggregate those informative features of the anchor points to enhance the quality of cluster features, where the order of rays plays an important roles in the feature aggregation.</p><p>To be specific, given the local features P (c) and P (f ) , the point masks M  features of each anchor points will be masked by setting the features of negative anchor points to zeros. We denote the masked features asP (c) andP (f ) .</p><p>To aggregate the learned features orderly based on the determined rays, we formulate a fusion stage to integrate point features in a predefined order of rays. The features of coarse and fine anchor points are aggregated with two separate branches. In coarse branch, the masked point features of n th ray, {? n . It is implemented by concatenating the features of anchor points in order before being projected to a 32-dimensional features:</p><formula xml:id="formula_9">r (c) n = F (c) point ({? (c) n,k } Kc k=1 , ),<label>(7)</label></formula><p>where means the concatenation operation. Then, in the same way, we concatenate all the ray features R (c) = {r (c) n } N n=1 with a determined order and apply a two-layer MLP to generate a 128-dimensional coarse feature:</p><formula xml:id="formula_10">? (c) = F (c) ray ({r (c) n } N n=1 , ).<label>(8)</label></formula><p>Note that the predefined order of both anchor points and rays are consistent for each proposal, but different ordering strategies do not affect the performance. The fine branch also adopts the same strategy as the coarse branch to generates a 128-dimensional feature ? (f ) . Finally, the coarse and fine features are fused as:</p><formula xml:id="formula_11">g = F fuse (? (c) , ? (f ) ).<label>(9)</label></formula><p>The fused feature g is finally combined with the cluster feature f to improve the performance of 3D object detection. In this way, our RBGNet models the surface geometry implicitly and roughly obtain the size and the position of a possible object in a class-agnostic way, which could greatly benefit the prediction of 3D bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Foreground Biased Sampling</head><p>The foreground points provide rich information on predicting their associated object locations and orientations, and force network to capture shape information for more accurate 3D box generation. However, the widely-adopted farthest point sampling algorithm in the backbone is agnostic to the downstream tasks and samples a lot of background points. It may bring negative effects for 3D detection. Therefore, we design a simple but effective strategy, Foreground Biased Sampling, to sample more points on foreground object surfaces while still keeping the coverage rate of the whole scene.</p><p>Given the point-wise features encoded by each set abstraction layer, we append a segmentation head for estimating the confidence of each points. The ground-truth segmentation mask is naturally provided by the 3D groundtruth boxes. To be specific, for example, after going through the first SA layer of standard PointNet++ <ref type="bibr" target="#b31">[32]</ref>, we obtain 2048 downsample point set D = {d j } 2048 j=1 with xyz j and 128-dimensional feature ? j . Then the segmentation head scores each point to be a foreground point or not as:</p><formula xml:id="formula_12">? j = F fore (? j , j ) ? [0, 1].<label>(10)</label></formula><p>We sort the confidence scores, select top ? to form a fore-</p><formula xml:id="formula_13">ground set D (f ) = {d (f ) j } ? j=1 and the rest are the back- ground set D (b) = {d (b) j } 2048-? j=1 .</formula><p>Due to the concentration of high score points, there is a trade-off between the recall of foreground points and the sampling coverage for the whole scene. Based on this observation, we apply farthest point sampling on foreground and background set separately, and combine them into the final sample set as follows: <ref type="bibr" target="#b10">(11)</ref> where</p><formula xml:id="formula_14">D( f ) = FPS(D (f ) ), D( b) = FPS(D (b) ), S = D( f ) ? D( b)</formula><formula xml:id="formula_15">D( f ) = {d( f ) j } ? j=1 and D( b) = {d( b)</formula><p>j } ? j=1 , ? and ? are the sample number of foreground and background set. S is the set of final sampled points, which contains more object points while still keeping the coverage rate of the whole scene. In our case, we sample most target points,(i.e., 87.5%) from the foreground set and a small number (i.e., 12.5%) from background set. For example, in downsample process of the 2 th SA layer (2048 ? 1024), ?, ? and ? are 1024, 896 and 128, respectively. We adopt the cross entropy loss for foreground segmentation. In inference, the confidence score is obtained by the margin between positive class and negative class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning Objective</head><p>The loss function consists of foreground biased sampling L fbs , voting regression L vote-reg , ray-based feature grouping L rbfg , objectness L obj-cls , bounding box estimation L box , and semantic classification L sem-cls losses.</p><formula xml:id="formula_16">L = ? vote-reg L vote-reg + ? fbs L fbs + ? rbfg L rbfg + ? obj-cls L obj-cls + ? box L box + ? sem-cls L sem-cls .<label>(12)</label></formula><p>Following the setting in VoteNet <ref type="bibr" target="#b28">[29]</ref>, we use the same label assignment and loss terms L vote-reg , L obj-cls , L box and L sem-cls . L fbs is a cross entropy loss used to supervise foreground sampling (see ?3.3). L rbfg is the sum loss of raybased feature grouping module defined as follows:</p><formula xml:id="formula_17">L rbfg = ? scale-reg L scale-reg + ? c-cls L c-cls + ? f-cls L f-cls . (13)</formula><p>As defined in ?3.2, L scale-reg is a smooth 1 loss, to explicitly supervise object scale of each proposal. L c-cls and L f-cls are both cross entropy losses, to supervise our model for querying valid point on each object surface. The detailed balancing factors are in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metric</head><p>We evaluate our method on two large-scale indoor 3D scene datasets, i.e., ScanNet V2 <ref type="bibr" target="#b7">[8]</ref> and SUN RGB-D <ref type="bibr" target="#b36">[37]</ref>, and we follow the standard data splits <ref type="bibr" target="#b28">[29]</ref> for both of them. SUN RGB-D [37] is a single-view RGB-D dataset for 3D scene understanding, which consists of 5K RGB-D training images annotated with the oriented 3D bounding boxes and the semantic labels for 10 categories. Following the standard data processing in <ref type="bibr" target="#b28">[29]</ref>, we convert the depth images to point clouds using the provided camera parameters. ScanNet V2 <ref type="bibr" target="#b7">[8]</ref> consists of richly-annotated 3D reconstructions of indoor scenes. It consists of 1513 training samples (reconstructed meshes converted to point clouds) with axisaligned bounding box labels for 18 object categories. Compared to SUN RGB-D, its scenes are larger and more complete with more objects. We sample point clouds from the reconstructed meshes by following <ref type="bibr" target="#b28">[29]</ref>.</p><p>For both datasets, the evaluation follows the same protocol as in VoteNet <ref type="bibr" target="#b28">[29]</ref> using mean average precision(mAP) under different IoU thresholds, i.e., 0.25 and 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details.</head><p>Network Architecture Details. For each 3D scene in the training set, we subsample 50000 points from the scene point cloud as the inputs. For the backbone and voting layers, we follow the same network structure of <ref type="bibr" target="#b28">[29]</ref>, but replace FPS with our proposed Foreground Biased Sampling (FBS) in 2 nd ? 4 th SA layers. More network details about other parts are given in Appendix. Training Scheme. Our network is end-to-end optimized by using the AdamW optimizer with the batch size 8 per-GPU and initial learning rate of 0.006 for ScanNet V2 and 0.004 for SUN RGB-D. We train the network for 360 epochs on both datasets, and the initial learning rate is decayed by 10x at the 240-th epoch and the 330-th epoch. The gradnorm clip <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b15">16]</ref> is applied to stabilize the training dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with state-of-the-art methods.</head><p>For performance benchmarking, we compare with a wide range state-of-the-art methods on ScanNet V2 and   <ref type="table">Table 3</ref>: 3D object detection results on the SUN-RGB-D <ref type="bibr" target="#b36">[37]</ref> val set. The main comparison is also based on the best results of multiple experiments between different methods. ?: reported by <ref type="bibr" target="#b20">[21]</ref>, which is better than the official paper. *: use RGB as addition inputs and our method is geometric only.</p><p>SUN RGB-D. We follow the previous work <ref type="bibr" target="#b20">[21]</ref> and also report both best results and average results. ScanNet V2. The results are summarized in <ref type="table" target="#tab_3">Table 2</ref>. With the same backbone network of a standard PointNet++, our approach achieves 70.2 mAP@0.25 and 54.2 mAP@0.5 using 66 rays and 256 object candidates, which is 2.5 and 3.3 better than previous best methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b6">7]</ref> using the same backbones. With stronger backbones and more sampled object candidates just like <ref type="bibr" target="#b20">[21]</ref>, i.e., 2? more channels and 512 candidates, our approach is also improved dramatically, achieving 70.6 mAP@0.25 and 55.2 mAP@0.5, which is still 1.5 and 2.4 better than <ref type="bibr" target="#b20">[21]</ref>. Notably, we also only use geometric input (point cloud) as previous works did. SUN RGB-D. We also evaluate our RBGNet against several competing approaches on SUN RGB-D dataset, which     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies and Discussions</head><p>In this section, a set of ablative studies are conducted on ScanNet V2 dataset, to investigate effectiveness of essential components of our algorithm. We follows <ref type="bibr" target="#b20">[21]</ref> and report the average performance of 25 trials by default. Effect of ray-based representation. We first ablate the effects of ray-based feature grouping in <ref type="table" target="#tab_5">Table 4</ref>, 5 and 6. As evidenced in the first three rows in <ref type="table" target="#tab_5">Table 4</ref>, with ray-based feature grouping, our model performs better, i.e., 66.2 ? 69.0, 48.2 ? 52.9 on mAP@0.25 and mAP@0.5. Note that our model is implemented based on a strong baseline. The first row in <ref type="table" target="#tab_5">Table 4</ref> is actually the VoteNet <ref type="bibr" target="#b28">[29]</ref> with corner loss regularization, vote sampling in vote aggregation layer and optimized hyper-parameters. Even on such a strong baseline (almost close to state-of-the-art already, 66.2 vs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT Ours</head><p>Group-free VoteNet chair sofa  Our ray-based feature grouping module also works well in a wide range of hyper-parameters, such as the number of rays. <ref type="table" target="#tab_6">Table 5</ref> shows its performance with different ray number. More rays can bring significant performance improvement, especially in the mAP@0.5. Compared with the setting without any rays, our ray-102 model performs much better on mAP@0.25 and mAP@0.5 by 2.8 and 4.9, respectively. For the recall of object points, the second column shows that more rays can capture surface points more completely. Considering the trade-off between memory usage and performance improvement, our model finally adopts the variant with 66 rays though more rays is better.</p><p>To further demonstrate the effectiveness of ray-based feature grouping module, we refer several grouping strategies in 3D object detection as baselines and compare with them. For a fair comparison, we only switch the feature aggregation mechanism while all other settings remain unchanged. <ref type="table" target="#tab_7">Table 6</ref> shows that our approach achieves more reliable detection results than others with a remarkable margin (1.5 on mAP@0.25 and 3.1 on mAP@0.5).</p><p>Effect of foreground biased sampling. <ref type="table" target="#tab_5">Table 4</ref> also demonstrates the effectiveness of the foreground biased sampling strategy. We can observe that, it improves the performance in both settings with and without feature grouping module. This verifies the necessity of sampling more foreground points for 3D object detection tasks. To further ablate the effectiveness of FBS, we compare the foreground points recall of 2 nd ? 4 th SA layers among different subsampling methods in <ref type="table" target="#tab_8">Table 7</ref>. Our sampling strategy draws better performance with a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Inference Speed.</head><p>The realistic inference speed of our method is competitive with other state-of-the-art methods. For a fair comparison, all experiments are run on the same workstation (single NVIDIA Tesla V100 GPU, 256G RAM, and Xeon E5-2650 v3). The results are shown in <ref type="table">Table.</ref> 8. Our method achieves better performance with a competitive speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented the RBGNet, a novel framework for 3D object detection from point clouds. We propose the ray-base feature grouping module, which can encode object surface geometry with determined rays and learn better geometric features to boost the performance of point-based 3D detectors. We also introduce the foreground biased sampling to sample more points on object surface while keeping the coverage rate for the whole scene. All of the above designs enable our model to achieve state-of-theart performance on ScanNet V2 and SUN RGB-D benchmarks with remarkable performance gains.</p><p>In the supplementary material, we first elaborate on fine sampling ( ?A). Then, more implementation details and raybased representation discussion are provided in ?B and ?C. Finally, we present per-category evaluation, visualization of positive anchor points and quantitative results from ?D.1 to ?D.3. We also discuss the limitation of RBGNet in ?E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details on Fine Sampling</head><p>In this section, we will provide more technical details on fine sampling. After obtaining coarse point masks,</p><formula xml:id="formula_18">M (c) = {m (c)</formula><p>n,k } Kc,N k=1,n=1 , we generate fine anchor points biased towards the dense part of its corresponding object. To achieve this goal, we apply inverse transform sampling to uniformly generate K f anchor points set</p><formula xml:id="formula_19">Q (f ) = {q (f ) k } K f k=1</formula><p>on positive regions of the n th ray (we remove the subscript n of Q (f ) n for simplicity) based on the predicted coarse point masks {m</p><formula xml:id="formula_20">(c) k } Kc k=1 .</formula><p>To be specific, we first normalize the coarse point masks asm</p><formula xml:id="formula_21">(c) k = m (c) k / Kc j=1 m (c) j</formula><p>to produce a piecewiseconstant probability density function (PDF). Then we translate it into the cumulative distribution function (CDF). Finally, sampling with the CDF at uniform steps concentrates samples around regions with positive coarse point masks.</p><p>To further illustrate it, we provide a demo case and visualize it in <ref type="figure">Fig.6</ref>. The number of coarse and fine anchor points on each ray is 8 and 10 respectively.</p><p>? The predicted coarse point masks of n th ray are: </p><p>? We compute the piece-wise PDF by normalizing m ? Finally, sample 10 points based on the CDF at uniform steps and inverse them to original distribution. As demonstration in <ref type="figure">Fig.6</ref>, the relative distance of fine points from object center are as follows: </p><p>B. Implementation Details.</p><p>B.1. RBGNet architecture details.</p><p>As mentioned in the main paper, the RBGNet architecture consists of a backbone with foreground biased sampling, a voting layer, a ray-based feature grouping module and a proposal module.  The backbone network, based on the PointNet++ architecture, has four set abstraction layers and two feature upsampling layers. We follow the same layer parameters (e.g. ball-region radius, number of sample points and MLP channels) as VoteNet <ref type="bibr" target="#b28">[29]</ref>. To sample points biased towards object surface, we append a segmentation head for estimating the foreground confidence of each point. The detailed layer parameters are shown in <ref type="table" target="#tab_12">Table 9</ref>. The voting module is the same as VoteNet. Note that, in training stage, we generate M proposals from the votes by vote FPS (samples M clusters based on votes' XYZ), in test stage, we apply vote FPS on ScanNet V2 and seed FPS on SUN RGB-D (sample on seed XYZ and then find the votes corresponding to the sampled seeds).</p><p>The ray-based feature grouping module consists of two parts, Ray Point Generation and Feature Enhancement by Determined Rays. After generating a set of vote cluster centers {c i } M i=1 based on the vote sampling and grouping, where c i = [v i , f i ] (the vote center position v i ? R 3 and its corresponding features f i ? R 128 ), M = 256 (the number of vote clusters). In our case, for each vote cluster, 66 rays are emitted uniformly from the cluster center with the determined angles and lengths generated in ?3.2.1. We use a MLP <ref type="bibr">[128,</ref><ref type="bibr">128]</ref> to regress the object scale of each cluster. As for the coarse-to-fine anchor point generation step, the number of coarse points (K c ) is 5 and fine points (K f ) is 3. To extract local feature of each anchor point, we apply two SA layers (coarse and fine), to aggregate the features of these seed points within a fixed radius (r = 0.2m) surrounding the query points. The two SA layers both have a receptive field specified by r=0.    The proposal module is a two-layer MLP <ref type="bibr">[128,</ref><ref type="bibr">128]</ref>. We follow <ref type="bibr" target="#b28">[29]</ref> on how to estimate the 3D bounding boxes, except for size prediction that we adopts class-agnostic head to regress bounding box size directly. The layer's output has 5+2NH+3+NC where the first five channels are for objectness classification and center regression (relative to the vote cluster center), 2NH channels are for heading bins classification and offsets regression, 3 is the scale regression for height, width and length, NC is the number of semantic classes. In SUN RGB-D: NH = 12, NC = 10, and in Scan-Net: NH = 1, NC = 18, due to the axis aligned bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. RBGNet loss function details.</head><p>As mentioned in the main paper, our model is trained end-to-end with a multi-task loss including foreground biased sampling L fbs , voting regression L vote-reg , ray-based feature grouping L rbfg , objectness L obj-cls , bounding box estimation L box , and semantic classification L sem-cls losses.</p><formula xml:id="formula_24">L = ? vote-reg L vote-reg + ? fbs L fbs + ? rbfg L rbfg + ? obj-cls L obj-cls + ? box L box + ? sem-cls L sem-cls .<label>(18)</label></formula><p>Following the setting in VoteNet <ref type="bibr" target="#b28">[29]</ref>, we use the same loss terms L vote-reg , L obj-cls , L box and L sem-cls , but L box is class-agnostic and contains an additional corner loss defined in <ref type="bibr" target="#b29">[30]</ref> for accurate bounding box estimation,</p><formula xml:id="formula_25">L box = ? size-reg L size-reg + ? corner L corner + ? angle-cls L angle-cls + ? angle-reg L angle-reg .<label>(19)</label></formula><p>As discussed in ?3.4, L fbs is a cross entropy loss used to supervise foreground sampling (see ?3.2). L rbfg is the sum loss of ray-based feature grouping module defined as follows:</p><formula xml:id="formula_26">L rbfg = ? scale-reg L scale-reg + ? c-cls L c-cls + ? f-cls L f-cls . (20)</formula><p>The balancing factors are set default as ? vote-reg =10.0, ? fbs =3.0, ? rbfg =10.0, ? obj-cls =5.0, ? box =10.0, ? sem-cls =1.0, ? size-reg =0.11, ? corner =0.33, ? angle-cls =0.1, ? angle-reg =0.11, ? scale-reg =0.11, ? c-cls =0.2 and ? f-cls =0.2. L scale-reg , ? size-reg and ? angle-reg are all the smooth 1 loss and their betas are 0.0625, 0.0625, 0.0400 separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Other Grouping Mechanisms</head><p>To further ablate the effectiveness of our ray-based feature grouping module, we refer several grouping strategies in 3D object detection as baselines and compare with them in ?4.4. For a fair comparison, we only switch the feature aggregation mechanism while all other settings remain unchanged (e.g. backbone with FBS, vote-FPS, proposal module). Here we give some detailed descriptions. Voting. The voting mechanism is first introduced by VoteNet <ref type="bibr" target="#b28">[29]</ref>. In our implementation, it is actually the VoteNet equipped with FBS, corner loss, vote-FPS in test stage and optimized hyperparameters.     <ref type="bibr" target="#b6">[7]</ref>. In our implementation, it is similar to RoI-Pooling described above, except that the prediction of bounding boxes is replaced by the maximum offsets of 6 directions. And then the points are aggregated by the balls uniformly sampled along the rays to enhance 3D bounding box estimation. Group-free. We replace the ray-based feature grouping module with a transformer network adopted in Groupfree <ref type="bibr" target="#b20">[21]</ref>. For a fair comparison, we use vote-FPS for initial object candidate sampling instead of KPS. Then, the same as group-free <ref type="bibr" target="#b20">[21]</ref>, we adopt the transformer as the decoder to leverage all the seed points to compute the object feature of each candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ray-based Representation Discussion</head><p>There are many choices for anchor point generation, such as classification <ref type="bibr" target="#b21">[22]</ref> and regression <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>? Regression: Given the center and surface points of an object, they want to represent the shape by polar coordi-nates. The length of n rays can be computed easily. Then, the model regresses the length of each ray and captures the points when the ray terminates somewhere. ? Classification: Given an instance, model predicts far bounds of all rays and samples a fixed number of potential query points on each ray, and then extracts local features and classifies those points whether belong to corresponding object to generate reasonable anchor points. Our model adopts this way and we will discuss why do we choose it.</p><p>In 2D perception community, some methods also represent object shape by rays <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b43">44]</ref> in regression way, which applies the angle and distance as the coordinate to locate points. However, due to the particular property of point clouds, this regression pipeline has many problems in 3D scenario, i.e., i) center is outside of the object, no intersection with the object surface at some angles, ii) limited expressive ability on concave shape, one ray may has multiple intersections. Compare to regression, classification pipeline is more reasonable to represent point clouds and doesn't have the above problems, so we choose it to generate anchor points. In <ref type="table" target="#tab_6">Table 15</ref>, we show the results of the two representations, classification approach performs better than regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Per-class Evaluation</head><p>We evaluate per-category on ScanNet V2 and SUN RGB-D under different IoU thresholds. <ref type="table" target="#tab_14">Table 10</ref> and <ref type="table" target="#tab_15">Table 11</ref> report the results on 18 classes of ScanNetV2 with 0.25 and 0.5 box IoU thresholds respectively. <ref type="table" target="#tab_18">Table 13</ref> and <ref type="table" target="#tab_5">Table 14</ref> show the results on 10 classes of SUN RGB-D with 0.25 and 0.5 box IoU thresholds. Our approach outperforms the baseline VoteNet <ref type="bibr" target="#b28">[29]</ref> and prior state-of-theart methods Group-free <ref type="bibr" target="#b20">[21]</ref> significantly in almost every category. These improvements are achieved by using raybased feature grouping and foreground biased sampling to better encode object surface geometry. <ref type="figure" target="#fig_11">Fig.7</ref> shows the scores of coarse anchor points predicted from our RBGNet in a typical SUN RGB-D scene. We clearly see that the high responses are almost on the object surface (bed, chair etc.) while low responses are on the empty space or background surface. This verifies that our method can really learn the shape distribution and boost point-based 3D detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Visualization of Positive Anchor Points</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Quantitative Results</head><p>We provide more qualitative comparisons between our method and the top-performing reference methods, such as Group-free <ref type="bibr" target="#b20">[21]</ref> and VoteNet <ref type="bibr" target="#b28">[29]</ref>, on the ScanNet V2 and SUN RGB-D datasets. Please see <ref type="figure">Fig. 8</ref> for more qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Limitations</head><p>Although our method achieves promising performance on multiple datasets, there are still some limitations. Compared with the previous approaches, the performance of RBGNet is significantly better in the case of a large number of rays. However, there is a trade-off between computational cost and performance improvement, as shown in main paper. In the future, we hope to discover approaches that can encode surface geometry more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Group-free VoteNet <ref type="figure">Figure 8</ref>: Qualitative results on ScanNet V2(top) and SUN RGB-D(down). The baseline methods are Group-free <ref type="bibr" target="#b20">[21]</ref> and VoteNet <ref type="bibr" target="#b28">[29]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>3D object detection in point clouds with a raybased feature grouping module. Given the point clouds of a 3D scene, our model aggregates the point-wise features on object surface by a group of ordered rays to boost the performance of 3D object detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The RBGNet architecture for 3D object detection from point cloud. (a) Generating more foreground seed points and a number of rays emitted from object centers. (b) Object shape encoding by ordered rays and 3D bounding box estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Demonstration of spherical coordinate system and the distribution of 18 rays.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>is denoted as ? (c) n,k . Finally, we append a binary classification module for estimating the positive mask m (c) n,k of point q (c) n,k based on cluster feature f and local feature ? (c) n,k as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(c) and M (f ) of each anchor point, the local self-seed points other-seed points ray positive coarse point negative coarse point fine point</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of coarse-to-fine anchor point generation. Fine sampling is biased towards the dense part of its corresponding object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>} Kc k=1 , are firstly fused into a single ray feature r (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Kc k=1 = {0, 1, 0, 1, 0, 0, 1, 0}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Kc k=1 = {0., 1/3, 0., 1/3, 0., 0., 1/3, 0.}.(15)? Then we convert PDF to CDF, {C Kc k=1 = {0., 1/3, 1/3, 2/3, 2/3, 2/3, 1., 0.}.<ref type="bibr" target="#b15">(16)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>=</head><label></label><figDesc>{0.1625, 0.2000, 0.2375, 0.4000, 0.4375, 0.4750, 0.7625, 0.8000, 0.8375, 0.875} ? RayScale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results of shape distribution our model learned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2204.02251v1 [cs.CV] 5 Apr 2022 +13.31) 53.91 (+14.00) 71.69 (+8.79) 50.71 (+10.80) 71.27 (+8.39) 50.68 (+10.77)</figDesc><table><row><cell>GT-Features (explicit GT-center)</cell><cell>GT-Features (implicit GT-center)</cell><cell>FgSamp mAP@0.25</cell><cell>mAP@0.50</cell></row><row><cell></cell><cell></cell><cell>62.90</cell><cell>39.91</cell></row><row><cell></cell><cell></cell><cell>76.21 (</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">Performance comparison on the ScanNetV2 [8] val</cell></row><row><cell cols="4">set with state-of-the-art. The main comparison is based on</cell></row><row><cell cols="4">the best results of multiple experiments, the number within</cell></row><row><cell cols="4">the bracket is the average result of 25 trials.  ?: reported</cell></row><row><cell cols="4">by [21], which is better than the official paper. *: use RGB</cell></row><row><cell>as addition inputs.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SUN RGB-D</cell><cell>Backbone</cell><cell cols="2">mAP@0.25 mAP@0.5</cell></row><row><cell>F-PointNet [30]*</cell><cell>PointNet</cell><cell>54.0</cell><cell>-</cell></row><row><cell>ImVoteNet [28]*</cell><cell>PointNet++</cell><cell>63.4</cell><cell>-</cell></row><row><cell>MTC-RCNN [25]*</cell><cell>PointNet++</cell><cell>65.3 (64.7)</cell><cell>48.6 (48.2)</cell></row><row><cell>3Detr [23]</cell><cell>PointNet++</cell><cell>59.1</cell><cell>32.7</cell></row><row><cell>VoteNet [29] ?</cell><cell>PointNet++</cell><cell>59.1</cell><cell>35.8</cell></row><row><cell>MLCVNet [43]</cell><cell>PointNet++</cell><cell>59.8</cell><cell>-</cell></row><row><cell>HGNet [5]</cell><cell>GU-net</cell><cell>61.6</cell><cell>-</cell></row><row><cell>H3DNet [51]</cell><cell>4?PointNet++</cell><cell>60.1</cell><cell>39.0</cell></row><row><cell>BRNet [7]</cell><cell>PointNet++</cell><cell>61.1</cell><cell>43.7</cell></row><row><cell>VENet [42]</cell><cell>PointNet++</cell><cell>62.5</cell><cell>39.2</cell></row><row><cell>Group-free [21]</cell><cell>PointNet++</cell><cell>63.0(62.6)</cell><cell>45.2(44.4)</cell></row><row><cell>Our(R66, O256)</cell><cell>PointNet++</cell><cell>64.1(63.6)</cell><cell>47.2(46.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Effect of ray-based feature grouping module and foreground biased sampling.</figDesc><table><row><cell cols="4">number of ray objp recall mAP@0.25 mAP@0.5</cell></row><row><cell>0</cell><cell>-</cell><cell>67.1</cell><cell>49.0</cell></row><row><cell>6</cell><cell>38.1</cell><cell>68.4</cell><cell>51.6</cell></row><row><cell>18</cell><cell>63.3</cell><cell>68.7</cell><cell>52.0</cell></row><row><cell>38</cell><cell>75.8</cell><cell>69.2</cell><cell>52.7</cell></row><row><cell>66</cell><cell>78.1</cell><cell>69.6</cell><cell>53.6</cell></row><row><cell>102</cell><cell>86.5</cell><cell>69.9</cell><cell>53.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on the performance of ray-base feature grouping module with different ray number.</figDesc><table><row><cell>method</cell><cell cols="2">mAP@0.25 mAP@0.5</cell></row><row><cell>Voting [29]</cell><cell>67.1</cell><cell>49.0</cell></row><row><cell>RoI-Pooling [35]</cell><cell>67.6</cell><cell>49.9</cell></row><row><cell>Back-tracing [7]</cell><cell>67.7</cell><cell>50.1</cell></row><row><cell>Group-free [21]</cell><cell>68.1</cell><cell>50.5</cell></row><row><cell>Our(R66)</cell><cell>69.6</cell><cell>53.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison with other grouping-based methods.</figDesc><table><row><cell>method</cell><cell>dataset</cell><cell cols="3">2 nd (1024) 3 rd (512) 4 th (256)</cell></row><row><cell>FPS [29]</cell><cell>ScanNet V2</cell><cell>31.1</cell><cell>30.8</cell><cell>30.3</cell></row><row><cell>F-FPS [48]</cell><cell>ScanNet V2</cell><cell>40.4</cell><cell>42.1</cell><cell>43.8</cell></row><row><cell>Ours</cell><cell>ScanNet V2</cell><cell>51.2</cell><cell>73.2</cell><cell>87.8</cell></row><row><cell>FPS [29]</cell><cell>SUN RGB-D</cell><cell>17.9</cell><cell>17.8</cell><cell>17.7</cell></row><row><cell cols="2">F-FPS [48] SUN RGB-D</cell><cell>21.3</cell><cell>22.9</cell><cell>23.5</cell></row><row><cell>Ours</cell><cell>SUN RGB-D</cell><cell>30.8</cell><cell>45.3</cell><cell>65.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Foreground percentage statistics on PointNet++ of different sampling approaches.</figDesc><table><row><cell>is also a standard benchmark for 3D object detection. The</cell></row><row><cell>results are summarized in Table 3. Our base model with</cell></row><row><cell>standard PointNet++ achieves 64.1 on mAP@0.25 and 47.2</cell></row><row><cell>on mAP@0.5, which outperforms all previous state-of-the-</cell></row><row><cell>arts point-only methods.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>table door</head><label>door</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>window</cell></row><row><cell></cell><cell></cell><cell></cell><cell>garbagebin</cell></row><row><cell></cell><cell></cell><cell></cell><cell>picture</cell></row><row><cell></cell><cell></cell><cell></cell><cell>desk</cell></row><row><cell></cell><cell></cell><cell></cell><cell>table</cell></row><row><cell></cell><cell></cell><cell></cell><cell>chair</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bookshelf</cell></row><row><cell></cell><cell></cell><cell></cell><cell>desk</cell></row><row><cell cols="4">Figure 5: Qualitative results on ScanNet V2 (top) and SUN RGB-D (bottom). The baseline methods are Group-free [21] and</cell></row><row><cell cols="4">VoteNet [29]. Our method can generate high-quality and compact bounding boxes compared with other methods.</cell></row><row><cell>method</cell><cell cols="3">mAP@0.25 mAP@0.5 frames/s</cell></row><row><cell>MLCVNet [43]</cell><cell>64.5</cell><cell>41.4</cell><cell>5.37</cell></row><row><cell>BRNet [7]</cell><cell>66.1</cell><cell>50.9</cell><cell>7.37</cell></row><row><cell>H3DNet [51]</cell><cell>67.2</cell><cell>48.1</cell><cell>3.75</cell></row><row><cell>Group-free [21]</cell><cell>67.3</cell><cell>48.9</cell><cell>6.64</cell></row><row><cell>Our(R6)</cell><cell>69.0</cell><cell>52.3</cell><cell>7.23</cell></row><row><cell>Our(R18)</cell><cell>69.0</cell><cell>52.6</cell><cell>5.70</cell></row><row><cell>Our(R38)</cell><cell>69.7</cell><cell>53.3</cell><cell>5.27</cell></row><row><cell>Our(R66)</cell><cell>70.2</cell><cell>54.2</cell><cell>4.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Comparison on realistic inference speed on Scan-Net V2. Note that mAP@0.25 and 0.5 are the best results of multiple experiments. 66.6 [21] on mAP@0.25), ray-based feature grouping module still boosts our model with a remarkable improvement.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /><note>Backbone network architecture: FBS parameters.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>90.79 90.07 90.78 60.22 53.83 43.71 55.56 12.38 66.85 66.02 52.37 52.05 63.94 97.40 52.32 92.57 43.37 62.90 MLCVNet [43] 42.45 88.48 89.98 87.40 63.50 56.93 46.98 56.94 11.94 63.94 76.05 56.72 60.86 65.91 98.33 59.18 87.22 47.89 64.48 BRNet [7] 49.90 88.30 91.90 86.90 69.30 59.20 45.90 52.10 15.30 72.00 76.80 57.10 60.40 73.60 93.80 58.80 92.20 47.10 66.10 H3DNet* [51] 49.40 88.60 91.80 90.20 64.90 61.00 51.90 54.90 18.60 62.00 75.90 57.30 57.20 75.30 97.90 67.40 92.50 53.60 67.20 Group-free [21] 55.40 86.60 91.80 86.60 73.30 54.50 49.40 47.70 13.10 63.30 82.40 63.30 53.20 74.00 99.20 67.70 91.70 55.80 67.20 Ours 52.62 91.34 93.07 89.71 73.57 60.10 51.96 53.53 20.01 72.65 82.57 63.58 59.79 76.03 99.28 74.79 92.67 55.88 70.20</figDesc><table><row><cell></cell><cell>cab</cell><cell>bed</cell><cell>chair</cell><cell>sofa</cell><cell>tabl</cell><cell>door</cell><cell>wind bkshf</cell><cell>pic</cell><cell>cntr</cell><cell>desk</cell><cell>curt</cell><cell>frig</cell><cell>showr</cell><cell>toil</cell><cell>sink</cell><cell>bath</cell><cell>ofurn</cell><cell>mAP</cell></row><row><cell>VoteNet [29]</cell><cell>47.87</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">2m, a MLP[128, 64, 32]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">for feature transform. But coarse layer samples 8 points by</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">ball query operation and fine layer is 4 points. In term of</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>3D object detection scores per category on the ScanNetV2 dataset, evaluated with mAP@0.25 IoU. * means that H3DNet [51] only provide the checkpoint with 4 PointNet++ backbones. 77.85 73.11 80.49 46.54 25.09 15.98 41.85 2.50 22.34 33.35 25.02 31.04 17.58 87.75 23.05 81.60 18.66 39.91 H3DNet* [51] 20.50 79.70 80.10 79.60 56.20 29.00 21.30 45.50 4.20 33.50 50.60 37.30 41.40 37.00 89.10 35.10 90.20 35.40 48.10 Group-free [21] 23.80 77.20 81.60 65.10 62.80 35.00 21.30 39.40 7.00 33.10 66.30 39.30 43.90 47.00 91.20 38.50 85.10 37.40 49.70 BRNet [7] 28.70 80.60 81.90 80.60 60.80 35.50 22.20 48.00 7.50 43.70 54.80 39.10 51.80 35.90 88.90 38.70 84.40 33.00 50.90 Ours 30.69 80.95 86.48 84.82 66.45 40.37 29.59 48.60 7.96 44.76 59.14 40.83 44.80 39.78 92.92 45.30 90.90 41.49 54.21</figDesc><table><row><cell></cell><cell>cab</cell><cell>bed</cell><cell>chair</cell><cell>sofa</cell><cell>tabl</cell><cell>door</cell><cell>wind bkshf</cell><cell>pic</cell><cell>cntr</cell><cell>desk</cell><cell>curt</cell><cell>frig</cell><cell>showr</cell><cell>toil</cell><cell>sink</cell><cell>bath</cell><cell>ofurn</cell><cell>mAP</cell></row><row><cell>VoteNet [29]</cell><cell>14.62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>3D object detection scores per category on the ScanNetV2 dataset, evaluated with mAP@0.50 IoU. * means that H3DNet<ref type="bibr" target="#b50">[51]</ref> only provide the checkpoint with 4 PointNet++ backbones.</figDesc><table><row><cell cols="4">layer name Input channel MLP Channels</cell></row><row><cell>F</cell><cell>(c) point</cell><cell>5?32</cell><cell>[32,]</cell></row><row><cell>F</cell><cell>(f ) point</cell><cell>3?32</cell><cell>[32,]</cell></row><row><cell cols="2">F ray (c)</cell><cell>66?32</cell><cell>[256, 128]</cell></row><row><cell cols="2">F ray (f )</cell><cell>66?32</cell><cell>[256, 128]</cell></row><row><cell cols="2">F f use</cell><cell>256</cell><cell>[256, 128]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>Detailed layer parameters of Feature Enhance module. In our case, the number of rays (N ), coarse points (K c ) and fine points (K f ) are 66, 5, and 3.the point mask prediction, we use a MLP[32+128, 32, 2] to estimate the positive mask based on corresponding cluster features and local features. In Feature Enhancement module, all the functions F * * are MLP networks. The detailed layer parameters are shown inTable 12.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>3D object detection scores per category on the SUN RGB-D dataset, evaluated with mAP@0.25 IoU. * means that H3DNet<ref type="bibr" target="#b50">[51]</ref> only provide the checkpoint with 4 PointNet++ backbones.</figDesc><table><row><cell></cell><cell>bathtub</cell><cell>bed</cell><cell cols="2">bookshelf chair</cell><cell cols="3">desk dresser nightstand</cell><cell>sofa</cell><cell>table toilet</cell><cell>mAP</cell></row><row><cell>VoteNet [29]</cell><cell>45.40</cell><cell>53.40</cell><cell>6.80</cell><cell cols="2">56.50 5.90</cell><cell>12.00</cell><cell>38.60</cell><cell>49.10 21.30 68.50 35.80</cell></row><row><cell>H3DNet* [51]</cell><cell>47.60</cell><cell>52.90</cell><cell>8.60</cell><cell cols="2">60.10 8.40</cell><cell>20.60</cell><cell>45.60</cell><cell>50.40 27.10 69.10 39.00</cell></row><row><cell>BRNet [7]</cell><cell>55.50</cell><cell>63.80</cell><cell>9.30</cell><cell cols="2">61.60 10.00</cell><cell>27.30</cell><cell>53.20</cell><cell>56.70 28.60 70.90 43.70</cell></row><row><cell>Group-free [21]</cell><cell>64.00</cell><cell>67.10</cell><cell>12.40</cell><cell cols="2">62.60 14.50</cell><cell>21.90</cell><cell>49.80</cell><cell>58.20 29.20 72.20 45.20</cell></row><row><cell>Ours</cell><cell>65.74</cell><cell>68.02</cell><cell>12.99</cell><cell cols="2">65.46 12.81</cell><cell>25.84</cell><cell>54.89</cell><cell>59.55 32.38 74.50 47.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 :</head><label>14</label><figDesc>3D object detection scores per category on the SUN RGB-D dataset, evaluated with mAP@0.50 IoU. * means that H3DNet<ref type="bibr" target="#b50">[51]</ref> only provide the checkpoint with 4 PointNet++ backbones.</figDesc><table><row><cell>method</cell><cell cols="2">mAP@0.25 mAP@0.5</cell></row><row><cell>Reg-Ray(R66)</cell><cell>68.0</cell><cell>50.2</cell></row><row><cell>Our(R66)</cell><cell>69.6</cell><cell>53.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 15 :</head><label>15</label><figDesc>The average performance of different ray representations on ScanNet V2.RoI-Pooling. For a given object proposal, the points within the predicted box are aggregated together. We adopt the similar implementation with<ref type="bibr" target="#b6">[7]</ref>, predict the bounding boxes based on the voting cluster features and aggregate the points within the corresponding boxes by max-pooling. Finally, the aggregated features and cluster features are concatenated for 3D object detection.</figDesc><table /><note>Back-Tracing. It is first formulated in</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient interactive annotation of segmentation datasets with polygon-rnn++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amlan</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A survey of augmented reality. Presence: teleoperators &amp; virtual environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronald T Azuma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Ogale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A survey of augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Billinghurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gun</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hierarchical graph network for 3d object detection on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biwen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 4</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Back-tracing representative points for votingbased 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Voxel r-cnn: Towards high performance voxel-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyong</forename><surname>Wen Gang Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning to sample. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Dovrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itai</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Alireza Fathi, Bastian Leibe, and Matthias Nie?ner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Bokeloh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d-sis: 3d semantic instance segmentation of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical point-edge interaction network for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pointgroup: Dual-set point grouping for 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-convex distributionally robust optimization: Non-asymptotic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jikai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Samplenet: Differentiable point cloud sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itai</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Polytransform: Deep polygon transformer for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rui Hu, and Raquel Urtasun. Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Group-free 3d object detection via transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An endto-end transformer model for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive hierarchical down-sampling for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Nezhadarya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multi-modality task cascade for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunze</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.04013</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep snake for real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaijin</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Centerpoly: real-time instance segmentation using bounding polygons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hughes</forename><surname>Perreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume-Alexandre</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Saunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maguelonne</forename><surname>H?ritier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imvotenet: Boosting 3d object detection in point clouds with image votes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Pointvoxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pv-rcnn++: Point-voxel feature set abstraction with local vector representation for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00463</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Monocular plan view networks for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coline</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.01151</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Collaborative visual navigation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Polarmask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Venet: Voting enhancement network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dening</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mlcvnet: Multi-level context votenet for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Explicit shape encoding for real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fubo</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hdnet: Exploiting hd maps for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pixor: Realtime 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Point-based 3d single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Centerbased 3d object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improved analysis of clipping algorithms for non-convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jikai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">H3dnet: 3d object detection using hybrid geometric primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<title level="m">Cia-ssd: Confident iou-aware single-stage object detector from point cloud. AAAI, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sessd: Self-ensembling single-stage object detector from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Target-driven visual navigation in indoor scenes using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
