<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runsheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Zhengzhong</roleName><forename type="first">Tu</forename><surname>2?</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Google Research</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>V2X</term>
					<term>Vehicle-to-Everything</term>
					<term>Cooperative perception</term>
					<term>Au- tonomous driving</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we investigate the application of Vehicle-to-Everything (V2X) communication to improve the perception performance of autonomous vehicles. We present a robust cooperative perception framework with V2X communication using a novel vision Transformer. Specifically, we build a holistic attention model, namely V2X-ViT, to effectively fuse information across on-road agents (i.e., vehicles and infrastructure). V2X-ViT consists of alternating layers of heterogeneous multiagent self-attention and multi-scale window self-attention, which captures inter-agent interaction and per-agent spatial relationships. These key modules are designed in a unified Transformer architecture to handle common V2X challenges, including asynchronous information sharing, pose errors, and heterogeneity of V2X components. To validate our approach, we create a large-scale V2X perception dataset using CARLA and OpenCDA. Extensive experimental results demonstrate that V2X-ViT sets new state-of-the-art performance for 3D object detection and achieves robust performance even under harsh, noisy environments. The code is available at https://github.com/DerrickXuNu/v2x-vit.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>perspective with limited sight-of-view. To address these issues, recent studies <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b22">23]</ref> leverage the advantages of multiple viewpoints of the same scene by investigating Vehicle-to-Vehicle (V2V) collaboration, where visual information (e.g., detection outputs, raw sensory information, intermediate deep learning features, details see Sec. 2) from multiple nearby AVs are shared for a complete and accurate understanding of the environment. Although V2V technologies have the prospect to revolutionize the mobility industry, it ignores a critical collaborator -roadside infrastructure. The presence of AVs is usually unpredictable, whereas the infrastructure can always provide supports once installed in key scenes such as intersections and crosswalks. Moreover, infrastructure equipped with sensors on an elevated position has a broader sight-of-view and potentially less occlusion. Despite these advantages, including infrastructure to deploy a robust V2X perception system is non-trivial. Unlike V2V collaboration, where all agents are homogeneous, V2X systems often involve a heterogeneous graph formed by infrastructure and AVs. The configuration discrepancies between infrastructure and vehicle sensors, such as types, noise levels, installation height, and even sensor attributes and modality, make the design of a V2X perception system challenging. Moreover, the GPS localization noises and the asynchronous sensor measurements of AVs and infrastructure can introduce inaccurate coordinate transformation and lagged sensing information. Failing to properly handle these challenges will make the system vulnerable.</p><p>In this paper, we introduce a unified fusion framework, namely V2X Vision Transformer or V2X-ViT, for V2X perception, that can jointly handle these challenges. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the entire system. AVs and infrastructure capture, encode, compress, and send intermediate visual features with each other, while the ego vehicle (i.e., receiver) employs V2X-Transformer to perform information fusion for object detection. We propose two novel attention modules to accommodate V2X challenges: 1) a customized heterogeneous multi-agent self-attention module that explicitly considers agent types (vehicles and infrastructure) and their connections when performing attentive fusion; 2) a multi-scale window attention module that can handle localization errors by using multi-resolution windows in parallel. These two modules will adaptively iteratively fuse visual features to capture inter-agent interaction and per-agent spatial relationship, correcting the feature misalignment caused by localization error and time delay. Moreover, we also integrate a delay-aware positional encoding to handle the time delay uncertainty further. Notably, all these modules are incorporated in a single transformer that learns to address these challenges end-to-end.</p><p>To evaluate our approach, we collect a new large-scale open dataset, namely V2XSet, that explicitly considers real-world noises during V2X communication using the high-fidelity simulator CARLA <ref type="bibr" target="#b9">[10]</ref>, and a cooperative driving automation simulation tool OpenCDA. <ref type="figure" target="#fig_0">Fig. 1</ref> shows a data sample in the collected dataset. Experiments show that our proposed V2X-ViT significantly advances the performance on V2X LiDAR-based 3D object detection, achieving a 21.2% gain of AP compared to single-agent baseline and performing favorably against leading intermediate fusion methods by at least 7.3%. Our contributions are:</p><p>-We present the first unified transformer architecture (V2X-ViT) for V2X perception, which can capture the heterogeneity nature of V2X systems with strong robustness against various noises. Moreover, the proposed model achieves state-of-the-art performance on the challenging cooperative detection task. 2 Related work V2X perception. Cooperative perception studies how to efficiently fuse visual cues from neighboring agents. Based on its message sharing strategy, it can be divided into 3 categories: 1) early fusion <ref type="bibr" target="#b5">[6]</ref> where raw data is shared and gathered to form a holistic view, 2) intermediate fusion <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b4">5]</ref> where intermediate neural features are extracted based on each agent's observation and then transmitted, and 3) late fusion <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> where detection outputs (e.g., 3D bounding box position, confidence score) are circulated. As early fusion usually requires large transmission bandwidth and late fusion fails to provide valuable scenario context <ref type="bibr" target="#b44">[45]</ref>, intermediate fusion has attracted increasing attention because of its good balance between accuracy and transmission bandwidth. Several intermediate fusion methods have been proposed for V2V perception recently. OPV2V <ref type="bibr" target="#b49">[50]</ref> implements a single-head self-attention module to fuse features, while F-Cooper employs maxout <ref type="bibr" target="#b14">[15]</ref> fusion operation. V2VNet <ref type="bibr" target="#b44">[45]</ref> proposes a spatial-aware message passing mechanism to jointly reason detection and prediction. To attenuate outlier messages, <ref type="bibr" target="#b40">[41]</ref> regresses vehicles' localization errors with consistent pose constraints. DiscoNet <ref type="bibr" target="#b24">[25]</ref> leverages knowledge distillation to enhance training by constraining the corresponding features to the ones from the network for early fusion. However, intermediate fusion for V2X is still in its infancy. Most V2X methods explored late fusion strategies to aggregate information from infrastructure and vehicles. For example, a late fusion two-level Kalman filter is proposed by <ref type="bibr" target="#b30">[31]</ref> for roadside infrastructure failure conditions. Xiangmo et al . <ref type="bibr" target="#b57">[58]</ref> propose fusing the lane mark detection from infrastructure and vehicle sensors, leveraging Dempster-Shafer theory to model the uncertainty.</p><p>LiDAR-based 3D object detection. Numerous methods have been explored to extract features from raw points, voxels, bird-eye-view (BEV) images, and their mixtures. PointRCNN <ref type="bibr" target="#b35">[36]</ref> proposes a two-stage strategy based on raw point clouds, which learns rough estimation in the first stage and then refines it with semantic attributes. The authors of <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b50">51]</ref> propose to split the space into voxels and produce features per voxel. Despite having high accuracy, their inference speed and memory consumption are difficult to optimize due to reliance on 3D convolutions. To avoid computationally expensive 3D convolutions, <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b51">52]</ref> propose an efficient BEV representation. To satisfy both computational and flexible receptive field requirements, <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b52">53]</ref> combine voxel-based and pointbased approaches to detect 3D objects.</p><p>Transformers in vision. The Transformer <ref type="bibr" target="#b42">[43]</ref> is first proposed for machine translation <ref type="bibr" target="#b42">[43]</ref>, where multi-head self-attention and feed-forward layers are stacked to capture long-range interactions between words. Dosovitskiy et al . <ref type="bibr" target="#b8">[9]</ref> present a Vision Transformer (ViT) for image recognition by regarding image patches as visual words and directly applying self-attention. The full selfattention in ViT <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>, despite having global interaction, suffers from heavy computational complexity and does not scale to long-range sequences or highresolution images. To ameliorate this issue, numerous methods have introduced locality into self-attention, such as Swin <ref type="bibr" target="#b29">[30]</ref>, CSwin <ref type="bibr" target="#b7">[8]</ref>, Twins <ref type="bibr" target="#b6">[7]</ref>, window <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b38">39]</ref>, and sparse attention <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49]</ref>. A hierarchical architecture is usually adopted to progressively increase the receptive fields for capturing longer dependencies.</p><p>While these vision transformers have proven efficient in modeling homogeneous structured data, their efficacy to represent heterogeneous graphs has been less studied. One of the developments related to our work is the heterogeneous graph transformer (HGT) <ref type="bibr" target="#b17">[18]</ref>. HGT was originally designed for web-scale Open Academic Graph where the nodes are text and attributes. Inspired by HGT, we build a customized heterogeneous multi-head self-attention module tailored for graph attribute-aware multi-agent 3D visual feature fusion, which is able to capture the heterogeneity of V2X systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this paper, we consider V2X perception as a heterogeneous multi-agent perception system, where different types of agents (i.e., smart infrastructure and AVs) perceive the surrounding environment and communicate with each other. To simulate real-world scenarios, we assume that all the agents have imperfect localization and time delay exists during feature transmission. Given this, our goal is to develop a robust fusion system to enhance the vehicle's perception capability and handle these aforementioned challenges in a unified end-to-end fashion. The overall architecture of our framework is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, which includes five major components: 1) metadata sharing, 2) feature extraction, 3) compression and sharing, 4) V2X vision Transformer, and 5) a detection head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main architecture design</head><p>V2X metadata sharing. During the early stage of collaboration, every agent i ? {1 . . . N } within the communication networks shares metadata such as poses, extrinsics, and agent type c i ? {I, V } (meaning infrastructure or vehicle) with each other. We select one of the connected AVs as the ego vehicle (e) to construct a V2X graph around it where the nodes are either AVs or infrastructure and the edges represent directional V2X communication channels. To be more specific, we assume the transmission of metadata is well-synchronized, which means each agent i can receive ego pose x ti e at the time t i . Upon receiving the pose of the ego vehicle, all the other connected agents nearby will project their own LiDAR point clouds to the ego-vehicle's coordinate frame before feature extraction. Feature extraction. We leverage the anchor-based PointPillar method <ref type="bibr" target="#b21">[22]</ref> to extract visual features from point clouds because of its low inference latency and optimized memory usage <ref type="bibr" target="#b49">[50]</ref>. The raw point clouds will be converted to a stacked pillar tensor, then scattered to a 2D pseudo-image and fed to the PointPillar backbone. The backbone extracts informative feature maps F ti i ? R H?W ?C , denoting agent i's feature at time t i with height H, width W , and channels C. Compression and sharing. To reduce the required transmission bandwidth, we utilize a series of 1?1 convolutions to progressively compress the feature maps along the channel dimension. The compressed features with the size (H, W, C ? ) (where C ? ? C) are then transmitted to the ego vehicle (e), on which the features are projected back to (H, W, C) using 1 ? 1 convolutions.</p><p>There exists an inevitable time gap between the time when the LiDAR data is captured by connected agents and when the extracted features are received by the ego vehicle (details see appendix). Thus, features collected from surrounding   agents are often temporally misaligned with the features captured on the ego vehicle. To correct this delay-induced global spatial misalignment, we need to transform (i.e., rotate and translate) the received features to the current egovehicle's pose. Thus, we leverage a spatial-temporal correction module (STCM), which employs a differential transformation and sampling operator ? ? to spatially warp the feature maps <ref type="bibr" target="#b18">[19]</ref>. An ROI mask is also calculated to prevent the network from paying attention to the padded zeros caused by the spatial warp.</p><formula xml:id="formula_0">I-V I-V V-I V-I V-V V-V V-V V-V I-I k</formula><formula xml:id="formula_1">V2X-ViT. The intermediate features H i = ? ? F ti i</formula><p>? R H?W ?C aggregated from connected agents are fed into the major component of our framework i.e., V2X-ViT to conduct an iterative inter-agent and intra-agent feature fusion using self-attention mechanisms. We maintain the feature maps in the same level of high resolution throughout the entire Transformer as we have observed that the absence of high-definition features greatly harms the objection detection performance. The details of our proposed V2X-ViT will be unfolded in Sec. 3.2. Detection head. After receiving the final fused feature maps, we apply two 1?1 convolution layers for box regression and classification. The regression output is (x, y, z, w, l, h, ?), denoting the position, size, and yaw angle of the predefined anchor boxes, respectively. The classification output is the confidence score of being an object or background for each anchor box. We use the smooth ? 1 loss for regression and a focal loss <ref type="bibr" target="#b27">[28]</ref> for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">V2X-Vision Transformer</head><p>Our goal is to design a customized vision Transformer that can jointly handle the common V2X challenges. Firstly, to effectively capture the heterogeneous graph representation between infrastructure and AVs, we build a heterogeneous multiagent self-attention module that learns different relationships based on node and edge types. Moreover, we propose a novel spatial attention module, namely multi-scale window attention (MSwin), that captures long-range interactions at various scales. MSwin uses multiple window sizes to aggregate spatial information, which greatly improves the detection robustness against localization errors. Lastly, these two attention modules are integrated into a single V2X-ViT block in a factorized manner (illustrated in <ref type="figure" target="#fig_3">Fig. 3a</ref>), enabling us to maintain highresolution features throughout the entire process. We stack a series of V2X-ViT blocks to iteratively learn inter-agent interaction and per-agent spatial attention, leading to a robust aggregated feature representation for detection.</p><p>Heterogeneous multi-agent self-attention The sensor measurements captured by infrastructure and AVs possibly have distinct characteristics. The infrastructure's LiDAR is often installed at a higher position with less occlusion and different view angles. In addition, the sensors may have different levels of sensor noise due to maintenance frequency, hardware quality etc. To encode this heterogeneity, we build a novel heterogeneous multi-agent self-attention (HMSA) where we attach types to both nodes and edges in the directed graph. To simplify the graph structure, we assume the sensor setups among the same category of agents are identical. As shown in <ref type="figure" target="#fig_3">Fig. 3b</ref>, we have two types of nodes and four types of edges, i.e., node type c i ? {I, V } and edge type</p><formula xml:id="formula_2">? (e ij ) ? {V ?V, V ?I, I ?V, I ?I}.</formula><p>Note that unlike traditional attention where the node features are treated as a vector, we only reason the interaction of features in the same spatial position from different agents to preserve spatial cues. Formally, HSMA is expressed as:</p><formula xml:id="formula_3">H i = Dense ci ?j?N (i) (ATT (i, j) ? MSG (i, j))<label>(1)</label></formula><p>which contains 3 operators: a linear aggregator Dense ci , attention weights estimator ATT, and message aggregator MSG. The Dense is a set of linear projectors indexed by the node type c i , aggregating multi-head information. ATT calculates the importance weights between pairs of nodes conditioned on the associated node and edge types:</p><formula xml:id="formula_4">ATT (i, j) = softmax ?j?N (i) ? m?[1,h] head m ATT (i, j) (2) head m ATT (i, j) = K m (j) W m,ATT ?(eij ) Q m (i) T 1 ? C (3) K m (j) = Dense m cj (H j ) (4) Q m (i) = Dense m ci (H i )<label>(5)</label></formula><p>where ? denotes concatenation, m is the current head number and h is the total number of heads. Notice that Dense here is indexed by both node type c i/j , and head number m. The linear layers in K and Q have distinct parameters. To incorporate the semantic meaning of edges, we calculate the dot product between Query and Key vectors weighted by a matrix W m,ATT ?(eij ) ? R C?C . Similarly, when parsing messages from the neighboring agent, we embed infrastructure and vehicle's features separately via Dense m cj . A matrix W m,MSG ?(eij ) is used to project the features based on the edge type between source node and target node:</p><formula xml:id="formula_5">MSG (i, j) = ? m?[1,h] head m MSG (i, j) (6) head m MSG (i, j) = Dense m cj (H j ) W m,MSG ?(eij ) .<label>(7)</label></formula><p>Multi-scale window attention We present a new type of attention mechanism tailored for efficient long-range spatial interaction on high-resolution detection, called multi-scale window attention (MSwin). It uses a pyramid of windows, each of which caps a different attention range, as illustrated in <ref type="figure" target="#fig_3">Fig. 3c</ref>. The usage of variable window sizes can greatly improve the detection robustness of V2X-ViT against localization errors (see ablation study in <ref type="figure" target="#fig_6">Fig. 5b</ref>). Attention performed within larger windows can capture long-range visual cues to compensate for large localization errors, whereas smaller window branches perform attention at finer scales to preserve local context. Afterward, the split-attention module <ref type="bibr" target="#b55">[56]</ref> is used to adaptively fuse information coming from multiple branches, empowering MSwin to handle a range of pose errors. Note that MSwin is applied on each agent independently without considering any inter-agent fusion; therefore we omit the agent subscript in this subsection for simplicity. Formally, let H ? R H?W ?C be an input feature map of a single agent. In branch j out of k parallel branches, H is partitioned using window size P j ?P j , into a tensor of shape ( H Pj ? W Pj , P j ? P j , C), which represents a H Pj ? W Pj grid of non-overlapping patches each with size P j ? P j . We use h j number of heads to improve the attention power at j-th branch. More detailed formulation can be found in Appendix. Following <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b16">17]</ref>, we also consider an additional relative positional encoding B that acts as a bias term added to the attention map. As the relative position along each axis lies in the range [?P j + 1, P j ? 1], we take B from a parameterized matrixB ? R (2Pj ?1)?(2Pj ?1) .</p><p>To attain per-agent multi-range spatial relationship, each branch partitions input tensor H with different window sizes i.e. {P j } k j=1 = {P, 2P, ..., kP }. We progressively decrease the number of heads when using a larger window size to save memory usage. Finally, we fuse the features from all the branches by a Split-Attention module <ref type="bibr" target="#b55">[56]</ref>, yielding the output feature Y. The complexity of the proposed MSwin is linear to image size HW , while enjoying long-range multi-scale receptive fields and adaptively fuses both local and (sub)-global visual hints in parallel. Notably, unlike Swin Transformer <ref type="bibr" target="#b29">[30]</ref>, our multi-scale window approach requires no masking, padding, or cyclic-shifting, making it more efficient in implementations while having larger-scale spatial interactions.</p><p>Delay-aware positional encoding Although the global misalignment is captured by the spatial warping matrix ? ? , another type of local misalignment, arising from object motions during the delay-induced time lag, also needs to be considered. To encode this temporal information, we leverage an adaptive delay-aware positional encoding (DPE), composed of a linear projection and a learnable embedding. We initialize it with sinusoid functions conditioned on time delay ?t i and channel c ? [1, C]:</p><formula xml:id="formula_6">p c (?t i ) = ? ? ? sin ?t i /10000 2c C , c = 2k cos ?t i /10000 2c C , c = 2k + 1 (8)</formula><p>A linear projection f : R C ? R C will further warp the learnable embedding so it can generalize better for unseen time delay <ref type="bibr" target="#b17">[18]</ref>. We add this projected embedding to each agents' feature H i before feeding into the Transformer so that the features are temporally aligned beforehand.</p><formula xml:id="formula_7">DPE (?t i ) = f (p (?t i )) (9) H i = H i + DPE (?t i )<label>(10)</label></formula><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">V2XSet: An open dataset for V2X cooperative perception</head><p>To the best of our knowledge, no fully public V2X perception dataset exists suitable for investigating common V2X challenges such as localization error and transmission time delay. DAIR-V2X <ref type="bibr" target="#b1">[2]</ref> is a large-scale real-world V2I dataset without V2V cooperation. V2X-Sim <ref type="bibr" target="#b23">[24]</ref> is an open V2X simulated dataset but does not simulate noisy settings and only contains a single road type. OPV2V <ref type="bibr" target="#b49">[50]</ref> contains more road types but are restricted to V2V cooperation. To this end, we collect a new large-scale dataset for V2X perception that explicitly considers these real-world noises during V2X collaboration using CARLA <ref type="bibr" target="#b9">[10]</ref> and OpenCDA <ref type="bibr" target="#b47">[48]</ref> together. In total, there are 11,447 frames in our dataset <ref type="bibr">(</ref>   Sensitivity to localization error. To assess the models' sensitivity to pose error, we sample noises from Gaussian distribution with standard deviation ? xyz ? [0, 0.5] m, ? heading ? [0?, 1.0?]. As <ref type="figure" target="#fig_4">Fig. 4</ref> depicts, when the positional and heading errors stay within a normal range (i.e., ? xyz ? 0.2m, ? heading ? 0.4? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47]</ref>), the performance of V2X-ViT only drops by less than 3%, whereas other intermediate fusion methods decrease at least 6%. Moreover, the accuracy of Early Fusion and Late Fusion degrade by nearly 20% in AP@0.7. When the noise is massive (e.g., 0.5 m and 1?std), V2X-ViT can still stay around 60% detection accuracy while the performance of other methods significantly degrades, showing the robustness of V2X-ViT against pose errors. Time delay analysis. We further investigate the impact of time delay with range [0, 400] ms. As <ref type="figure" target="#fig_4">Fig. 4c</ref> shows, the AP of Late Fusion drops dramatically below No Fusion with only 100 ms delay. Early Fusion and other intermediate fusion methods are relatively less sensitive, but they still drop rapidly when delay keeps increasing and are all below the baseline after 400 ms. Our V2X-ViT, in contrast, exceeds No Fusion by 6.8% in AP@0.7 even under 400 ms delay, which is much larger than usual transmission delay in real-world system <ref type="bibr" target="#b37">[38]</ref>. This clearly demonstrates its great robustness against time delay. Infrastructure vs. vehicles. To analyze the effect of infrastructure in the V2X system, we evaluate the performance between V2V, where only vehicles can share information, and V2X, where infrastructure can also transmit messages. We denote the number of agents as the total number of infrastructure and vehicles that can share information. As shown in <ref type="figure" target="#fig_6">Fig. 5a</ref>, both V2V and V2X have better performance when the number of agents increases. The V2X system has better APs compared with V2V in our collected scenes. We argue this is due to the      better sight-of-view and less occlusion of infrastructure sensors, leading to more informative features for reasoning the environmental context. Effects of transmission size. The size of the transmitted message can significantly affect the transmission delay, thereby affecting the detection performance.</p><p>Here we study the model's detection performance with respect to transmitted data size. The data transmission time is calculated by t c = f s /v, where f s denotes the feature size and transmission rate v is set to 27 Mbps <ref type="bibr" target="#b2">[3]</ref>. Following <ref type="bibr" target="#b31">[32]</ref>, we also include another system-wise asynchronous delay that follows a uniform distribution between 0 and 200 ms. See supplementary materials for more details. From <ref type="figure" target="#fig_6">Fig. 5c</ref>    with ground truths, while other approaches exhibit larger displacements. More importantly, V2X-ViT can identify more dynamic objects (more ground-truth bounding boxes have matches), which proves its capability of effectively fusing all sensing information from nearby agents. Please see Appendix for more results. Attention map visualization. To understand the importance of infra, we also visualize the learned attention maps in <ref type="figure" target="#fig_9">Fig. 7</ref>, where brighter color means more attention ego pays. As shown in <ref type="figure" target="#fig_9">Fig. 7a</ref>, several objects are largely occluded (circled in blue) from both ego and AV2's perspectives, whereas infrastructure can still capture rich point clouds. Therefore, V2X-ViT pays much more attention to infra on occluded areas <ref type="figure" target="#fig_9">(Fig. 7d</ref>) than other agents ( <ref type="figure" target="#fig_9">Figs. 7b and 7c)</ref>, demonstrating the critical role of infra on occlusions. Moreover, the attention map for infra is generally brighter than the vehicles, indicating more importance on infra seen by the trained V2X-ViT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation studies</head><p>Contribution of major components in V2X-ViT. Now we investigate the effectiveness of individual components in V2X-ViT. Our base model is Point-Pillars with naive multi-agent self-attention fusion, which treats vehicles and infrastructure equally. We evaluate the impact of each component by progressively adding i) MSwin, ii) split attention, iii) HMSA, and iv) DPE on the Noisy Setting. As Tab. 2 demonstrates, all the modules are beneficial to the performance gains, while our proposed MSwin and HMSA have the most significant contributions by increasing the AP@0.7 4.1% and 6.6%, respectively. MSwin for localization error. To validate the effectiveness of the multiscale design in MSwin on localization error, we compare three different window configurations: i) using a single small window branch (SW), ii) using a small and a middle window (SMW), and iii) using all three window branches (SMLW). We simulate the localization error by combining different levels of positional and heading noises. From <ref type="figure" target="#fig_6">Fig. 5b</ref>, we can clearly observe that using a large and small window in parallel remarkably increased its robustness against localization error, which validates the design benefits of MSwin. DPE Performance under delay. Tab. 3 shows that DPE can improve the performance under various time delays. The AP gain increases as delay increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a new vision transformer (V2X-ViT) for V2X perception. Its key components are two novel attention modules i.e. HMSA and MSwin, which can capture heterogeneous inter-agent interactions and multi-scale intraagent spatial relationship. To evaluate our approach, we construct V2XSet, a new large-scale V2X perception dataset. Extensive experiments show that V2X-ViT can significantly boost cooperative 3D object detection under both perfect and noisy settings. This work focuses on LiDAR-based cooperative 3D vehicle detection, limited to single sensor modality and vehicle detection task. Our future work involves multi-sensor fusion for joint V2X perception and prediction.</p><p>Broader impacts and limitations. The proposed model can be deployed to improve the performance and robustness of autonomous driving systems by incorporating V2X communication using a novel vision Transformer. However, for models trained on simulated datasets, there are known issues on data bias and generalization ability to real-world scenarios. Furthermore, although the design choice of our communication approach (i.e., project LiDAR to others at the beginning) has an advantage of accuracy (see supplementary for details), its scalability is limited. In addition, new concerns around privacy and adversarial robustness may arise during data capturing and sharing, which has not received much attention. This work facilitates future research on fairness, privacy, and robustness in visual learning systems for autonomous vehicles.</p><p>In this supplementary material, we first discuss the design choice and scalability issue of our method (Appendix A). Then, we provide more model details and analysis (Appendix B) in regards to the proposed architecture, including the mathematical details of the spatial-temporal correction module (Appendix B.1), details of the proposed <ref type="figure" target="#fig_1">MSwin (Appendix B.2)</ref>, and the overall architectural specifications <ref type="figure" target="#fig_3">(Appendix B.3</ref>). Afterwords, additional information and visualizations of the proposed V2XSet dataset are shown in Appendix C. In the end, we present more quantitative experiments, qualitative detection results, attention map visualizations, and details about the effects of the transmission size experiment in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Discussion of design choice</head><p>Scalability of ego vehicles. Our approach can be scalable in two ways: 1) Decentralized: the ablation studies conducted in this paper ( <ref type="figure" target="#fig_6">Fig. 5a</ref>) and OPV2V <ref type="bibr" target="#b49">[50]</ref> indicate that, when the number of collaborators is larger than 4, the performance gain becomes marginal while the computation still increases linearly. In practice, each agent only needs to share features with a limited number of agents. For example, Who2Com <ref type="bibr" target="#b28">[29]</ref> studies which agent to request/transmit data, largely reducing computation. Moreover, the computation of selected PointPillar backbone is efficient, e.g., around 4 ms for 4 agents with full parallelization and 16 ms in sequence computing on RTX3090. 2) Centralized: Within a certain communication range, only one ego agent is selected to aggregate all the features from neighbors to predict bounding boxes and share the results with other agents. This solution requires only one computation node for a group of agents, thus being scalable. Design choices for communication. Compared to the broadcasting approach (i.e., compute the features in each cav's own space and transform the feature maps directly on the ego side), our approach has more advantages in terms of detection accuracy. Most LiDAR detection methods often largely crop the LiDAR range based on the evaluation range to reduce computation. As the figure below shows, the CAVs crop the LiDAR data based on their own evaluation range in the broadcasting method, which leads to redundant data. Our approach, on the contrary, always does cropping based on the ego's evaluation range, thus guaranteeing more effective feature transmission. We further validate this by comparing our framework with the broadcasting approach. The Tab. T0 below shows that our design outperforms broadcasting by 8.5% and 8.9% for DiscoNet and V2X-ViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Model Details and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Spatial-Temporal Correction Module</head><p>During the early stage of collaboration, when each connected agent i receives ego vehicle's pose at time t i , the observed point clouds of agent i will be projected to ego vehicle's pose x ti e at time t i before feature extraction. However, due to the time delay, the ego vehicle observes the data at a different time t e . Thus, the received features from connected agents are centered around a delayed ego vehicle's pose (i.e., x ti e ) while the ego vehicle's features are centered around current pose (i.e., x te e ), leading to a delay-induced spatial misalignment. To correct this misalignment between the received features and ego-vehicle's features, a global transformation ? x t i e ,x te e ? se(3) from ego vehicle's past pose x ti e to its current pose x te e is required. To this end, we employ a differential 2D transformation ? ? (?) to warp the intermediate features spatially <ref type="bibr" target="#b18">[19]</ref>. To be more specific, we will transform features' positions by using affine transformation:</p><formula xml:id="formula_8">x s y s = ? ? ( ? ? x t y t 1 ? ? ) = R 11 R 12 ?x R 21 R 22 ?y ? ? x t y t 1 ? ?<label>(11)</label></formula><p>where (x s , y s ) and (x t , y t ) are the source and target coordinates. As the calculated coordinates may not be integers, we use bilinear interpolation to sample input feature vectors. An ROI mask is also calculated to prevent the network from paying attention to the padded zeros caused by the spatial warp. This mask will be used in heterogeneous multi-agent self-attention to mask padded values' attention weights as zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Multi-Scale Window Attention (MSwin)</head><p>Detailed formulation. Let H ? R H?W ?C be an input feature of a single agent. Let h j be the number of attention heads used in branch j (i.e. head dimension d hj = C/h j ), applying self-attention within each non-overlapping window P j ?P j for branch j out of k branches on feature H can be formulated as:</p><formula xml:id="formula_9">H = [H 1 , H 2 , ..., H HW/(Pj ) 2 ], for branch j<label>(12)</label></formula><formula xml:id="formula_10">H i m = Attention(H i W Q m , H i W K m , H i W V m ), i = 1, ..., HW/(P j ) 2 (13) Y m = [? 1 m ,? 2 m , ...,? HW/(Pj ) 2 m ], m = 1, ..., h j (14) Y j = [Y 1 , Y 2 , ..., Y hj ],<label>(15)</label></formula><p>where? i m ? R P 2 j ?d h j and W Q m , W K m , W V m represent the query, key, and value projection matrices. Y m is the output of the m-th head for branch j. Afterwards, the outputs for all heads 1, 2, ..., h j are concatenated to obtain the final output Y j . Here the Attention operation denotes the relative self-attention, similar to the usage in Swin <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_11">Attention(Q, K, V) = softmax(( QK T ? d + B)V)<label>(16)</label></formula><p>where Q, K, V ? R P 2 j ?d denote the query, key, and value matrices. d is the dimension of query/key, while P 2 j denotes the window size for branch j. Following <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b16">17]</ref>, we also consider an additional relative positional encoding B that acts as a bias term added to the attention map. As the relative position along each axis lies in the range [?P j + 1, P j ? 1], we take B from a parameterized ma-trixB ? R (2Pj ?1)?(2Pj ?1) . To adaptively fuse features from all the k branches, we adopt the split-attention module <ref type="bibr" target="#b55">[56]</ref> for the parallel feature aggregation:</p><formula xml:id="formula_12">Y = SplitAttention(Y 1 , Y 2 , ...Y k ),<label>(17)</label></formula><p>Time complexity. As mentioned in the paper, we have k parallel branches. Each branch has P j ?P j window size and h j heads where P j = jP and h j = h/j. After partitioning, the input tensor H ? R H?W ?C is split into h j features with shape ( H Pj ? W Pj , P j ? P j , C/h j ). Following <ref type="bibr" target="#b29">[30]</ref>, we focus on the computation for vector-matrix multiplication and attention weight calculation. Thus, the complexity of MSwin can be written as:</p><formula xml:id="formula_13">O( k j=1 HW P 2 j ? C h j ? (P j ? P j ) 2 ? h j + 4 HW P 2 j ? P 2 j ? ( C h j ) 2 ? h j ) = O( k j=1 P 2 j HW C + 4HW C 2 h j ) = O( k j=1 j 2 P 2 HW C + 4HW C 2 j h ) = O( 1 3 k 3 P 2 HW C + 2HW C 2 k 2 h )<label>(18)</label></formula><p>where the first term corresponds to attention weight calculation, the second term is associated with vector-matrix multiplication, and the last equality is due to the fact that k j=1 j 2 = O( k 3 3 ) and k j=1 j = O( k 2 2 ). Thus the overall complexity is</p><formula xml:id="formula_14">FLOPs(MSwin) = O(( k 3 P 2 C 3 + 2k 2 C 2 h )HW ) ? O(HW ),<label>(19)</label></formula><p>which is linear with respect to the image size. The comparison of time complexity of different types of transformers is shown in Tab. T1 where N denotes the number of input pixels, or (here N = HW ). Our MSwin obtains multi-scale spatial interactions with a linear complexity with respect to N , while other long-range attention mechanisms like ViT <ref type="bibr" target="#b8">[9]</ref>, Axial <ref type="bibr" target="#b43">[44]</ref>, and CSwin <ref type="bibr" target="#b7">[8]</ref> requires more than linear complexity, which are not scalable to high-resolution dense prediction tasks such as object detection and segmentation. <ref type="table" target="#tab_3">Table T1</ref>: Computational complexity comparisons of our proposed MSwin attention with (a) full attention in ViT <ref type="bibr" target="#b8">[9]</ref>, (b) Axial <ref type="bibr" target="#b43">[44]</ref>, (c) Swin <ref type="bibr" target="#b29">[30]</ref>, (d) CSwin <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Models Complexity</head><p>ViT <ref type="bibr" target="#b8">[9]</ref> O(4HW C 2 + 2(HW ) 2 C) ? O(N 2 ) Axial <ref type="bibr" target="#b43">[44]</ref> O(HW C(4C + H + W )) ? O(N ? N ) Swin <ref type="bibr" target="#b29">[30]</ref> O(4HW C 2 + 2P 2 HW C) ? O(N ) CSwin <ref type="bibr" target="#b7">[8]</ref> O</p><formula xml:id="formula_15">(HW C(4C + sH + sW )) ? O(N ? N ) MSwin (ours) O( 1 3 k 3 P 2 HW C + 2HW C 2 k 2 h ) ? O(N )</formula><p>Effective receptive field. The comparisons of receptive fields between different transformers are shown in <ref type="figure" target="#fig_11">Fig. 8</ref>. Swin <ref type="bibr" target="#b29">[30]</ref> enlarge the receptive fields by using shifted window but it requires sequential blocks to accumulate. Axial Transformer <ref type="bibr" target="#b43">[44]</ref> conducts attention on both row-wise and column-wise directions. Similarly, CSwin <ref type="bibr" target="#b7">[8]</ref> proposes to perform attention on horizontal and vertical stripes with asymmetrical receptive range in different directions, but requires polynomial time complexity-O(N 1.5 ). In contrast, our proposed MSwin can aggregate features from multi-scale branches to increase fields in parallel, which has more symmetrical receptive fields and linear complexity with respect to N .   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Architectural Configurations</head><p>Given all these definitions, the entire V2X-ViT model can be formulated as:</p><formula xml:id="formula_16">z i = PointPillar(x i ), x i ? R P ?4 , z i ? R H?W ?C for agent i (20) z 0 = STCM([z 0 , ..., z M ]) + DPE([?t 0 , ..., ?t M ]),</formula><p>for ego AV (21)</p><formula xml:id="formula_17">z ? ? = z ??1 + MSwin(HSMA(LN(z 0 ))), z 0 ? R M ?H?W ?C ? = 1, ..., L (22) z ? = z ? ? + MLP(LN(z ? ? )), ? = 1, ..., L (23) y = Head(z L ),<label>(24)</label></formula><p>where the input x i denotes the raw LiDAR point clouds captured on each agent, which are fed into the PointPillar Encoder <ref type="bibr" target="#b21">[22]</ref>, yielding visually informative 2D features z i for each agent i. These tensors are then compressed, shared, decompressed, and further fed into the spatial-temporal correction module (STCM) to spatially warp the features. Then, we add the delay-aware positional encoded (DPE) features conditioned on each agent's time delay ?t i to the output of STCM. Afterwords, the gathered features from M agents are processed using our proposed V2X-ViT, which consists of L layers of V2X-ViT blocks. Each V2X-ViT block contains a HSMA, a MSwin, and a standard MLP network <ref type="bibr" target="#b8">[9]</ref>. Following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30]</ref>, we use the Layer Normalization <ref type="bibr" target="#b3">[4]</ref> before feeding into the Transformer/MLP module. We show the detailed specifications of V2X-ViT architecture in <ref type="table" target="#tab_5">Table T2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C V2XSet Dataset</head><p>Statistics We gather 55 representative scenes covering 5 different roadway types and 8 towns in CARLA. Each scene is limited to 25 seconds, and in each scene, there are at least 2 and at most 7 intelligent agents that can communicate with each other. Each agent is equipped with 32-channel LiDAR and has 120 meters data range. We mount sensors on top of each AV while we only deploy infrastructure sensors in the intersection, mid-block, and entrance ramp at the height of 14 feet since these scenarios are typically more congested and challenging <ref type="bibr" target="#b15">[16]</ref>. We record LiDAR point clouds at 10 Hz and save the corresponding positional data and timestamp. Infrastructure deployment. The infrastructure sensors are installed on the traffic light poles or steet light poles at the intersection, mid-block, and entrance ramp at the height of 14 feet. For road type like rural curvy road, there is no infrastructure installed and only V2V collaboration exists. Dataset visualization. As <ref type="figure">Fig. 9</ref> displays, there are 5 different roadway types in V2XSet dataset (i.e., straight segment, curvy segment, midblock, entrance ramp, and intersection), covering the most common driving scenarios in real life. We collect more intersection scenes than other types as it is usually more challenging due to the high traffic volume and severe occlusions. Data samples from different roadway types can be found in <ref type="figure" target="#fig_0">Fig. 10</ref>. From the figure, we can observe that the infrastructure sensors at the entrance ramp and intersection have different measurement patterns especially near its installation position compared with vehicle sensors. This is caused by the different installation heights between vehicle and infrastructure sensors. Such observation again shows the necessity of capturing the heterogeneity nature of V2X system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intersection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>48%</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Straight Segment 21%</head><p>Curvy Segment 23% Midblock 5% Entrance Ramp 3% <ref type="figure">Fig. 9</ref>: Data distribution of 5 roadway types in the proposed dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D More Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Performance for identifying dynamic objects</head><p>We group the test set based on object speeds v (km/h) and compare AP@IoU=0.7 under the noisy setting for all intermediate fusion models. As shown in Tab. T3, V2X-ViT outperforms all other intermediate fusion methods under various speed range. It is noticeable that the objects with higher speed range generally have lower AP scores as the same time delay can produce more positional misalignments for the high-speed vehicles. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Performance for different road types</head><p>We also group the test scenes based on their road types and calculate the AP@IoU=0.7 scores under the noisy setting. As shown in Tab. T4, V2X-ViT ranks the first for all 5 road categories, demonstrating its detection robustness on different scenes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Qualitative results</head><p>Figs. 11 to 13 demonstrate more detection visualizations of V2VNet <ref type="bibr" target="#b44">[45]</ref>, OPV2V <ref type="bibr" target="#b49">[50]</ref>, F-Cooper <ref type="bibr" target="#b4">[5]</ref>, DiscoNet <ref type="bibr" target="#b24">[25]</ref>, and our V2X-ViT in different scenarios under Noisy Setting. V2X-ViT yields more robust performance in general with fewer regression displacements and fewer undetected objects. When the scenario is challenging with high-density traffic flow and more occlusions (e.g., Scene 7 in <ref type="figure" target="#fig_0">Fig. 13</ref> ), our model can still identify most of the objects accurately. The LiDAR points of ego vehicle, the other connected autonomous vehicle (cav), and infrastructure are plotted in blue, green, and red respectively. The brighter color in the attention map means more attention ego vehicle pays. Generally, the color of infrastructure attention maps is brighter than others, especially for the occluded regions of other agents, indicating the more importance ego vehicle assigns to the infrastructure. This observation agrees with our intuition that the sensor observation of infrastructure has fewer occlusions, which leads to better feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Attention visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Explanation on effects of transmission size</head><p>Here we provide more explanations of the data transmission size experiment in our paper. Different fusion strategies usually have distinct bandwidth requirements e.g., early fusion requires large bandwidth to transmit raw data, whereas late fusion only delivers minimal size of data. This communication volume will significantly influence the time delay, thus we need to simulate a more realistic time delay setting to study the effects of transmission size. Following <ref type="bibr" target="#b31">[32]</ref>, we decompose the total time delay into two parts: i) the data transmission time t c during broadcasting, ii) the idle time t i caused by the lack of synchronization between the perception system and communication system. The total time delay is calculated as t total = t c + t i <ref type="bibr" target="#b24">(25)</ref> As mentioned in the paper, the data transmission time has</p><formula xml:id="formula_18">t c = f s /v (26)</formula><p>where f s is the data size and v is the transmission rate. Idle time t i can be further decoupled into the idle time on the sender side and the time on the receiver side i.e., t i = t i,1 + t i,2 . For t i,1 , the worst case in terms of delay happens when the communication system just misses a perception cycle and needs to wait for the next round. Similarly, for t i,2 , the worst case occurs when new data is received just after a new cycle of the perception system has started. Assume both perception system and communication system have the same rate of 10Hz, then 0 ms &lt; t i &lt; 200 ms. We employ a uniform distribution U (0, 200) to model this uncertainty. In summary, we use the following equation to mimic the real-world time delay.</p><formula xml:id="formula_19">t c = f s /v + U (0, 200)<label>(27)</label></formula><p>which captures the influence of transmission size and asynchrony-caused uncertainty. In practice, we sample the time delay according to Eq. 26 and discretize it to the observed timestamps, which are discrete in a 10Hz update system.  V2VNet <ref type="bibr" target="#b44">[45]</ref> OPV2V <ref type="bibr" target="#b49">[50]</ref> DiscoNet <ref type="bibr" target="#b24">[25]</ref> V2X-ViT (ours) <ref type="figure" target="#fig_0">Fig. 11</ref>: Qualitative comparison on scenarios 1-3. Green and red 3D bounding boxes represent the groun truth and prediction respectively. Our method yields more accurate detection results. V2VNet <ref type="bibr" target="#b44">[45]</ref> OPV2V <ref type="bibr" target="#b49">[50]</ref> DiscoNet <ref type="bibr" target="#b24">[25]</ref> V2X-ViT (ours) <ref type="figure" target="#fig_0">Fig. 12</ref>: Qualitative comparison on scenarios 4-6. Green and red 3D bounding boxes represent the groun truth and prediction respectively. V2VNet <ref type="bibr" target="#b44">[45]</ref> OPV2V <ref type="bibr" target="#b49">[50]</ref> DiscoNet <ref type="bibr" target="#b24">[25]</ref> V2X-ViT (ours) <ref type="figure" target="#fig_0">Fig. 13</ref>: Qualitative comparison on scenarios 7-8. Green and red 3D bounding boxes represent the groun truth and prediction respectively. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>A data sample from the proposed V2XSet. (a) A simulated scenario in CARLA where two AVs and infrastructure are located at different sides of a busy intersection. (b) The aggregated LiDAR point clouds of these three agents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of our proposed V2X perception system. It consists of five sequential steps: V2X metadata sharing, feature extraction, compression &amp; sharing, V2X-ViT, and the detection head. The details of each individual component are illustrated in Sec. 3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>V2X-ViT architecture. (a) The architecture of our proposed V2X-ViT model. (b) Heterogeneous multi-agent self-attention (HMSA) presented in Sec. 3.2. (c) Multi-scale window attention module (MSwin) illustrated in Sec. 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Robustness assessment on positional and heading errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Ablation studies. (a) AP vs. number of agents. (b) MSwin for localization error with window sizes: 4 2 (S), 8 2 (M), 16 2 (L). (c) AP vs. data size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative comparison in a congested intersection and a highway entrance ramp. Green and red 3D bounding boxes represent the ground truth and prediction respectively. Our method yields more accurate detection results. More visual examples are provided in the supplementary materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 :</head><label>7</label><figDesc>Aggregated LiDAR points and attention maps for ego. Several objects are occluded (blue circle) from both AV's perspectives, whereas infra can still capture rich point clouds. V2X-ViT learned to pay more attention to infra on occluded areas, shown in (d). We provide more visualizations in Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 :</head><label>8</label><figDesc>Visualizations of approximated receptive fields (blue shaded pixels) for the green pixel for (a) Swin<ref type="bibr" target="#b29">[30]</ref> (b) Axial<ref type="bibr" target="#b43">[44]</ref>, (c) CSwin<ref type="bibr" target="#b7">[8]</ref> and (d) MSwin attention. MSwin obtains multi-scale long-range interactions at linear complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 shows</head><label>14</label><figDesc>more attention map visualizations of V2X-ViT under noisy setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 10 :</head><label>10</label><figDesc>Data samples of 5 different roadway types. Left is the snapshot of simulation and right is the corresponding aggregated LiDAR point clouds from multiple agents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 14 :</head><label>14</label><figDesc>Additional attention map visualizations on 3 different scenes. V2X-ViT learned to pay more attention to infra features on occluded areas from AV's perspectives, thus yielding more robust detection under occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>-</head><label></label><figDesc>We propose a novel heterogeneous multi-agent attention module (HMSA) tailored for adaptive information fusion between heterogeneous agents.-We present a new multi-scale window attention module (MSwin) that simultaneously captures local and global spatial feature interactions in parallel. -We construct V2XSet, a new large-scale open simulation dataset for V2Xperception, which explicitly accounts for imperfect real-world conditions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc><ref type="bibr" target="#b32">33</ref>,081 samples if we count frames per agent in the same scene), and the train/validation/test splits are 6,694/1,920/2,833, respectively. Compared with existing datasets, V2XSet incorporates both V2X cooperation and realistic noise simulation. Please refer to the supplementary material for more details.4.2 Experimental setupTh evaluation range in x and y direction are [?140, 140] m and [?40, 40] m respectively. We assess models under two settings: 1) Perfect Setting, where the pose is accurate, and everything is synchronized across agents; 2) Noisy Setting, where pose error and time delay are both considered. In the Noisy Setting, the positional and heading noises of the transmitter are drawn from a Gaussian distribution with a default standard deviation of 0.2 m and 0.2?respectively,</figDesc><table><row><cell>0.2 0.3 0.4 0.5 0.6 0.7 AP @ IoU 0.7</cell><cell>0.0</cell><cell cols="2">0.1 (a) Positional error std (m) 0.2 0.3 0.4</cell><cell>0.5</cell><cell>0.35 0.40 0.45 0.50 0.55 0.60 0.70 0.65 AP @ IoU 0.7</cell><cell>0.0</cell><cell cols="2">0.2 (b) Heading error std (?) 0.4 0.6 0.8</cell><cell>1.0</cell><cell>0.7 0.3 0.4 0.5 0.6 AP @ IoU 0.7</cell><cell>0.0</cell><cell>0.1 (c) Time delay (s) 0.2 0.3</cell><cell>0.4</cell></row><row><cell></cell><cell cols="2">No fusion</cell><cell>Late fusion</cell><cell cols="3">Early fusion</cell><cell>OPV2V</cell><cell cols="2">F-Cooper</cell><cell cols="2">V2VNet</cell><cell>DiscoNet</cell><cell>V2X-ViT</cell></row></table><note>following the real-world noise levels [47,26,1]. The time delay is set to 100 ms for all the evaluated models to have a fair comparison of their robustness against asynchronous message propagation. Evaluation metrics. The detection performance is measured with Average Precisions (AP) at Intersection-over-Union (IoU) thresholds of 0.5 and 0.7. In</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>3D detection performance comparison on V2XSet. We show Average Precision (AP) at IoU=0.5, 0.7 on Perfect and Noisy settings, respectively. Tab. 1 shows the performance comparisons on both Perfect and Noisy Setting. Under the Perfect Setting, all the cooperative methods significantly outperform No Fusion baseline. Our proposed V2X-ViT outperforms SOTA intermediate fusion methods by 3.8%/1.7% for AP@0.5/0.7. It is even higher than the ideal Early fusion by 0.2% AP@0.7, which receives complete raw information. Under noisy setting, when localization error and time delay are considered, the performance of Early Fusion and Late Fusion drastically drop to 38.4% and 30.7% in AP@0.7, even worse than single-agent baseline No Fusion. Although OPV2V<ref type="bibr" target="#b49">[50]</ref>, F-Cooper<ref type="bibr" target="#b4">[5]</ref> V2VNet<ref type="bibr" target="#b44">[45]</ref>, and DiscoNet<ref type="bibr" target="#b24">[25]</ref> are still higher than No fusion, their performance decrease by 17.7%, 21.1%, 18.4% and 15.4% in AP@0.7, respectively. In contrast, V2X-ViT performs favorably against the No fusion method by a large margin, i.e. 23% and 21.2% higher</figDesc><table><row><cell></cell><cell cols="2">Perfect</cell><cell cols="2">Noisy</cell></row><row><cell>Models</cell><cell>AP0.5</cell><cell>AP0.7</cell><cell>AP0.5</cell><cell>AP0.7</cell></row><row><cell>No Fusion</cell><cell>0.606</cell><cell>0.402</cell><cell>0.606</cell><cell>0.402</cell></row><row><cell>Late Fusion</cell><cell>0.727</cell><cell>0.620</cell><cell>0.549</cell><cell>0.307</cell></row><row><cell>Early Fusion</cell><cell>0.819</cell><cell>0.710</cell><cell>0.720</cell><cell>0.384</cell></row><row><cell>F-Cooper [5]</cell><cell>0.840</cell><cell>0.680</cell><cell>0.715</cell><cell>0.469</cell></row><row><cell>OPV2V [50]</cell><cell>0.807</cell><cell>0.664</cell><cell>0.709</cell><cell>0.487</cell></row><row><cell>V2VNet [45]</cell><cell>0.845</cell><cell>0.677</cell><cell>0.791</cell><cell>0.493</cell></row><row><cell>DiscoNet [25]</cell><cell>0.844</cell><cell>0.695</cell><cell>0.798</cell><cell>0.541</cell></row><row><cell>V2X-ViT (Ours)</cell><cell>0.882</cell><cell>0.712</cell><cell>0.836</cell><cell>0.614</cell></row></table><note>this work, we focus on LiDAR-based vehicle detection. Vehicles hit by at least one LiDAR point from any connected agent will be included as evaluation targets. Implementation details. During training, a random AV is selected as the ego vehicle, while during testing, we evaluate on a fixed ego vehicle for all the compared models. The communication range of each agent is set as 70 m based on [20], whereas all the agents out of this broadcasting radius of ego vehicle is ignored. For the PointPillar backbone, we set the voxel resolution to 0.4 m for both height and width. The default compression rate is 32 for all intermediate fusion methods. Our V2X-ViT has 3 encoder layers with 3 window sizes in MSwin: 4, 8, and 16. We first train the model under the Perfect Setting, then fine-tune it for Noisy Setting. We adopt Adam optimizer [21] with an initial learning rate of 10 ?3 and steadily decay it every 10 epochs using a factor of 0.1. All models are trained on Tesla V100. Compared methods. We consider No Fusion as our baseline, which only uses ego-vehicle's LiDAR point clouds. We also compare with Late Fusion, which gathers all detected outputs from agents and applies Non-maximum suppression to produce the final results, and Early Fusion, which directly aggregates raw Li- DAR point clouds from nearby agents. For intermediate fusion strategy, we eval- uate four state-of-the-art approaches: OPV2V [50], F-Cooper [5] V2VNet [45], and DiscoNet [25]. For a fair comparison, all the models use PointPillar as the backbone, and every compared V2V methods also receive infrastructure data, but they do not distinguish between infrastructure and vehicles.4.3 Quantitative evaluation Main performance comparison.in AP@0.5 and AP@0.7. Moreover, when compared to the Perfect Setting, V2X- ViT only drops by less than 5% and 10% in AP@0.5 and AP@0.7 under Noisy Setting, demonstrating its robustness against normal V2X noises. The real-time performance of V2X-ViT is also shown in Tab. 4. The inference time of V2X-ViT is 57 ms, and by using only 1 encoder layer, V2X-ViT S can still beat DiscoNet while reaching only 28 ms inference time, which achieves real-time performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Component ablation study.</figDesc><table><row><cell cols="5">MSwin, SpAttn, HMSA, DPE represent</cell></row><row><cell cols="5">adding i) multi-scale window attention, ii)</cell></row><row><cell cols="5">split attention, iii) heterogeneous multi-</cell></row><row><cell cols="5">agent self-attention, and iv) delay-aware</cell></row><row><cell cols="5">positional encoding, respectively.</cell></row><row><cell>M S w i n</cell><cell>S p A t t n</cell><cell>H M S A</cell><cell>D P E</cell><cell>AP0.5 / AP0.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.719 / 0.478</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>0.748 / 0.519</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>0.786 / 0.548</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>0.823 / 0.601</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>0.836 / 0.614</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Effect of DPE w.r.t. time delay on AP@0.7.</figDesc><table><row><cell cols="3">Delay/Model w/o DPE w/ DPE</cell></row><row><cell>100 ms</cell><cell>0.639</cell><cell>0.650</cell></row><row><cell>200 ms</cell><cell>0.558</cell><cell>0.572</cell></row><row><cell>300 ms</cell><cell>0.496</cell><cell>0.514</cell></row><row><cell>400 ms</cell><cell>0.458</cell><cell>0.478</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Inference time measured on GPU Tesla V100.</figDesc><table><row><cell>Model</cell><cell cols="2">Time AP0.7(prf/nsy)</cell></row><row><cell cols="2">V2X-ViT S 28ms</cell><cell>0.696 / 0.591</cell></row><row><cell>V2X-ViT</cell><cell>57ms</cell><cell>0.712 / 0.614</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table T0 :</head><label>T0</label><figDesc>Comparison between our design choice and broadcasting approach.</figDesc><table><row><cell></cell><cell cols="2">DiscoNet (broad. / ours) V2X-ViT (broad. / ours)</cell></row><row><cell>AP@0.7 (perfect)</cell><cell>0.610 / 0.695</cell><cell>0.623 / 0.712</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table T2 :</head><label>T2</label><figDesc>Detailed architectural specifications for V2X-ViT.</figDesc><table><row><cell></cell><cell>Output size</cell><cell></cell><cell>V2X-ViT framework</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Voxel samp. reso. 0.4m, Scatter, 64</cell></row><row><cell></cell><cell></cell><cell cols="4">Conv3x3, 64, stride 2, BN, ReLU ? 3</cell></row><row><cell></cell><cell></cell><cell cols="4">Conv3x3, 128, stride 2, BN, ReLU ? 5</cell></row><row><cell></cell><cell>M ? 352 ? 96 ? 256</cell><cell cols="4">Conv3x3, 256, stride 2, BN, ReLU ? 8</cell></row><row><cell>PointPillar</cell><cell></cell><cell cols="4">ConvT3x3, 128, stride 1, BN, ReLU ? 1</cell></row><row><cell>Encoder</cell><cell></cell><cell cols="4">ConvT3x3, 128, stride 2, BN, ReLU ? 1</cell></row><row><cell></cell><cell></cell><cell cols="4">ConvT3x3, 128, stride 4, BN, ReLU ? 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Concat3, 384</cell><cell></cell><cell></cell></row><row><cell></cell><cell>M ? 176 ? 48 ? 256</cell><cell cols="3">Conv3x3, 256, stride 2, ReLU Conv3x3, 256, stride 1, ReLU</cell><cell>? 1</cell></row><row><cell>Delay-aware Pos. Encoding</cell><cell>M ? 176 ? 48 ? 256</cell><cell></cell><cell>sin-cos pos. encoding Linear, 256 ? 1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>HSMA, dim 256, head 8</cell><cell>?</cell><cell></cell></row><row><cell>Transformer Backbone</cell><cell>M ? 176 ? 48 ? 256</cell><cell>? ? ? ? ?</cell><cell>MSwin, dim 256, head {16, 8, 4}, ws. {4 ? 4, 8 ? 8, 16 ? 16}</cell><cell>? ? ? ? ?</cell><cell>? 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MLP, dim 256</cell><cell></cell><cell></cell></row><row><cell>Detection Head</cell><cell>176 ? 48 ? 16</cell><cell cols="4">Cls. head: Conv1x1, 2, stride 1 Regr. head: Conv1x1, 14, stride 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table T3 :</head><label>T3</label><figDesc>Perception performance for objects with different speed (km/h), measured in AP@0.7 under noisy setting.</figDesc><table><row><cell>Model</cell><cell>v &lt; 20</cell><cell>20 ? v ? 40</cell><cell>v &gt; 40</cell></row><row><cell>F-Cooper</cell><cell>0.539</cell><cell>0.487</cell><cell>0.354</cell></row><row><cell>OPV2V</cell><cell>0.552</cell><cell>0.498</cell><cell>0.346</cell></row><row><cell>V2VNet</cell><cell>0.598</cell><cell>0.518</cell><cell>0.406</cell></row><row><cell>DiscoNet</cell><cell>0.639</cell><cell>0.580</cell><cell>0.420</cell></row><row><cell>V2X-ViT</cell><cell>0.693</cell><cell>0.634</cell><cell>0.488</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table T4 :</head><label>T4</label><figDesc>Perception performance for different road types, measured in AP@0.7 under noisy setting.</figDesc><table><row><cell>Model</cell><cell>Straight</cell><cell>Curvy</cell><cell>Intersection</cell><cell>Midblock</cell><cell>Entrance</cell></row><row><cell>F-Cooper</cell><cell>0.483</cell><cell>0.558</cell><cell>0.458</cell><cell>0.431</cell><cell>0.375</cell></row><row><cell>OPV2V</cell><cell>0.478</cell><cell>0.604</cell><cell>0.492</cell><cell>0.460</cell><cell>0.380</cell></row><row><cell>V2VNet</cell><cell>0.496</cell><cell>0.556</cell><cell>0.517</cell><cell>0.489</cell><cell>0.360</cell></row><row><cell>DiscoNet</cell><cell>0.519</cell><cell>0.594</cell><cell>0.572</cell><cell>0.472</cell><cell>0.440</cell></row><row><cell>V2X-ViT</cell><cell>0.645</cell><cell>0.686</cell><cell>0.615</cell><cell>0.530</cell><cell>0.487</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">V2X-ViT: V2X Perception with Vision Transformer</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This material is supported in part by the Federal Highway Administration Exploratory Advanced Research (EAR) Program, and by the US National Science Foundation through Grants CMMI # 1901998. We thank Xiaoyu Dong for her insightful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rt3000</surname></persName>
		</author>
		<ptr target="https://www.oxts.com/products/rt3000-v3" />
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">U</forename></persName>
		</author>
		<title level="m">Vehicle-infrastructure cooperative autonomous driving: Dair-v2x dataset</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">9</biblScope>
		</imprint>
		<respStmt>
			<orgName>Institue for AI Industry Research (AIR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An overview of vehicular communications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Internet</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">F-cooper: Feature based cooperative perception for autonomous vehicle edge computing system using 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM/IEEE Symposium on Edge Computing</title>
		<meeting>the 4th ACM/IEEE Symposium on Edge Computing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cooper: Cooperative perception for connected autonomous vehicles based on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 39th International Conference on Distributed Computing Systems (ICDCS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13840</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00652</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CARLA: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Annual Conference on Robot Learning</title>
		<meeting>the 1st Annual Conference on Robot Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rgb and lidar fusion based 3d semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>El Madawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El Sallab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rafm: Recurrent atrous feature modulation for accurate monocular depth estimating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/LSP.2022.3189597</idno>
		<ptr target="https://doi.org/10.1109/LSP.2022.31895971" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Svt-net: Super light-weight sparse voxel transformer for large scale place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning on monocular object pose detection and tracking: A comprehensive overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating the effectiveness of integrated connected automated vehicle applications applied to freeway managed lanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spatial transformer networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dedicated short-range communications (dsrc) standards in the united states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Kenney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 99</title>
		<meeting>the IEEE 99</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.08560</idno>
		<title level="m">Latency-aware collaborative perception</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">V2x-sim: Multiagent collaborative perception dataset and benchmark for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning distilled collaboration graph for multi-agent perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Toward location-enabled iot (le-iot): Iot positioning techniques, error sources, and error mitigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Who2com: Collaborative perception via learnable handshake communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Glaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A method of vehicle-infrastructure cooperative perception based vehicle state information fusion using improved kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Analysis of v2x communication parameters for the development of a fusion architecture for cooperative perception systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Klanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Car2x-based perception in a high-level fusion architecture for cooperative perception systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Klanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rasshofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Collaborative automated driving: A machine learningbased method to enhance the accuracy of shared information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Rawashdeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Speeding up semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Treml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arjona-Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Friedmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schuberth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmarcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS workshop MLITS</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Autoc2x: Open-source software to realize v2x cooperative perception among autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Esaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 92nd Vehicular Technology Conference (VTC2020-Fall)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Maxim: Multi-axis mlp for image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01697</idno>
		<title level="m">Maxvit: Multi-axis vision transformer</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vadivelu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.05289</idno>
		<title level="m">Learning to communicate and correct pose errors</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Stand-alone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">V2vnet: Vehicle-to-vehicle communication for joint perception and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manivasagam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Uformer: A general u-shaped transformer for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03106</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Advancing estimation accuracy of sideslip angle by fusing vehicle kinematics and dynamics information with fuzzy logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Opencda: an open cooperative driving automation framework integrated with co-simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Cobevt: Cooperative bird&apos;s eye view semantic segmentation with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.02202</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Opv2v: An open benchmark dataset and fusion pipeline for perception with vehicle-to-vehicle communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07644</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Keypoints-based deep feature fusion for cooperative vehicle detection of autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3054" to="3061" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yueqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boxun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05518</idno>
		<title level="m">Tracking objects as pixel-wise distributions</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Safe occlusion-aware autonomous driving via game-theoretic active perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Fisac</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08169</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A cooperative vehicle-infrastructure based urban driving environment perception method using a ds theory-based credibility map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Prehofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Vin: Voxel-based implicit network for joint 3d object detection and segmentation for lidars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02980</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">R-msfm: Recurrent multi-scale feature modulation for monocular depth estimating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
