<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Omni-frequency Channel-selection Representations for Unsupervised Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangning</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuwen</forename><surname>Pan</surname></persName>
						</author>
						<title level="a" type="main">Omni-frequency Channel-selection Representations for Unsupervised Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Anomaly detection</term>
					<term>omni-frequency decoupling</term>
					<term>unsupervised learning</term>
					<term>reconstruction-based network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Density-based and classification-based methods have ruled unsupervised anomaly detection in recent years, while reconstruction-based methods are rarely mentioned for the poor reconstruction ability and low performance. However, the latter requires no costly extra training samples for the unsupervised training that is more practical, so this paper focuses on improving this kind of method and proposes a novel Omni-frequency Channel-selection Reconstruction (OCR-GAN) network to handle anomaly detection task in a perspective of frequency. Concretely, we propose a Frequency Decoupling (FD) module to decouple the input image into different frequency components and model the reconstruction process as a combination of parallel omnifrequency image restorations, as we observe a significant difference in the frequency distribution of normal and abnormal images. Given the correlation among multiple frequencies, we further propose a Channel Selection (CS) module that performs frequency interaction among different encoders by adaptively selecting different channels. Abundant experiments demonstrate the effectiveness and superiority of our approach over different kinds of methods, e.g., achieving a new state-of-the-art 98.3 detection AUC on the MVTec AD dataset without extra training data that markedly surpasses the reconstruction-based baseline by +38.1? and the current SOTA method by +0.3?. Source code will be available at https://github.com/zhangzjn/OCR-GAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>datasets. However, semantic AD task detects images with label shifts, assuming that normal and abnormal come from different semantic distributions, such as the one-class detection task in CIFAR-10 <ref type="bibr" target="#b9">[10]</ref>. This work focus on solving the sensory AD task but also evaluate on the related semantic AD dataset.</p><p>In anomaly detection, obtaining abnormal samples and detecting novel abnormalities are time-consuming and costly objects that force us to develop unsupervised methods for more practical applications. Current unsupervised anomaly detection methods are mainly divided into three categories: density-based ( <ref type="figure" target="#fig_1">Fig. 2a</ref>), classification-based ( <ref type="figure" target="#fig_1">Fig. 2b</ref>) and reconstruction-based ( <ref type="figure" target="#fig_1">Fig. 2c</ref>) methods. a) Density-based methods generally employ a pre-trained model to extract meaningful vectors of the input image. The anomaly score can be obtained by calculating the similarity between the embedding representation of the test image and the reference density distribution. This kind of method <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref> achieves a high AUC score on the popular MVTec AD <ref type="bibr" target="#b1">[2]</ref> dataset, but they need pre-trained models and are insufficient for the model interpretability. b) Classification-based methods try to find the classification boundaries of normal data. Self-supervised methods are representative of classification-based methods, which use the model trained by the proxy task to detect anomalies. Thus, self-supervised methods rely on how well the proxy tasks match the test data. For example, CutPaste <ref type="bibr" target="#b13">[14]</ref> performs well in anomaly detection on MVTec AD dataset. However, it is difficult for this method to perform well on other datasets. Also, these methods need pre-trained models and  Normal and abnormal data have noticeable frequency distribution differences. (b) Development of three kinds of methods. Our approach surpasses the SOTA reconstruction-based method without extra training data by a large margin, i.e., +18.3?. Note that the current SOTA Draem <ref type="bibr" target="#b18">[19]</ref> is not a classical reconstruction-based method, which requires a new training strategy and extra training data.</p><p>extra training data. c) Reconstruction-based methods <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref> contain a generator to reconstruct the input image, and the anomaly score is the more interpretable reconstruction error. These methods do not need pre-trained models and extra training data. However, current reconstruction-based methods without extra training data are much less expressive than other methods for the generator's poor reconstruction ability. In summary, current unsupervised anomaly detection approaches are still suffering from two main challenges: (1)) Some works achieve high AUC score but require abnormal samples or extra training data that are hard to obtain and costly for practical use.</p><p>(2)) Current reconstruction-based methods are more practical and do not need pre-trained models and extra training data but suffer from low performance. This paper focuses on studying the reconstruction-based method as it requires no extra training data and only normal samples that is more practical.</p><p>To improve the performance of the reconstruction-based method, we need to enhance the reconstruction ability of the generator for the anomaly detection task. For an im-age, different frequency bands contain different types of information, e.g., low frequency represents more semantic information while high frequency represents more detailed texture information. Also, we find that the model performance can be improved from the frequency domain perspective in many computer vision tasks, e.g., in image super-resolution task, <ref type="bibr" target="#b19">[20]</ref> separates the different frequency components to compensate for the loss of information in different frequency bands of real LR images to improve the performance of the model. Motivated by the idea, we analyze the frequency distribution of normal and abnormal images in the anomaly detection task. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>(a), we count the frequency energy distribution of normal and abnormal images, as the energy distribution of the Fourier-transformed image is reflected in the amplitude spectrum. We re-analyze this paradigm and find that normal and abnormal samples have different frequency distributions in sensory AD. So it may be difficult and unsuitable for only one generator to learn the full-frequency reconstruction of the RGB image. Therefore, we propose an anomaly detection framework using multiple frequency branches to reconstruct information from different frequency bands respectively. In order to differentiate the use of information from different frequency bands, we propose an effective Frequency Decoupling (FD) module to pre-obtain omni-frequency representation of the input image and use parallel generators to reconstruct images of multiple frequencies.</p><p>Considering the model efficiency, we conduct experiments with 2 or 3 frequency branches in this paper. Different frequency branches in the framework are independent by default. However, an image contains information in multiple frequency bands, and the information in different frequency bands is not completely unrelated to each other but complementary in the real world. So, we design a tailored Channel Selection (CS) module to further realize omni-frequency interaction among multiple branches that can adaptively select different channel features. Based on the above modules and the baseline Skip-GANomaly <ref type="bibr" target="#b20">[21]</ref>, we propose a novel Omni-frequency Channel-selection Reconstruction (OCR-GAN) network. Our method achieves state-of-the-art (SOTA) results on multiple public datasets consistently. Specifically, our OCR-GAN improves +0.3? than current SOTA method Draem <ref type="bibr" target="#b18">[19]</ref> and significantly +18.3? than SOTA reconstruction-based DGAD <ref type="bibr" target="#b16">[17]</ref> without extra training data on MVTec AD in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>, which strongly proves that the reconstruction-based method can also perform well even without extra training data and pretrained models. To the best of our knowledge, this paper is the first attempt to explore omni-frequency information with reconstruction-based anomaly detection method. Our main contributions can be summarized as follows:</p><p>? We rethink the difference between normal and abnormal images from the frequency domain perspective and propose a novel framework for anomaly detection based on omni-frequency reconstruction. ? We propose an effective FD module to obtain different frequency bands information of the image that enables the omni-frequency reconstruction by multiple branches. <ref type="bibr">?</ref>  The remainder of the paper is organized as follows. In Section II, we review some related works. Details of the proposed OCR-GAN method are given in Section III. Experimental results are presented in Section IV. And we conclude the paper with discussion and summary in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Anomaly detection methods can be mainly divided into density-based, classification-based and reconstruction-based methods as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Density-based methods</head><p>Density-based methods build a density estimation model for the distribution of normal training data. And this kind of method assumes that normal data have a higher likelihood under this model than abnormal data during inference. Parameter density estimation assumes that the density of normal data can be represented by some reference distribution. A pre-trained network is used to extract meaningful vectors representing the whole image or patch image for anomaly detection. The similarity between the representation vector of the test image and the reference vector is set as anomaly score. Some researches <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b26">[27]</ref> train the model on the entire image, while works <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> on the patch image. The normal distribution reference can be the parameter of the Gaussian distribution of the normal image embedding vectors <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b29">[30]</ref>, the mixed Gaussian distribution <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, the Poisson distribution <ref type="bibr" target="#b32">[33]</ref>, the center of the sphere containing the embedding from normal images <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b33">[34]</ref>, the entire set of normal embedding vectors <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b34">[35]</ref>, the feature of the last layer in the network <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, or the mid-level feature representation <ref type="bibr" target="#b37">[38]</ref>. Mahalanobis distance is used to calculate the anomaly score between the embedding vector of the test image and the reference vector of the normal training distribution. These methods have achieved good performance recently, but they lack interpretability that it is difficult to clearly distinguish which part of the image causes high abnormal scores. Also, this kind of method requires the pre-trained model for extracting vectors that is less practical for various real scenarios.</p><p>Another method of density estimation is normalizing flows. Normalizing flows are used to learn bijective transformations between data distributions with a special property. Differ-Net <ref type="bibr" target="#b38">[39]</ref> using normalizing flows to estimate the precise likelihood achieved good anomaly detection performance on MVTec AD. Since flow-based methods have no dimensional reduction, the computation cost is significant. And this kind of method also needs pre-trained models to extract features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Classification-based methods</head><p>Classification-based methods <ref type="bibr" target="#b39">[40]</ref> try to find the classification boundaries of normal data. DeepSVDD <ref type="bibr" target="#b33">[34]</ref> first introduces one-class classification to anomaly detection. Samples that deviate from the normal training sample description are considered abnormal. Moreover, there are some selfsupervised learning methods to design good proxy tasks to help the model detect anomalies from normal samples. One classical self-supervised anomaly detection method is isolation forest <ref type="bibr" target="#b40">[41]</ref>. This method is based on abnormal samples that can be isolated in fewer steps compared to normal samples. Other proxy tasks for self-supervised anomaly detection methods include image transformation prediction <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b41">[42]</ref>, contrastive learning <ref type="bibr" target="#b42">[43]</ref> and proxy binary classification <ref type="bibr" target="#b13">[14]</ref>. <ref type="bibr" target="#b13">[14]</ref> uses data augmentation to generate pseudo-anomaly data and then does a binary classification proxy task with normal training samples to train the feature extraction model. The selfsupervised method relies on the design of proxy tasks, which is difficult to perform well on multiple data sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ? ?</head><formula xml:id="formula_0">I 2 , . . . } from pre-processed Gaussian images {I G 1 , I G 2 , . . . }. Then {I 1 , I 2 , . . . } are fed into multiple generators {? 1 , ? 2 , . . . } to reconstruct corresponding images {? 1 ,? 2 , . . . },</formula><p>which are added to obtain the final output?. The proposed Channel Selection (CS) module performs omni-frequency interaction among different</p><formula xml:id="formula_1">encoders, i.e., {? E 1 , ? E 2 , . . . }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Reconstruction-based methods</head><p>One of the reconstruction-based methods is sparse reconstruction which assumes that normal samples can be reconstructed with a limited number of basis functions while abnormal samples are more expensive to reconstruct. L 1 normbased kernel PCA <ref type="bibr" target="#b43">[44]</ref> and low-rank embedded networks <ref type="bibr" target="#b44">[45]</ref> are belong to sparse reconstruction methods.</p><p>Abnormal images would get higher reconstruction error as they have a different data distribution than normal images so that the reconstruction error can be used as the anomaly score for the reconstruction-based method. The autoencoder (AE) <ref type="bibr" target="#b45">[46]</ref> and generative adversarial networks (GAN) <ref type="bibr" target="#b46">[47]</ref> can reconstruct samples from the normal training data. <ref type="bibr" target="#b47">[48]</ref> propose to use an autoencoder for the reconstruction process and structural similarity to measure reconstruction error. Some studies <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b48">[49]</ref> have shown that using adversarial network training would improve generation results. Moreover, GANbased methods have more suitable metrics that can play the role of anomaly score, e.g., output of the discriminator <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> and latent space distance <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. It is difficult to ensure the poor reconstruction for abnormal samples as the capacity of the generator is strong, so OCGAN <ref type="bibr" target="#b52">[53]</ref> uses denoising autoencoder, latent discriminator, visual discriminator, and classifier to ensure that any example generated from the learned latent space is indeed from the normal class. For GAN-based methods, the discriminator is usually used to distinguish the reconstructed image from the original image, but OGNet <ref type="bibr" target="#b53">[54]</ref> redefines the role of the discriminator that is used to distinguish reconstructed images of different qualities. Recently, <ref type="bibr" target="#b54">[55]</ref> utilize backpropagated gradients as representations to characterize anomalies, and DGAD <ref type="bibr" target="#b16">[17]</ref> learns representation by the guidance of the discriminator to improve the model performance. The generation ability of the generator has a significant influence on the effect of the reconstructionbased method, so <ref type="bibr" target="#b55">[56]</ref> propose to construct GAN ensembles for anomaly detection as GAN ensembles often outperform the single GAN. And <ref type="bibr" target="#b56">[57]</ref> propose multistage GAN to detect fabric defect. These methods indiscriminately reconstruct all frequencies of the RGB image that may be difficult for the generator, leading to poor results in anomaly detection. Also, we find that normal and abnormal samples have different frequency distributions, so we propose a new paradigm that uses parallel branches to reconstruct omni-frequency images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>In this section, we aim at improving the current reconstruction-based approach without extra training data and designing a generalized network for anomaly detection. As the difference between normal and abnormal images varies in different frequency bands, we perform anomaly detection from the perspective of the frequency domain. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, our method derives from a frequency-decoupling idea that comprises multiple generators, i.e., G={? 1 , ? 2 , . . . }, to reconstruct omni-frequency images {? 1 ,? 2 , . . . }, which is trained alternately with a discriminator D to further boost the model performance. Concretely, we propose an effective FD module to decouple the input image I to omni-frequency images {I 1 , I 2 , . . . } and a CS module to realize omni-frequency interaction by adaptively selecting channels among encoders</p><formula xml:id="formula_2">{? E 1 , ? E 2 , . . . }.</formula><p>When the model finishes the training, the abnormal images would be poorly reconstructed and get higher anomaly scores than normal images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Frequency Decoupling</head><p>Pixel distributions reflect the spatial frequency of the image. Different frequency components contain different information, e.g., the low frequency of the image contains more semantic information while the high frequency includes more details and texture information. As previously mentioned, normal and abnormal images have obvious frequency distribution differences, which derive from the abnormal elements in abnormal data, e.g., holes, cracks, and scratches in the MVTec AD dataset. For a more thorough analysis, we counted the frequency energy distribution of normal and abnormal images. The energy distribution of the Fourier-transformed image is reflected in the amplitude spectrum. And <ref type="figure" target="#fig_2">Fig. 3</ref>(a) also shows the difference between normal and abnormal images in the frequency domain. Therefore, we consider that the importance of information in different frequency bands varies in anomaly detection tasks, especially sensory anomaly detection.</p><p>Motivated by the difference in the frequency distribution of normal and abnormal images shown in <ref type="figure" target="#fig_2">Fig. 3</ref>(a), we propose a tailored Frequency Decoupling (FD) module to pre-obtain informative omni-frequency representations. Specifically, FD contains the following three processes.</p><p>(1) Convolving original image I with the Gaussian kernel Gau 1 : </p><formula xml:id="formula_3">Gau 1 = 1 256 ? ? ? ? ? ?</formula><formula xml:id="formula_4">? ? ? ? ? ? ,<label>(1)</label></formula><p>and then removing even rows and columns of the blurred image to obtain intermediate down-sampled image I blur .</p><p>(2) I blur would be exactly one-quarter the area of I and goes through a ?2? up-sampling to restore the original resolution, with the new even rows and columns filled with zeros. Then a similar convolution operation with the Gaussian kernel Gau 2 = 4 * Gau 1 is applied to approximate missing pixels, i.e., zeros in even rows and columns, and we obtain first-level blurred image I G N ?1 , where n represents the branch number (The smaller the N is, the less high-frequency information) and I G N is initialized with I, denoted as:</p><formula xml:id="formula_5">I G N = I, I G N ?1 = Up(Down(I G N * Gau 1 )) * Gau 2 ,<label>(2)</label></formula><p>where * means the convolution operation, while Down and Up are above-mentioned down-sampling and up-sampling operations. We would obtain a set of blurred images {I G1 , I G2 , . . . , I Gn } by repeating the above processes.</p><p>(3) The blurred images I Gn , n = 1, 2, . . . , N ? 1 lost some high-frequency information in varying degrees, and we further calculate the difference between adjacent images to obtain omni-frequency images:</p><formula xml:id="formula_6">I 1 = I G1 ,</formula><formula xml:id="formula_7">I 2 = I G1 ? I G2 , . . . I N = I G N ?1 ? I G N .<label>(3)</label></formula><formula xml:id="formula_8">1 2 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? element-wise summation element-wise product ? ? Fig. 5. Schematic diagram of CS.</formula><p>For simplicity, a two-branch case is shown here that F l and F h represent low-/high-frequency features, i.e., features in branch one and two. Augmented F l and F h are fed into following layers.</p><p>FD can be effectively applied to frequency-sensitive tasks such as anomaly detection by decoupling different-frequency components as needed, and we set branch number two in the paper by default. <ref type="figure" target="#fig_3">Fig. 4</ref> shows that different frequency components are reconstructed by multiple independent generators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Channel Selection</head><p>Different frequency branches are relatively independent in our anomaly detection framework with only the FD module, which goes against the objective fact that different frequencies complement each other. Besides, as shown in <ref type="figure">Fig. 5</ref>, the features of different channels in omni-frequency features are various. Attention <ref type="bibr" target="#b57">[58]</ref> can help us to achieve the selection of information in different frequency bands. Therefore, we design a novel Channel Selection (CS) module to realize omnifrequency interaction among multiple branches and adaptive selection of different channel features. <ref type="figure">Fig. 5</ref> shows the detailed structure of the CS module with a two-branch case that contains low and high-frequency features, but it is easy to extend to multiple branches. Concretely, for the given two feature maps F h ? R H?W ?C with high-frequency information and F l ? R H?W ?C with low-frequency information, we fuse them via an element-wise summation:</p><formula xml:id="formula_9">F = F l + F h .<label>(4)</label></formula><p>Then we apply Global Average Pooling to embed the global information and obtain channel-wise statistics z 1 ? R C :</p><formula xml:id="formula_10">z 1 c = F GAP (F c ) = 1 H ? W H i=1 W j=1 F c (i, j),<label>(5)</label></formula><p>where c is the c-th channel with C channels totally. After that, we use a fully connected layer to reduce the dimension of the embedded z 1 from C to d and obtain z 2 ? R d , which is able to provide a precise and adaptive selection:</p><formula xml:id="formula_11">z 2 = F F C (z 1 ).<label>(6)</label></formula><p>Finally, we use compact feature descriptor z 2 to regress c-th channel attentions for different frequency branch by: <ref type="bibr" target="#b5">6</ref> where L, H ? R C?d represent the parameter weights, while l and h denote the channel attention vectors for F l and F h . The augmented feature maps F l and F h are obtained through the channel attention operations as follows:</p><formula xml:id="formula_12">l c = e Lcz 2 e Lcz 2 + e Hcz 2 , h c = e Hcz 2 e Lcz 2 + e Hcz 2 ,<label>(7)</label></formula><formula xml:id="formula_13">F lc = l c ? F lc , F hc = h c ? F hc .<label>(8)</label></formula><p>CS module augments feature maps of different frequency branches by adaptively selecting channels, i.e., using l and h to re-weight F l and F h . Note that the attention vectors of two branches complement each other, i.e., l c + h c = 1, and this module can be easily extended to multiple branches by adding corresponding weights in Equation 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training</head><p>Our OCR-GAN is trained from scratch end-to-end with only normal samples and tested with both normal and abnormal samples. We expect the GAN to correctly reconstruct normal samples both in image and latent vector space. Consistent with the previous reconstruction-based methods, our OCR-GAN assumes that out-of-distribution (i.e., anomaly pixels) cannot be well reconstructed as the model is never trained on abnormal samples. Therefore, the difference between the reconstructed image and the input image in either image space or latent space is much greater for abnormal samples. Moreover, inspired by CutPaste <ref type="bibr" target="#b13">[14]</ref>, we also use data augmentation to generate forgery abnormal samples to assist the training process. As shown in <ref type="figure">Fig. 6</ref>, the forgery abnormal samples generated by data augmentation and the samples generated by the generator are both used as positive inputs to the discriminator D, while original normal samples are used as negative inputs. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, we adopt three losses for model training to ensure that OCR-GAN can well reconstruct normal samples. Content Loss. The first term L con ensures accurate reconstruction between the input normal image I and the reconstructed image? by 1 error:</p><formula xml:id="formula_14">L con = E I?pn [I ??] 1 .<label>(9)</label></formula><p>This loss enables learning how to reconstruct similar images from the training data directly. Adversarial Loss. The second term L adv employs a discriminator D for adversarial training <ref type="bibr" target="#b46">[47]</ref>, which significantly improves the quality of the constructed image. Our model is to minimize it for G and maximize for D. Adversarial loss allows the generator to reconstruct the image as realistically as possible, while the discriminator distinguishes the normal images from the reconstructed and forgery abnormal images:</p><formula xml:id="formula_15">L adv = E? ?pr [D(?)] + E I?p f [D( I)] ? E I?pn [D(I)],<label>(10)</label></formula><p>Latent Loss. Latent loss <ref type="bibr" target="#b20">[21]</ref> penalizes the similarity between the positive and negative images in the latent space. In OCR-GAN, we use the features of the last convolutional layer of the discriminator D as latent space features. Then, the 2 error between latent space features is used as the latent loss:</p><formula xml:id="formula_16">L lat = E I?pn [D lat (I) ? D lat (?) ? D lat ( I)] 2 .<label>(11)</label></formula><p>Positive Negative <ref type="figure">Fig. 6</ref>. Schematic diagram of using forgery abnormal samples to assist the training process Note that? = G(I), I is the forged abnormal data obtained by data augmentation, p n , p r and p f are normal, reconstructed and forged image distributions, and D lat (?) denotes the feature extraction of D for the penultimate layer. The total loss L all is a weighted sum of above losses:</p><formula xml:id="formula_17">L all = ? con L con + ? adv L adv + ? lat L lat ,<label>(12)</label></formula><p>where weight parameters are chosen as ? rec = 50, ? adv = 1, and ? lat = 1 for a balance training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Inference</head><p>Anomaly score proposed in <ref type="bibr" target="#b58">[59]</ref> is used to detect anomalies during inference. For a test image I, its anomaly score is defined as:</p><formula xml:id="formula_18">A(I) = ?L con (I) + (1 ? ?)L lat (I),<label>(13)</label></formula><p>where L con (I) is the reconstruction error that measures the content similarity between the input and reconstructed images, while L lat (I) is the latent representation error based on the latent loss. ? is the weight parameter and is set to 0.9. Based on Equation <ref type="bibr" target="#b12">13</ref>, we are able to compute the anomaly score for each test sample in the test set. The set of anomaly scores for all samples in the test set is A. Following <ref type="bibr" target="#b20">[21]</ref>, we scale A to [0, 1]. Therefore, the final anomaly score for a test image I is:</p><formula xml:id="formula_19">A (I) = A(I) ? min(A) max(A) ? min(A)<label>(14)</label></formula><p>Threshold of anomaly score can be set according to requirements in real world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In order to assess the effectiveness of the proposed OCR-GAN, we consider two types of anomaly detection tasks: sensory AD and semantic AD. We evaluate the performance of the proposed OCR-GAN in two cases against state-of-the-art methods using public-available datasets. A. Experimental Setup 1) Datasets: This paper focuses on the sensory AD task. Thus, we use MVTec AD <ref type="bibr" target="#b1">[2]</ref>, DGAM <ref type="bibr" target="#b59">[60]</ref> and Kolek-torSDD <ref type="bibr" target="#b8">[9]</ref> to evaluate the performance of OCR-GAN in sensory AD. To further validate that OCR-GAN can improve the generation ability of generators in anomaly detection tasks, we use CIFAR-10 [10] to evaluate the performance of OCR-GAN in semantic AD. MVTec AD [2] contains 5,354 high-resolution color images that consist of 10 kinds of objects and 5 kinds of textures, which is widely used for the anomaly detection task. The image resolution ranges from 700 to 1,024, and we downscale all images to 256?256 resolution for all experiments. The number of training samples for each category ranges from 60 to 320, and the abnormal samples in the test set contain more than 70 defects, e.g., cracks, scratches, deformation, and holes. DGAM [60] is a well-known benchmark database for surface defect detection. It contains images of various surfaces with artificially generated defects. Surfaces and defects are split into 10 classes of various difficulties. It is a weakly supervised dataset, and there are 8,050 training and testing sets each, and the ratio of positive and negative samples for each type is approximately 1:7. OCR-GAN is trained only on anomaly-free training samples for all experiments. KolektorSDD <ref type="bibr" target="#b9">[10]</ref> is constructed from images of defected electrical commutators. Specifically, microscopic fractions or cracks are observed on the surface of the plastic embedding in electrical commutators. The dataset contains 50 commutator samples, each with 8 surfaces, totaling 399 images in 500?1,240, of which 347 images are without any defect, and 52 images are with visible defects. The anomalies are tiny and visually similar to the background, making this dataset challenging for anomaly detection. CIFAR-10 [10] consists of 60,000 color images in 32?32 with 10 classes. Semantic AD experiments regard one class as normal and the other classes as abnormal. CIFAR-10 is a challenging semantic AD dataset because images differ substantially across classes, and the background of images are not aligned.</p><p>2) Implementation Details.: Our method is implemented by PyTorch 1.2.0 <ref type="bibr" target="#b64">[65]</ref> and CUDA 10.2, and all experiments run with a TITAN RTX GPU. We use Adam <ref type="bibr" target="#b65">[66]</ref> optimizer and set ? 1 = 0.5, ? 2 = 0.999, weight-decay=1e ?4 , and learning rate=0.002. Unless otherwise specified, the batchsize is set to 32 for MVTec AD dataset, 64 for DGAM dataset, 64 for KolektorSDD dataset and 64 for CIFAR-10 dataset, and we use two frequencies (denoted as low-/high-frequency branches) for experiments. We choose Skip-GANomaly <ref type="bibr" target="#b20">[21]</ref> as our baseline.</p><p>3) Evaluation Metrics: The Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) Curve is used as a standard evaluation metric for anomaly detection. It is calculated by gradually changing the threshold of anomaly scores. AUC is accumulated to a score for the performance evaluation. A higher AUC score means better anomaly detection performance. 4) Default Setting: Following the previous unsupervised anomaly detection methods settings, our ablation experiments and interpretability experiments are all conducted on the MVTec AD dataset unless otherwise specified. Considering the number of parameters and the computational cost, we choose two frequency branches (high frequency and low frequency) for the experiments if not specified. Moreover, our OCR-GAN keeps the same parameter settings as the baseline. If not specified, the number of feature channels of each generator is set to 64. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>AUC skipGAN <ref type="bibr" target="#b20">[21]</ref> 55.1 Puzzle AE <ref type="bibr" target="#b60">[61]</ref> 55.4 DifferNet <ref type="bibr" target="#b38">[39]</ref> 84.9 InTra <ref type="bibr" target="#b61">[62]</ref> 70.1 CutPaste <ref type="bibr" target="#b13">[14]</ref> 60.2 Draem <ref type="bibr" target="#b18">[19]</ref> 85.9 Ours 91.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Compare with SOTA Methods</head><p>We evaluate the performance of our OCR-GAN on four popular datasets to verify the superiority of our method over other SOTA methods. 1) Sensory AD. The difference between normal and abnormal images in the sensory AD task is covariate shift. And anomalies usually appear in the form of defects, such as cracks, scratches and holes. Actually, we design our OCR-GAN based on the motivation of difference in the frequency distribution of normal and abnormal samples in the MVTec AD dataset (shown in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>). And the frequency analysis can be extended to other sensory AD datasets. Therefore, we choose three different sensory AD datasets to evaluate the effectiveness of our method in the sensory AD task.</p><p>MVTec AD. <ref type="table" target="#tab_2">Table I</ref> shows the detection AUC results of different methods on the MVTec AD dataset, and our experiments run five times using different random seeds without extra training data. We report the mean AUC score with corresponding standard error for each category and the average AUC for texture, object, and all categories. Results indicate that our approach achieves a new SOTA on the MVTec AD dataset without extra training data, i.e., obtaining 98.3 detection AUC score. OCR-GAN improves the AUC score by a significant +18.3? compared with SOTA classical reconstruction-based method DGAD <ref type="bibr" target="#b16">[17]</ref> without extra training data, by 3.6? compared with SOTA density-based method DifferNet <ref type="bibr" target="#b38">[39]</ref>, and by 1.2? compared with SOTA classification-based method Cut-Paste <ref type="bibr" target="#b13">[14]</ref>. Our OCR-GAN belongs to classical reconstructionbased approaches that use the generator to reconstruct the image and use the reconstruction error to detect anomalies. The current SOTA method, Draem <ref type="bibr" target="#b18">[19]</ref>, trains two models (reconstruction model and anomaly segmentation model) us-ing extra training data and uses the results of the anomaly segmentation to detect anomalies, which is very different from the classical reconstruction-based approaches. However, our OCR-GAN achieves a higher AUC score than Draem <ref type="bibr" target="#b18">[19]</ref> by +0.3 ? without using extra training data, proving the effectiveness of our approach. Although our OCR-GAN does not achieve the best performance in every category, we obtain the highest overall score, and the AUC score of each category surpasses 95 (c.f . Ours ? in the table) proves the robustness and practicality of our method. In conclusion, it is remarkable that we firstly achieve SOTA on the MVTec AD dataset with a classical reconstruction-based method without extra training data while previous classical reconstruction-based methods fail to perform so well on sensory AD. DGAM. Our OCR-GAN is trained only on normal samples for the DGAM dataset. <ref type="table" target="#tab_2">Table II</ref> shows that supervised methods have achieved near-perfect AUC on the DGAM dataset. Compared with supervised methods, previous unsupervised methods (including classification-based methods, reconstructionbased methods, and density-based methods) did not perform well on this dataset. However, our OCR-GAN achieves a 99.3 detection AUC score without extra training data. Our performance is comparable to supervised methods, which is a remarkable result. KolektorSDD. OCR-GAN is compared with other unsupervised methods, and results are shown in <ref type="table" target="#tab_2">Table III</ref>. As the anomaly elements in the dataset are small and similar to the background, previous unsupervised methods do not perform well on the KolektorSDD dataset. Without extra training data, our OCR-GAN achieves a 91.4 detection AUC score that increases by +5.5? over the suboptimal method Draem which is trained with extra data. This result shows that our approach also performs reliably in challenging sensory AD dataset.</p><p>2) Semantic AD. Experiments on the semantic anomaly detection task are performed to assist in proving the effectiveness of our approach. The difference between normal and abnormal images in the semantic AD <ref type="bibr" target="#b66">[67]</ref> task is label shift. There is no variability across frequency bands of normal and abnormal in this task. However, our method can also improve the generation ability of the generator. We choose a public dataset CIFAR-10, which is widely used for oneclass detection (semantic AD), to verify the effectiveness of our method on all types of anomaly detection tasks. CIFAR-10. As shown in <ref type="table" target="#tab_2">Table IV</ref>, reconstruction-based methods perform better in the semantic AD compared to the  density-based methods and the classification-based methods. Our OCR-GAN outperforms all compared unsupervised methods and achieves 89.4 AUC on the CIFAR-10 dataset, which is +16.3? higher than the suboptimal method. The results verify that our OCR-GAN performs well in one-class detection (semantic AD). Although the difference in frequency bands is not suitable for semantic anomaly detection, the improvement to structural design implicitly improves the model's reconstruction ability. Therefore the performance of the model in semantic AD is also enhanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>Influence of Different Components. We further conduct an ablation study on the MVTec AD dataset to investigate the effectiveness of each component of the proposed OCR-GAN. We choose Skip-GANomaly <ref type="bibr" target="#b20">[21]</ref> as our baseline and gradually add different component, performing following seven experiments: (1) Baseline; (2) Adding FD module; (3) Adding both FD and CS modules, i.e., OCR-GAN in the paper; (4) Using three frequency branches; (5) Training with forgery abnormal images by the cutout data augmentation; <ref type="bibr" target="#b5">(6)</ref> Training with forgery abnormal images by the cutpaste data augmentation; <ref type="bibr" target="#b6">(7)</ref> Training with forgery abnormal images by both cutout and cutpaste data augmentations. As shown in <ref type="table" target="#tab_5">Table V</ref>, our baseline only obtains a 60.2 AUC score because this reconstruction-based method suffers from the poor reconstruction ability of the generator. When the FD module is added, the model performance increases by a significant +14.6?, and our proposed CS module further improves the AUC score by +22.1? to 96.9. The results strongly demonstrate the effectiveness of our proposed two modules for the anomaly detection task. Moreover, our approach improves by +0.6? when using three frequency branches, meaning that more frequencies contribute to the model performance. We set the frequency number as two in the paper for balancing model effectiveness and efficiency. And we use data augmentation to generate forgery abnormal samples to assist the training process. As shown in  Anomaly Score Anomaly Score Anomaly Score Anomaly Score Anomaly Score  different frequencies contain different information, we conduct an ablation study on the MVTec AD dataset to explore the anomaly detection performance using different frequency branches. As shown in <ref type="table" target="#tab_2">Table VI</ref>, we conduct a set of experiments using only a high-frequency branch, only a lowfrequency branch, two independent frequency branches, and two frequency branches with CS module (two-frequencybranch OCR-GAN). Results show that using only highfrequency information performs better than low-frequency information, meaning that abnormal elements contain more highfrequency information. Nevertheless, using two-frequency branches independently is not ideal for lacking the information interaction between different frequency branches. Our designed CS module can well handle this problem and further improve the model performance. Influence of Number of Parameters. Our OCR-GAN reconstructs the information of different frequency bands respectively, which means that one frequency band requires one generator. Considering the model efficiency, we conduct experiments mostly with two frequency branches. Compared with baseline, our OCR-GAN has a larger number of method parameters. So, we reduce the number of feature channels of the generator to explore the influence of model parameters on the model performance. As shown in <ref type="figure" target="#fig_4">Fig. 7</ref>, when we reduce the number of feature channels, the model performs better. Our model performs best when the number of channels is set to 4, achieving 98.7 AUC on the MVTec AD dataset. This means that our lightweight model can even perform better in anomaly detection task. Since the lightweight model is not the focus of this study, we will further fully study the design of the lightweight anomaly detection model and why the lightweight model can bring certain performance improvement in the future work. For fair comparison, all of our experiments in this paper (except this one) use the standard model, i.e., the number of feature channels equals 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottle</head><p>Transistor Pill </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wood Leather Grid</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ori Rec Diff Rec Diff Rec Diff</head><p>Skip-GAN Puzzle-AE OCR-GAN <ref type="figure" target="#fig_0">Fig. 11</ref>. Reconstruction results of three reconstruction-based methods.</p><p>Ori: Original images with anomaly segmentation ground truth. Rec: Reconstructed images. Diff: Differences between original and reconstructed images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Interpretability of OCR-GAN</head><p>Analysis of Histogram. We visualize the anomaly score histogram for each category to further prove the effectiveness of our OCR-GAN. As shown in <ref type="figure" target="#fig_6">Fig. 8</ref>, abnormal samples would get higher anomaly scores while normal samples get lower anomaly scores, and there is a clear distinction between normal and abnormal samples, meaning that our model can well distinguish abnormal samples from normal samples by the anomaly score. <ref type="figure" target="#fig_7">Fig. 9</ref> shows that normal samples and abnormal samples can not be distinguished by anomaly score in the histogram of the baseline. We further assess our proposed FD and CS module and results similarly indicate that each module contributes to the model.</p><p>Visualization of latent-space features. We map the latentspace features from the last convolution layer of the D for each test sample to a two-dimensional subspace. <ref type="figure" target="#fig_0">Fig. 10</ref> shows that our proposed OCR-GAN yields promising separation between normal and abnormal samples in the latent space.</p><p>Reconstruction results. The reconstruction ability of the generator has a significant influence on the performance of the GAN-based method in the anomaly detection task. As shown in <ref type="figure" target="#fig_0">Fig. 11</ref>, we visualize the reconstructed images and the difference images between the reconstructed and original images to explore the reconstruction ability of different methods. Reconstructed results indicate that our OCR-GAN has a better reconstruction ability for details, and the abnormal areas are more prominent in the difference images than other classical reconstruction-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper proposes a novel reconstruction-based OCR-GAN for anomaly detection from a perspective of frequency domain. Specifically, we propose FD module to decouple the input image into different frequencies and model the reconstruction process as a combination of parallel omni-frequency image restorations. To better perform frequency interaction among different encoders, we propose a tailored CS module to adaptively select different channels among multiple branches. Our approach achieves new SOTA results over current SOTA methods on both sensory AD and semantic AD tasks even without extra training data, meaning that the proposed OCR-GAN is robust and effective for practical applications.</p><p>In the future, we will further explore the design of the lightweight model for AD tasks, while building more difficult practical dataset and testing the corresponding effects of our and other methods, hoping to contribute to the development of this field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustrations of sensory anomaly detection (Left) and semantic anomaly detection (Right) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Pipeline illustrations of three kinds of unsupervised anomaly detection methods in column. Bottom two rows indicate whether Pretrained Model and Extra Training Data are used for each kind of method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>(a) Energy distribution with frequencies for normal and abnormal samples in MVTec AD dataset, and the shadow represents standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Overview of proposed OCR-GAN. Input image I goes through Frequency Decoupling (FD) module to obtain omni-frequency images {I 1 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Influence of channel numbers. With the number of generator feature channels decreases, the AUC first increases and then decreases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Histogram of anomaly scores for the normal and abnormal samples for each category in the MVTec AD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Comparison of anomaly score histograms for all category. (a):Baseline. (b):Adding FD. (c):Adding both FD and CS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>t-SNE visualization of normal and abnormal samples for eight categories in MVTec AD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We propose a CS module to realize omni-frequency interaction among multiple branches and adaptive selection of different channel features. Abundant experiments demonstrate the superiority of our OCR-GAN over SOTA methods, e.g., we achieve a new SOTA 98.3 detection AUC on the MVTec AD dataset without extra training data, which markedly surpasses the SOTA reconstruction-based method without extra training data by +18.3? and the SOTA method by +0.3?.</figDesc><table /><note>?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I</head><label>I</label><figDesc>AUC RESULTS WITH SOTAS ON MVTEC AD DATASET. THREE TO TEN COLUMNS ARE RECONSTRUCTION-BASED METHODS WHILE THE FOLLOWING FOUR COLUMNS ARE DENSITY-BASED AND CLASSIFICATION-BASED METHODS. BOLD AND UNDERLINE REPRESENT OPTIMAL AND SUBOPTIMAL RESULTS. ' MEANS THE YEAR OF PUBLICATION. ? MEANS USING THE PRE-TRAINED MODEL WITH EXTRA DATASET. ? MEANS OUR TRAINING WITH FORGERY ABNORMAL SAMPLES IN SECTION IV-C.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">AGAN AE1</cell><cell cols="10">AE2 SkipG GradC P-AE DGAD Draem Diff CutPaste CutPaste  ? InTra</cell><cell></cell></row><row><cell></cell><cell>Items</cell><cell>[59]</cell><cell>[48]</cell><cell>[48]</cell><cell>[21]</cell><cell>[55]</cell><cell>[61]</cell><cell>[17]</cell><cell>[19]</cell><cell>[39]</cell><cell>[14]</cell><cell>[14]</cell><cell>[62]</cell><cell>Ours</cell><cell>Ours  ?</cell></row><row><cell></cell><cell></cell><cell>17'</cell><cell>18'</cell><cell>18'</cell><cell>19'</cell><cell>20'</cell><cell>20'</cell><cell>21'</cell><cell>21'</cell><cell>21'</cell><cell>21'</cell><cell>21'</cell><cell>21'</cell><cell></cell></row><row><cell></cell><cell>Carpet</cell><cell>49.0</cell><cell>67.0</cell><cell>50.0</cell><cell>70.9</cell><cell>89.3</cell><cell>65.7</cell><cell>52.0</cell><cell>97.0</cell><cell>92.9</cell><cell>93.1</cell><cell>100.0</cell><cell>98.8</cell><cell cols="2">98.9?0.5 99.4?0.3</cell></row><row><cell>texture</cell><cell>Grid Leather Tile</cell><cell>51.0 52.0 51.0</cell><cell>69.0 46.0 52.0</cell><cell>78.0 44.0 77.0</cell><cell>47.7 60.9 29.9</cell><cell>71.6 69.3 63.4</cell><cell>75.4 72.9 65.5</cell><cell>67.0 94.0 83.0</cell><cell>99.9 100.0 99.6</cell><cell>84.0 97.1 99.4</cell><cell>99.9 100.0 93.4</cell><cell>99.1 100.0 99.8</cell><cell cols="3">100.0 99.6?0.2 99.6?0.2 100.0 97.1?0.6 97.1?0.8 98.2 92.2?0.8 95.5?1.5</cell></row><row><cell></cell><cell>Wood</cell><cell>68.0</cell><cell>83.0</cell><cell>74.0</cell><cell>19.9</cell><cell>76.7</cell><cell>89.5</cell><cell>72.0</cell><cell>99.1</cell><cell>99.8</cell><cell>98.6</cell><cell>99.8</cell><cell>98.0</cell><cell cols="2">95.8?1.6 95.7?1.1</cell></row><row><cell></cell><cell>Average</cell><cell>54.2</cell><cell>63.4</cell><cell>64.6</cell><cell>45.86</cell><cell>74.1</cell><cell>73.8</cell><cell>73.6</cell><cell>99.1</cell><cell>94.6</cell><cell>95.7</cell><cell>99.7</cell><cell>99.0</cell><cell cols="2">96.6?0.3 97.5?0.3</cell></row><row><cell></cell><cell>Bottle</cell><cell>69.0</cell><cell>88.0</cell><cell>80.0</cell><cell>85.2</cell><cell>52.0</cell><cell>94.2</cell><cell>97.0</cell><cell>99.2</cell><cell>99.0</cell><cell>98.3</cell><cell>100.0</cell><cell cols="3">100.0 99.6?0.2 99.6?0.1</cell></row><row><cell></cell><cell>Cable</cell><cell>53.0</cell><cell>61.0</cell><cell>56.0</cell><cell>54.4</cell><cell>58.7</cell><cell>87.9</cell><cell>90.0</cell><cell>91.8</cell><cell>95.9</cell><cell>80.6</cell><cell>96.2</cell><cell>84.2</cell><cell cols="2">99.2?0.5 99.1?0.6</cell></row><row><cell></cell><cell>Capsule</cell><cell>58.0</cell><cell>61.0</cell><cell>62.0</cell><cell>54.3</cell><cell>55.6</cell><cell>66.9</cell><cell>60.0</cell><cell>98.5</cell><cell>86.9</cell><cell>96.2</cell><cell>95.4</cell><cell>86.5</cell><cell cols="2">95.4?0.4 96.2?0.6</cell></row><row><cell>object</cell><cell>Hazelnut Metal Nut Pill</cell><cell>50.0 50.0 62.0</cell><cell>54.0 54.0 60.0</cell><cell>88.0 73.0 62.0</cell><cell>24.5 81.4 67.1</cell><cell>91.4 56.0 92.4</cell><cell>91.2 66.3 71.6</cell><cell>80.0 95.0 76.0</cell><cell>100.0 98.7 98.9</cell><cell>99.3 96.1 88.8</cell><cell>97.3 99.3 64.7</cell><cell>99.9 98.6 93.3</cell><cell>95.7 96.9 90.2</cell><cell cols="2">88.2?2.0 98.5?1.3 98.7?0.2 99.5?0.3 98.5?0.4 98.3?0.2</cell></row><row><cell></cell><cell>Screw</cell><cell>35.0</cell><cell>51.0</cell><cell>69.0</cell><cell>87.9</cell><cell>78.2</cell><cell>57.8</cell><cell>67.0</cell><cell>93.9</cell><cell>96.3</cell><cell>86.3</cell><cell>86.6</cell><cell>95.7</cell><cell cols="2">100.0?0.0 100.0?0.0</cell></row><row><cell></cell><cell cols="2">Toothbrush 57.0</cell><cell>74.0</cell><cell>98.0</cell><cell>58.6</cell><cell>98.0</cell><cell>97.8</cell><cell>93.0</cell><cell>100.0</cell><cell>98.6</cell><cell>98.3</cell><cell>90.7</cell><cell>99.7</cell><cell cols="2">98.2?0.9 98.7?0.7</cell></row><row><cell></cell><cell>Transistor</cell><cell>67.0</cell><cell>52.0</cell><cell>71.0</cell><cell>84.5</cell><cell>72.8</cell><cell>86.0</cell><cell>88.0</cell><cell>93.1</cell><cell>91.1</cell><cell>95.5</cell><cell>97.5</cell><cell>95.8</cell><cell cols="2">94.9?0.3 98.3?1.5</cell></row><row><cell></cell><cell>Zipper</cell><cell>59.0</cell><cell>80.0</cell><cell>80.0</cell><cell>76.1</cell><cell>56.6</cell><cell>75.7</cell><cell>82.0</cell><cell>100</cell><cell>95.1</cell><cell>99.4</cell><cell>99.9</cell><cell>99.4</cell><cell cols="2">97.6?0.4 99.0?0.2</cell></row><row><cell></cell><cell>Average</cell><cell>56.0</cell><cell>63.5</cell><cell>73.9</cell><cell>67.4</cell><cell>71.2</cell><cell>79.5</cell><cell>82.8</cell><cell>97.4</cell><cell>94.7</cell><cell>94.3</cell><cell>95.8</cell><cell>94.4</cell><cell cols="2">97.0?0.2 98.7?0.3</cell></row><row><cell></cell><cell>All</cell><cell>55.0</cell><cell>63.0</cell><cell>71.0</cell><cell>60.2</cell><cell>72.1</cell><cell>77.6</cell><cell>80.0</cell><cell>98.0</cell><cell>94.7</cell><cell>95.2</cell><cell>97.1</cell><cell>95.9</cell><cell cols="2">96.9?0.2 98.3?0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II AUC</head><label>II</label><figDesc>RESULTS WITH SOTAS ON DGAM DATASET. BOLD AND UNDERLINE REPRESENT OPTIMAL AND SUBOPTIMAL UNSUPERVISED RESULTS.</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="11">Class1 Class2 Class3 Class4 Class5 Class6 Class7 Class8 Class9 Class10 Average</cell></row><row><cell></cell><cell>skipGAN [21]</cell><cell>58.3</cell><cell>56.1</cell><cell>55.1</cell><cell>53.7</cell><cell>57.4</cell><cell>66.8</cell><cell>52.4</cell><cell>53.7</cell><cell>52.3</cell><cell>52.2</cell><cell>55.8</cell></row><row><cell>Unsup.</cell><cell>Puzzle AE [61] CutPaste [14] DifferNet [39]</cell><cell>50.7 56.1 59.7</cell><cell>50.5 87.8 82.9</cell><cell>58.7 57.1 69.8</cell><cell>70.0 71.3 97.3</cell><cell>63.6 47.4 61.2</cell><cell>92.3 68.8 97.0</cell><cell>54.0 96.5 68.5</cell><cell>49.1 53.4 52.1</cell><cell>54.6 51.9 78.2</cell><cell>49.6 74.7 79.1</cell><cell>59.3 66.0 74.6</cell></row><row><cell></cell><cell>Draem [19]</cell><cell>96.1</cell><cell>98.3</cell><cell>99.5</cell><cell>99.6</cell><cell>92.1</cell><cell>100</cell><cell>99.7</cell><cell>99.9</cell><cell>98.9</cell><cell>96.0</cell><cell>98.0</cell></row><row><cell></cell><cell>Ours</cell><cell>99.1</cell><cell>100</cell><cell>99.1</cell><cell>99</cell><cell>100</cell><cell>97.5</cell><cell>99.8</cell><cell>99.8</cell><cell>99.5</cell><cell>99.2</cell><cell>99.3</cell></row><row><cell>Sup.</cell><cell>Lin et al. [63] B? et al. [64]</cell><cell>100 100</cell><cell>94.0 100</cell><cell>100 100</cell><cell>100 100</cell><cell>100 99.9</cell><cell>100 100</cell><cell>100 100</cell><cell>99.0 100</cell><cell>100 100</cell><cell>100 100</cell><cell>99.3 100</cell></row><row><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">AUC RESULTS WITH SOTAS ON KOLEKTORSDD DATASET. BOLD AND</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">UNDERLINE REPRESENT OPTIMAL AND SUBOPTIMAL RESULTS.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV AUC</head><label>IV</label><figDesc>RESULTS WITH SOTAS ON CIFAR-10 DATASET. BOLD AND UNDERLINE REPRESENT OPTIMAL AND SUBOPTIMAL RESULTS.</figDesc><table><row><cell>Methods</cell><cell>Plane</cell><cell>Car</cell><cell>Bird</cell><cell>Cat</cell><cell>Deer</cell><cell>Dog</cell><cell>Frog</cell><cell>Horse</cell><cell>Ship</cell><cell>Truck</cell><cell>Average</cell></row><row><cell>OCSVM [36]</cell><cell>63.0</cell><cell>44.0</cell><cell>64.9</cell><cell>48.7</cell><cell>73.5</cell><cell>50.0</cell><cell>72.5</cell><cell>53.3</cell><cell>64.9</cell><cell>50.8</cell><cell>58.6</cell></row><row><cell>AnoGAN [59]</cell><cell>67.1</cell><cell>54.7</cell><cell>52.9</cell><cell>54.5</cell><cell>65.1</cell><cell>60.3</cell><cell>58.5</cell><cell>62.5</cell><cell>75.8</cell><cell>66.5</cell><cell>61.8</cell></row><row><cell>skipGAN [21]</cell><cell>44.8</cell><cell>95.3</cell><cell>60.7</cell><cell>60.2</cell><cell>61.5</cell><cell>93.1</cell><cell>78.8</cell><cell>79.7</cell><cell>65.9</cell><cell>90.7</cell><cell>73.1</cell></row><row><cell>OCGAN [53]</cell><cell>75.7</cell><cell>53.1</cell><cell>64.0</cell><cell>62.0</cell><cell>72.3</cell><cell>62.0</cell><cell>72.3</cell><cell>57.5</cell><cell>82.0</cell><cell>55.4</cell><cell>65.7</cell></row><row><cell>Gradcon [55]</cell><cell>76.0</cell><cell>59.8</cell><cell>64.8</cell><cell>58.6</cell><cell>73.3</cell><cell>60.3</cell><cell>68.4</cell><cell>56.7</cell><cell>78.4</cell><cell>67.8</cell><cell>66.4</cell></row><row><cell>Puzzle AE [61]</cell><cell>78.9</cell><cell>78.0</cell><cell>70.0</cell><cell>54.9</cell><cell>75.5</cell><cell>66.0</cell><cell>74.8</cell><cell>73.3</cell><cell>83.3</cell><cell>70.0</cell><cell>72.5</cell></row><row><cell>Draem [19]</cell><cell>58.8</cell><cell>56.5</cell><cell>55.6</cell><cell>58.5</cell><cell>53.0</cell><cell>64.7</cell><cell>59.0</cell><cell>54.3</cell><cell>51.0</cell><cell>54.4</cell><cell>56.6</cell></row><row><cell>CutPaste [14]</cell><cell>70.0</cell><cell>62.0</cell><cell>62.6</cell><cell>62.0</cell><cell>53.8</cell><cell>62.9</cell><cell>62.4</cell><cell>59.9</cell><cell>51.8</cell><cell>57.6</cell><cell>60.5</cell></row><row><cell>Intra [62]</cell><cell>50.2</cell><cell>48.9</cell><cell>57.8</cell><cell>49.2</cell><cell>55.4</cell><cell>60.3</cell><cell>44.5</cell><cell>65.7</cell><cell>73.8</cell><cell>64.9</cell><cell>57.1</cell></row><row><cell>Ours</cell><cell>99.9</cell><cell>80.2</cell><cell>82.5</cell><cell>85.4</cell><cell>98.5</cell><cell>87.3</cell><cell>98.6</cell><cell>76.9</cell><cell>99.8</cell><cell>85.2</cell><cell>89.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell cols="5">ABLATION STUDY ON MVTEC AD DATASET. BN REPRESENTS BRANCH</cell></row><row><cell></cell><cell></cell><cell>NUMBER.</cell><cell></cell><cell></cell></row><row><cell cols="5">BN FD CS cutout cutpaste AUC</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>60.2</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>74.8+14.6</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>96.9+36.7</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell>97.5+37.3</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>97.4+37.2</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>97.5+37.3</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>98.3+38.1</cell></row><row><cell></cell><cell></cell><cell>TABLE VI</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">ABLATION STUDY FOR FREQUENCY BRANCHES.</cell></row><row><cell>Category</cell><cell>high frequency</cell><cell>low frequency</cell><cell>two branches</cell><cell>OCR-GAN</cell></row><row><cell>texture</cell><cell>85.2</cell><cell>69.8</cell><cell>73.6</cell><cell>96.6</cell></row><row><cell>object</cell><cell>81.3</cell><cell>75.1</cell><cell>75.4</cell><cell>97.0</cell></row><row><cell>all</cell><cell>82.6</cell><cell>73.3</cell><cell>74.8</cell><cell>96.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Archive film defect detection and removal: an automatic restoration framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirmehdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3757" to="3769" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Thoracic disease identification and localization with limited supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8290" to="8299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classifier two sample test for video anomaly detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-03" />
			<biblScope unit="page">71</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Localizing anomalies from weakly-labeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4505" to="4515" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domaintransformable sparse representation for anomaly detection in movingcamera videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jardim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Thomaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Netto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1329" to="1343" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmentation-Based Deep-Learning Approach for Surface-Defect Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tabernik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?ela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Skvar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Manufacturing</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sub-image anomaly detection with deep pyramid correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02357</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Patch svdd: Patch-level svdd for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Padim: A patch distribution modeling framework for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Audigier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="475" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cutpaste: Self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9664" to="9674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anomaly detection in video sequence with appearance-motion correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1273" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Discriminativegenerative representation learning for one-class anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.12753</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A lightweight spatial and temporal multi-feature fusion network for defect detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="472" to="486" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Draem-a discriminatively trained reconstruction embedding for surface anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zavrtanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8330" to="8339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning omni-frequency region-adaptive representations for real image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1975" to="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Skip-ganomaly: Skip connected and adversarially trained encoder-decoder anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ak?ay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transfer representation-learning for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Are pre-trained cnns good feature extractors for anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Nazare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ponti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08495</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Classification-based anomaly detection for general data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling the distribution of normal data in pre-trained deep features for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Merhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6726" to="6733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dfr: Deep feature reconstruction for unsupervised anomaly segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07122</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reference-based defect detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6637" to="6647" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Region-based anomaly localisation in crowded scenes via trajectory analysis and path prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wiliem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 International Conference on Digital Image Computing: Techniques and Applications (DICTA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised anomaly detection with multi-scale interpolated gaussian descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10043</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Anomaly detection over noisy data using learned probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eskin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mixture densities, maximum likelihood and the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Redner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="239" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Poisson factorization for peer-based anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turcotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcphall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Intelligence and Security Informatics (ISI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="208" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep nearest neighbor anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10445</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">One-class svm for learning in image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2001 International Conference on Image Processing (Cat. No. 01CH37205)</title>
		<meeting>2001 International Conference on Image Processing (Cat. No. 01CH37205)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="34" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepanomaly: Fully convolutional neural network for fast anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Moayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Towards total recall in industrial anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pemula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08265</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Same same but differnet: Semi-supervised defect detection with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1907" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image outlier detection and feature extraction via l1-norm-based 2d probabilistic pca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4834" to="4846" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Isolation forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 eighth ieee international conference on data mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Csi: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th Conference on Neural Information Processing Systems (NeurIPS) 2020. Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">L1 norm based kpca for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="389" to="396" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Lren: Low-rank embedded network for sample-free hyperspectral anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4139" to="4146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</title>
		<meeting>the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>VISAPP</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3379" to="3388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Ganomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="622" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Ocgan: One-class novelty detection using gans with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2898" to="2906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Old is gold: Redefining the adversarially learned one-class classifier training paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Astrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Backpropagated gradient representations for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prabhushankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Temel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="206" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Gan ensemble for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4090" to="4097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multistage gan for fabric defect detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3388" to="3400" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Weakly supervised learning for industrial optical inspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wieler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM symposium in</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Puzzle-ae: Novelty detection in images through solving puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Rohban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Rabiee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12959</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Inpainting transformer for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pirnay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13897</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An efficient network for surface defect detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page">6085</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">End-to-end training of a twostage neural network for defect detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bo?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tabernik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5619" to="5626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning deep features for one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5450" to="5463" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
