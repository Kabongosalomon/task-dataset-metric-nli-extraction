<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CWS-PResUNet: Music Source Separation with Channel-wise Subband Phase-aware ResUNet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohe</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sound, Audio, and Music Intelligence (SAMI) Group</orgName>
								<address>
									<country>ByteDance</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sound, Audio, and Music Intelligence (SAMI) Group</orgName>
								<address>
									<country>ByteDance</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sound, Audio, and Music Intelligence (SAMI) Group</orgName>
								<address>
									<country>ByteDance</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CWS-PResUNet: Music Source Separation with Channel-wise Subband Phase-aware ResUNet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>License Authors of papers retain copyright and release the work under a Creative Commons Attribution 4.0 International License (CC BY 4.0). In partnership with</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Introductions and Related Works</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Music source separation (MSS) shows active progress with deep learning models in recent years. Many MSS models perform separations on spectrograms by estimating bounded ratio masks and reusing the phases of the mixture. When using convolutional neural networks (CNN), weights are usually shared within a spectrogram during convolution regardless of the different patterns between frequency bands. In this study, we propose a new MSS model, channel-wise subband phase-aware ResUNet (CWS-PResUNet), to decompose signals into subbands and estimate an unbound complex ideal ratio mask (cIRM) for each source. CWS-PResUNet utilizes a channel-wise subband (CWS) feature to limit unnecessary global weights sharing on the spectrogram and reduce computational resource consumptions. The saved computational cost and memory can in turn allow for a larger architecture. On the MUSDB18HQ test set, we propose a 276-layer CWS-PResUNet and achieve state-of-the-art (SoTA) performance on vocals with an 8.92 signal-to-distortion ratio (SDR) score. By combining CWS-PResUNet and Demucs, our ByteMSS system ranks the 2nd on vocals score and 5th on average score in the 2021 ISMIR Music Demixing (MDX) Challenge limited training data track (leaderboard A). Our code and pre-trained models are publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introductions and Related Works</head><p>Music source separation aims at decomposing a music mixture into several soundtracks, such as Vocals, Bass, Drums, and Other tracks. It is closely related to topics like music transcription, remixing, and retrieval. Based on deep learning models, most of the early studies <ref type="bibr" target="#b2">(Jansson et al., 2017;</ref><ref type="bibr" target="#b13">Takahashi et al., 2018)</ref> perform separations in the frequency domain by estimating the ideal ratio masks (IRM) of the magnitude spectrogram and reusing the phase of the mixture. Later, time-domain models <ref type="bibr" target="#b1">(D?fossez et al., 2019)</ref> start to demonstrate SoTA performance using direct waveform modeling, which does not involve transformations like short-time fourier transform (STFT). In this case, phase information can be implicitly estimated and models will not be restricted with the fixed time-frequency resolution. To enhance the MSS performance, Y.  chose to employ a self-attension mechanism and Dense-UNet architecture. <ref type="bibr" target="#b0">Choi et al. (2019)</ref> compared the performance of several types of UNet built with different intermediate blocks. To alleviate the computational cost, <ref type="bibr" target="#b3">Kadandale et al. (2020)</ref> designed a multi-task model to replace source-dedicated models. Also, H.  proposed to use the channel-wise subband feature to reduce resource consumptions and improve separation performance. Recently, <ref type="bibr" target="#b4">Kong et al. (2021)</ref> conducted an experiment on the MSS system theoretical upper bound, which proved the limitation of IRMs and the importance of phase estimation.</p><p>In the next section, we will introduce the detailed architecture of CWS-PResUNet as well as ByteMSS, the system we submitted for the MDX Challenge . Method CWS-PResUNet is a ResUNet (H. <ref type="bibr" target="#b5">Liu et al., 2021)</ref> based model integrating the CWS feature (H.  and the cIRM estimation strategies described in <ref type="bibr" target="#b4">Kong et al. (2021)</ref>. The overall pipeline is summarized in <ref type="figure" target="#fig_0">Figure 1a</ref>. We modeling separation on the subband spectrogram and phase domains. The analysis and synthesis filters in subband operations are designed by optimizing reconstruction error using the open-source toolbox 3 .</p><p>As is illustrated in <ref type="figure" target="#fig_0">Figure 1b</ref>, the CWS feature has a lower frequency dimension and more channels compared with the full band spectrogram. To adapt the conventional full band CNN-based model to the CWS input feature, it just needs to modify the input and final output channel with internal CNN blocks unchanged. In this way, the internal feature map of the model becomes smaller, leading to a direct reduction in computational cost. Also, models become more efficient by enlarging receptive fields and diverging subband information into different channels.</p><p>The detailed computation procedure of our CWS-PResUNet model is described as follows. For a stereo mixture signal x ? R 2?L , where L stands for signal length, we first utilize a set of analysis filters h (j) , j = 1, 2, 3, 4 to perform subband decompositions:</p><formula xml:id="formula_0">x 8? L 4 = [DS 4 (x 2?1?L * h (j) 1?64 )] j=1,2,3,4 ,</formula><p>where DS 4 (?), * , and [?] denote the downsampling by 4, convolution, and stacking operators, respectively. The analysis filters we used are uniform filter banks with a filter length of 64. Then we calculate the STFT of the downsampled subband signals x to obtain their magnitude spectrograms |X | 8?T ? F 4 , which is the input of Phase-aware ResUNet.  As is shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the phase-aware ResUNet is a symmetric architecture containing a down-sampling and an up-sampling path with skip-connections between the same level. It accepts |X | as input and estimates four tensors with the same shape: mask estimationM , phase variationP r ,P i , and direct magnitude predictionQ. The complex spectrogram can be reconstructed with the following equation:</p><formula xml:id="formula_1">S = relu(|X | sigmoid(M ) +Q) exp j(?X +??) ,</formula><p>in which cos?? =P r /( P 2 r +P 2 i ) and sin?? =P i /( P 2 r +P 2 i ). We pass the mask estimationM through a sigmoid function to obtain a mask with values between 0 and 1. Then by estimatingQ and?, models can avoid using mixture phase and estimating mask with only bounded values to calculate the unbounded cIRM. We use relu activation to ensure the positve magnitude value. Finally, after the inverse STFT, we perform subband reconstructions to obtain the source estimation?:</p><formula xml:id="formula_2">s 2?L = 4 j=1 (US 4 (? 2?4? L 4 ) * g (j) 4?64 ),</formula><p>where g (j) , j = 1, 2, 3, 4 are the pre-defined synthesis filters and US 4 (?) is the zero-insertion upsampling function.</p><p>Our model for vocals is optimized by calculating L1 loss between? and its target source s. Although we also use a model dedicated to separating the other track, we notice estimating and optimizing four sources together in one model can result in a 0.2 SDR <ref type="bibr" target="#b15">(Vincent et al., 2006)</ref> gain on other. In this case, we not only use L1 loss on the waveform, but also employ energy-conservation loss, which calculates the L1 loss between the mixture and the sum of four source estimations. Our CWS-PResUNet models for bass and drums reported in the next section employ the same setup as the model for other. In our ByteMSS system, we set up Demucs <ref type="bibr" target="#b1">(D?fossez et al., 2019)</ref> to separate bass and drums tracks because it performs better than CWS-PResUNet on these two sources. Demucs is a time-domain MSS model. In our study, we adopted the open-sourced pre-trained Demucs 4 and do not apply the shift trick because it will slow down the inference speed. To separate the vocals track, we train a 276-layer CWS-PResUNet. For the other track, which is usually prone to overfitting due to the limited training data, we setup a smaller 166-layer CWS-PResUNet in order to achieve better generalization ability on the hidden test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Our models are optimized using the training subset of MUSDB18HQ <ref type="bibr" target="#b9">(Rafii et al., 2019)</ref>. We calculate the STFT of the downsampled 11.05 kHz subband signals with a window length of 512 and a window shift of 110. We use Adam optimizer with an initial learning rate of 0.001 and exponential decay. CWS-PResUNet takes approximately four days to train on a Tesla V100 GPU. During inference, we utilize a 10-second long boxcar windowing function <ref type="bibr" target="#b11">(Schuster et al., 2008)</ref> with no overlapping to segment the signal. For evaluation, we report the SDR on the MUSDB18HQ test set with the open-sourced museval tool <ref type="bibr" target="#b12">(St?ter et al., 2018)</ref>.</p><p>The subband analysis and synthesis operations usually cannot achieve perfect reconstruction. To assess the errors introduced by subband operations, we decompose the test set vocals tracks into 2,4, and 8 subbands and reconstruct them back to evaluate the reconstruction error of the filterbanks. We perform the computation using 32 bits float numbers. As is presented in <ref type="table">Table 1</ref>, in all cases subband reconstructions achieve high performance with neglectable errors, which show an increasing trend with more subband numbers. <ref type="table">Table 2</ref> lists the results of the baselines and our proposed systems. Our CWS-PResUNets achieve an SDR of 8.92 and 5.84 on vocals and other sources, respectively, outperforming the baseline X-UMX <ref type="bibr" target="#b10">(Sawata et al., 2021)</ref>, D3Net <ref type="bibr" target="#b14">(Takahashi &amp; Mitsufuji, 2020)</ref>, and Demucs systems by a large margin. Demucs performs better than CWS-PResUNet on bass and drums tracks. We assume that is because time-domain models can learn better representations than time-frequency features so are more suitable for separating percussive and band-limited sources. The average performance of our ByteMSS system is 6.97, marking a SoTA performance on MSS. Considering the high performance of the vocals model, we also attempt to separate three instrumental sources from mixture minus vocals. In this case, the average score remains 6.97, in which the drums score increase to 6.72 but the other three sources drop slightly. In the future, we will address the integration of time and frequency models for the compensations in both domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>Our experiment result shows CWS-PResUNet can achieve a leading performance on the separation of vocals and other tracks. And channel-wise subband feature is an effective alternative to magnitude spectrogram on music source separation task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the CWS-PResUNet and a comparison between using magnitude spectrogram and channel-wise subband spectrogram as the input feature. 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2</head><label></label><figDesc>We use mono signal for simple illustration. 3 https://www.mathworks.com/matlabcentral/fileexchange/40128-filter-bank-design</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of Phase-aware ResUNet</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Open sourced at: https://github.com/haoheliu/2021-ISMIR-MSS-Challenge-CWS-PResUNet CWS-PResUNet: Music Source Separation with Channel-wise Subband Phase-aware ResUNet. Proceedings of the MDX Workshop, 2021.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/facebookresearch/demucs CWS-PResUNet: Music Source Separation with Channel-wise Subband Phase-aware ResUNet. Proceedings of the MDX Workshop, 2021.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">CWS-PResUNet: Music Source Separation with Channel-wise Subband Phase-aware ResUNet. Proceedings of the MDX Workshop, 2021.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This project is funded by ByteDance Inc. We acknowledge the supports from Haonan Chen for testing our system.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Investigating u-nets with various intermediate blocks for spectrogram-based singing voice separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02591</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Demucs: Deep extractor for music sources with extra unlabeled data remixed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01174</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Singing voice separation with deep u-net convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Montecchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multi-task u-net for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Kadandale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Montesinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>G?mez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Decoupling magnitude and phase estimation with deep ResUNet for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05418</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.13731</idno>
		<title level="m">VoiceFixer: Toward general speech restoration with neural vocoder</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Channel-wise subband input for better voice and accompaniment separation on high resolution music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05216</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Voice and accompaniment separation in music using self-attention convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thoshkahna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kristjansson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08954</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fabbro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13559</idno>
		<title level="m">Music demixing challenge at ISMIR 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">MUSDB18-HQ-an uncompressed version of MUSDB18</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">All for one and one for all: Improving music separation by bridging networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sawata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="51" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The influence of windowing on bias and variance of DFT-based frequency and phase estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scheiblhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stelzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and Measurement</title>
		<imprint>
			<biblScope unit="page" from="1975" to="1990" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="293" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MMDenseLSTM: An efficient combination of convolutional and recurrent neural networks for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Workshop on Acoustic Signal Enhancement</title>
		<imprint>
			<biblScope unit="page" from="106" to="110" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">D3net: Densely connected multidilated densenet for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>F?votte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cws-Presunet</surname></persName>
		</author>
		<title level="m">Music Source Separation with Channel-wise Subband Phase-aware ResUNet. Proceedings of the MDX Workshop</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
