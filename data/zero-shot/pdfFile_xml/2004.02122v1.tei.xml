<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anisotropic Convolutional Networks for 3D Semantic Scene Completion *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Wollongong</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Anisotropic Convolutional Networks for 3D Semantic Scene Completion *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As a voxel-wise labeling task, semantic scene completion (SSC) tries to simultaneously infer the occupancy and semantic labels for a scene from a single depth and/or RGB image. The key challenge for SSC is how to effectively take advantage of the 3D context to model various objects or stuffs with severe variations in shapes, layouts and visibility. To handle such variations, we propose a novel module called anisotropic convolution, which properties with flexibility and power impossible for the competing methods such as standard 3D convolution and some of its variations. In contrast to the standard 3D convolution that is limited to a fixed 3D receptive field, our module is capable of modeling the dimensional anisotropy voxel-wisely. The basic idea is to enable anisotropic 3D receptive field by decomposing a 3D convolution into three consecutive 1D convolutions, and the kernel size for each such 1D convolution is adaptively determined on the fly. By stacking multiple such anisotropic convolution modules, the voxel-wise modeling capability can be further enhanced while maintaining a controllable amount of model parameters. Extensive experiments on two SSC benchmarks, NYU-Depth-v2 and NYUCAD, show the superior performance of the proposed method. Our code is available at https://waterljwant.github.io/SSC/ .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>To behave in the 3D physical world, it requires an accurate understanding of both the 3D geometry as well as the semantics of the environment. Humans can easily infer such geometrical and semantic information of a scene from partial observations. An open topic in computer vision is to study how to enable machines such an ability, which is desirable in many applications such as navigation <ref type="bibr" target="#b3">[4]</ref>, grasping <ref type="bibr" target="#b20">[20]</ref>, 3D home design <ref type="bibr" target="#b0">[1]</ref>, to name a few.</p><p>Semantic scene completion (SSC) <ref type="bibr" target="#b15">[16]</ref> is a computer vision task teaching the machine how to perceive the 3D world from the static depth and/or RGB image. The task has two coupled objectives: one is 3D scene completion, which aims at inferring the volumetric occupancy of the scene, and the other is 3D scene labeling, which requires to predict the semantic labels voxel-wisely. As the objects within the physical scene carry severe variations in shapes, layouts, and visibility due to occlusions, the main challenge thereon is how to model the 3D context to learn each voxel effectively.</p><p>Recently, promising progress has been achieved for SSC <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13]</ref> by employing deep convolutional neural networks (CNNs). A direct solution is to use 3D convolutional neural network <ref type="bibr" target="#b15">[16]</ref> to model the volumetric context, which consists of a stack of conventional 3D convolutional layers. This solution, however, suffers from apparent limitations. On the one hand, 3D convolution renders a fixed receptive field that does not cater to the variations of the objects. On the other hand, 3D convolution is resource demanding, which causes massive computational and memory consumption. 3D convolution variations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">21]</ref> are proposed to address such shortcomings. For example, a lightweight dimensional decomposition network is proposed in <ref type="bibr" target="#b9">[10]</ref> to alleviate the resource consumption, but it still leaves the object variation issue unattended.</p><p>In this work, we propose a novel module, termed anisotropic convolution, to model object variation, which properties with flexibility and power impossible for competing methods. In contrast to standard 3D convolution and some of its variations that are limited to the fixed receptive field, the new module adapts to the dimensional anisotropy property voxel-wisely and enables receptive field with varying sizes, a.k.a anisotropic receptive field. The basic idea is to decompose a 3D convolution operation into three consecutive 1D convolutions and equip each such 1d convolution with a mixer of different kernel sizes. The combination weights of such kernels along each 1D convolution are learned voxel-wisely and thus anisotropic 3D context can essentially be modeled by consecutively performing such adaptive 1D convolutions. Although we use multiple kernels, e.g. 3, due to the dimensional decomposition scheme, our module is still parameter-economic comparing to the 3D counterpart. By stacking multiple such modules, a more flexible 3D context, as well as an effective mapping function from such context to the voxel output, can be obtained.</p><p>The contributions of this work are as follows:</p><p>? We present a novel anisotropic convolutional network (AIC-Net) for the task of semantic scene completion. It renders flexibility in modeling the object variations in a 3D scene by automatically choosing proper receptive fields for different voxels.</p><p>? We propose a novel module, termed anisotropic convolution (AIC) module, which adapts to the dimensional anisotropy property voxel-wisely and thus implicitly enables 3D kernels with varying sizes.</p><p>? The new module is much less computational demanding with higher parameter efficiency comparing to the standard 3D convolution units. It can be used as a plugand-play module to replace the standard 3D convolution unit.</p><p>We thoroughly evaluate our model on two SSC benchmarks. Our method outperforms existing methods by a large margin, establishing the new state-of-the-art. Code will be made available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic Scene Completion</head><p>SSCNet <ref type="bibr" target="#b15">[16]</ref> proposed by Song et al. is the first work that tries to simultaneously predict the semantic labels and volumetric occupancy of a scene in an end-to-end network. The expensive cost of 3D CNN, however, limits the depth of the network, which hinders the accuracy achieved by SS-CNet. Zhang et al. <ref type="bibr" target="#b21">[21]</ref> introduced spatial group convolution (SGC) into SSC for accelerating the computation of 3D dense prediction task. However, its accuracy is slightly lower than that of SSCNet. By combining the 2D CNN and 3D CNN, Guo and Tong <ref type="bibr" target="#b7">[8]</ref> proposed the view-volume network (VVNet) to efficiently reduce the computation cost and enhance the network depth. Li et al. <ref type="bibr" target="#b10">[11]</ref> use both depth and voxels as the inputs of a hybrid network and consider the importance of elements at different positions <ref type="bibr" target="#b23">[23]</ref> while training.</p><p>Garbade et al. <ref type="bibr" target="#b5">[6]</ref> proposed a two-stream approach that jointly leverages the depth and visual information. In specific, it first constructs an incomplete 3D semantic tensor for the inferred 2D semantic information, and then adopts a vanilla 3D CNN to infer the complete 3D semantic tensor. Liu et al. <ref type="bibr" target="#b12">[13]</ref> also used RGB-D image as input and proposed a two-stage framework to sequentially carry out the 2D semantic segmentation and 3D semantic scene completion, which are connected via a 2D-3D re-projection layer. However, their two-stage method can suffer from the error accumulation, producing inferior results. Although significant improvements have been achieved, these methods are limited by the cost of 3D convolution and the fixed receptive fields. Li et al. <ref type="bibr" target="#b9">[10]</ref> introduced a dimensional decomposition residual network (DDRNet) for the 3D SSC task. Although it achieves good accuracy with less parameters, it still leaves the limitation of using fixed receptive field unattended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Going Beyond Fixed Receptive Field</head><p>Most existing models utilize fixed-size kernel to model fixed visual context, which are less robust and flexible when dealing with objects with various sizes.</p><p>Inception family <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b17">18]</ref> take receptive field with multiple sizes into account, and it implements this concept by launching multi-branch CNNs with different convolution kernels. The similar idea appears in atrous spatial pyramid pooling (ASPP) <ref type="bibr" target="#b1">[2]</ref>, multi-scale information was captured via using several parallel convolutions with different atrous(dilation) rates on the top of feature map. These strategies essentially embrace the idea of multi-scale fusion, and the same fusion strategy is uniformly applied to all the positions. Zhang et al. <ref type="bibr" target="#b22">[22]</ref> choose a more suitable receptive field by weighting convolutions with different kernel sizes.</p><p>STN <ref type="bibr" target="#b8">[9]</ref> designs a Spatial Transformer module to achieve invariance in terms of translation, rotation, and scale. However, it treats the whole image as a unit, rather than adjusts the receptive field pixel-wisely. Deformable CNN (DCNv1) <ref type="bibr" target="#b2">[3]</ref> attempts to adaptively adjust the spatial distribution of receptive fields according to the scale and shape of the object. Specifically, it utilizes offset to control the spatial sampling. DCNv2 <ref type="bibr" target="#b25">[25]</ref> increases the modeling power by stacking more deformable convolutional layers to improve its modelling ability and proposes to use a teacher network to guide the training process. However, DCNv2 still struggles to control the offset in order to focus on relevant pixels only. Different from the above methods, the proposed AIC module is tailored for 3D tasks, in particular for SSC in this paper. It is capable of handling objects with variations in shapes, layouts and visibility by learning anisotropic receptive field voxel-wisely. At the same time, it achieves trade-off between semantic completion accuracy and computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Anisotropic Convolutional Networks</head><p>In this section, we introduce our anisotropic convolutional networks (AIC-Net) for 3D semantic scene completion. At the core of AIC-Net is our proposed anisotropic convolutional (AIC) module. Given a singleview RGB-D image of a 3D scene, AIC-Net predicts a dense 3D voxel representation and maps each voxel in the view frustum to one of the labels C = {c 1 , c 2 , ? ? ? , c N +1 },  <ref type="figure">Figure 1</ref>. The overall network structure of AIC-Net. AIC-Net has two feature extractors in parallel to capture the features from RGB and depth images, respectively. The feature extractor contains a projection layer to map the 2D feature to 3D space. After that, we use stacked AICs to obtain information with adaptive receptive fields. The multi-scale features are concatenated and then fused through another two AICs followed by three voxel-wise convolutions to predict occupancy and object labels simultaneously.</p><p>where N is the number of object classes, c N +1 represents the empty voxels, {c 1 , c 2 , ? ? ? , c N } represent the voxels occupied by objects of different categories. <ref type="figure">Fig. 1</ref> illustrates the overall architecture of our AIC-Net. It consists of a hybrid feature extractor for feature extraction from the depth map and RGB image, a multi-stage feature aggregation module with a stack of AIC modules to aggregate features obtained by the hybrid feature extractor, two extra AIC modules to fuse multi-stage information, followed by a sequence of voxel-wise 3D convolution layers to reconstruct the 3D semantic scene. The hybrid feature extractor contains two parallel branches to extract features for the depth map and the RGB image, respectively. Each branch contains a hybrid structure of 2D and 3D CNNs. The 2D and 3D CNNs are bridged by a 2D-3D projection layer, allowing the model to convert the 2D feature maps into 3D feature maps that are suitable for 3D semantic scene completion. The structure of our hybrid feature extractor follows that of DDRNet <ref type="bibr" target="#b9">[10]</ref>. The multi-stage feature aggregation module consists of a sequence of AIC modules, each of which can voxel-wisely adjust the 3D context on the fly. The outputs of these AIC modules are concatenated together, and another two AIC modules fuse such multi-stage information. The 3D semantic scene can then be reconstructed by applying a sequence of voxel-wise 3D convolutional layers on the fused feature.</p><p>In the rest of this section, we will introduce our AIC module (section 3.1), the multi-path kernel selection mechanism achieved by stacking our AIC modules (section 3.2), and the training loss for our model (section 3.3) in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Anisotropic Convolution</head><p>Considering the variations in object shapes, layouts as well as the varying levels of occlusion in SSC, it will be beneficial to model different context information to infer the occupancy and semantics for different voxel positions. The anisotropic convolution (AIC) module is proposed to adapt to such variations, allowing the convolution to accommodate 3D geometric deformation. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the structure of our AIC module. Instead of using the 3D kernels ( k 1 ? k 2 ? k 3 ) that are limited to the fixed 3D receptive field, we model the dimensional anisotropy property by enabling the kernel size for each 3D dimension to be learnable. To achieve this, we first decompose the 3D convolution operation as the combination of three 1D convolution operations along each dimension x, y, z. In each dimension, we can inject multiple (e.g. 3 in our implementation) kernels of different sizes to enable more flexible context modeling. For example, for dimension x, we can have three kernels as</p><formula xml:id="formula_0">1 ? 1 ? 1 1 ? 2 ? 1 1 ? 1 ? 1 1 ? 1 ? 1 2 ? 1 ? 1 1 ? 1 ? 3 1 ? 1 ? 2 1 ? 3 ? 1 3 ? 1 ? 1 1 ? 1 ? 1 ?1 ? ? ? ?</formula><formula xml:id="formula_1">(1 ? 1 ? k x 1 ), (1 ? 1 ? k x 2 ), and (1 ? 1 ? k x 3 )</formula><p>. A set of selection weights, a.k.a. modulation factors, will be learned to select proper kernels along each of the three dimensions. Note that the kernel candidates for different dimensions are not necessary to be the same. When there are n, m, and l candidate kernels along x, y, and z dimensions respectively, the possible kernel combinations can grow exponentially as,</p><formula xml:id="formula_2">{k z 1 , k z 2 , ? ? ? , k z l } ? {k y 1 , k y 2 , ? ? ? , k y m } ? {k x 1 , k x 2 , ? ? ? , k x n }.</formula><p>The AIC module can learn to select different kernels for each dimension, forming an anisotropic convolution to capture anisotropic 3D information. Modulation factors To enable the model to determine the optimal combination of the candidate kernels and consequently adaptively controlling the context to model different voxels, we introduce a modulation module in the AIC module. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, assume the input to an AIC module is a tensor X t?1 ? R L?W ?H?D , where L, W , H denotes the length, width, height of the tensor, and D indicates the dimensionality of the feature. The output X t ? R L?W ?H?D can be formulated as,</p><formula xml:id="formula_3">X t = F z (F y (F x (X t?1 ))) + X t?1 ,<label>(1)</label></formula><p>where F u represents the anisotropic convolution along the u ? {x, y, z} dimension. We adopt a residual structure to obtain the output by element-wisely summing up the input tensor and the output of three consecutive anisotropic 1D convolutions. Without losing generality, we represent F x (X t?1 ) as,</p><formula xml:id="formula_4">X x t = n i=1 f x (X t?1 , ? x i ) g x (X t?1 , ? x )[i],<label>(2)</label></formula><p>where f x (X t?1 , ? x i ) represents performing convolution to X t?1 using parameter ? x i which has kernel size (1, 1, k</p><formula xml:id="formula_5">x i ) with k x i ? {k x 1 , k x 2 , ? ? ?</formula><p>, k x n }, n is the toal numner of candidate kernels for dimension x, and denotes element-wise multiplication. g x (X t?1 , ? x ) is a mapping function from the input tensor to the weights or modulation factors used to select the kernels along dimension x and ? x denotes the parameters of the mapping function. We perform sof tmax to g u (?, ?)[i] in order that the weights for the kernels of each  <ref type="figure">Figure 4</ref>. Illustration of multi-path kernel selection in one dimension. In this example, four AIC modules are stacked and for each module the kernel sizes for each dimension are {3, 5, 7}. The background darkness of the kernel indicates the value of the modulation factor, and thus reflects the selection tendency for this kernel. Stacking multiple AIC modules can increase the range of receptive fields exponentially. dimension u ? {x, y, z} sum up to 1, that is,</p><formula xml:id="formula_6">p?{n,m,l} i=1 g u (?, ? u )[i] = 1, g u (?, ? u )[i] ? 0.<label>(3)</label></formula><p>In this sense, we adopt a soft constraint with a set of weights to determine the importance of different kernels. The two extreme cases are that the learned modulation factor is 1 or 0, indicating that the corresponding kernel will be the unique selected or be ignored. By using soft values, we can control the contributions of these kernels more flexibly. In <ref type="figure" target="#fig_1">Fig. 2, we</ref> show an example of the AIC module with m = n = l = 3 and as seen, g u (?, ?) is realized by a 1-layer 3D convolution with kernel (1 ? 1 ? 1). Bottleneck anisotropic convolution To further reduce the parameters of our AIC module, we propose a bottleneck based AIC module. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, for each AIC module, a (1 ? 1 ? 1) convolution is added both before and after the AIC operation. These two convolutions are responsible for reducing and restoring the feature channels, allowing the AIC module to have a more compact input. In the remainder of the paper, unless stated otherwise, AIC refers to the bottleneck based AIC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-path Kernel Selection</head><p>Despite the attractive properties in a single AIC module, here we show that greater flexibility can be achieved by stacking multiple AIC modules. Stacking multiple AIC modules forms multiple possible paths between layers implicitly and consequently enables an extensive range of receptive field variations in the model. <ref type="figure">Fig. 4</ref> shows a stack of four AIC modules, and each module sets the kernel sizes to {3, 5, 7} along all three dimensions. For one specific dimension, when each module tends to select the kernel size 7, a maximum receptive field of 25 will be obtained for this dimension. On the contrary, a minimum receptive field of 9 can be obtained for a dimension, if kernel size 3 dominates the selections of all four AIC modules in this dimension. In theory, the receptive field for this particular dimension  <ref type="table">Table 2</ref>. Results on the NYUCAD dataset <ref type="bibr" target="#b24">[24]</ref>. Bold numbers represent the best scores.</p><p>can freely vary in the range of <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b25">25)</ref>. When considering three dimensions simultaneously, the number of 3D receptive fields supported by our AIC network will grow exponentially, which will provide flexibility and power for modeling object variations impossible for competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Loss</head><p>Our proposed AIC-Net can be trained in an end-to-end fashion. We adopt the voxel-wise cross-entropy loss function <ref type="bibr" target="#b15">[16]</ref> for the network training. The loss function can be expressed as,</p><formula xml:id="formula_7">L = i,j,k w ijk L sm (p ijk , y ijk ),<label>(4)</label></formula><p>where L sm is the cross-entropy loss, y ijk is the ground truth label for coordinates (i, j, k), p ijk is the predicted probability for the same voxel, and w ijk is the weight to balance the semantic categories. We follow <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10]</ref> and use the same weights in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we start by introducing some key implementation details, followed by the description of the datasets as well as the evaluation metrics. Then we present some quantitative comparisons between the propose AIC-Net and some other existing works. Furthermore, qualitative comparisons are given through visualization. Finally, comprehensive ablation studies are performed to inspect some critical aspects of AIC-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>In our AIC-Net, we stack three AIC modules for each branch in the multi-stage feature aggregation part, and two AIC modules are adopted to fuse these features. All the AIC modules used are the bottleneck version as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. For the three AIC modules in feature aggregation, the bottleneck layer is used to decrease the dimensionality of the features from D = 64 to D = 32. For the AIC modules in feature fusion part, the dimensionalities of features before and after the bottleneck layer are D = 256 and D = 64. Unless stated otherwise, we use three candidate kernels with kernel size {3, 5, 7} for each dimension of all AIC modules. More details about the network structure can be found in the supplements.</p><p>Our model is trained by using SGD with a momentum of 0.9 and a weight decay of 10 ?4 . The initial learning rate is set to be 0.01, which decays by a factor of 10 every 15 epochs. The batch size is 4. We implement our model using PyTorch. All the experiments are conducted on a PC with 4 NVIDIA RTX2080TI GPUs.</p><p>Datasets. We evaluate the proposed AIC-Net on two SSC datasets. One dataset is the NYU-Depth-V2 <ref type="bibr" target="#b14">[15]</ref>, which is also known as the NYU dataset. The NYU dataset consists of 1,449 depth scenes captured by a Kinect sensor. Following SSCNet <ref type="bibr" target="#b15">[16]</ref>, we use the 3D annotations provided by <ref type="bibr" target="#b13">[14]</ref> for semantic scene completion task. The second dataset is the NYUCAD dataset <ref type="bibr" target="#b4">[5]</ref>. This dataset uses the depth maps generated from the projections of the 3D annotations to reduce the misalignment of depths and the annotations and thus can provide higher-quality depth maps.</p><p>Evaluation metrics. For semantic scene completion, we measure the intersection over union (IoU) between the predicted voxel labels and ground-truth labels for all object classes. Overall performance is also given by computing the average IoU over all classes. For scene completion, all voxels are to be categorized into either empty or occupied.  A voxel is counted as occupied if it belongs to any of the semantic classes. For scene completion, apart from IoU, precision and recall are also reported. Note that the IoU for semantic scene completion is commonly accepted as a more important metric in the SSC task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the State-of-the-Art</head><p>We compare our AIC-Net with the state-of-the-art methods on NYU and NYUCAD. The results are reported in <ref type="table" target="#tab_0">Table 1 and Table 2</ref>  <ref type="table">Table 3</ref>. The performance of AIC-Net under different kernel sets. We use the same kernel set k = (k1, k2, ? ? ? , kn) for each dimension. Results are reported on NYUCAD <ref type="bibr" target="#b24">[24]</ref>  cantly outperforms other methods in overall accuracy. The proposed AIC-Net achieves 2.9% better than the cuttingedge approach DDRNet <ref type="bibr" target="#b9">[10]</ref> in terms of the average IoU. For scene completion, our method is slightly outperformed by DDRNet <ref type="bibr" target="#b9">[10]</ref>. The scene completion task requires to predict the volumetric occupancy, which is class-agnostic. Since our AIC-Net aims at modeling the object variation voxel-wisely, its advantage will fade in the binary completion task. In <ref type="table">Table 2</ref>, our AIC-Net achieves the best semantic segmentation performance as well, and our average IoU outperforms the second-best approach by 3%. For scene completion, our method also observes superior performance, although the advantage is not as significant.</p><p>Among the comparing methods, SSCNet <ref type="bibr" target="#b15">[16]</ref> is built using standard 3D convolution. The inferior performance lies twofold. First, the fixed receptive field is not ideal for addressing object variations. Second, 3D convolution is resource demanding, which can limit the depth of the 3D network and consequently sacrifices the modeling capability. Another interesting observation from these two tables is that our AIC-Net tends to obtain better performance on some categories that have more severe shape variations, e.g. chair, table, objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative Results</head><p>In <ref type="figure" target="#fig_4">Fig. 5</ref>, we show some visualization results to evaluate the effectiveness of our AIC-Net qualitatively. Generally, we can see that the proposed AIC-Net can handle diverse objects with various shapes and thus give more accurate semantic predictions and shape completion than SS-CNet <ref type="bibr" target="#b15">[16]</ref> and DDRNet <ref type="bibr" target="#b9">[10]</ref>. Some challenging examples include "chairs" and "tables" in Row 1, Row 3, and Row 5, which require a model to adaptively adjust the receptive field voxel-wisely. For example, for some more delicate parts like "legs", a smaller receptive field can be more beneficial. It shows that our AIC-Net can identify such objects more clearly. While for some other objects like "windows" in Row 5 and Row 7, it expects to see the larger context. Both SSCNet and DDRNet fail in this case, but our method still successfully identifies them from other surrounding distractors. The "bed" in Row 2, the "wall" in Row 6, and the "sofa" in Row 4 also demonstrate the superiority of our approach. In Row 8, the "objects" marked by the red dashed rectangle are in a messy environment. Our AIC-Net is less vulnerable to the influence of surrounding objects and more accurately distinguishes the categories and shapes of these "objects".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this section, we dive into the AIC-Net to investigate its key aspects in detail. Specifically, we try to answer the following questions. 1). Is it beneficial to use multiple candidate kernels along each dimension of the AIC module? 2). Is the performance improvement simply coming from multiple kernels? 3). Will that work if the AIC module is used as a plug-and-play module? 4). The trade-off between SSC performance and cost.</p><p>The effectiveness of using multiple kernels In our AIC module, we use multiple candidate kernels in each dimension x, y, z, and use the learned modulation factors to choose proper kernels along each of these dimensions. Since we expect our AIC-Net to be able to deal with objects of varying shapes, the kernels in AIC should be sufficiently distinct. In our experiments, we set the kernel set to be {3, 5, 7} across all three dimensions. The first question needs to be clarified is that will it be enough to use only the maximum kernel, i.e. 7 in our network? Then, are three kernels better than two? From the results of <ref type="table">Table 3</ref>  <ref type="table">Table 6</ref>. Params, FLOPs and Performance of our approach compared with other methods. 3D conv, k = (k1, k2, k3) denotes we replace our AIC module with a 3D convolution unit with 3D kernel (k1, k2, k3). AIC-Net * denotes a AIC-Net with one AIC module in feature fusion part, while by default we use two AIC modules. outperform kernel 7. Since the maximum receptive field for all these three options is 7, the results demonstrate the benefits of using multiple kernels. At the same time, three kernels outperform two kernels by about 1% because it renders more flexibility in modeling the context.</p><p>Is it necessary to use modulation factors? In the above paragraph, we show the benefit of using multiple kernels along each dimension. However, another question arises that is the improvement simply coming from multiple kernels? In other words, is that necessary to learn modulation factors to adaptively select the kernels voxel-wisely? From <ref type="table" target="#tab_3">Table 4</ref>, we can see when we discard the modulation factors in AIC modules, the performance of AIC-Net observes obvious degradation on both NYU and NYUCAD datasets. These results show that the superior performance of AIC-Net relies on modeling the dimensional anisotropy property by adaptively selecting proper kernels along each dimension. To further inspect the anisotropic nature of the learned kernels, we observed the statistical values of the modulation factors and found that: 1.) the selected kernel sizes are basically consistent with the object sizes; 2.) the modulation values for different voxels vary a lot within one scene; 3.) the modulation values among the three separable dimensions have significant variation. This indicates the learned "3D receptive field" are anisotropic and adaptive.</p><p>AIC module used as a plug-and-play module Due to its ability to model the anisotropic context, our AIC module is expected to be able to benefit other networks when it is used as a plug-and-play module. To validate this, we choose the DDRNet <ref type="bibr" target="#b9">[10]</ref> as the test-bed, and use the AIC module to replace its building blocks, DDR and ASPP. DDR block models 3D convolution in a lightweight manner with the fixed receptive field. ASPP is a feature fusion scheme commonly used in semantic segmentation to take advantage of the multi-scale context. <ref type="table">Table 5</ref> shows the comparison. When we use AIC to replace the DDR module in DDR-Net <ref type="bibr" target="#b9">[10]</ref>, the SSC-IoU is improved by 1.6%. When we replace ASPP by our AIC module, we still observe a 0.6% improvement in semantic segmentation. Finally, when we replace both DDR and ASPP by AIC, the result can be further boosted.</p><p>Trade-off in performance and cost Since we decompose the 3D convolution into three consecutive 1D convolutions, the model parameters and computation grow linearly with the number of candidate kernels in each dimension. While for standard 3D convolution, the parameters and computation will have cubic growth. <ref type="table">Table 6</ref> presents some comparisons in terms of both efficiency and accuracy. For the 3D conv, k = (k 1 , k 2 , k 3 ) in the table, it means we use this particular 3D convolution to replace our AIC module. As can be seen, when the 3D kernel size is <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b4">5)</ref>, it will result in 3 times of parameters and FLOPs comparing to our AIC-Net. When the kernel size is increased to <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b6">7)</ref>, the parameter and computation scale will be 8 times more than ours. DDRNet is a lightweight structure, which consumes the least parameters and has the lowest computation complexity, but it observes a glaring performance gap comparing to our method. Thus, our AIC-Net achieves a better trade-off between performance and cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a novel AIC-Net, to handle the object variations in the semantic scene completion (SSC) task. At the core of AIC-Net is our proposed AIC module, which can learn anisotropic convolutions by adaptively choosing the convolution kernels along all three dimensions voxel-wisely. By stacking multiple such AIC modules, it allows us more flexibly to control the receptive field for each voxel. This AIC module can be freely inserted into existing networks as a plug-and-play module to effectively model the 3D context in a parameter-economic manner. Thorough experiments were conducted on two SSC datasets, and the AIC-Net outperforms existing methods by a large margin, establishing the new state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More Details of AIC-Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Detailed Architectures</head><p>The details of the proposed network structure are shown in <ref type="table">Table 7</ref>. PWConv represents the point-wise convolution, and it is used to adjust the number of channels of the feature map. The down-sample layer in our network is composed of a max-pooling layer and a convolution layer with stride set as 2. The outputs of the two layers are concatenated before fed into the subsequent layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Details of Each AIC Module</head><p>In <ref type="table">Table 7</ref>, we show the details of the Anisotropic Convolution module (AIC). We use three candidate kernels with kernel size {3, 5, 7} for each dimension of all AIC modules. Since we use bottleneck version AIC, the channel dimension D within each AIC is lower than the output dimension D. We set D = 32 for the first six AIC modules and set D = 64 for the last two AIC modules. The stride and dilation rates of each AIC are all set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. 2D to 3D Projection</head><p>Each point in depth can be projected to a position in the 3D space. We voxelize this entire 3D space with meshed grids to obtain a 3D volume. In the projection layer, every feature tensor is projected into the 3D volume at the location corresponding to its position in depth. With the feature projection layer, the 2D feature maps extracted by the 2D CNN are converted to a view-independent 3D feature volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Qualitative Results</head><p>As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, our completed semantic 3D scenes are less cluttered and show a higher voxel-wise accuracy compared to DDRNet <ref type="bibr" target="#b9">[10]</ref> and SSCNet <ref type="bibr" target="#b14">[15]</ref>.</p><p>In <ref type="figure">Fig. 6</ref>, the chair in the first row shows that our result is much more meticulous than the results of the other two methods. In AIC-Net, the irrelevant voxels less interfere with the prediction. In the second row, the windows are relatively difficult to distinguish, and our method can still distinguish them effectively, while other methods fail. As shown in rows 3 to 8, the prediction of our AIC-Net is more accurate than other methods. The predicted shape of AIC-Net is more suitable for the actual shape of the object, and the predicted semantic category is more accurate than the other two methods. We mark the representative areas in <ref type="figure">Fig. 6</ref> with a red dotted bounding box for easy comparison.  <ref type="table">chair  bed  sofa  table  tvs</ref> furn. objects <ref type="figure">Figure 6</ref>. Qualitative results on NYUCAD <ref type="bibr" target="#b21">[21]</ref>. Left to right: input RGB-D image, the ground truth, results generated by SSCNet <ref type="bibr" target="#b14">[15]</ref>, DDRNet <ref type="bibr" target="#b9">[10]</ref> and the proposed AIC-Net. (Best viewed in color.)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Anisotropic convolution. For each dimension, we set 3 parallel convolution with different kernel sizes as an example. The learned modulation factors for different kernels are denoted with different colors. The values of the modulation factors are positive and the values of each row sum up to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Bottleneck version AIC module. The first convolution reduces the number of channels from D to D (D &lt; D) and the last convolution increases the channels back to D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results on NYUCAD. From left to right are input RGB-D image, the ground truth, results generated by SSCNet<ref type="bibr" target="#b15">[16]</ref>, DDRNet<ref type="bibr" target="#b9">[10]</ref> and the proposed AIC-Net. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>scene completion semantic scene completion Methods prec. recall IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. Lin et al. 61.1 19.3 94.8 28.0 12.2 19.6 57.0 50.5 17.6 11.9 35.6 15.3 32.9 AIC-Net 62.4 91.8 59.2 23.2 90.8 32.3 14.8 18.2 51.1 44.8 15.2 22.4 38.3 15.7 33.3 Results on the NYU [15] dataset. Bold numbers represent the best scores.</figDesc><table><row><cell>[12]</cell><cell>58.5</cell><cell cols="2">49.9 36.4 0.0</cell><cell cols="3">11.7 13.3 14.1</cell><cell>9.4</cell><cell cols="2">29.0 24.0</cell><cell>6.0</cell><cell>7.0</cell><cell>16.2</cell><cell>1.1</cell><cell>12.0</cell></row><row><cell cols="2">Geiger et al. [7] 65.7</cell><cell cols="5">58.0 44.4 10.2 62.5 19.1 5.8</cell><cell>8.5</cell><cell cols="2">40.6 27.7</cell><cell>7.0</cell><cell>6.0</cell><cell>22.6</cell><cell>5.9</cell><cell>19.6</cell></row><row><cell>SSCNet [16]</cell><cell>57.0</cell><cell cols="5">94.5 55.1 15.1 94.7 24.4 0.0</cell><cell cols="4">12.6 32.1 35.0 13.0</cell><cell>7.8</cell><cell cols="3">27.1 10.1 24.7</cell></row><row><cell>EsscNet [21]</cell><cell>71.9</cell><cell cols="5">71.9 56.2 17.5 75.4 25.8 6.7</cell><cell cols="4">15.3 53.8 42.4 11.2</cell><cell>0</cell><cell cols="3">33.4 11.8 26.7</cell></row><row><cell>DDRNet [10]</cell><cell>71.5</cell><cell cols="5">80.8 61.0 21.1 92.2 33.5 6.8</cell><cell cols="8">14.8 48.3 42.3 13.2 13.9 35.3 13.2 30.4</cell></row><row><cell>VVNet [8]</cell><cell cols="2">69.8 83.1 scene completion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">semantic scene completion</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="14">prec. recall IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg.</cell></row><row><cell cols="2">Zheng et al. [24] 60.1</cell><cell>46.7 34.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Firman et al. [5]</cell><cell>66.5</cell><cell>69.7 50.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SSCNet [16]</cell><cell>75.4</cell><cell cols="5">96.3 73.2 32.5 92.6 40.2 8.9</cell><cell cols="4">33.9 57.0 59.5 28.3</cell><cell>8.1</cell><cell cols="3">44.8 25.1 40.0</cell></row><row><cell>TS3D [6]</cell><cell>80.2</cell><cell cols="13">91.0 74.2 33.8 92.9 46.8 27.0 27.9 61.6 51.6 27.6 26.9 44.5 22.0 42.1</cell></row><row><cell>DDRNet [10]</cell><cell>88.7</cell><cell cols="9">88.5 79.4 54.1 91.5 56.4 14.9 37.0 55.7 51.0 28.8</cell><cell>9.2</cell><cell cols="3">44.1 27.8 42.8</cell></row><row><cell>VVNet [8]</cell><cell>86.4</cell><cell>92.0 80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AIC-Net</cell><cell>88.2</cell><cell cols="9">90.3 80.5 53.0 91.2 57.2 20.2 44.6 58.4 56.2 36.2</cell><cell>9.7</cell><cell cols="3">47.1 30.4 45.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, respectively. In Table 1, we can see that for the semantic scene completion our method signifi-scene completion semantic scene completion Methods prec. recall IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. AIC-Net, k={3, 5, 7} 88.2 90.3 80.5 53.0 91.2 57.2 20.2 44.6 58.4 56.2 36.2 9.7 47.1 30.4 45.8 AIC-Net, k={5, 7} 88.3 89.5 79.9 51.0 91.3 56.8 18.6 41.3 58.6 59.4 34.6 4.8 46.7 30.9 44.9 AIC-Net, k={7} 86.3 90.3 79.1 50.7 91.7 54.5 21.2 38.0 55.5 57.1 33.2 7.9 44.9 29.4 44.0 AIC-Net, k={5} 87.8 88.2 78.4 49.6 91.3 55.3 15.7 38.7 58.6 52.8 30.9 0. 43.9 30.2 42.5</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. The importance of the modulation factors. AIC-Net-noMFs denotes we set all the modulation factors to be 1. Results are reported on the NYU<ref type="bibr" target="#b14">[15]</ref> and NYUCAD<ref type="bibr" target="#b24">[24]</ref> datasets.</figDesc><table><row><cell></cell><cell></cell><cell>dataset.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">scene completion</cell><cell>semantic scene completion</cell><cell></cell></row><row><cell cols="3">Methods prec. recall NYU</cell><cell></cell><cell></cell></row><row><cell cols="2">AIC-Net-noMFs 71.4</cell><cell cols="4">79.0 59.9 22.3 90.8 32.0 14.4 14.5 47.5 41.3 12.6 16.8 32.8 12.7 30.7</cell></row><row><cell>AIC-Net</cell><cell>62.4</cell><cell cols="4">91.8 59.2 23.2 90.8 32.3 14.8 18.2 51.1 44.8 15.2 22.4 38.3 15.7 33.3</cell></row><row><cell>NYUCAD</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AIC-Net-noMF</cell><cell>87.2</cell><cell cols="2">90.3 79.6 51.1 91.7 57.0 18.5 39.3 51.4 51.8 30.7</cell><cell>1.3</cell><cell>45.0 30.1 42.5</cell></row><row><cell>AIC-Net</cell><cell>88.2</cell><cell cols="2">90.3 80.5 53.0 91.2 57.2 20.2 44.6 58.4 56.2 36.2</cell><cell>9.7</cell><cell>47.1 30.4 45.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, we can see, either two kernels {5, 7} or three kernels {3, 5, 7} can scene completion semantic scene completion method prec. recall IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. DDRNet-DDR-ASPP [10] 88.7 88.5 79.4 54.1 91.5 56.4 14.9 37.0 55.7 51.79.1 51.7 91.5 56.4 16.5 44.1 56.3 56.4 35.4 12.3 46.1 30.4 45.2Table 5. AIC module as plug-and-play modules. The components of DDRNet<ref type="bibr" target="#b9">[10]</ref> are replaced by the AIC modules. Results are reported on NYUCAD<ref type="bibr" target="#b24">[24]</ref> dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0 28.8</cell><cell>9.2</cell><cell>44.1 27.8 42.8</cell></row><row><cell>DDRNet-AIC-ASPP</cell><cell>87.9</cell><cell cols="4">89.1 79.4 48.0 90.9 56.1 20.1 41.6 56.6 55.0 33.1 12.6 45.3 29.0 44.4</cell></row><row><cell>DDRNet-DDR-AIC</cell><cell>88.0</cell><cell cols="3">89.6 79.7 49.0 91.4 57.6 19.7 40.5 52.3 52.9 32.5</cell><cell>6.1</cell><cell>44.6 30.7 43.4</cell></row><row><cell cols="5">DDRNet-AIC-AIC 89.3 Methods 87.5 Params/k FLOPs/G SC-IoU SSC-IoU</cell></row><row><cell>SSCNet [16]</cell><cell>930.0</cell><cell>163.8</cell><cell>73.2</cell><cell>40.0</cell></row><row><cell>DDRNet [10]</cell><cell>195.0</cell><cell>27.2</cell><cell>79.4</cell><cell>42.8</cell></row><row><cell>3D conv, k=(3, 3, 3)</cell><cell>440.1</cell><cell>61.0</cell><cell>-</cell><cell>-</cell></row><row><cell>3D conv, k=(5, 5, 5)</cell><cell>1443.6</cell><cell>191.1</cell><cell>-</cell><cell>-</cell></row><row><cell>3D conv, k=(7, 7, 7)</cell><cell>3675.9</cell><cell>480.4</cell><cell>-</cell><cell>-</cell></row><row><cell>AIC-Net  *  , k={3, 5, 7}</cell><cell>628.7</cell><cell>85.5</cell><cell>79.1</cell><cell>45.2</cell></row><row><cell>AIC-Net, k={3, 5, 7}</cell><cell>847.0</cell><cell>113.7</cell><cell>80.5</cell><cell>45.8</cell></row><row><cell>AIC-Net, k={5, 7}</cell><cell>716.0</cell><cell>96.77</cell><cell>79.9</cell><cell>44.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Planner5d</surname></persName>
		</author>
		<ptr target="https://planner5d.com/.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scalable place recognition under appearance change for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh-Dzung</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasir</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jun</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9319" to="9328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Structured prediction of unobserved voxels from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5431" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Two stream 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03550</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint 3d object and layout inference from a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="183" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">View-volume network for semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="726" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rgbd based dimensional decomposition residual network for 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7693" to="7702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth based semantic scene completion with position importance aware loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="219" to="226" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Holistic scene understanding for 3d object detection with rgbd cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">See and think: Disentangling semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shice</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beibei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhe</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="263" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Completing 3d object shape from one depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Thorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2484" to="2493" />
		</imprint>
	</monogr>
	<note>Daeyun Shin, and Derek Hoiem</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<imprint>
			<pubPlace>Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shape completion enabled robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chad</forename><surname>Dechant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaqu?n</forename><surname>Ruales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. IROS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2442" to="2447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient semantic scene completion network with spatial group convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yaoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongen</forename><surname>Liaoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="733" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pixel-wise deep function-mixture network for spectral super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanning</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Yanning Zhang, and Anton van den Hengel. Adaptive importance learning for improving lightweight image superresolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond point clouds: Scene understanding by reasoning geometry and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsushi</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3127" to="3134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
