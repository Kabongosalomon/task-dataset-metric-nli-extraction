<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MULTI-VIEW MATCHING (MVM): FACILITATING MULTI-PERSON 3D POSE ESTIMATION LEARNING WITH ACTION-FROZEN PEOPLE VIDEO</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-14">April 14, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeji</forename><surname>Shen</surname></persName>
							<email>yejishen@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">Jay</forename><surname>Kuo</surname></persName>
							<email>cckuo@sipi.usc.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MULTI-VIEW MATCHING (MVM): FACILITATING MULTI-PERSON 3D POSE ESTIMATION LEARNING WITH ACTION-FROZEN PEOPLE VIDEO</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-14">April 14, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To tackle the challeging problem of multi-person 3D pose estimation from a single image, we propose a multi-view matching (MVM) method in this work. The MVM method generates reliable 3D human poses from a large-scale video dataset, called the Mannequin dataset, that contains actionfrozen people immitating mannequins. With a large amount of in-the-wild video data labeled by 3D supervisions automatically generated by MVM, we are able to train a neural network that takes a single image as the input for multi-person 3D pose estimation. The core technology of MVM lies in effective alignment of 2D poses obtained from multiple views of a static scene that has a strong geometric constraint. Our objective is to maximize mutual consistency of 2D poses estimated in multiple frames, where geometric constraints as well as appearance similarities are taken into account simultaneously. To demonstrate the effectiveness of 3D supervisions provided by the MVM method, we conduct experiments on the 3DPW and the MSCOCO datasets and show that our proposed solution offers the state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human pose estimation is a long-standing problem in computer vision research. It has numerous applications such as sports, augmented reality, motion analysis, visual avatar creation, etc. In the past ten years, a major advance has been made in building large-scale human pose datasets and developing deep-learning-based models for human pose estimation. As a result, estimating multi-person 2D poses and/or a single-person 3D pose in complicated scenes become mature. Yet, multi-person 3D pose estimation is still a challeging problem due to the lack of large-scale high-quality datasets for this application. This is further hindered by inadequate depth estimation algorithms in monocular images.</p><p>It is nontrivial to acquire high quality 3D supervisions in dynamic scenes. Depth sensors (e.g., Kinect) can provide useful data, yet the acquisition is typically limited to indoor environments. Furthermore, it often demands a large amount of manual work in capturing and processing. As an alternative, Li et al. <ref type="bibr" target="#b0">[1]</ref> attempted to estimate dense depth maps using the multi-view stereo (MVS) method from video clips captured in stationary scenes.</p><p>The main objective of our current research is to obtain high quality 3D skeletons from multiple 2D poses obtained in each frame of action-frozen people video. Such a dataset was collected by focusing on a special kind of Youtube video. That is, people imitate mannequins and freeze in elaborate and natural poses in a scene while a hand-held camera tours the scene to create the desired video. It is called the Mannequin dataset. This type of video is suitable for human pose estimation since it can provide diverse poses from people of different ages and genders with different scales in a wide variety of outdoor scenes. We propose a multi-view matching (MVM) method to achieve this goal. With a large amount of in-the-wild video data labeled by 3D supervisions automatically generated by MVM, we are able to train a neural network that takes a single image as the input and generate the associated multi-person 3D pose estimation as the output. arXiv:2004.05275v1 [cs.CV] 11 Apr 2020 Our approach has several advantages in multi-person 3D pose estimation. First, the scene does not have to be an indoor and/or lab environment. Second, there are a considerable number of video clips available online. If needed, it is possible to make more action-frozen people video at a low cost. Third, we can get high quality 3D skeleton information by exploiting the static scene assumption. To the best of our knowledge, there is only one in-the-wild 3D human pose dataset, called MPII-3DPW <ref type="bibr" target="#b1">[2]</ref>. It relies on Inertial Measurement Unit (IMU) sensors attached to a few actors or actresses. As compared to MPII-3DPW, our approach is more scalable. It contains more diverse contents in terms of subjects and environments. Some exemplary video clips in the Mannequin dataset are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. With the help of such video clips, we can obtain diverse 3D human poses in a wide range of scenes in daily life.</p><p>Estimating 3D skeletons from predicted 2D poses of multiple views has been studied previously in several settings, e.g. the synchronized multi-camera lab environment <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, the synthesized sports video <ref type="bibr" target="#b4">[5]</ref>. However, existing methods do not work well in the current setting (namely, video of action-frozen people captured by a hand-held camera) for the following reason. In the action-frozen video clip, each frame can be essentially regarded as a single view of the scene. A typical 10-second video sequence sampled at 25fps will yield 250 views of the scene from 250 viewing angles. As observed in <ref type="bibr" target="#b2">[3]</ref>, the main challenge is to build the correspondence of predicted 2D poses in different frames. The optimization method in <ref type="bibr" target="#b2">[3]</ref> was developed to minimize the cycle consistency loss in a multi-camera lab environment with a small number of views (e.g. five cameras). The solution in <ref type="bibr" target="#b2">[3]</ref> is computationally intractable in our current case, which has a larger number of views and more people in the scene.</p><p>Besides, pose tracking methods like <ref type="bibr" target="#b5">[6]</ref> can be a potential option when we need to determine which 2D poses in different frames corresponds to the same person. However, we argue that they are also not efficient enough in our case. The reason is that, when identifying the correspondence of multiple 2D poses, the 3D geometric constraints plays a more important role compared to the visual similarity clue that is usually what the pose tracking methods rely on, which is further confirmed by our empirical experiments. As it is not common for pose tracking methods to make assumption of static scenes, the 3D geometric constraints are not fully utilized. The pipeline of the proposed MVM method is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. First, we use the OpenPifPaf method <ref type="bibr" target="#b6">[7]</ref> to estimate multi-person 2D poses at each frame. Second, we find the correspondence of 2D poses across multiple frames. This is achieved by adopting an approximation to the optimization objective in <ref type="bibr" target="#b2">[3]</ref>, where both geometric constraints and appearance similarities are taken into account. Third, we apply triangulation to groups of matched 2D poses and conduct bundle adjustment to recover multi-person 3D poses. We provide both qualitative and quantative results to show the quality of the multi-person 3D poses in Youtube video.</p><p>In practice, we are interested in multi-person 3D pose estimation from a single image. To accomplish this goal, we train a monocular multi-person 3D pose network, which is a modification of the network proposed in <ref type="bibr" target="#b6">[7]</ref>, from multiple frames using the 3D supervisions obtained by MVM. To demonstrate the effectiveness of 3D supervisions provided by the MVM method, we conduct experiments on the 3DPW and the MSCOCO datasets. Evaluation shows performance improvement in the 3D human pose estimation accuracy for the 3DPW dataset and the 2D human pose estimation accuracy for the MSCOCO dataset.</p><p>There are four main contributions of our current research.</p><p>? We develop an efficient method, called the MVM method, that levarages geometric constraints and appearance similarities existing in video clips of static scenes. for reliable 3D human poses estimation.</p><p>? We collect a large-scale Youtube video clips containing action-frozen people with in-the-wild scenes and call it the Mannequin dataset. It can be used as a training dataset for multi-person 3D pose estimation.</p><p>? Although there is no groundtruth, we use the MVM method to generate weak 3D supervisions for the Mannequin dataset so that the dataset can be used to train a multi-person 3D neural network that is applied to a single image.</p><p>? We use the Mannequin dataset as the training dataset and show the effectiveness of the weak 3D supervisions provided by MVM through extensive experimental results conducted for the 3DPW dataset and the 2D MSCOCO dataset.</p><p>The rest of this paper is organized as follows. Related work is reviewed in Sec. 2. The Mannequin dataset is introduced in Sec. 3. The MVM method is proposed in Sec. 4. The solution to the multi-person 3D pose estimation problem from a single image is discussed in Sec. 5. Experimental results are shown in Sec. 6. Finally, concluding remarks are given in Sec. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Review of Related Work</head><p>Several previous research related to our current work is reviewed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">3D Human Pose Estimation</head><p>There are two common representations for 3D human poses: the skeleton-based representation and the parametricmodel-based representation. The skeleton-based representation serves as the minimal representation of human poses. It finds applications such as in sports motion analysis <ref type="bibr" target="#b7">[8]</ref>, among many others. One approach to skeleton-based 3D poses estimation is the use of the 2D-to-3D lifting mechanism. Martinez et al. <ref type="bibr" target="#b8">[9]</ref> proposed a method that uses a cascade of fully-connected (FC) layers to infer 3D skeletons from 2D skeletons. Recently, Ci et al.</p><p>[10] adopted a modified graph neural network to achieve the same goal. Yang et al. <ref type="bibr" target="#b10">[11]</ref> proposed a data augmentation method to synthesize virtual candidate poses, which improves performance consistently.</p><p>Besides using RGB images as the input, the skeleton-based representation can also be applied to range images for 3D human pose estimation. For example, Marin-Jimenez et al. <ref type="bibr" target="#b11">[12]</ref> examined a convolutional neural network (CNN) based method that estimates 3D human poses from depth images directly. Zhang et al. <ref type="bibr" target="#b12">[13]</ref> proposed a clustering based method with hybrid features by integrating the geodesic distance and the superpixel-based mid-level representation.</p><p>The parametric-body-model-based representation offers richer information of the human body. Early work used the SCAPE model <ref type="bibr" target="#b13">[14]</ref> that fits a body model to annotated 2D keypoints in the image. More recently, the SMPL model <ref type="bibr" target="#b14">[15]</ref> becomes popular. Kanazawa et al. <ref type="bibr" target="#b15">[16]</ref> proposed a method that fits the SMPL model by minimizing the re-projection error with respect to the 2D keypoint annotations while taking the human pose prior such as joint angles and shape parameters into account. Lassner et al. <ref type="bibr" target="#b16">[17]</ref> included the 2D silhouette projection in the loss function for data with segmentation mask annotations (e.g., the MSCOCO dataset).</p><p>Although they are useful in several contexts, both representations cannot be used in our multi-person 3D pose estimation framework directly. For the former, it is not easy to generalize the skeleton model to tackle with a varying number of people in an image. For the latter, the parametric model is too heavy for multi-person 3D pose estimation since it has too many parameters. It is worthwhile to mention that there exists work that targeted at obtaining 3D human poses directly. For example, Papandreou et al. <ref type="bibr" target="#b17">[18]</ref> showed that a coarse-to-fine volumetic representation is better than the simpliest K-by-3 vector representation, where K is the number of keypoints of the skeleton. A similar conclusion was observed in <ref type="bibr" target="#b18">[19]</ref>, which further proposed an integral regression on 3D skeletons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-Person Pose Estimation</head><p>To solve the problem of multi-person pose estimation from a single image, we can categorize current methods into two types: the top-down approach <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> and the bottom-up approach <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref>. The top-down approach runs a person detector and then estimates body joints in the detected bounding box. The associated methods benefit from advances in person detectors and a vast amount of labeled bounding boxes for people. The ability to leverage the labeled data turns the requirement of a person detector into an advantage. Yet, when multiple bounding boxes overlap, most single-person pose estimators do not work well. Unfortunetely, there are many such cases in the Mannequin dataset of our interest so that the top-down approach is not applicable. In contrast with the top-down approach, the bottom-up approach does not rely on a person detector. Kreiss et al. <ref type="bibr" target="#b6">[7]</ref> first estimated each body joint and, then, grouped them to form a unique pose by a method called the Parts Asscociation Field (PAF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Human Pose Tracking</head><p>A single-frame multi-person pose estimator cannot ensure consistency of identity across frames. Depending on how multiple frames are utilized, we categorize pose tracking methods into two types: offline methods and online methods.</p><p>To address the identity correspondence problem across frames, Many offline methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> were proposed to enforce temporal consistency of poses in video clips. Since it usually demands the solution of some difficult-to-optimize formula in form of spatio-temporal graphs, their solution speed is slow.</p><p>For online methods, a common technique to handle the multi-person identification problem across frames is to maintain temporal graphs in neural networks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. Rohit et al. <ref type="bibr" target="#b31">[32]</ref> proposed a 3D extension of Mask-RCNN, called person tubes, to connect people across time. Yet, its tracking result is no better than the simple baseline Hungarian Algorithm <ref type="bibr" target="#b27">[28]</ref> even more time and memory are needed to support the grouped convolution operations in the implementation for a couple of frames. Joint Flow <ref type="bibr" target="#b2">[3]</ref> exploited the concept of the Temporal Flow Field to connect keypoints across two frames. However, the flow-based representation suffers from ambiguity when subjects moved slowly (or being stationary). It demands special handling of such cases in the tracking process. Apparently, this method is not applicable to the Mannequin dataset that contains action-frozen people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Design Choices of Proposed MVM</head><p>In the proposed MVM method, we choose an approximating offline optimization method to balance accuracy and computing efficiency. Furthermore, we choose the bottom-up method since it is a better fit for the context of our interest. Furthermore, we adopt the "2D-skeleton-plus-relative-depth" representation for multi-person 3D pose representation. The above design choices have several advantages. First, unlike most top-down methods which are difficult to generalize to scenes containing a varying number of people in an image, our method can scale from the one-person case to the multi-person case elegently. Second, both 2D skeletons and their depth information can be expressed using local image coordinates projected to the camera matrix for each corresponding frame independently. This greatly relieves the dependence on camera's locations. As a result, our multi-person 3D pose estimation network can be trained in the camera-location-independent setting. In other words, our method can work in scenes with simple camera caliberation. The original Mannequin dataset <ref type="bibr" target="#b0">[1]</ref> has around 2000 sequences which consist of around 170K annotated frames with both camera poses and dense depthmaps available. However, some of the video clips are no longer available. We do not need the provided dense depthmaps. To build our own dataset, we manually choose 100 sequences and collect ten clips for each sequence, where each clip is less than 10 seconds. Our dataset consists of 47,263 frames in total. As compared with the original Mannequin dataset, we only use the camera intrinsics information. This is because our re-sampled frames have very few overlaps with the annotated frames in the original sequences and we need to re-compute the camera pose information using the COLMAP method <ref type="bibr" target="#b33">[34]</ref> in sampled frames. Unlike the original application of the Mannequin dataset in depth estimation which requires geometric consistency of most pixels in a frame, we only demand consistency of a few pixels corresponding to 2D keypoints. Thus, we have a different criterion in selecting valid frames.</p><p>The 100 sequences are chosen such that they contain a large amount of body motion, pose and appearance variations. They also contain severe body part occlusion and truncation. They are attributed to occlusions with other people or objects and the fact that persons may disappear partially or completely and re-appear again. The person scale also varies across the video clip due to camera's movement. Thus, the number of visible people and body parts varies across a video clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multi-View Matching (MVM) Method</head><p>The overall pipeline of the proposed 3D multi-view matching (MVM) method is illustrated in 2. For each input clip, we first estimate the camera poses for all frames. Then, a 2D pose estimator is independently applied to those frames to get the initial 2D pose predictions. Next, we run the matching algorithm to find the correspondence of 2D poses across frames. After that, we use triangulation on 2D poses that belong to the same person to get the initial 3D skeleton. Finally, the 3D skeleton is further finetuned by the bundle adjustment algorithm for better consistency of the estimated 3D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Camera Pose Estimation</head><p>By following an approach similar to that adopted by <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b34">[35]</ref>, we use ORB-SLAM2 <ref type="bibr" target="#b35">[36]</ref> to identify trackable sequences in each video clip and estimate the initial camera pose for each frame. In this stage, we set the camera intrinsics the same as the one provided in the original Mannequin dataset, and process a lower-resolution version of the video clip for efficiency. Afterwards, we process each sequence at a higher resolution using a visual SfM system <ref type="bibr" target="#b36">[37]</ref> to refine initial camera poses and intrinsic parameters. This method extracts and matches features across frames. Then, it conducts the global bundle adjustment optimization. Our implementation is based on an open-sourced multi-view stereo (MVS) system called COLMAP <ref type="bibr" target="#b33">[34]</ref>. Two camera pose estimation examples are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. We show an exemplary image frame in the left and the estimated camera pose trajectory curve of for the corresponding video clip in red in the right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">2D Pose Estimation</head><p>We use the OpenPifPaf method in <ref type="bibr" target="#b6">[7]</ref> as the 2D human pose estimation baseline network. It is trained using results from the keypoint detection task of the MSCOCO dataset. For fair comparison with other similar work in <ref type="bibr" target="#b37">[38]</ref> and <ref type="bibr" target="#b38">[39]</ref>, we adopt a ResNet-152 <ref type="bibr" target="#b39">[40]</ref> backbone feature extractor and run the 2D pose estimator on each input frame independently to get the 2D pose predictions x ij along with confidence score w ij ? [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Matching</head><p>The objective of this matching step is to determine a set of 2D poses that belong to the same person. This can be mathematically stated as follows. Suppose that there are T frames in a video clip. We use x ij ? [0, 1] C?2 to indicate the j th 2D pose in the i th frame with C joints for each 2D pose in the normalized image coordinates. For each person k, we would like to determine a group of 2D poses, denoted by</p><formula xml:id="formula_0">G k = {x i1j1 , x i2j2 , ? ? ? , x i M k j M k },</formula><p>that are associated with the same person and M k is the number of frames this person shows up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Affinity Matrix</head><p>The matching criterion is based on the affinity function, denoted by A(x u , x v ), of two 2D poses x u and x v . The affinity function takes two factors into account; namely, appearance similarity S and geometric distance D. It is expressed as</p><formula xml:id="formula_1">A(x u , x v ) = S(x u , x v ) ? 1 1 + exp ?D(x u , x v ) ,<label>(1)</label></formula><p>where 1 1+exp (?D) converts a distance measure to a similarity measure and</p><formula xml:id="formula_2">D(x u , x v ) = 1 2C C c=1 d(x c u , L uv (x c v )) + d(x c v , L vu (x c u ))<label>(2)</label></formula><p>is a geometric distance measure between two poses. Furthermore, in Eq. <ref type="formula" target="#formula_2">(2)</ref>, L uv (x c v ) indicates the epipolar line of the c th joint of 2D pose x from view u to view v and d(.) is the Euclidean distance between a point and a line.</p><p>The appearance similarity, S(x u , x v ), in Eq. (1) is calculated using the cosine similarity of the features extracted from the last conv layer of the network described in Sec. 4.2. The geometric distance, D(x u , x v ), compute the average distance between the epipolar lines of the 2D keypoints in one frame and the corresponding 2D keypoints in the other frame. The overall affinity is a product of the appearance affinity and the geometric affinity as shown in Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Mutual Consistency Maximization</head><p>Suppose that there are N people in total in the entire set of input frames. We use y ij ? [1..N ] to indicate the associated person index of pose x ij . Our goal is to maximize the following objective function:</p><formula xml:id="formula_3">maximize y N k=1 (i1,j1)?G k (i2,j2)?G k w i1j1 w i2j2 A(x i1j1 , x i2j2 ), subject to {G k } is a partition for {x ij }.<label>(3)</label></formula><p>In principle, Eq. (3) can be solved by finding the partition of the affinity matrix with the spectral clustering method. However, this classical method is too slow to be practical in our current context. To speed up this optimization process, we develop a greedy algorithm that finds an approximation to the original objective. The main idea is described below in words. The algorithm maintains a set of corresponding 2D poses. It begins with the 2D pose that has the largest confidence from the pool of 2D poses, where the confidence score is generated by the 2D pose network. At each time, we choose the 2D pose with the highest affinity score from the pool, and repeat the process until we cannot find any x ij that has an affinity score above threshold ? . The pseudo codes of the proposed greedy 2D poses matching algorithm are given in Algorithm 1.</p><p>Algorithm 1: Pseudo codes for greedy 2D poses matching.</p><formula xml:id="formula_4">Input :2D Poses x ij , Affinity Matrix A Output :G k for k ? [1..N ] Initialize visited set V = ?; for k ? 1 to N do Find x i0j0 / ? V with the largest confidence; G k ? {x i0j0 }; Find x ij such that xpq?G k w ij w pq A(x ij , x pq ) is the largest and i = p for any x pq ? G k .; while Confidence of x ij is above ? do G k ? G k ? x ij ; Find x ij / ? V such that xpq?G k w ij w pq A(x ij , x pq ) is the largest and i = p for any x pq ? G k .; end V ? V ? G k ; end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Triangulation and Bundle Adjustment</head><p>The reconstruction of 3D poses from a group of corresponding 2D poses is a well-studied question in the 3D geometry literature. Here, we use the direct linear transform (DLT) algorithm <ref type="bibr" target="#b40">[41]</ref> to estimate the 3D keypoints from multiple corresponding 2D keypoints. Moreover, we use the RANSAC algorithm to eliminate outliers. The triangulation is applied to different joints independently. Triangulated 3D poses may suffer from small parallax angles, which results in a very large reconstruction error. Some reconstructed poses are even not possible for human beings.</p><p>The common practice is to apply bundle adjustment on triangulated 3D poses to get high quality estimates. To implement this idea, we minimize an error function that considers the reprojection error and the human pose prior introduced in <ref type="bibr" target="#b41">[42]</ref> jointly. It is formulated by a Gaussian mixture (? l , ? l ) for l = 1, ? ? ? , 8 on a dataset of diverse 3D poses <ref type="bibr" target="#b3">[4]</ref> </p><formula xml:id="formula_5">in form of minimize X xu E R (X, x u ) + ?E P (X),, subject to X ? R 3?C , ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">E P (X) = ? log 8 l=1 N (X|? l , ? l ) .<label>(5)</label></formula><p>After we obtain reconstructed 3D poses, we will project them back to each frame to generate a 3D supervision that will be used in the training of a 3D pose estimation network from a single image as discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multi-Person 3D Pose Estimation from Single Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">System Overview</head><p>In this section, we study how to train a convolutional neural network (CNN) to solve the problem of multi-person 3D pose estimation from a single image. To address this problem, we represent estimated 3D poses with two complementary components: 1) 2D poses and 2) keypoint depths. Our CNN architecture is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. It is a variant of a state-of-the-art bottom-up human pose estimation network called PifPaf <ref type="bibr" target="#b6">[7]</ref>. It has three key modules:</p><p>? a backbone feature extractor (ResNet-152);</p><p>? three prediction modules; namely, Parts Intensity Fields (PIF), Parts Association Fields (PAF), and Parts Depths Fields (PDF);</p><p>? a 3D pose encoder/decoder.</p><p>It is worthwhile to mention that PIF and PAF are the same as <ref type="bibr" target="#b6">[7]</ref> while PDF is new. Furthermore, most existing multi-person 3D pose networks, e.g., <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>, take multiple frames as the input and perform some sort of 2D-to-3D operations before producing the final 3D poses. In contrast, our network can be trained in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Key Components</head><p>We will discuss the roles and implementations of PIF, PAF, PDF, and the 3D pose encoder and decoder here.</p><p>PIF is used to describe where the 2D keypoints are so that the body parts can be localized. Different joints are processed independently. As far as the implementation is concerned, there are 5 components for each location of joint j in the output map. They are c, p x , p y , b and ?, where c is the confidence score, (p x , p y ) denote the coordinates of the point that is closest to joint j, b is used to characterize the adaptive regression loss for (p x , p y ), and ? stands for the standard deviation of the Gaussian component at location (p x , p y ) when werecover the high-resolution confidence map of joint j from a low-resolution output map produced by the network.</p><p>PAF is used to characterize the link location of a skeleton, where links are used to connect joints detected by PIF. The joints and links are associated together to form an instance of a skeleton. In our skeleton representation as illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>, some PAFs correspond to real bones while others do not. They are just some virtual connections between joints, e.g. the connection between the left ear and the left shoulder. Since both PIF and PAF are the same as those in <ref type="bibr" target="#b6">[7]</ref>, we refer to the orginal paper for further details.</p><p>PDF is used to regress the relative depth of joints. We use the relative depth because of inherent scale ambiguity of 3D poses recovered from multiple 2D poses. Note that 3D poses used as the groundtruth are expressed in the relative scale. Being similar to PIF, there are 2 PDF components for each location (i, j). They are d ij and ? ij . The former represents the relative depth at locaiton (i, j) while the latter denotes the radius of the Gaussian component centered at the location of the closest joint, denoted by (p ij x , p ij y ). The actual value of the high resolution depth map at location (x, y) satisfies  the following:</p><formula xml:id="formula_7">D xy = i,j d ij N (x, y|(p ij x , p ij y ), ? ij ).<label>(6)</label></formula><p>The 3D pose encoder is used to encode three parts fields to the 3D poses while the 3D pose decoder is used to decode the 3D poses to three part fields. As to the 2D poses plus the depth representation of 3D poses, we encoder/decode the 2D pose part in the same way as that is given in <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Loss Function</head><p>Since the 3D pose encoder is not differentiable, we need to compute the loss between encoded groundtruth parts fields and the predicted parts fields. The overall loss function is given below:</p><formula xml:id="formula_8">L = ? 1 L P IF + ? 2 L P AF + ? 3 L P DF .<label>(7)</label></formula><p>Because our camera poses are estimated through the SfM-based method, scale ambiguity is inevitable in our case. To solve the ambiguity problem, we use the following relative depths loss in our network:</p><formula xml:id="formula_9">L P DF (d 1 , d 2 ) = V ar[log d 1 d 2 ] = 1 N log d 1 d 2 2 ? 1 N 2 log d 1 d 2 2 .<label>(8)</label></formula><p>Therefore, our estimated keypoint depths will try to fit the relative scale of the depth label. It means that the estimated depths and the depth labels are the same if the ratio of these two depths at all locations are consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Length Scale Calibration</head><p>Although the predicted depths are relative, we can still make use of the prior information hidden in human poses to have a rough estimation of the missing scale, s. We need to find s such that the squared difference between the scaled bone length sl i and the average bone lengthl i is minimized as given below.</p><formula xml:id="formula_10">minimize s i 1 2 (sl i ?l i ) 2<label>(9)</label></formula><p>It is not difficult to show that Eq. (9) has the optimal solution at</p><formula xml:id="formula_11">s * = il i l i i l 2 i .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Network Training</head><p>Our proposed CNN for multi-person 3D pose estimation from a single image is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. It can be trained by data with imcomplete labels, e.g. images with 2D supervisions only, human pose labels with some missing joints. This training flexibility comes from the pose encoder that can generate independent maps for PIF, PAF and PDF separately. If some fields are missing in the ground truth labels, we can simply set the weights of these fields to zero while keeping labels of other fields in the gradient computation process. When training images have 3D supervisions, we project the 3D skeleton information back to the view of each frame so that our network can learn camera-independent multi-person 3D poses.</p><p>Before we train the proposed CNN with the Mannequin dataset, we first train it with 64,115 images in the 2017 COCO training set that have 2D pose annotations. In this pre-training process, the PDF branch is not affected since the COCO dataset does not have the depth information. The pre-training of 2D pose related fields can improve the robustness of the ultimate 3D pose estimation network significantly.</p><p>In the network training with the Mannequin dataset, we apply the same data augmentation as that is done in <ref type="bibr" target="#b6">[7]</ref>. To create uniform batches, we crop images into squares whose side is around 95-100% of the short edge of the image with a randomly selected location. The large crops are done to preserve the size of training images as much as possible.</p><p>Training images and annotations are horizontally flipped randomly. We use the SGD optimizer with a learning rate of 0.001, momentum of 0.95, a batch size of 8 and no weight decay. We employ model averaging to extract stable models for validation. At each optimization step, we update an exponentially weighted version of the model parameters. The decay constant is 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We first conduct experiments to verify the validity of the proposed MVM method quantitatively with the Mannequin dataset. Then, we do performance benchmarking with two public datasets; namely, 3DPW and MSCOCO. The purpose is to show how additional training based on the 3D supervision offered by the proposed MVM method as well as the Mannequin dataset helps improve the performance of the multi-person 3D pose network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">MVM Evaluation with Mannequin Dataset</head><p>We will demonstrate the effectiveness of the proposed MVM method using the Mannquin challenge dataset in both qualitative and quantitative ways. For the qualitative performance of generated 3D poses, some predicted examples are shown in <ref type="figure">Fig. 6</ref>. There are two people sitting on the carpet with severe occluded poses in the first column of this figure.</p><p>We see from this example the capability of the proposed MVM method in recovering complicated poses. In some scenarios, we may have a low confidence score of a predicted human pose (e.g. the orange girl in the third column) or a small parallax angle to result in the failure of 3D reconstruction, (e.g., some part of the joints associated with the orange-and-pink person in the second column). The MVM method tends to filter out such instances.</p><p>For quantitative evalutions, since the Mannequin dataset does not have any labels, we use the following metric to quantify the validity of generated 3D poses: the reprojection error E R of estimated 3D poses with their corresponding 2D poses. If the average reprojection error is low enough, it is reasonable to trust quality of the generated 3D poses.</p><p>Matching Algorithm  <ref type="table">Table 2</ref>: Performance comparison between several matching algorithms, where * means only evaluated on clips with less than 50 frames. Otherwise, it would take too much time to run Dong's algorithm. Smaller reconstruction error E R implies more consistent 3D poses. A larger number of corresponding 2D keypoints |G k | as well as a smaller number of outliers means a more robust 3D reconstruction process and thus potentially better 3D poses.</p><formula xml:id="formula_12">E R (px) ? Outliers ? |G k | ? E R (px) w/o RANSAC ? |G k | w/</formula><p>We compare the MVM method with three other methods in <ref type="table">Table 2</ref>. They are:</p><p>1. a baseline that uses sequential matching; 2. a shortest path matching method <ref type="bibr" target="#b41">[42]</ref>; 3. the state-of-the-art optimization-based matching algorithm <ref type="bibr" target="#b2">[3]</ref>.</p><p>The triangulation is computed joint-wise. We also compare the effect of RANSAC in eliminating outliers. The quantity, |G k |, in the table means the number of the 2D keypoints chosen for triangulation. <ref type="table">Table 2</ref>, the proposed MVM method achieves comparable performance with the state-of-the-art optimization method <ref type="bibr" target="#b2">[3]</ref> on short clips at a much lower computational cost. These video clips are short enough for the optimization method to converge within a reasonable amount of time (e.g. an hour). For longer video clips, our MVM method outperforms both the baseline method and the shortest path matching algorithm by a large margin.  <ref type="table">Table 3</ref>: Comparison among different affinity matrices. Furthermore, we show how different similarity metrics affect the quality of generated 3D poses in <ref type="table">Table 3</ref>. It is clear from the table the proposed geometric distance contributes the most performance gain in terms of the average reprojection error. As to the appearance similarity, although it reduces the total number of matched 2D poses slightly, we can obtain the best overall reprojection error by combining the geometric distance and the appearance similarity since their integration introduces more constraints to matched poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">3DPW Evaluation</head><p>Very few multi-person 3D human pose datasets are available to the public. One recently released dataset, called the MPII-3DPW <ref type="bibr" target="#b1">[2]</ref>, contains 60 clips. It contains outdoor video clips captured by a mobile phone with 17 IMUs attached to the subjects. The IMU data allow people to accurately compute 3D poses and use them as the ground truth. The test set consists of 24 video clips. We use the 14 keypoints that are common in both MSCOCO and SMPL skeletons. The same setting was also used in <ref type="bibr" target="#b41">[42]</ref>. We evaluate on those frames that have enough visible keypoints for 3D pose estimation and ignore subjects that have less than seven 2D visible keypoints. We compute the Procrustes Aligned <ref type="figure">Figure 6</ref>: Visualization of the Mannequin dataset results: (1st row) original images with 2D poses, (2nd row) generated 3D poses. The first column indicates a successful estimation of 3D poses in the scene. Due to the low confidence score (e.g., the orange girl in the third column) or a small parallax angle which can cause the failure of 3D reconstruction (e.g., some part of the joints associated with the orange-and-pink person in the second column), the MVM method filters out such instances in a scene.</p><p>Mean Per Joint Position Error (PA-MPJPE) <ref type="bibr" target="#b41">[42]</ref> independently for each pose and, then, average errors for each tracked person in each video clip. This process implies that we count video clips with two people twice, etc. Finally, we average over the entire dataset. single frame PA-MPJPE (mm) ? Popa et al. <ref type="bibr" target="#b43">[44]</ref> 108.2 SMPLify, Bogo et al. <ref type="bibr" target="#b44">[45]</ref> 108.1 Train w/ Our network 82.3 Train + Mannequin dataset w/ our network 78.8 Train w/ HMR <ref type="bibr" target="#b15">[16]</ref> 81.3 Train + Mannequin dataset w/ HMR <ref type="bibr" target="#b15">[16]</ref> 78.2 <ref type="table">Table 4</ref>: Evaluation of estimated 3D poses for the MPII-3DPW dataset. <ref type="table">Table 4</ref> shows how incorporating additional data from our Mannequin dataset improves results on 3DPW testing set. Training our proposed network with our data can improve the PA-MPJPE by 3.5mm. Since 3DPW dataset has annotations for the full parametric body model, SMPL model <ref type="bibr" target="#b14">[15]</ref>, it is expected to have better performance if the network can leverage the such information. Thus, we compare the performance of HMR model trained in 3DPW dataset and the performance trained in 3DPW and Mannequin dataset. The PA-MPJPE was improved by 3.1mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">MSCOCO Evaluation</head><p>Besides facilitating 3D pose estimation, we are able to improve the accuracy of 2D pose estimation by training a learning system using our Mannequin dataset. <ref type="table">Table 5</ref> provides quantitative results for the COCO dataset. The evaluation metrics in COCO dataset are based on the mean average precision (AP) over 10 object keypoint similarity (OKS) thresholds as the main competition metric <ref type="bibr" target="#b45">[46]</ref>. The OKS score can measure the similarity between two 2D poses, which essentially serves as a similar role like what IoU does in object detection or segmentation. Our dataset can improve the baseline performance by 1.9 in mAP. The training benefits from the additional data in our dataset which has a lot of weird poses. Apparently, it is more likely for people to make strange and challenging poses in the shooting of a video clip to be uploaded to the Youtube. As a result, our dataset helps recover difficult cases. mAP@OKS ? AP@OKS=0.5 ? Train 64.6 85.9 Train + Mannequin dataset 66.5 87.1 <ref type="table">Table 5</ref>: Evaluation of estimated 2D poses in the COCO validation set.</p><p>These experiments show how one can effectively use the Mannequin dataset to improve the per-frame 2D pose model in multiple datasets. Besides, in <ref type="table">Table 6</ref>, we compute C var to empirically compare the consistency of the initial 2D pose estimator. As is defined in <ref type="bibr">Equation 11</ref>, C var stands for the variance of pairwise triangulated 3D keypoints of 2D pose predictions,</p><formula xml:id="formula_13">C var ({x c u }) = 1 M (M ? 1)/2 (F tri (x c u , x c v ) ? ? c X ) 2 ,<label>(11)</label></formula><p>where ? is the threshold to eliminiate outliers. As shown in the table, OpenPifPaf can produce a more consistent set of 2D poses. This is the reason we used OpenPifPaf as the default 2D pose estimator.</p><p>2D pose network C var @? = 0.5 C var @? = 0.9 Mask-RCNN <ref type="bibr" target="#b19">[20]</ref> 581.8 233.6 OpenPifPaf <ref type="bibr" target="#b6">[7]</ref> 351.0 185.7 <ref type="table">Table 6</ref>: Comparison between two 2D pose estimation networks, where we report the variance, C var , of the pairwise triangulated 3D poses, which is used to measure consistency of predicted 2D poses, and ? is the threshold to eliminiate outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>An MVM method was proposed to generate reliable 3D supervisions from unlabeled action-frozen video in the Mannequin dataset so as to improve the estimation accuracy of multi-person 2D and 3D poses. The MVM method attempts to match 2D human poses estimated across multiple frames. The key to efficient matching lies in taking the geometric constraint (of static scenes in the Mannequin dataset) and appearance similarity into account jointly. Afterwards, through the triangulation of a group of matched 2D poses, optimization by considering the human pose prior and re-projection errors jointly and bundle adjustment, we can obtain reliable 3D supervisions. These 3D supervisions are used to train a multi-person 3D pose network. It was demonstrated by experimental results that both 3D pose estimation accuracy (tested for the 3DPW dataset) and 2D pose estimation accuracy (tested for the MSCOCO dataset) can be improved.</p><p>There are several research directions worth further exploration. They are elaborated below.</p><p>? Acquiring more information from videos of action-frozen people such as 3D surfaces, textures, etc. In the static scene setting, it is feasible to obtain reliable estimates of various scene information with no manual labels.</p><p>? Utilizing weak 3D supervisions obtained from the Mannequin dataset. Although the 3D poses generated by MVM method are accurate, we may have an incomplete number of joints. This is caused by insufficient parallax (viewing angle). To give an example, the right ear might be missing since it is not seen in input frames. One might "inpaint" incomplete pose supervisions to obtain a pseudo groundtruth.</p><p>? Utilizing 3D point cloud information when running COLMAP. COLMAP can be used to estimate camera poses and the 3D point cloud information. Currently, we only use the camera pose information. The 3D point cloud set may provide some extra useful information for better 3D pose estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Exemplary video clips from the Mannequin dataset, where action-frozen people keep certain poses in diverse environments such as cafe, school, living room, hall, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the generation of multi-person 3D poses by the proposed MVM method. The input contains a sequence of image frames (shown in the first block), 2D pose estimation at each frame (shown in the second block), building correspondence of 2D poses across multiple frames (shown in the third block), recovering 3D poses from matched 2D poses (shown in the fourth block).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of estimated camera poses: exemplary image frames in two video clips (left) and the corresponding estimated camera pose trajectory curves in red (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The proposed CNN for multi-person 3D pose estimation from a single image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The skeleton representation of human poses in our framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the Mannequin dataset.</figDesc><table><row><cell>3 Mannequin Dataset</cell><cell></cell></row><row><cell># of</cell><cell>Count</cell></row><row><cell>Sequences</cell><cell>100</cell></row><row><cell>Sampled clips</cell><cell>550</cell></row><row><cell>Frames</cell><cell>47263</cell></row><row><cell>Unmatched 2D Poses</cell><cell>525901</cell></row><row><cell>Matched 2D Poses</cell><cell>176876</cell></row><row><cell>Generated 3D Poses</cell><cell>2172</cell></row><row><cell>Ave. clips per sequence</cell><cell>5.5</cell></row><row><cell>Ave. frames per clip</cell><cell>85.9</cell></row><row><cell cols="2">Ave. 2D joints per triangulation 41.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Ave. E R Ave. |G k |</figDesc><table><row><cell>Shortest Path w/ MSE</cell><cell>25.7</cell><cell>14.7</cell></row><row><cell>Shortest Path w/ Geo Dist</cell><cell>20.5</cell><cell>22.6</cell></row><row><cell>Shortest Path w/ Geo Dist + Appearance</cell><cell>19.6</cell><cell>19.8</cell></row><row><cell>MVM w/ MSE</cell><cell>N/A</cell><cell>15.8</cell></row><row><cell>MVM w/ Geo Dist</cell><cell>16.8</cell><cell>45.5</cell></row><row><cell>MVM w/ Geo Dist + Appearance (ours)</cell><cell>14.6</cell><cell>41.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning the depths of moving people by watching frozen people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4521" to="4530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast and robust multi-person 3d pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7792" to="7801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="190" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-person 3d pose estimation and tracking in sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Bridgeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5167" to="5176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Synergetic reconstruction from 2d pose and 3d motion for wide-space multi-person video motion capture in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Ohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Ikegami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05613</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2262" to="2271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via exemplar augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanru</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="371" to="379" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d human pose estimation from depth maps using a deep combination of poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Romero-Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Mu?oz-Salinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Medina-Carnicer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="627" to="639" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d human pose estimation from range images with depth difference and geodesic distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehui</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="272" to="282" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005 Papers</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="408" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6050" to="6059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">View invariant 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge-guided deep fractal neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiquan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1246" to="1259" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Monocular three-dimensional human pose estimation using local-topology preserved sparse retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingtian</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">33008</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multi-person pose estimation with enhanced feature aggregation and selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xixia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10238</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Arttrack: Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6457" to="6465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Posetrack: Joint multi-person pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A robust likelihood function for 3d human pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5374" to="5389" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04596</idno>
		<title level="m">Joint flow: Temporal flow fields for multi person tracking</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detect-and-track: Efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A gaussian process guided particle filter for tracking 3d human pose in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Sedai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du Q</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4286" to="4300" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enliang</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="501" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Stereo magnification: Learning view synthesis using multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09817</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Artal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tard?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1255" to="1262" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4104" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05656</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deepfuse: An imu-aware network for real-time 3d human pose estimation from multi-view image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuxia</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04071</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep multitask architecture for integrated 2d and 3d human sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6289" to="6298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
