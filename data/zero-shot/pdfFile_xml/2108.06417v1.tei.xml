<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Is Pseudo-Lidar needed for Monocular 3D Object detection?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rares</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ambrus</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Research Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Is Pseudo-Lidar needed for Monocular 3D Object detection?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress in 3D object detection from single images leverages monocular depth estimation as a way to produce 3D pointclouds, turning cameras into pseudo-lidar sensors. These two-stage detectors improve with the accuracy of the intermediate depth estimation network, which can itself be improved without manual labels via large-scale self-supervised learning. However, they tend to suffer from overfitting more than end-to-end methods, are more complex, and the gap with similar lidar-based detectors remains significant. In this work, we propose an end-to-end, single stage, monocular 3D object detector, DD3D, that can benefit from depth pre-training like pseudo-lidar methods, but without their limitations. Our architecture is designed for effective information transfer between depth estimation and 3D detection, allowing us to scale with the amount of unlabeled pre-training data. Our method achieves state-of-theart results on two challenging benchmarks, with 16.34% and 9.28% AP for Cars and Pedestrians (respectively) on the KITTI-3D benchmark, and 41.5% mAP on NuScenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Detecting and accurately localizing objects in 3D space is crucial for many applications, including robotics, autonomous driving, and augmented reality. Hence, monocular 3D detection is an active research area <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b71">71]</ref>, owing to its potentially wide-ranging impact and the ubiquity of cameras. Leveraging exciting recent progress in depth estimation <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b31">31]</ref>, pseudo-lidar detectors <ref type="bibr" target="#b71">[71,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b59">59]</ref> first use a pre-trained depth network to compute an intermediate pointcloud representation, which is then fed to a 3D detection network. The strength of pseudo-lidar methods is that they monotonically improve with depth estimation quality, e.g., thanks to large scale training of the depth network on raw data. * equal contribution Code: https://github.com/TRI-ML/dd3d <ref type="figure">Figure 1</ref>: We introduce a single-stage 3D object detector, DD3D, that combines the best of both pseudo-lidar methods (scaling with depth pre-training) and end-to-end methods (simplicity and generalization performance). Our detector features a simple training protocol of depth pre-training and detection fine-tuning, compared to pseudo-lidar methods that require an additional depth fine-tuning step and tend to overfit to depth errors.</p><p>However, regressing depth from single images is inherently an ill-posed inverse problem. Consequently, errors in depth estimation account for the major part of the gap between pseudo-lidar and lidar-based detectors, a problem compounded by generalization issues that are not fully understood yet <ref type="bibr" target="#b58">[58]</ref>. Simpler end-to-end monocular 3D detectors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">39]</ref> seem like a promising alternative, although they do not enjoy the same scalability benefits of unsupervised pre-training due to their single stage nature.</p><p>In this work, we aim to get the best of both worlds: the scalability of pseudo-lidar with raw data and the simplicity and generalization performance of end-to-end 3d detectors. To this end, our main contribution is a novel fully convolutional single-stage 3D detection architecture, DD3D (for Dense Depth-pre-trained 3D Detector), that can effectively leverage monocular depth estimation for pre-training (see <ref type="figure">Figure 1</ref>). Using a large corpus of unlabeled raw data, we show that DD3D scales similarly to pseudo-lidar methods, and that depth pre-training improves upon pre-training on large labeled 2D detection datasets like COCO <ref type="bibr" target="#b37">[37]</ref>, even with the same amount of data.</p><p>Our method sets a new state of the art on the task of monocular 3D detection on KITTI-3D <ref type="bibr" target="#b14">[14]</ref> and nuScenes <ref type="bibr" target="#b4">[5]</ref> with significant improvements compared to previous state-of-the-art methods. The simplicity of its training procedure and its end-to-end optimization allows for effective use of large-scale depth data, leading to impressive multi-class detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Monocular 3D detection. A large body of work in imagebased 3D detection builds upon 2D detectors, and aims to lift them to 3D using various cues from object shapes and scene geometry. These priors are often injected by aligning 2D keypoints with their projective 3D counterparts <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b52">52]</ref>, or by learning a low-dimensional shape representation <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b43">43]</ref>. Other methods try to leverage geometric consistency between 2D and 3D structures, formulating the inference as a constrained optimization problem <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr">12,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b34">34]</ref>. These methods commonly use additional data (e.g. 3D CAD models, instance segmentation), or assume rigidity of objects. Inspired by Lidarbased methods <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b30">30]</ref>, another line of work employs view transformation (i.e. birds-eye-view) to overcome the limitations of perspective range view, e.g. occlusion or variability in size <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b60">60]</ref>. These methods often require precise camera extrinsics, or are accurate only in close vicinity. End-to-end 3D detectors. Alternatively, researchers have attempted to directly regress 3D bounding boxes from CNN features <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b76">76,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">9]</ref>. Typically, these approaches extend standard 2D detectors (single-stage <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b76">76]</ref> or twostage <ref type="bibr" target="#b55">[55]</ref>) by adding heads that predict various 3D cuboid parameterizations . The use of depth-aware convolution or dense 3D anchors <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b53">53]</ref> enabled higher accuracy. Incorporating uncertainty in estimating the 3D box (or its depth) has also been shown to greatly improve detection accuracy <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b58">58]</ref>. In <ref type="bibr" target="#b59">[59]</ref>, the authors proposed a disentangled loss for 3D box regression that helps stabilize training. DD3D also falls in the end-to-end 3D detector category, however, our emphasis is on learning a good depth representation via large-scale self-supervised pre-training on raw data, which leads to robust 3D detection. Pseudo-Lidar methods. Starting with the pioneering work of <ref type="bibr" target="#b67">[67]</ref>, these methods leverage advances in monocular depth estimation and train Lidar-based detectors on the resulting pseudo pointcloud, producing impressive results <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b42">42]</ref>. Recent methods have improved upon <ref type="bibr" target="#b67">[67]</ref> by correcting the monocular depth with sparse Lidar readings <ref type="bibr" target="#b71">[71]</ref>, decorating the pseudo pointcloud with colors <ref type="bibr" target="#b42">[42]</ref>, using 2D detection to segment foreground pointcloud regions <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b41">41]</ref>, and structured sparsification of the monocular pointcloud <ref type="bibr" target="#b63">[63]</ref>. Recently, <ref type="bibr" target="#b58">[58]</ref> has shown a bias in the PL results on the representative KITTI-3D <ref type="bibr" target="#b14">[14]</ref> benchmark, while this paradigm remains the state-of-the-art. Multiple researchers showed that inaccurate depth estimation is a major source of error in PL methods <ref type="bibr" target="#b58">[58,</ref><ref type="bibr" target="#b66">66]</ref>. In this work, we build our reference PL method based on <ref type="bibr" target="#b49">[49]</ref> and <ref type="bibr" target="#b41">[41]</ref> to investigate the benefits of using large-scale depth data and compare it with DD3D. Monocular depth estimation. Estimating per-pixel depth is a key task for both DD3D (as a pre-training task) and PL (as the first of a two-stage pipeline). It is itself a thriving research area: the community has pushed toward accurate dense depth prediction via supervised <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b31">31]</ref> as well as self-supervised <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b64">64]</ref> methods. We note that supervised monocular depth training used in this work require no annotations from human, allowing us to scale our methods to a large amount of raw data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dense depth pre-training for 3D detection</head><p>Given a single image and its camera intrinsics matrix as input, the goal of monocular 3D detection is to generate a set of multi-class 3D bounding boxes relative to camera coordinates. During inference, DD3D does not require any additional data, such as per-pixel depth estimates, 2D bounding boxes, segmentations, or 3D CAD models. DD3D is also camera-aware: it scales the depth of putative 3D boxes according to the camera intrinsics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>DD3D is a fully convolutional single-stage network that extends FCOS <ref type="bibr" target="#b61">[61]</ref> to perform 3D detection and dense depth prediction. The architecture (see <ref type="figure" target="#fig_0">Figure 2</ref>) is composed of a backbone network and three sub-networks (or heads) that are shared among all multi-scale features. The backbone takes an RGB image as input, and computes convolutional features at different scales. As in <ref type="bibr" target="#b61">[61]</ref>, we adopt a feature pyramid network (FPN) <ref type="bibr" target="#b36">[36]</ref> as the backbone.</p><p>Three head networks are applied to each feature map produced by the backbone, and perform independent prediction tasks. The classification module predicts object category. It produces C real values, where C is the number of object categories. The 2D box module produces classagnostic bounding boxes and center-ness by predicting 4 offsets from the feature location to the sides of each bounding box and a scalar associated with center-ness. We refer the readers to <ref type="bibr" target="#b61">[61]</ref> for more details regarding the 2D detection architecture. 3D detection head. This head predicts 3D bounding boxes and per-pixel depth. It takes FPN features as input, and applies four 2D convolutions with 3 ? 3 kernels that generate 12 real values for each feature location. These are decoded into 3D bounding box, per-pixel depth map, and 3D prediction confidence, as described below:</p><p>? q = (q w , q x , q y , q z ) is the quaternion representation of allocentric orientation <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b39">39]</ref> of the 3D bounding box. It is normalized and transformed to an egocentric orientation <ref type="bibr" target="#b43">[43]</ref>. Note that we predict orientations with the full 3 degrees of freedom.</p><p>? z {c,p} represent the depth predictions. z c is decoded to the z-component of 3D bounding box centers and therefore only associated with foreground features, while z p is decoded to the monocular depth to the closest surface and is associated with every pixel. To decode them to metric depth, we unnormalized them using per-level parameters as follows:</p><formula xml:id="formula_0">d = c p ? (? l ? z + ? l ),<label>(1)</label></formula><formula xml:id="formula_1">p = 1 f 2 x + 1 f 2 y ,<label>(2)</label></formula><p>where z ? {z c , z p } are network output, d ? {d c , d p } are predicted depths, (? l , ? l ) are learnable scaling factors and offsets defined for each FPN level, p is the pixel size computed from the focal lengths, f x and f y , and c is a constant.</p><p>We note that this design of using camera focal lengths endows DD3D with camera-awareness, by allowing us to infer the depth not only from the input image, but also from the pixel size. We found that this is particularly useful for stable training. Specifically, when the input image is resized during training, we keep the ground-truth 3D bounding box unchanged, but modify the camera resolution as follows:</p><formula xml:id="formula_2">K = r x r y 1 ? ? f x 0 p x 0 f y p y 0 0 1 ? ? ,<label>(3)</label></formula><p>where r x and r y are resize rates, and K is the new camera intrinsic matrix that is used in Eqs. 2 and 4. Finally, {z p } collectively represent low-resolution versions of the dense depth maps computed from each FPN features. To recover the full resolution of the dense depth maps, we apply bilinear interpolation to match the size of the input image.</p><p>? o = (? u , ? v ) represent offsets from the feature location to the 3D bounding box center projected onto the camera plane. This is decoded to the 3D center via unprojection:</p><formula xml:id="formula_3">C = K ?1 ? ? u b + ? l ? u v b + ? l ? v 1 ? ? d c ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">(u b , v b )</formula><p>is the feature location in image space, ? l is a learnable scaling factor assigned to each FPN level.</p><p>? ? = (? W , ? H , ? H ) represents the deviation in the size of the 3D bounding box from the class-specific canonical size, i.e. s = (W 0 e ? W , H 0 e ? H , D 0 e ? D ). As in <ref type="bibr" target="#b57">[57]</ref>, (W 0 , H 0 , D 0 ) is the canonical box size for each class, and is pre-computed from the training data as its average size.</p><p>? ? 3D represents the confidence of the 3D bounding box prediction <ref type="bibr" target="#b59">[59]</ref>. It is transformed into a probability: p 3D = (1 + e ?? 3D ) ?1 , and multiplied by the class probability computed from the classification head <ref type="bibr" target="#b61">[61]</ref> to account for the relative confidence to the 2D confidence. The adjusted probability is used as the final score for the candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Losses</head><p>We adopt the classification loss and 2D bounding box regresssion loss from FCOS <ref type="bibr" target="#b61">[61]</ref>:</p><formula xml:id="formula_5">L 2D = L reg + L cls + L ctr ,<label>(5)</label></formula><p>where the 2D box regression loss L reg is the IOU loss <ref type="bibr" target="#b73">[73]</ref>, the classification loss L cls is the binary focal loss (i.e. onevs-all), and the center-ness loss L ctr is the binary cross entropy loss. For 3D bounding box regression, we use the disentangled L1 loss described in <ref type="bibr" target="#b59">[59]</ref>, i.e.</p><formula xml:id="formula_6">L 3D (B * ,B) = 1 8 ||B * ?B|| 1 ,<label>(6)</label></formula><p>where B * andB are the 8 vertices of ground-truth and candidate 3D boxes. This loss is replicated 4 times by using only one of the predicted 3D box components (orientation, projected center, depth, and size), while replacing other three with their ground-truth values. Also as in <ref type="bibr" target="#b59">[59]</ref>, we adopt the self-supervised loss for 3D confidence which uses the error in 3D box prediction to compute a surrogate target for 3D confidence (relative to the 2D confidence):</p><formula xml:id="formula_7">p * 3D = e ? 1 T L 3D (B * ,B) ,<label>(7)</label></formula><p>where T is the temperature parameter. The confidence loss L conf is the binary cross entropy between p 3D and p * 3D . In summary, the total loss of DD3D is defined as follows:</p><formula xml:id="formula_8">L DD = L 2D + L 3D + L conf .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Depth pre-training</head><p>During pre-training, we use per-pixel depth predictions from all FPN levels, i.e. {z p }. We consider pixels that have valid ground-truth depth from the sparse Lidar pointclouds projected onto the camera plane, and compute L1 distance from the predicted values:</p><formula xml:id="formula_9">L l depth = ||D * ?D l || 1 M,<label>(9)</label></formula><formula xml:id="formula_10">L depth = l L l depth ,<label>(10)</label></formula><p>where D * is the ground-truth depth map,D l is the predicted depth map from the l-th level in FPN (i.e. interpolated z p ), and M is the binary indicator for valid pixels. We observed using all FPN levels in the objective, rather than e.g. using only the highest resolution features, enables stable training, especially when training from scratch. We also observed that the L1 loss yields stable training with large batch sizes and high-resolution input, compared to SILog loss <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b31">31]</ref> that is popular in monocular depth estimation literature. We note that the two paths in DD3D from the input image to the 3D bounding box and to the dense depth prediction differ only in the last 3?3 convolutional layer, and thus share nearly all parameters. This allows for effective transfer from the pre-trained representation to the target task.</p><p>While pre-training, the camera-awareness of DD3D allows us to use camera intrinsics that are substantially different from the ones of the target domain, while still enjoying effective transfer. Specifically, from Eqs. 1 and 2, the error in depth prediction, z, caused by the difference in resolution between the two domains is corrected by the difference in pixel size, p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Pseudo-Lidar 3D detection</head><p>An alternative way to utilize a large set of image-Lidar frames is to adopt the Pseudo-Lidar (PL) paradigm and aim to improve the depth network component using the large scale data. PL is a two-stage method: first, given an image it applies a monocular depth network to predict per-pixel depth. The dense depth map is transformed into a 3D point cloud, and then a Lidar-based 3D detector is used to predict 3D bounding boxes. The modularity of PL methods enables us to quantify the role of improved depth predictors brought by a large-scale image-LiDAR dataset (see the supplementary material for additional details.) Monocular depth estimation. The aim of monocular depth estimation is to compute the depthD = f D (I (p)) for each pixel p ? I. Similarly to Eq. 9, given the ground truth depth measurement D * acquired from Lidar pointclouds, we define a loss by the error between the predicted and the ground-truth depth. Here we instead use the SILog loss <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b31">31]</ref>, which yields better performance than L1 for PackNet. Network architecture. As the depth network, we use Pack-Net <ref type="bibr" target="#b17">[17]</ref>, a state-of-the-art monocular depth prediction architecture which uses packing and unpacking blocks with 3D convolutions. By avoiding feature sub-sampling, Pack-Net recovers fine structures in the depth map with high precision; moreover, PackNet has been shown to generalize better thanks to its increased capacity <ref type="bibr" target="#b17">[17]</ref>. 3D detection. To predict 3D bounding boxes from the input image and the estimated depth map, we follow the method proposed by <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b41">41]</ref>. We first convert the estimated depth map into a 3D pointcloud similarly to Eq. 4, and concatenate each 3D point with the corresponding pixel values. This results in a 6D tensor encompassing colors along with 3D coordinates. We use an off-the-shelf 2D detector to identify proposal regions in input images, and apply a 3D detection network to each RoI region of the 6-channel image to produce 3D bounding boxes. Backbone, detection head and 3D confidence. We follow <ref type="bibr" target="#b41">[41]</ref> and process each RoI with a ResNet-18 <ref type="bibr" target="#b20">[20]</ref> backbone that uses Squeeze-and-Excitation layers <ref type="bibr" target="#b22">[22]</ref>. As the RoI contains both objects as well as background pixels, the resulting features are filtered via foreground masks computed based on the associated RoI depth map <ref type="bibr" target="#b42">[42]</ref>. The detection head follows <ref type="bibr" target="#b41">[41]</ref> and operates in 3 distance ranges, producing one bounding box for each range. The final output is then selected based on the mean depth of the input RoI. Following <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b58">58]</ref>, we modify the detection head to also output a 3D confidence value ? per detection, which is linked to the 3D detection loss. Loss function. The 3D regression loss <ref type="bibr" target="#b49">[49]</ref> is defined as:</p><formula xml:id="formula_11">L PL 3D = Lcenter + Lsize + L heading + Lcorners.<label>(11)</label></formula><p>In addition, we define a loss that links the predicted 3D confidence ? with the 3D bounding box coordinates loss <ref type="bibr" target="#b59">[59]</ref> using a Binary Cross Entropy (BCE) formulation with target? = e ?Lcorners . The final PL 3D detection loss is:</p><formula xml:id="formula_12">L PL = L PL 3D + L PL conf .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>KITTI-3D. The KITTI-3D detection benchmark <ref type="bibr" target="#b14">[14]</ref> consists of urban driving scenes with 8 object classes. The benchmark evaluates 3D detection accuracy on three classes (Car, Pedestrian, and Cyclist) using two average precision (AP) metrics computed with class-specific thresholds on intersection-over-union (IoU) of 3D bounding boxes or Bird-Eye-View (2D) bounding boxes. We refer to these metrics as 3D AP and BEV AP. We use the revised AP| R40 metrics <ref type="bibr" target="#b57">[57]</ref>. The training set consists of 7481 images, the test set of 7518 images. The objects in the test set are organized into three partitions according to their difficulty level (easy, moderate, hard), and are evaluated separately. For the analysis in Section 6.2, we follow the common practice of splitting the training set into 3712 and 3769 images <ref type="bibr" target="#b6">[7]</ref>, and report validation results on the latter. We refer to these splits as KITTI-3D train and KITTI-3D val.</p><p>nuScenes. The nuScenes 3D detection benchmark <ref type="bibr" target="#b4">[5]</ref> consists of 1000 multi-modal videos with 6 cameras covering the full 360-degree field of view. The videos are split into 700 for training, 150 for validation, and 150 for testing. The benchmark requires to report 3D bounding boxes of 10 object classes over 2Hz-sampled video frames. The evaluation metric, nuScenes detection score (NDS), is computed by combining the detection accuracy (mAP) computed over four different thresholds on center distance with five truepositive metrics. We report NDS and mAP, along with the three true-positive metrics that concern 3D detection, i.e. ATE, ASE, and AOE.</p><p>KITTI-Depth. We use the KITTI-Depth dataset <ref type="bibr" target="#b14">[14]</ref> to fine-tune the depth networks of our PL models. It contains over 93 thousands depth maps associated with the images in the KITTI raw dataset. The standard monocular depth protocol <ref type="bibr" target="#b74">[74,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b16">16]</ref> is to use the Eigen splits <ref type="bibr" target="#b11">[11]</ref>. However, as described in <ref type="bibr" target="#b58">[58]</ref>, up to a third of its training images overlap with KITTI-3D images, leading to biased results for PL models. To avoid this bias, we generate a new split by removing training images that are geographically close (i.e. within 50m) to any of the KITTI-3D images. We denote this split by Eigen-clean and use it to fine-tune the depth networks of our PL models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DDAD15M.</head><p>To pre-train DD3D and our PL models, we use an in-house dataset that consists of 25000 multi-camera videos of urban driving scenes. DDAD15M is a larger version of DDAD <ref type="bibr" target="#b16">[16]</ref>: it contains high-resolution Lidar sensors to generate pointclouds and 6 cameras synchronized with 10 Hz scans. Most videos are 10-second long, which amounts to approximately 15M image frames in total. Unless noted differently, we use the entire dataset for pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation detail</head><p>DD3D. We use V2-99 <ref type="bibr" target="#b33">[33]</ref> extended to an FPN as the backbone network. When pre-training DD3D, we first initialize the backbone with parameters pre-trained on the 2D detection task using the COCO dataset <ref type="bibr" target="#b37">[37]</ref>, and perform the pre-training on dense depth prediction using the DDAD15M dataset.</p><p>We use a test-time augmentation by resizing and flipping the input images. We observed 2.3% gain in "Car" BEV AP on KITTI val, but no improvement on the nuScenes validation set. All metrics on DD3D in Section 6.2 are averages over 4 training runs. We observed the variance over the runs to be small, i.e. 0.5 ? 1.2% BEV AP.</p><p>PL. When training PackNet <ref type="bibr" target="#b16">[16]</ref>, we use only the front camera images of DDAD15M to pre-train PackNet, and train until convergence. We then fine-tune the network on KITTI Eigen-clean split for 5 epochs. For more details on training DD3D and PL, please refer to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Benchmark evaluation</head><p>In this section, we evaluate DD3D on the KITTI-3D and nuScenes benchmarks. DD3D is pre-trained on DDAD15M, and then fine-tuned on the training set of each dataset. We also evaluate PL on KITTI-3D. Its depth network is pre-trained on DDAD15M and fine-tuned on KITTI Eigen-clean, and its detection network is trained on the KITTI-3D train.</p><p>KITTI-3D. In <ref type="table" target="#tab_1">Table 1</ref> we compare the accuracy of DD3D to state-of-the-art methods on the KITTI-3D benchmark. DD3D achieves a significant improvement over all methods with 16.34% 3D AP on Moderate Cars, which amounts to a 23% improvement from the previous best method (13.25%). Qualitative visualization is presented in <ref type="figure" target="#fig_1">Figure 3</ref>. In <ref type="table" target="#tab_5">Table 4</ref> we evaluate DD3D also on the Pedestrian and Cyclist classes. DD3D outperforms all other approaches on the Pedestrian category, with an 80.5% improvement from the previous best method (9.30% vs 5.14% 3D AP). On Cyclist DD3D achieves the second best result, reducing the gap to <ref type="bibr" target="#b27">[27]</ref> that uses ground-truth pointclouds to train a per-instance pointcloud reconstruction module and a two-stage regression network to refine 3D object proposals.</p><p>We next report the accuracy of our PL detector in Table 1. The accuracy of our PL detector is on par with stateof-the-art methods, however it performs significantly worse than DD3D (13.05% vs. 16.34%). We will discuss this result in the context of generalizability of PL methods in Section 6.2. nuScenes. In <ref type="table" target="#tab_2">Table 2</ref> we compare DD3D with other monocular methods reported on the nuScenes detection bench-    <ref type="bibr" target="#b30">[30]</ref>, which is a Lidar based detector.</p><p>In <ref type="table" target="#tab_6">Table 5</ref> we compare per-class accuracy of DD3D with other methods on the three major categories, with various thresholds on distance. In general, DD3D offers significant improvements across all categories and thresholds. In particular, DD3D performs significantly better on the stricter criteria: comparing to the previous best method <ref type="bibr" target="#b65">[65]</ref>, the  <ref type="table">Table 3</ref>: Ablative analysis of DD3D and PL on the KITTI-3D validation set. As pre-training methods, KITTI denotes dense depth prediction using Eigen-clean split of KITTI-Depth dataset, DDAD15M denotes dense depth prediction using DDAD15M dataset, and COCO denotes initial pre-training phase on 2D detection. The right arrow (?) denotes sequential pre-training phases. We report BEV AP| R40 metrics on Car. The analysis is presented in Section 6.2.</p><p>relative improvements averaged across the 3 object classes are 103.7% and 35.5% for 0.5m and 1.0m thresholds, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Analysis</head><p>Here we provide a detailed analysis of DD3D, focusing on the role of depth pre-training and comparison with our PL approach. After pre-training the two models under various settings, we fine-tune them on KITTI-3D train on the 3D detection task and report AP metrics on KITTI-3D val.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Is depth-pretraining effective?</head><p>Ablating large-scale depth pre-training. We first ablate the effect of dense depth pre-training on the DDAD15M dataset, with results reported in <ref type="table">Table 3</ref>. When we omit the depth pre-training and directly fine-tune DD3D with the DLA-34 backbone on the detection task, we observe a 5.3% loss in Car Moderate BEV AP. In contrast, when we instead remove the initial COCO-pretraining (i.e. pre-training on DDAD15M from-scratch), we observe a relatively small loss, i.e. 2.0%. For the larger backbone of V2-99, the effect of removing the depth pre-training is even more significant, i.e. ?10.7%. Dense depth prediction as pre-training task. To better quantify the effect of depth pre-training, we design a    Starting from a common initial model (COCO), we pretrain DD3D using two different tasks using the same set of images. nusc-det denotes 2D detection task, nusc-depth denotes dense prediction task, both using nuScenes images. We report the accuracy on the KITTI-3D validation set.</p><p>controlled experiment to further isolate its effect <ref type="table" target="#tab_7">(Table 6</ref>).</p><p>Starting from a single set of initial parameters (COCO), we consider two tasks for pre-training DD3D, 2D detection and dense depth prediction. The two pre-training phases use a common set of images that are annotated with both 2D bounding box labels and sparse depth maps obtained by projecting Lidar pointcloud. To further ensure that the comparison is fair, we applied the same number of training steps and batch size (15K and 512). The data used for this pretraining experiment is composed of 136571 images from the nuScenes <ref type="bibr" target="#b4">[5]</ref> training set. The experiment shows that, even with the smaller scale of pre-training data compared to DDAD15M (137K vs. 15M), the dense depth pre-training yields a notable differ-ence in the downstream 3D detection accuracy (21.8% vs. 20.5% BEV AP on Car Moderate). How does depth-pretraning scale? We next investigate how the unsupervised depth pre-training scales with respect to the size of pre-training data <ref type="figure">(Figure 4</ref>). For this experiment, we randomly subsample 1K, 5K, and 10K videos from DDAD15M to generate a total of 4 pre-training splits (including the complete dataset), that consist of 0.6M, 3M, 6M and 15M images respectively. Importantly, the downsampled splits contain fewer images as well as less diversity, as we downsample from the set of videos. We pre-train both DD3D and the PackNet of PL on each split, and subsequently train the detectors on KITTI-3D train. We note that DD3D and PL perform similarly at each checkpoint, and continue to improve as more depth data is used in pretraining, at least up to 15M images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">The limitations of PL methods.</head><p>In-domain depth fine-tuning of PL. Recall that training our PL 3D detector entails fine-tuning of the depth network in the target domain (i.e. Eigen-clean), after it is pre-trained on DDAD15M. We ablate the effect of the in-domain finetuning step in training the PL detector ( <ref type="table">Table 3</ref>). Note that in this experiment, the depth network (PackNet) is trained only on the pre-training domain (DDAD15M) and is directly applied on KITTI-3D without any adaptation. In this setting, we observe a significant loss in performance (30.1% ? 19.1% BEV AP). This indicates that in-domain fine-tuning of the depth network is crucial for PL-style 3D detectors. This poses a practical hurdle in using PL methods, since one has to curate a separate in-domain dataset specifically for fine-tuning the depth network. We argue that this is the main reason that PL methods are exclusively reported on KITTI-3D, which is accompanied by KITTI-Depth as a convenient source for in-domain fine-tuning. This is unnecessary for end-to-end detectors, as shown in <ref type="table" target="#tab_1">Tables 1 and 3</ref>. Limited generalizability of PL. With the large-scale depth pre-training and in-domain fine-tuning, our PL detector offers excellent performance on KITTI-3D val <ref type="table">(Table 3)</ref>. However, the gain from the depth pre-training is not transferred to the benchmark results <ref type="table" target="#tab_1">(Table 1)</ref>. While the loss in accuracy between KITTI-3D val and the test sets is consistent with other methods <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref>, PL suffers particularly more (30.1% ? 18.6% BEV AP), when compared to other methods including DD3D (29.4% ? 22.56%). This reveals a subtle issue in the generalization of PL that is not well understood yet. We argue that the in-domain fine-tuning overfits to some image statistics that cause the performance gap between KITTI-3D val and KITTI-3D test, particularly more than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We proposed DD3D, an end-to-end single-stage 3D object detector that enjoys the benefit of Pseudo Lidar meth-  <ref type="figure">Figure 4</ref>: DD3D and PL are pre-trained with varying sizes of depth data, fine-tuned for 3D detection on the KITTI-3D train, and evaluated on KITTI-3D val. We pretrain the two methods on 4 subsets of DDAD15M with 0.6M, 3M, 6M, and 15M images. ods (i.e. scaling accuracy using large-scale depth data), but without its limitation (i.e. impractical training, issues in generalization). This is enabled by pre-training DD3D using a large-scale depth dataset, and fine-tuning on the target task end-to-end. DD3D achieves excellent accuracy on two challenging 3D detection benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details of training DD3D and PL</head><p>We provide the training details used for supervised monocular depth pre-training of both DD3D and PackNet. DD3D. During pre-training, we use 512 as a batch size, and train for 375K steps until convergence. The learning rate starts at 0.02, decayed by 0.1 at the 305K-th and 365K-th steps. The size of the input images (and projected depth map) is 1600 ? 900, and we resize them to 910 ? 512. When resizing the depth maps, we preserve the sparse depth values by assigning all non-zero depth values to the nearest-neighbor pixel in the resized image space (note that this is different from naive nearest-neighbor interpolation, where the target depth value is assigned zero, if the nearestneighbor pixel in the original image does not have depth value.) We observed that training converges after 30 epochs. We use the Adam optimizer with ? = 0.99. For all supervised depth pre-training splits, we use an L1 loss between predicted depth and projective ground-truth depth.</p><p>When training as 3D detectors, the learning rate starts at 0.002, and is decayed by 0.1, when the training reaches 85% and 95% of the entire duration. We use a batch size of 64, and train for 25K and 120K steps for KITTI-3D and nuScenes, respectively. The ? l and ? l are initialized as the mean and standard deviation of the depth of the 3D boxes that are associated with each FPN level, ? l as the stride size of the associated FPN level, and c is fixed to 1 500 . The raw predictions are filtered by non-maxima suppression (NMS) using IoU criteria on 2D bounding boxes. For the nuScenes benchmark, to address duplicated detections in the overlapping frustums of adjacent cameras, an additional BEVbased NMS is applied across all 6 synchronized images (i.e. a sample) after converting the detected boxes to the global reference frame. PackNet. When training PackNet <ref type="bibr" target="#b16">[16]</ref>, the depth network of PL, we use a batch size of 4 and a learning rate of 5 ? 10 ?5 with input resolution of 640 ? 480. We use only front camera images of DDAD15M to pre-train Pack-Net, and train until convergence, and for 5 epochs over the KITTI Eigen-clean split during fine-tuning. The PL detector is trained with a learning rate of 1 ? 10 ?4 for 100 epochs, decayed by 0.1 after 40 and 80 epochs, respectively. For both networks we use the Adam <ref type="bibr" target="#b40">[40]</ref> optimizer with ? 1 = 0.9 and ? 2 = 0.999. Both DD3D and PL are implemented using Pytorch <ref type="bibr" target="#b47">[47]</ref> and trained on 8 V100 GPUs.</p><p>B. DD3D architecture details <ref type="bibr">FPN [36]</ref> is composed of a bottom-up feed-forward CNN that computes feature maps with a subsampling factor of 2, and a top-down network with lateral connections that recovers high-resolution features from low-resolution ones. The FPNs yield 5 levels of feature maps. DLA-34 <ref type="bibr" target="#b72">[72]</ref> FPN yields three levels of feature maps (with strides of <ref type="bibr">8, 16,</ref>  and 32). We add two lower resolution features (with strides of 64, 128) by applying two 3 ? 3 2D convs with stride of 2 (see <ref type="figure" target="#fig_0">Figure 2</ref>). V2-99 <ref type="bibr" target="#b33">[33]</ref>  2D detection head. We closely follow the decoder architecture and loss formulation of <ref type="bibr" target="#b61">[61]</ref>. In addition, we adopt the positive instance sampling approach introduced in the updated arXiv version <ref type="bibr" target="#b65">[65]</ref>. Specifically, only the centerportion of the ground truth bounding box is used to assign positive samples in L reg and L 3D (Eq. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pseudo-Lidar 3D confidence head</head><p>Our PL 3D detector is based on <ref type="bibr" target="#b41">[41]</ref>, and outputs 3D bounding boxes with 3 heads, separated based on distance (i.e. near, medium and far). Following <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b58">58]</ref> we modify each head to output a 3D confidence, trained through the 3D bounding box loss. Specifically, each 3D box estimation head consists of 3 fully connected layers with dimensions 512 ?? 512 ?? 256 ?? (?, ?), where ? denotes the bounding box parameters as described in <ref type="bibr" target="#b41">[41]</ref>, and ? denotes the 3D bounding box confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The impact of data on Pseudo-Lidar depth and 3D detection accuracy</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>DD3D is a fully convolutional single-stage architecture that performs monocular 3D detection and dense depth prediction. To maximize reuse of pre-trained features, the inference of dense depth and 3D bounding box share most of the parameters. The heads are shared among all FPN layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative visualization of DD3D detections. The first two rows are from the KITTI-3D dataset and the last row is from nuScenes. None of the images were seen during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>We evaluate depth performance (abs rel) against PL 3D detection performance (Car Mod. 3D AP| R40 ) at each pre-training step. All results are computed on the KITTI-3D validation split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>KITTI-3D test set evaluation on Car. We report AP| R40 metrics. Bold and underline denote the best and second best results.</figDesc><table><row><cell>Metric</cell><cell cols="5">AP[%]? ATE[m]? ASE[1-IoU]? AOE[rad]? NDS?</cell></row><row><cell>CenterNet  *</cell><cell>33.8</cell><cell>0.66</cell><cell>0.26</cell><cell>0.63</cell><cell>0.40</cell></row><row><cell>AIML-ADL  *</cell><cell>35.2</cell><cell>0.70</cell><cell>0.26</cell><cell>0.39</cell><cell>0.43</cell></row><row><cell>DHNet  *</cell><cell>36.3</cell><cell>0.67</cell><cell>0.26</cell><cell>0.40</cell><cell>0.44</cell></row><row><cell>PGDepth  *</cell><cell>37.0</cell><cell>0.66</cell><cell>0.25</cell><cell>0.49</cell><cell>0.43</cell></row><row><cell>P.Pillars [30]</cell><cell>31.0</cell><cell>0.52</cell><cell>0.29</cell><cell>0.50</cell><cell>0.45</cell></row><row><cell cols="2">MonoDIS [57] 30.4</cell><cell>0.74</cell><cell>0.26</cell><cell>0.55</cell><cell>0.38</cell></row><row><cell>FCOS3D [65]</cell><cell>35.8</cell><cell>0.69</cell><cell>0.25</cell><cell>0.45</cell><cell>0.43</cell></row><row><cell>DD3D</cell><cell>41.8</cell><cell>0.57</cell><cell>0.25</cell><cell>0.37</cell><cell>0.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>nuScenes detection test set evaluation. We present summary metrics of the benchmark. * denotes results reported on the benchmark that do not have associated publications at the time of writing. The underline denotes the second best published approach. Note that PointPillars<ref type="bibr" target="#b30">[30]</ref> is a Lidar-based detector.mark. The metrics are averages over all 10 categories of the dataset. DD3D outperforms all other methods with a 17% improvement in mAP compared to the previously best published method<ref type="bibr" target="#b65">[65]</ref> (41.8% vs. 35.8% mAP) as well as a 13% improvement compared to the best (unpublished) method. We note that DD3D even surpasses Point-Pillars</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>KITTI-3D test set Pedestrian and Cyclist results.</figDesc><table><row><cell>Methods</cell><cell>0.5m</cell><cell cols="2">Car [%] ? 1.0m 2.0m</cell><cell>4.0m</cell><cell>0.5m</cell><cell cols="2">Pedestrian [%] ? 1.0m 2.0m</cell><cell>4.0m</cell><cell>0.5m</cell><cell cols="2">Bicycle [%] ? 1.0m 2.0m</cell><cell>4.0m</cell></row><row><cell>CenterNet  *</cell><cell>20.0</cell><cell>45.8</cell><cell>68.0</cell><cell>80.6</cell><cell>7.9</cell><cell>26.7</cell><cell>49.6</cell><cell>65.9</cell><cell>4.3</cell><cell>13.8</cell><cell>28.4</cell><cell>36.2</cell></row><row><cell>AIML-ADL  *</cell><cell>14.2</cell><cell>36.8</cell><cell>58.5</cell><cell>71.0</cell><cell>9.6</cell><cell>30.8</cell><cell>54.6</cell><cell>69.4</cell><cell>5.2</cell><cell>21.6</cell><cell>37.3</cell><cell>46.2</cell></row><row><cell>DHNet  *</cell><cell>15.2</cell><cell>37.9</cell><cell>59.4</cell><cell>71.5</cell><cell>10.5</cell><cell>31.7</cell><cell>55.7</cell><cell>69.9</cell><cell>5.6</cell><cell>24.2</cell><cell>38.9</cell><cell>48.0</cell></row><row><cell>PGDepth  *</cell><cell>17.0</cell><cell>43.6</cell><cell>67.2</cell><cell>80.3</cell><cell>9.1</cell><cell>31.0</cell><cell>53.9</cell><cell>69.1</cell><cell>7.6</cell><cell>24.1</cell><cell>40.1</cell><cell>49.2</cell></row><row><cell>MonoDis (single) [59]</cell><cell>10.7</cell><cell>37.5</cell><cell>69.0</cell><cell>85.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MonoDis (multi) [57]</cell><cell>10.6</cell><cell>36.1</cell><cell>65.0</cell><cell>80.5</cell><cell>6.7</cell><cell>30.0</cell><cell>48.5</cell><cell>64.7</cell><cell>4.4</cell><cell>17.5</cell><cell>32.8</cell><cell>43.9</cell></row><row><cell>FCOS3D [65]</cell><cell>15.3</cell><cell>43.8</cell><cell>68.9</cell><cell>81.7</cell><cell>8.7</cell><cell>30.3</cell><cell>52.9</cell><cell>67.1</cell><cell>7.9</cell><cell>25.0</cell><cell>39.2</cell><cell>47.1</cell></row><row><cell>DD3D</cell><cell>30.2</cell><cell>59.7</cell><cell>77.4</cell><cell>84.1</cell><cell>18.7</cell><cell>42.4</cell><cell>61.9</cell><cell>70.2</cell><cell>15.7</cell><cell>32.6</cell><cell>45.2</cell><cell>50.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Detailed results on the nuScenes test set. We report AP metrics on Car, Pedestrian, and Bicycle with varying thresholds on distance. The underline denotes the second best published approach.</figDesc><table><row><cell>Pre-train task</cell><cell>Car</cell><cell>BEV AP Ped.</cell><cell>Cyclist</cell><cell>Car</cell><cell cols="2">3D AP Ped. Cyclist</cell></row><row><cell>COCO</cell><cell>20.2</cell><cell>7.8</cell><cell>3.8</cell><cell>13.9</cell><cell>6.0</cell><cell>3.1</cell></row><row><cell>+ nusc-det</cell><cell>20.5</cell><cell>7.9</cell><cell>3.8</cell><cell>14.0</cell><cell>6.0</cell><cell>3.0</cell></row><row><cell>+ nusc-depth</cell><cell>21.8</cell><cell>8.5</cell><cell>3.8</cell><cell>15.2</cell><cell>6.6</cell><cell>3.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Depth vs. 2D detection as pre-training task.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>by default produces 4 levels of features (strides = 4, 8, 16, and 32), so only one additional conv is used to complete 5 levels feature maps. Note that the final resolution of FPN features derived from DLA-34 and V2-99 network are different, strides=8, 16, 32, 64, 128 for DLA-34, strides= 4, 8, 16, 32, 64 for V2-99.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We evaluate depth quality against the 3D detection accuracy of the PL detector, with results shown in <ref type="figure">Figure 5</ref>.</p><p>Our results indicate an almost perfect linear relationship between depth quality as measured by the abs rel metric and 3D detection accuracy for our PL-based detector.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The earth ain&apos;t flat: Monocular reconstruction of vehicles on steep and graded roads from a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Junaid Ahmed Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshuman</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Krishna</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Madhava Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2018</title>
		<meeting><address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8404" to="8410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Barabanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Artemov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vyacheslav</forename><surname>Murashkin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05618</idno>
		<title level="m">Monocular 3d object detection via geometric reasoning on keypoints</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9287" to="9296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kinematic 3d object detection in monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="135" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep manta: A coarseto-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celine</forename><surname>Teuliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<editor>C</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">3d bounding box estimation for autonomous vehicles by cascaded geometric constraints and depurated 2d detections using 3d results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaojiao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingtao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guizhong</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Brostow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01260v3</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2485" to="2494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantically-guided representation learning for self-supervised monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Rares Ambrus, and Adrien Gaidon</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust semi-supervised monocular depth estimation with reprojected distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mono3d++: Monocular 3d vehicle detection with two-scale 3d hypotheses and task priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-02-01" />
			<biblScope unit="page" from="8409" to="8416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection and box fitting trained endto-end using intersection-over-union loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eskil</forename><surname>J?rgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08070</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1521" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning based vehicle position and orientation estimation via inverse perspective mapping image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsuk</forename><surname>Kum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="317" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Aike</forename><surname>Term?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06936</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instance-level 3d object reconstruction via render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
	<note>Vasileios Belagiannis, Federico Tombari, and Nassir Navab</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Kyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Wook</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Hong</forename><surname>Suh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Single-image depth estimation based on fourier domain analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Han</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyeok</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Rae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="330" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Centermask: Real-time anchor-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep fitting degree scoring network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Smoke: singlestage monocular 3d object detection via keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>T?th</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="996" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking pseudo-lidar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2069" to="2078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multiview reprojection architecture for orientation estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><forename type="middle">Min</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoa</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonsuk</forename><surname>Hyun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7074" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Shift r-cnn: Deep monocular 3d object detection with closed-form geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andretti</forename><surname>Naiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Paunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongmo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Byeong-Moon Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leordeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="61" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Superdepth: Self-supervised, super-resolved monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01849</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Geonet: Geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="283" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">End-to-end pseudo-lidar for image-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5881" to="5890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Triangulation learning network: From monocular to stereo 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08188</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection: From single to multiclass recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Lopez Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Demystifying pseudolidar for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05796</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>L?pez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning 2d to 3d lifting for object detection in 3d for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4504" to="4511" />
			<date type="published" when="2019-11-03" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Refinedmpl: Refined monocular pseudolidar for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean Marie Uwabeza</forename><surname>Vianney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhra</forename><surname>Aich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09712</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07804</idno>
		<title level="m">Sfmnet: Learning of structure and motion from video</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Fcos3d: Fully convolutional one-stage monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10956</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Task-aware monocular depth estimation for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Pseudolidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Train in germany, test in the usa: Making 3d object detectors generalize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11713" to="11723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection with pseudo-lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Pixor: Realtime 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06310</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Stereo magnification: Learning view synthesis using multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09817</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
