<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entropy-driven Sampling and Training Scheme for Conditional Diffusion Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengming</forename><surname>Li</surname></persName>
							<email>shengming22@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">1?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Zheng</surname></persName>
							<email>guangcongzheng@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
							<email>wanghui17@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiping</forename><surname>Yao</surname></persName>
							<email>taipingyao@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Youtu Lab, Tencent</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
							<email>wizyangchen@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Youtu Lab, Tencent</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouhong</forename><surname>Ding</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Youtu Lab, Tencent</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Institute for Advanced Study</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Entropy-driven Sampling and Training Scheme for Conditional Diffusion Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Denoising Diffusion probabilistic Model</term>
					<term>Conditional Gener- ation</term>
					<term>Distribution Entropy</term>
					<term>Gradient Vanishing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Denoising Diffusion Probabilistic Model (DDPM) is able to make flexible conditional image generation from prior noise to real data, by introducing an independent noise-aware classifier to provide conditional gradient guidance at each time step of denoising process. However, due to the ability of classifier to easily discriminate an incompletely generated image only with high-level structure, the gradient, which is a kind of class information guidance, tends to vanish early, leading to the collapse from conditional generation process into the unconditional process. To address this problem, we propose two simple but effective approaches from two perspectives. For sampling procedure, we introduce the entropy of predicted distribution as the measure of guidance vanishing level and propose an entropy-aware scaling method to adaptively recover the conditional semantic guidance. For training stage, we propose the entropyaware optimization objectives to alleviate the overconfident prediction for noisy data. On ImageNet1000 256?256, with our proposed sampling scheme and trained classifier, the pretrained conditional and unconditional DDPM model can achieve 10.89% (4.59 to 4.09) and 43.5% (12 to 6.78) FID improvement respectively. Code is available at https://github.com/ ZGCTroy/ED-DPM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional denoising process</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Previous method</head><p>Proposed method <ref type="figure" target="#fig_5">Fig. 1</ref>: The visualization of denoising sampling process. The classifier gradient, a kind of class information in conditional generation, quickly converge to 0 in the previous method. It will lead to the collapse from conditional generation to unconditional generation, while our method recovers the gradient guidance and succeed to generate fine-grained features in the subsequent iterations. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref>. However, when the condition requirement is changed, the generative models will be retrained, which is very inconvenient.</p><p>Denoising Diffusion Probabilistic Model (DDPM) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33]</ref> is a class of iterative generation models, which has made remarkable performance in unconditional image generation recently. The flexibility of DDPM <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref> is that it can be easily extended to conditional variants by introducing an independent noise-aware classifier. Recent researches modeled the prior denoising distribution by training an unconditional DDPM, following the training scheme of Denoising Score Matching <ref type="bibr" target="#b38">[39]</ref>, and computed likelihood score by backwarding the classifier gradient. Dhariwal et al. <ref type="bibr" target="#b5">[6]</ref> further proposed fixed scaling factor to improve the predicted probability of generated samples for DDPM, achieving superior performance than GAN on several image generation benchmarks.</p><p>In conditional generation process of DDPM, by backwarding the gradient of classification probability to image, the classifier provides high-level semantic information in the early stage of iterations, and gradually strengthens finegrained features in the subsequent iterations, both of which are indispensable. However, there exists a huge gap between discriminating the class of a image and generating a specific class of image with fine-grained textures. As shown in <ref type="figure" target="#fig_5">Fig.1</ref>, the predicted distribution of the classifier for noisy images tends to quickly converge to the desired class distribution, which is one-hot distribution, leading to the early vanishing of conditional gradient guidance. This is because that the incompletely generated image, which is still a noisy image and lacks fine-grained features, can be easily classified in the middle of denoising process. In this way, the image is considered to have been completely generated, and will no longer be guided by classifier gradient containing class information. As a result, the conditional generation process will degrade into an unconditional generation process in the later stage.</p><p>Therefore, our motivation is to enable the classifier to continuously give conditional guidance throughout the entire denoising process. We propose two simple but effective schemes from the procedure of sampling and the design of classifier training.</p><p>From the perspective of sampling procedure, we focus on how to detect the gradient vanishing and rescale the gradient to avoid the existence of gradient vanishing or recover the gradient when the vanishing does happen. We propose Entropy-Driven conditional Sampling (EDS) method, which is able to adaptively measure the level of gradient vanishing and rescale the gradient guidance to a appropriate level. In design of training classifier, we propose Entropy-Constraint Training (ECT), which will penalize the classifier when it gives a overconfident classification probability to a generated noisy image, thus constraining the classifier to provide more gentle guidance.</p><p>Our contributions can be summarized as follows:</p><p>? We are the first to discover the problem of vanishing gradient guidance for ddpm-based conditional generation methods, and point out that category information guidance should be continuously provided throughout the entire generation process.</p><p>? We propose EDS to alleviate the vanishing guidance by dynamically measuring and rescaling gradient guidance. At the training stage of classifier, for alleviating the vanishing gradient caused by one-hot label supervision, we utilize discrete uniform distribution to build an entropy-aware optimization term, which is Entropy-Constraint Training scheme (ECT).</p><p>? We conduct experiments on ImageNet1000 and achieve state-of-the-art FID (Fr?chet Inception Distance) results at various resolutions. On ImageNet1000 256?256, with our proposed sampling scheme and trained classifier, the pretrained conditional and unconditional DDPM model can achieve 10.89% (4.59 to 4.09) and 43.5% (12 to 6.78) FID (Fr?chet Inception Distance) improvement respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Denoising Diffusion Probabilistic Model</head><p>Denoising diffusion probabilistic models (DDPM) is the latest generation model which achieve superior generation performance than traditional generative models like Generative Adversial Networks (GAN) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2]</ref>, Variational Autoencoders (VAE) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref> on several benchmarks about unconditional generation. Its key idea is to model the diffusion process based on total T time steps, which adds noise gradually to the clean data, and its reverse process, which denoises the white noise into the clean sample. Accordingly, its diffusion process is modeled as a fixed Markov Chain and its transition kernel is formulated as:</p><formula xml:id="formula_0">q(x t |x t?1 ) := N (x t ; 1 ? ? t x t?1 , ? t I),<label>(1)</label></formula><p>where ? 1 , ..., ? T are the fixed variance parameters, which are not learnable. According to the transition kernel above, when the clean data x 0 is given, the noisy data x t can be sampled with a close-formed distribution:</p><formula xml:id="formula_1">q(x t |x 0 ) = N (x t ; ? ? t x 0 , (1 ? ? t )I),<label>(2)</label></formula><p>where ? t = 1?? t and? t = t s=1 ? s . When t is close to T , x T can be approximated as a Gaussian distribution.</p><p>Given a prior diffusion process, DDPM aims to model its reverse process to sample from the data distribution. The optimization objective of the reverse transition can be derived from a variational bound <ref type="bibr" target="#b12">[13]</ref>. Thus, DDPM introduced the variational solution <ref type="bibr" target="#b12">[13]</ref> and assumed that its reverse transition kernel also subjects to Gaussian distribution, which is the same as the diffusion process. In this way, the generation process parameterized the mean of the Gaussian transition distribution and fixed its variance as follow:</p><formula xml:id="formula_2">p ? (x t?1 |x t ) = N (x t?1 ; ? ? (x t ), ? 2 t I) ? ? (x t ) = 1 ? ? t (x t ? 1 ? ? t ? 1 ?? t ? ? (x t )),<label>(3)</label></formula><p>where ? ? (x t ) is a noise estimator modeled by a neural network. The variance is designed as the hyperparameters for training diffusion models.</p><p>Recently, there were some researches <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b5">6]</ref> which indicated that the noise estimator can be regarded as an approximation of score function so that the sampling process is equivalent to solving a stochastic differential equation. Based on above, Song.et <ref type="bibr" target="#b33">[34]</ref> proposed an effective sampling process that shares the same training objectives as DDPM and its corresponding denoising process, which is also the iteration solution to solve the stochastic differential equation, is designed as followed:</p><formula xml:id="formula_3">x t?1 = ? ? t?1 f ? (x t , t) + 1 ? ? t?1 ? ? 2 t ? ? (x t ) + ? 2 t z,<label>(4)</label></formula><p>where f ? (x t , t) is the prediction of clean data x 0 when the noisy data x t is observed and noise prediction ? ? (x t ) is given. The specific form of f ? (x t , t) can be expressed as:</p><formula xml:id="formula_4">f ? (x t , t) = x t ? ? 1 ? ? t ? ? (x t ) ? ? t ,<label>(5)</label></formula><p>When the variance ? t is set to 0, the sampling process becomes deterministic. At the same time, the non-Markovian diffusion process <ref type="bibr" target="#b33">[34]</ref> allows that the generation quality remains unchanged within fewer denoising steps, which is called DDIM sampling process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Conditional Image Generation</head><p>Conditional image generation aims to generate samples with desired condition information. The condition can be extended to multi-modal information, such as class <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42]</ref>, text <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40]</ref>, and low-resolution image <ref type="bibr" target="#b2">[3]</ref>. Most previous work modeled this by the joint training scheme with both condition and random noise, utilizing generative models like GAN or VAE.</p><p>Considering that Denoising Diffusion Probabilistic Model (DDPM) has made remarkable progress in recent years <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref>, there raised many researches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref> which applied DDPM to conditional generation. Due to the ideal theoretical properties of DDPM, it can be extended flexibly to conditional variants utilizing Bayes theorem without retraining, which is similar with score-based generative models <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b18">19]</ref>. In this paper, we focus on the class-conditional generation task, in which the condition is represented by a class discrete distribution, and design a more effective sampling and training scheme to improve the generation quality for DDPM, further exploring its potential in image generation aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Methods</head><p>We start by introducing the conditional generation process for diffusion models with classifier guidance (Sect.(3.1)). For ease of description, we will firstly introduce our dynamic scaling technology to recover gradient guidance adaptively in the sampling process and its motivation (Sect.(3.2)). Then, we describe the entropy-aware optimization loss for alleviating vanishing conditional guidance from training perspective (Sect.(3.3)), utilizing the uniform distribution, which is a more dense distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conditional Diffusion Generation</head><p>The goal of conditional image generation is to model probability density p(x|y). To be specific, in class-conditional image generation, y represents the desired class label, which tends to be one-hot distribution, and x represents the image sample. For diffusion models, it can be implemented by introducing an independent classifier, as shown in <ref type="figure" target="#fig_1">Fig.2</ref>.</p><p>Specifically, the goal is converted into modeling the conditional transition distribution p(x t?1 |x t , y), derived from Markov chain sampling scheme:</p><formula xml:id="formula_5">p ? (x 0 |y) = p ? (x 0:T |y)dx 1:T , p ? (x 0:T |y) = p(x T ) T t=1 p ? (x t?1 |x t , y),<label>(6)</label></formula><p>where ? represents the model and T is the total length of Markov chain, which tends to be large. Then, we decompose the conditional transition distribution into two independent terms:</p><formula xml:id="formula_6">p ? (x t?1 |x t , y) = Zp ? (x t?1 |x t )p ? (y|x t ),<label>(7)</label></formula><p>where Z is a normalizing constant independent from x t?1 and ? can be seen as the combination of models ? and ?, which is proven theoretically <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b5">6]</ref>. Furthermore, the log density of Eq. <ref type="formula" target="#formula_6">(7)</ref> can be approximated as a Gaussian distribution <ref type="bibr" target="#b5">[6]</ref>:</p><formula xml:id="formula_7">log(p ? (x t?1 |x t , y)) ? log p(z) + log Z z ? N ( 1 ? ? t (x t ? 1 ? ? t ? 1 ?? t ? ? (x t )) + ? 2 t g, ? 2 t I),<label>(8)</label></formula><p>where g = s? xt log p ? (y|x t ) and s is the gradient scale. g can be derived by backwarding the gradient of a pretrained classifier about noisy data x t . Usually, s is set to a constant <ref type="bibr" target="#b5">[6]</ref> to improve the predicted probability. ? ? (x t ) is the pretrained noise estimator for diffusion models and ? 2 t is the hyperparameters for unconditional DDPM to control the variance.</p><p>In this way, unconditional diffusion models with parameterized noise estimator ? ? can be extended to conditional generative models by introducing the conditionaware guidance ? xt log p ? (y|x t ), as shown in <ref type="figure" target="#fig_1">Fig.2</ref>. Specifically, we start with the prior noise x T ? N (0, I), and utilize DDPM-based (Eq.(8)) sampler to make transition iteratively to generate the samples conditioned on desired class y. The sampler can also be extended into DDIM iteration method (Eq.(3)). The extension details for conditional process with classifier guidance can refer to Dhariwal et al. <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b5">6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entropy-driven Conditional Sampling</head><p>During the conditional generation process, we observe that the guidance provided from noise-aware classifier tends to vanish prematurely, which could be attributed to the discrepancy between generative pattern and discriminative pattern. For example, the noisy samples with high-level semantic information, such as contour or the color, may be guided iteratively by the classifier with nearly one-hot distribution, in which situation the gradient guidance tends to be weak or vanish, while the samples still lack the condition-aware semantic details. In this way, the condition-aware textures of generated samples are guided by unconditional denoising process (Eq.(3)).   An intuitive solution is to manually select a time step during the sampling process, after which the semantic details tend to vanish in most instances, and rescale the conditional gradient by an empirical constant after the selected time step. However, since the stochasticity in the generation process of diffusion models <ref type="bibr" target="#b3">[4]</ref>, the denoising trajectories for generated samples would differ from each other. It will lead to the various initial vanishing points, as shown in <ref type="figure" target="#fig_2">Fig.3</ref>. At the same time, considering the learning bias of classifier for different conditional classes, the level of recovery factor for each class may also differ. In summary, the effective scaling factor can be related to the current time step, the class condition, and the stochasticity of generation process. The experiment design and results about more intuitive approaches can be seen in Sect.4.3.</p><p>Motivated from above, we propose an dynamic scaling technology for the conditional diffusion generation to recover the semantic details adaptively for each sample. We noticed that, when time step is close to T , the predicted distribution tends to be dense, which can be nearly approximated as uniform distribution. The reason is that the noisy data derived from Eq.(1) can be approximated as random noise, N (0, I), in which state the gradient guidance is obvious. As time step declines to 0 inversely, the noise hidden in sample will be removed Algorithm 1 Entropy-driven sampling scheme (DDPM/DDIM) Require: a pretrained diffusion model ? ? (xt), classifier p ? (?|xt), and desired class condition</p><formula xml:id="formula_8">y 1: xT ? N (0, I) 2: for t = T, . . . , 1 do 3: s ? ? * H(U (?)) H(p ? (?|x t )) if EDS, else s ? ? 4: if use DDPM then 5: z ? N (0, I) if t &gt; 1, else z ? 0 6: g ? s ? ?x t log p ? (y|xt), 7: xt?1 ? 1 ? ? t xt ? 1?? t ? 1?? t ? ? (xt) + ? 2 t g + ?tz 8: else if use DDIM then 9:? ? ? ? (xt) ? s ? ?x t log p ? (y|xt), 10: xt?1 ? ?? t?1( x t ? ? 1?? t? ?? t ) + ? 1 ??t?1? 11:</formula><p>end if 12: end for 13: return x0 gradually and the predicted distribution tends to be close to one-hot, in which case the classifier gradient is invalid to provide semantic details for generation. Statistically, entropy can represent the sparsity of the predicted distribution, inspiring us to take it into consideration:</p><formula xml:id="formula_9">H(p ? (?|x t )) = ?E? |xt log p ? (?|x t ) = ? |Y | i=1 p ? (? i |x t ) log p ? (? i |x t ),<label>(9)</label></formula><p>where p ? (?|x t ) represents the predicted distribution from classifier and Y represents the set of all class conditions. In this paper, we utilize H(p ? (?|x t )) to adaptively fit various gradient vanishing time step. Furthermore, the entropy H(p ? (?|x t )) can also capture bias caused by different conditions, due to its sample-aware rescaling effect. Thus, when we sample from a pretrained noise estimator ? ? in Eq.((8)) conditionally, we reformulate the gradient term g as following, which is shown in <ref type="figure" target="#fig_1">Fig.2</ref>:</p><formula xml:id="formula_10">g ? = s(x t , ?) * ? xt log p ? (y|x t ), s(x t , ?) = ? * H(U(?)) H(p ? (?|x t ))<label>(10)</label></formula><p>where ? is a hyper-parameter to balance the guiding gradient and entropy-aware scaling factor s(x t , ?). In order to maintain the numerical range, we renormalize the entropy by its theoretical upper bound H(U(?)), where U(?) represents the uniform distribution of class variable, so that the gradients are almost not rescaled when t is close to T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Entropy-constraint training scheme.</head><p>Require: training set D, a neural classifier ?, training set D, total time steps T 1:</p><formula xml:id="formula_11">repeat 2: (x0, y) ? sample from D 3: t ? U({1, . . . , T }) 4: xt ? q(xt|x0) (Eq.2) 5: LCE ? y log p ? (?|xt) 6:</formula><p>if use ECT then 7:</p><p>LECT ? ?H(p ? (?|xt)) 8:</p><p>Take gradient descent step on ? ? (LCE + ?LECT ) 9: else 10:</p><p>Take gradient descent step on ? ? LCE 11:</p><p>end if 12: until converged</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Noise-aware Classifier with Entropy Constraint</head><p>From the training perspective, the vanishing gradient can be partly attributed to the label supervision pattern for noise-aware classifier. Since one-hot distribution is very sparse and is utilized to supervise the noisy data, the predicted distributions are inclined to converge to one-hot under noisy samples in the sampling process, so that the gradient guidances are too weak to generate condition-aware semantic details at sampling stage.</p><p>Specifically, given the dataset (x 0 , y) ? D and a prior diffusion process (Eq.(1)), the classifier will be trained under noisy data x t to build the gradient field in Eq.(8) of each time step. To alleviate the weak guidance caused by the sparsity, we utilize discrete uniform distribution, which is a dense distribution and has maximum entropy, as a perturbing distribution and introduce the optimization term at the training stage of the classifier to constrain the predicted distribution from the classifier as followed:</p><formula xml:id="formula_12">L ECT (x t , y) = D KL (p ? (?|x t )||U(?)) = E? |xt log p ? (?|x t ) ? E? |xt log U(?) = ?H(p ? (?|x t )) + C,<label>(11)</label></formula><p>where C is a constant term independent from the parameter ?. This loss term is equivalent to maximizing entropy of the predicted distribution p(?|x t ). The whole training loss of guiding classifier is composed of the normal cross-entropy loss and entropy constraint training loss (ECT), which is formally given by:</p><formula xml:id="formula_13">L tot (x t , y) = L CE (x t , y) + ?L ECT (x t , y),<label>(12)</label></formula><p>where ? is a hyper-parameter to adjust the divergence about predicted label distribution and the uniform distribution. Different from Entropy-driven Sampling, the proposed training scheme tries to alleviate the vanishing guidance by adjusting the gradient direction in sampling process, instead of the gradient scale. Thus, entropy-constrain training scheme can be complementary with entropy-driven sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present experiments to verify the effectiveness and motivation of our proposed schemes. More visualization of generated samples and ablation experiments about hyperparameters can be found in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Dataset We perform our experiments mainly on ImageNet dataset <ref type="bibr" target="#b27">[28]</ref> at 256?256 resolutions. ImageNet contains 14,197,122 images with 1000 classes in total, which is a very challenging benchmark for conditional image generation.</p><p>Implementation Details. For verifying the effect of proposed schemes, we apply the neural network architecture, ablated diffusion model (ADM), proposed by Dhariwal.et al <ref type="bibr" target="#b5">[6]</ref>. ADM is mainly based on the UNet, with increased depth versus width, the number of attention heads, and rescaling residual connections with 1 ? 2 . It is also possible to train a conditional diffusion models. We call the conditional diffusion architecture <ref type="bibr" target="#b5">[6]</ref> as CADM for short. Correspondingly, UADM means the unconditional diffusion architecture. UADM-G and CADM-G additionally use noise-aware classifier guidance to perform conditional generation, separately.</p><p>Our training hyperparameters of noise-aware classifier, including batch size, total number of iterations, and decay rate, are kept the same with Dhariwal et al. <ref type="bibr" target="#b5">[6]</ref> for fair comparison. We adopt the fixed linear variance schedule ? 1 , ..., ? T [23,10] for prior noising process Eq.(1) and choose T as 1000. In this paper, we set ? to 0.2 to keep a slight disturbance during training stage of classifier in all experiments. The selection details of ? can be seen in supplementary materials.</p><p>All the experiments are conducted on 16 NVIDIA 3090s.</p><p>Evaluation Metrics. We select FID (Fr?chet Inception Distance) <ref type="bibr" target="#b8">[9]</ref> as our default evaluation metric, which is the most widely used metric for generation evaluation. FID <ref type="bibr" target="#b8">[9]</ref> measures KL divergence of two Gaussian distributions, which is computed by the real reference samples and the generated samples, in the feature space of the Inception-V3. To capture more spatial relationships, sFID are prposed as a variant of FID, which is more sensitive to the consistent image distribution with high-level structures.</p><p>Besides, we apply several other metrics for more comprehensive evaluations. Inception Score (IS) was proposed <ref type="bibr" target="#b0">[1]</ref>, to measure the mutual information between input sample and the predicted class. Improved Precision and Recall metrics are proposed <ref type="bibr" target="#b14">[15]</ref> for further evaluation of generative models. Precision is computed by estimating the proportion of generated samples that fall into real data manifold, measuring the sample fidelity. By contraries, recall is computed by estimating the proportion of real samples which fall into generated data manifold, measuring the sample diversity. Following Dhariwal.et al <ref type="bibr" target="#b5">[6]</ref>, we randomly generated 50k images to compute all metrics above based on 10k real images when compared with previous methods. For consistent comparisons, we use the evaluation metrics for all methods based on the same codebase as Dhariwal et al. <ref type="bibr" target="#b5">[6]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-art Methods</head><p>In this section, we show the comparison results of our proposed schemes with other SOTA methods. based on UADM and CADM architectures <ref type="bibr" target="#b5">[6]</ref>. All results of other previous methods are cited from Dhariwal et al. <ref type="bibr" target="#b5">[6]</ref>. As shown in <ref type="table">Table.</ref>1, we achieve the best results in term of FID metric. Compared with UADM-G in ImageNet 256?256, our methods achieve relatively about 40% improvement on FID metric (from 14.21 to 8.28, from 12.0 to 6.78) based on both DDPM and DDIM sampling iteration methods, with comparable or even better precision and recall. When based on the CADM architecture, our proposed schemes still maintain a significant improvement margin on FID metric (from 5.44 to 4.67, from 4.59 to 4.09), and comparable precision and recall. It is worth mentioning that our method based on UADM architecture outperformed BigGAN-deep in term of FID by 0.17 margin (6.78 vs 6.95), with no dependency on conditional architecture CADM, which has not been achieved in previous work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23]</ref>. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Effect of proposed schemes. To further verify the contribution of each component of proposed schemes, we conduct ablation experiments on both UADM and CADM architectures, as shown in <ref type="table" target="#tab_1">Table.2 and Table.</ref>3. It can be concluded that EDS and ECT both improves the generation quality. Combining with two  schemes, the generation results can be further improved, with more semantic details achieved from improved direction (ECT) and scale (EDS) aspects of guidance gradient.</p><p>Effect of entropy-driven sampling. In this part, we design various intuitive scaling methods and compare them under optimal hyperparameters with EDS. We select the scaling method with constant recovery factor for all time range <ref type="bibr" target="#b5">[6]</ref> as our baseline. Intuitively, we can manually select vanishing time point through observing <ref type="figure" target="#fig_2">Fig.3 and Fig.1</ref> i.e., 700, and finetune the constant rescaling factor to adjust the weak gradient for all generated samples, which we called Constant (range 0-700). The constant scale can be further adjusted to time-aware form, such like T ? t. We call this method as Timestep-aware, which can dynamically adjust the scaling factor according to time step in sampling process. In this way, the sample-aware vanishing characteristic is ignored and the scale cannot fit the various vanishing level.  To verify that entropy-aware scaling could better fit the initial time steps of vanishing guidance, we design another approach which is based on norm of gradient map. Specifically, we empirically select a norm bound M for gradient. When the norm of gradient map is smaller than the threshold vanishing norm bound M , we regard that the gradient guidance is weak and need to be rescaled. Thus, s in Eq.10 is rewritten as followed:</p><formula xml:id="formula_14">s = 1, ?? xt log p ? (y|x t )? 2 &lt; M C, Otherwise<label>(13)</label></formula><p>The rescaling factor C is a large constant. Compared with methods above, it can be verified experimentally that EDS not only fits the sample-aware initial time steps of weak guidance, but also provides reasonable rescaling factor for recovery, as shown in <ref type="table">Table.</ref>4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Results</head><p>Gradient Map Visualization. In this part, we collect several gradient maps g derived from classifier in the previous sampling process and our EDS process, with an equal time interval. From <ref type="figure" target="#fig_3">Fig.4</ref>, it can be observed that the classifier provides high-level semantic guidance at the beginning. Gradually, the classifier will provide the condition-aware texture guidance. Compared with EDS, which can maintain guidance of semantic details throughout the denoising process for refined generation results, sampling scheme based on the fixed scaling factor lost a lot of condition-aware details (first line) or introduce unnatural details (third line) at the later sampling stage.</p><p>Generation Results Visualization. In this part, we visualize various generated images conditioned on different classes. Based on UADM architecture, we compared our final generated results with that of previous SOTA method <ref type="bibr" target="#b5">[6]</ref>.</p><p>Considering the stochastic noise involved in ddpm sampling process, we adopted DDIM, the deterministic generation process, to generate samples with shared initial noise. From <ref type="figure" target="#fig_4">Fig.(5)</ref>, previous SOTA method can not generate condition-aware semantic details such like beaks of birds <ref type="figure" target="#fig_4">Fig.(5b)</ref> or petal texture <ref type="figure" target="#fig_4">Fig.(5i)</ref>, while our method is able to generate more refined textures, with similar high-level structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed an entropy-aware scaling technology for the guiding classifier in sampling process. From the perspective of training, we propose an entropy-aware optimization loss to constrain the predicted distribution, alleviating the overconfident prediction for noisy sample in the sampling process. Experiments demonstrate that our proposed methods can recover more textures in generated samples, achieving state-of-the-art generation results. In this section, we show the curve about the selection of hyperparameter ?. For efficient evaluation, we collect 5000 generated images and calculate FID metric based on 10000 real images from ImageNet1000.     <ref type="figure" target="#fig_3">Fig. 4</ref>: The curve plot of optimal hyperparameter C based on 5k images for Gradient Norm method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Effectiveness of intuitive sampling schemes.</head><p>An intuitive solution for sampling scheme is to manually select vanishing time point, i.e., 700 and finetune the constant rescaling factor to adjust the weak gradient for all generated samples, which we called Constant (range 0-700):</p><formula xml:id="formula_15">g = ? xt log p ? (y|x t ), t &gt; 700 C * ? xt log p ? (y|x t ), Otherwise<label>(1)</label></formula><p>Another intuitive scaling design mentioned in main paper is to rescale the gradient guidance according to current time step t. When t is close to T , the scaling effect should be insignificant. Thus, time-aware gradient guidance can be formulated as following: g = C * (T ? t) * ? xt log p ? (y|x t ),</p><p>where C is the constant hyperparameter to balance the scaling and gradient effects. We call this method as Timestep-aware, which can dynamically adjust the scaling factor according to time step in sampling process. The ablations about the hyperparameter C for two methods above are shown in <ref type="figure" target="#fig_1">Fig.2</ref>. We design another approach which is based on norm of gradient map to adaptively adjust the gradient scale. Specifically, we empirically select a norm bound M , i.e., 0.2 for gradient. When the norm of gradient map is smaller than the threshold vanishing norm bound M , we regard that the gradient guidance is weak and need to be rescaled. Thus, the gradient term is rewritten as followed:</p><formula xml:id="formula_17">g = ? xt log p ? (y|x t ), ?? xt log p ? (y|x t )? 2 &lt; 0.2 C * ? xt log p ? (y|x t ), Otherwise<label>(3)</label></formula><p>where C is the constant hyperparameter to balance the scaling and gradient effects. The ablation about C in this method can be seen in <ref type="figure" target="#fig_3">Fig.4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Comparisons with SOTA results</head><p>In this section, we show the experiment results on ImageNet1000 at 64?64, 128?128 resolutions, to further verify the effectiveness of our proposed methods. From <ref type="table">Table.</ref>1, it can be concluded that our proposed methods can adapt to low resolution image generation. All results based on previous methods in this table are cited from Dhariwal et al. <ref type="bibr">[?]</ref>, except for CADM-G <ref type="bibr" target="#b24">(25)</ref> in ImageNet 128?128, of which the evaluation metric is achieved by our reproducing result.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Pipeline for Entropy-driven Sampling process. Sampler represents a class of iteration method (DDPM or DDIM), which is non-parametric. All models are pretrained without gradient updating in the sampling process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The various gradient vanishing points for different samples. We randomly generate 5000 samples and define the time point, in which the gradient norm is smaller than 0.15, as the gradient vanishing point. Premature vanishing of gradients occurs in almost every sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Gradient map comparison of UADM-G<ref type="bibr" target="#b5">[6]</ref> (the first line of each image box) and our UADM-G + EDS (the second line of each image box) on ImageNet1000 256?256 using DDIM 25 iterations. For better visualization, we pick 8 maps at the same interval from 25 consecutive maps and magnify all pixels of a image by a factor of 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Sample quality comparison of UADM-G [6] (the first line of each image box) and our UADM-G + ECT + EDS (the second line of each image box) on ImageNet1000 256?256 using DDIM 25 iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 1 :</head><label>1</label><figDesc>The curve plot of optimal EDS hyperparameter ? based on 5k images evaluated with FID. arXiv:2206.11474v5 [cs.CV] 20 Sep 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 2 :</head><label>2</label><figDesc>The curve plot of optimal EDS hyperparameter C based on 5000 images evaluated with FID for Constant and Time-aware methods separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 :</head><label>3</label><figDesc>Ablation of Gradient Norm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 :</head><label>5</label><figDesc>Generated images in ImageNet 256?256 from CADM with EDS and ECT (DDPM 250 steps).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 :</head><label>6</label><figDesc>Generated images in ImageNet 256?256 from CADM-G with EDS and ECT (DDIM 25 steps).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 :</head><label>7</label><figDesc>Generated images in ImageNet 128?128 from CADM-G with EDS and ECT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 :</head><label>8</label><figDesc>Generated images in ImageNet 64?64 from CADM-G with EDS and ECT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison results with state-of-the-art generative models on Ima-geNet1000 256?256. Annotation '<ref type="bibr" target="#b24">(25)</ref>' means the DDIM<ref type="bibr" target="#b33">[34]</ref> sampling method with 25 steps. Otherwise, it means DDPM sampling method with 250 steps.</figDesc><table><row><cell>Method DCTransformer [22]</cell><cell>FID ? 36.51</cell><cell>sFID ? 8.24</cell><cell>IS ? -</cell><cell>Prec ? 0.36</cell><cell>Rec ? 0.67</cell></row><row><cell>VQ-VAE-2 [27]</cell><cell>31.11</cell><cell>17.38</cell><cell>-</cell><cell>0.36</cell><cell>0.57</cell></row><row><cell>IDDPM [23]</cell><cell>12.26</cell><cell>5.42</cell><cell>-</cell><cell>0.70</cell><cell>0.62</cell></row><row><cell>SR3 [29]</cell><cell>11.30</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BigGAN-deep [2]</cell><cell>6.95</cell><cell>7.36</cell><cell>-</cell><cell>0.87</cell><cell>0.28</cell></row><row><cell>UADM-G [6](25)</cell><cell>14.21</cell><cell>8.53</cell><cell>83</cell><cell>0.7</cell><cell>0.46</cell></row><row><cell>UADM-G+ECT+EDS (25)</cell><cell>8.28</cell><cell>6.37</cell><cell>163.17</cell><cell>0.76</cell><cell>0.44</cell></row><row><cell>UADM-G [6]</cell><cell>12</cell><cell>10.4</cell><cell>95.41</cell><cell>0.76</cell><cell>0.44</cell></row><row><cell>UADM-G+ECT+EDS</cell><cell>6.78</cell><cell>6.56</cell><cell>168.78</cell><cell>0.81</cell><cell>0.45</cell></row><row><cell>CADM [6]</cell><cell>10.94</cell><cell>6.02</cell><cell>100.98</cell><cell>0.69</cell><cell>0.63</cell></row><row><cell>CADM-G (25) [6]</cell><cell>5.44</cell><cell>5.32</cell><cell>194.48</cell><cell>0.81</cell><cell>0.49</cell></row><row><cell>CADM-G+ECT+EDS (25)</cell><cell>4.67</cell><cell>5.12</cell><cell>235.24</cell><cell>0.82</cell><cell>0.47</cell></row><row><cell>CADM-G [6]</cell><cell>4.59</cell><cell>5.25</cell><cell>186.70</cell><cell>0.82</cell><cell>0.52</cell></row><row><cell>CADM-G+ECT+EDS</cell><cell>4.09</cell><cell>5.07</cell><cell>221.57</cell><cell>0.83</cell><cell>0.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Effect of our proposed methods based on UADM-G under DDIM 25 steps and DDPM 250 steps on ImageNet1000 256?256.</figDesc><table><row><cell cols="5">DDIM ECT EDS FID ? sFID ?</cell><cell>IS ?</cell><cell cols="2">Precision ? Recall ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>12.0</cell><cell>10.4</cell><cell>95.41</cell><cell>0.76</cell><cell>0.44</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell>10.79</cell><cell>10.49</cell><cell>117.56</cell><cell>0.79</cell><cell>0.41</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>7.98</cell><cell>10.61</cell><cell>178.73</cell><cell>0.82</cell><cell>0.40</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>6.78</cell><cell>6.56</cell><cell>168.78</cell><cell>0.81</cell><cell>0.45</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>14.21</cell><cell>8.53</cell><cell>83</cell><cell>0.7</cell><cell>0.46</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell>12.21</cell><cell>8.14</cell><cell>100.95</cell><cell>0.74</cell><cell>0.44</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell>10.09</cell><cell>6.86</cell><cell>133.71</cell><cell>0.73</cell><cell>0.45</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>8.28</cell><cell>6.37</cell><cell>163.17</cell><cell>0.76</cell><cell>0.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Effect of our proposed methods based on CADM-G under DDIM 25 steps and DDPM 250 steps on ImageNet1000 256?256.</figDesc><table><row><cell cols="5">DDIM ECT EDS FID ? sFID ?</cell><cell>IS ?</cell><cell cols="2">Precision ? Recall ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>4.59</cell><cell>5.25</cell><cell>186.7</cell><cell>0.82</cell><cell>0.52</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell>4.62</cell><cell>5.16</cell><cell>182.48</cell><cell>0.81</cell><cell>0.53</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>4.01</cell><cell>5.15</cell><cell>217.25</cell><cell>0.82</cell><cell>0.52</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>4.09</cell><cell>5.07</cell><cell>221.57</cell><cell>0.83</cell><cell>0.50</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>5.46</cell><cell>5.32</cell><cell>194.48</cell><cell>0.81</cell><cell>0.48</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell>5.34</cell><cell>5.3</cell><cell>196.8</cell><cell>0.81</cell><cell>0.49</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell>4.82</cell><cell>5.04</cell><cell>218.97</cell><cell>0.80</cell><cell>0.50</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>4.67</cell><cell>5.12</cell><cell>235.24</cell><cell>0.82</cell><cell>0.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Sample quality comparison with several intuitive sampling schemes. All results are evaluated under 5,000 generated samples for more efficient comparisons.</figDesc><table><row><cell>Method</cell><cell>FID</cell><cell>IS</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>Baseline</cell><cell>20.17</cell><cell>84.53</cell><cell>0.71</cell><cell>0.59</cell></row><row><cell>Constant (range 0-700)</cell><cell>19.84</cell><cell>88.42</cell><cell>0.70</cell><cell>0.61</cell></row><row><cell>Timestep-aware</cell><cell>19.42</cell><cell>87.42</cell><cell>0.70</cell><cell>0.63</cell></row><row><cell>Gradient Norm</cell><cell>19.08</cell><cell>93.14</cell><cell>0.71</cell><cell>0.63</cell></row><row><cell>Entropy-driven</cell><cell>16.56</cell><cell>133.26</cell><cell>0.73</cell><cell>0.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 :</head><label>1</label><figDesc>Comparison results with state-of-the-art generative models based on ImageNet 128?128, 64?64. Annotation '(25 )' after the method means its sampling process is based on DDIM with 25 steps. Otherwise, it means the normal sampling method in DDPM, with 250 steps.</figDesc><table><row><cell>Method</cell><cell>FID ?</cell><cell>sFID ?</cell><cell>Prec ?</cell><cell>Rec ?</cell></row><row><cell>ImageNet 64?64</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BigGAN-deep [?]</cell><cell>4.06</cell><cell>3.96</cell><cell>0.79</cell><cell>0.48</cell></row><row><cell>IDDPM [?]</cell><cell>2.92</cell><cell>3.79</cell><cell>0.74</cell><cell>0.62</cell></row><row><cell>CADM-G [?]</cell><cell>2.07</cell><cell>4.29</cell><cell>0.74</cell><cell>0.63</cell></row><row><cell>CADM-G+EDS+ECT</cell><cell>1.88</cell><cell>4.51</cell><cell>0.76</cell><cell>0.61</cell></row><row><cell>ImageNet 128?128</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BigGAN-deep [?]</cell><cell>6.02</cell><cell>7.18</cell><cell>0.86</cell><cell>0.35</cell></row><row><cell>LOGAN [?]</cell><cell>3.36</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CADM [?]</cell><cell>5.91</cell><cell>5.09</cell><cell>0.70</cell><cell>0.65</cell></row><row><cell>CADM-G (25) [?]</cell><cell>6.58</cell><cell>7.52</cell><cell>0.77</cell><cell>0.50</cell></row><row><cell>CADM-G+EDS+ECT (25)</cell><cell>6.22</cell><cell>7.10</cell><cell>0.78</cell><cell>0.49</cell></row><row><cell>CADM-G [?]</cell><cell>2.97</cell><cell>5.09</cell><cell>0.78</cell><cell>0.59</cell></row><row><cell>CADM-G+EDS+ECT</cell><cell>2.68</cell><cell>5.10</cell><cell>0.80</cell><cell>0.56</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A note on the inception score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">To learn image super-resolution, use a gan to learn how to do image degradation first</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="185" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ilvr: Conditioning method for denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="14367" to="14376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diffusion schr?dinger bridge with applications to score-based generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Bortoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02544</idno>
		<title level="m">Large scale adversarial representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<title level="m">Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A variational perspective on diffusionbased generative models and score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00630</idno>
		<title level="m">Variational diffusion models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09761</idno>
		<title level="m">Diffwave: A versatile diffusion model for audio synthesis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved precision and recall metric for assessing generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kynk??nniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.02446</idno>
		<title level="m">Diffsinger: Singing voice synthesis via shallow diffusion mechanism</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Knowledge distillation in iterative generative models for improved sampling speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luhman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02388</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A conditional point diffusion-refinement paradigm for 3d point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.03530</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.01073</idno>
		<title level="m">Sdedit: Image synthesis and editing with stochastic differential equations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07582</idno>
		<title level="m">Non gaussian denoising diffusion models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03841</idno>
		<title level="m">Generating images with sparse representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09672</idno>
		<title level="m">Improved denoising diffusion probabilistic models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Controllable and compositional generation with latent-space energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<title level="m">Neural discrete representation learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<title level="m">Zero-shot text-to-image generation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00446</idno>
		<title level="m">Generating diverse high-fidelity images with VQ-VAE-2</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<title level="m">Imagenet large scale visual recognition challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:2104.07636</idno>
		<title level="m">Image super-resolution via iterative refinement</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unit-ddpm: Unpaired image translation with denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Willcocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05358</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">D2c: Diffusion-decoding models for fewshot conditional generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03585</idno>
		<title level="m">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Maximum likelihood training of score-based diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:1907.05600</idno>
		<title level="m">Generative modeling by estimating gradients of the data distribution</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03898</idno>
		<title level="m">Nvae: A deep hierarchical variational autoencoder</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cycle-consistent inverse gan for text-toimage synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tedigan: Text-guided diverse face image generation and manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pandey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05201</idno>
		<title level="m">Conditional generation of synthetic geospatial images from pixel-level and feature-level inputs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
