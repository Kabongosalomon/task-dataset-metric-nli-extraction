<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MPI For Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Saarland Informatics Campus [3</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MPI For Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Saarland Informatics Campus [3</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MPI For Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Saarland Informatics Campus [3</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MPI For Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Saarland Informatics Campus [3</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MPI For Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Saarland Informatics Campus [3</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MPI For Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Saarland Informatics Campus [3</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new single-shot method for multiperson 3D pose estimation in general scenes from a monocular RGB camera. Our approach uses novel occlusion-robust pose-maps (ORPM) which enable full body pose inference even under strong partial occlusions by other people and objects in the scene. ORPM outputs a fixed number of maps which encode the 3D joint locations of all people in the scene. Body part associations [8] allow us to infer 3D pose for an arbitrary number of people without explicit bounding box prediction. To train our approach we introduce MuCo-3DHP, the first large scale training data set showing real images of sophisticated multi-person interactions and occlusions. We synthesize a large corpus of multiperson images by compositing images of individual people (with ground truth from mutli-view performance capture). We evaluate our method on our new challenging 3D annotated multi-person test set MuPoTs-3D where we achieve state-of-the-art performance. To further stimulate research in multi-person 3D pose estimation, we will make our new datasets, and associated code publicly available for research purposes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single-person pose estimation, both 2D and 3D, from monocular RGB input is a challenging and widely studied problem in vision <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>. It has many applications, e.g., in activity recognition and content creation for graphics. While methods for 2D multi-person pose estimation exist <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37]</ref>, most 3D pose estimation methods are restricted to a single un-occluded subject. Natural human activities take place with multiple people in cluttered scenes hence exhibiting not only self-occlusions of the body, but also strong inter-person occlusions or occlusions by objects.</p><p>This work was funded by the ERC Starting Grant project CapReal (335545). We would also like to thank Hyeongwoo Kim and Eldar Insafutdinov for their assistance.</p><p>This makes the under-constrained problem of inferring 3D pose (of all subjects) from monocular RGB input even harder and leads to drastic failure of existing single person 3D pose estimation methods.</p><p>Recent work approaches this more general 3D multiperson pose estimation problem by decomposing it into multiple single-person instances <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b48">49]</ref>, often with significant redundancy in the decomposition <ref type="bibr" target="#b48">[49]</ref>. The single person predictions are post-processed to filter, refine and fuse the predictions into a coherent estimate. Bottom-up joint multi-person reasoning remains largely unsolved, and the multi-person 3D pose estimation lacks appropriate performance benchmarks.</p><p>We propose a new single shot CNN-based method to estimate multi-person 3D pose in general scenes from monocular input. We call our method single shot since it reasons about all people in a scene jointly in a single forward pass, and does not require explicit bounding box proposals by a separate algorithm as a preprocessing step <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b39">40]</ref>. The latter may fail under strong occlusions and may be expensive to compute in dense multi-person scenes. Our fully-convolutional method jointly infers 2D and 3D joint locations using our new occlusion-robust pose-map (ORPM) formulation. ORPM enables multi-person 3D pose estimates under strong (self-)occlusions by incorporating redundancy in the encoding, while using a fixed number of outputs regardless of the number of people in the scene. Our subsequent hierarchical read-out strategy starts with a base pose estimate, and is able to refine the estimate based on which joints of a person are visible, leading to robust 3D pose results.</p><p>To train our CNN we introduce a new multi-person 3D pose data set MuCo-3DHP. While there are several single-person datasets with 3D joint annotations, there are no annotated multi-person datasets containing large corpora of real video recordings of humanhuman interaction with large person and background diversity. Important advances in this direction have been made by Joo et al . <ref type="bibr" target="#b23">[24]</ref> using a multi-camera studio setup but background diversity remains limited. <ref type="figure">Figure 1</ref>. Qualitative results of our approach shown on MPII 2D <ref type="bibr" target="#b2">[3]</ref> dataset (blue), as well as our new MuPoTS-3D evaluation set (green). Our pose estimation approach works for general scenes, handling occlusions by objects or other people. Note that our 3D pose predictions are root-relative, and scaled and overlaid only for visualization.</p><p>Some prior work creates 3D annotated multi-person images by using 2D pose data augmented with 3D poses from motion capture datasets <ref type="bibr" target="#b48">[49]</ref>, or by finding 3D consistency in 2D part annotations from multi-view images recorded in a studio <ref type="bibr" target="#b52">[53]</ref>. To create training data with much larger diversity in person appearance, camera view, occlusion and background, we transform the MPI-INF-3DHP single-person dataset <ref type="bibr" target="#b32">[33]</ref> into the first multi-person set that shows images of real people in complex scenes. MuCo-3DHP is created by compositing multiple 2D person images with groundtruth 3D pose from multi-view marker-less motion capture. Background augmentation and shading-aware foreground augmentation of person appearance enable further data diversity. To validate the generalizability of our approach to real scenes, and since there are only very few annotated multi-person test sets <ref type="bibr" target="#b11">[12]</ref> showing more than two people, we contribute a new multiperson 3D test set, MuPoTS-3D. It features indoor and outdoor scenes, challenging occlusions and interactions, varying backgrounds, more than two persons, and ground truth from commercial marker-less motion capture. All datasets will be made publicly available. In summary, we contribute:</p><p>? A CNN-based single-shot multi-person pose estimation method based on a novel multi-person 3D pose-map formulation to jointly predict 2D and 3D joint locations of all persons in the scene. Our method is tailored for scenes with occlusion by objects or other people. ? The first multi-person dataset of real person images with 3D ground truth that contains complex inter-person occlusions, motion, and background diversity. ? A real in-the-wild test set for evaluating multiperson 3D pose estimation methods that contains diverse scenes, challenging multi-person interactions, occlusions, and motion.</p><p>Our method achieves state-of-the-art performance on challenging multi-person scenes where single-person methods completely fail. Although designed for the much harder multi-person task, it performs competitively on single-person test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We focus on most directly related work estimating the pose of multiple people in 2D or a single person in 3D from monocular RGB input. <ref type="bibr" target="#b49">[50]</ref> provide a more comprehensive review.</p><p>Multi-Person 2D Pose Estimation: A common approach for multi-person 2D pose estimation is to first detect single persons and then 2D pose <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40]</ref>. Unfortunately, these methods fail when the detectors fail-a likely scenario with multiple persons and strong occlusions. Hence, a body of work first localizes the joints of each person with CNN-based detectors and then find the correct association between joints and subjects in a post-processing step <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Single-Person 3D Pose Estimation: Existing monocular single-person 3D pose methods show good performance on standard datasets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27]</ref>. However, since many methods train a discriminative predictor for 3D poses <ref type="bibr" target="#b4">[5]</ref>, they often do not generalize well to natural scenes with varied poses, appearances, backgrounds and occlusions. This is due to the fact that most 3D datasets are restricted to indoor setups with limited backgrounds and appearance. The advent of large real world image datasets with 2D annotations made 2D pose estimation in the wild remarkably accurate. However, annotating images with 3D pose is much harder with many recent work focusing on leveraging 2D image datasets for 3D human pose estimation or multi-view settings <ref type="bibr" target="#b23">[24]</ref>. Additional annotations to these 2D image datasets allow some degree of 3D reasoning, either through body joint depth ordering constraints <ref type="bibr" target="#b41">[42]</ref> or dense shape correspondences <ref type="bibr" target="#b13">[14]</ref>. Some works split the problem in two: first estimate 2D joints and then lift them to 3D <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b1">2]</ref>, e.g., by database matching, neural network regression, or fitting the SMPL body model <ref type="bibr" target="#b29">[30]</ref>. Some works integrate SMPL within the CNN to exploit 3D and 2D annotations in an end-to-end fashion <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Other work leverages the features learned by a 2D pose estimation CNN for 3D pose estimation. For example, <ref type="bibr" target="#b59">[60]</ref> learn to merge features from a 2D and 3D joint prediction network. Another approach is to train a network with separate 2D and 3D losses for the different data sources <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b30">31]</ref>. Some approaches jointly reason about 2D and 3D pose with multi-stage belief maps <ref type="bibr" target="#b61">[62]</ref>. The advantage of such methods is that they can be trained end to end. A simpler yet very effective approach is to refine a network trained for 2D pose estimation for the task of 3D pose estimation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b32">33]</ref>. A major limitation of methods that rely on 2D joint detections directly or on bounding boxes is that they easily fail under body occlusion or with incorrect 2D detections, both of which are common in multi-person scenes. In contrast, our approach is more robust to occlusions since a base 3D body pose estimate is available even under significant occlusion. <ref type="bibr" target="#b33">[34]</ref> showed that 3D joint prediction works best when the receptive field is centered around the joint of interest. We build upon this insight to refine the base body pose where 2D joint detections are available.</p><p>Multi-Person 3D Pose Estimation: To our knowledge, only <ref type="bibr" target="#b48">[49]</ref> tackle multi-person 3D pose estimation from single images. 1 They first identify bounding boxes likely to contain a person using <ref type="bibr" target="#b46">[47]</ref>. Instead of a direct regression to pose, the bounding boxes are classified into a set of K-poses similar to <ref type="bibr" target="#b44">[45]</ref>. These poses are scored by a classifier and refined using a regressor. The method implicitly reasons using bounding boxes and produces multiple proposals per subject that need to be accumulated and fused. However, performance of their method under large person-person occlusions is unclear. In contrast, our approach produces multi-person 2D joint locations and 3D pose maps in a single shot, from which the 3D pose can be inferred even under severe person-person occlusion.</p><p>3D Pose Datasets: Existing pose datasets are either for a single person in 3D <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b32">33]</ref> or multi-person with only 2D pose annotations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref>. Of 1 A second approach <ref type="bibr" target="#b66">[67]</ref> was published in the review period. the two exceptions, the MARCOnI dataset <ref type="bibr" target="#b11">[12]</ref> features 5 sequences but contains only 2 persons simultaneously, and there are no close interactions. The other is the Panoptic dataset <ref type="bibr" target="#b23">[24]</ref> which has a limited capture volume, pose and background diversity. There is work on generating synthetic images <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b9">10]</ref> from mocap data, however the resulting images are not plausible. We choose to leverage the person segmentation masks available in MPI-INF-3DHP <ref type="bibr" target="#b32">[33]</ref> to generate annotated multi-person 3D pose images of real people at scale through compositing using the available segmentation masks. Furthermore, we captured a 3D benchmark dataset featuring multiple closely interacting persons which was annotated by a video-based multi-camera motion capture system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Person Dataset</head><p>Generating data by combining in-the-wild multiperson 2D pose data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref> and multi-person multiview motion capture for 3D annotation would be a straightforward extension of previous (single-person) approaches <ref type="bibr" target="#b32">[33]</ref>. However, multi-person 3D motion capture under strong occlusions and interactions is challenging even for commercial systems, often requiring manual pose correction constraining 3D accuracy. Hence, we merely employ purely multi-view marker-less motion capture to create the 20 sequences of MuPoTs-3D, the first expressive in-the-wild multi-person 3D pose benchmark. For the much larger training set MuCo-3DHP, we resort to a new compositing and augmentation scheme that leverages the single-person image data of real people in MPI-INF-3DHP <ref type="bibr" target="#b32">[33]</ref> to composite an arbitrary number of multi-person interaction images under user control, with 3D pose annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">MuCo-3DHP: Compositing-Based Training Set</head><p>The MPI-INF-3DHP <ref type="bibr" target="#b32">[33]</ref> single-person 3D pose dataset provides marker-less motion capture based annotations for real images of 8 subjects, each captured with 2 clothing sets, using 14 cameras at different elevations. We build upon these person segmentation masks to create per-camera composites with 1 to 4 subjects, with frames randomly selected from the 8 ? 2 sequences available per camera. Since we have groundtruth 3D skeleton pose for each video subject in the same space, we can composite in a 3D-aware manner resulting in correct depth ordering and overlap of subjects. We refer to this composited training set as the Multiperson Composited 3D Human Pose dataset (see <ref type="figure" target="#fig_0">Fig. 2</ref> for examples). The compositing process results in plausible images covering a range of simulated inter-person overlap and activity scenarios. Furthermore, user-control over the desired pose and occlu- sion distribution, and foreground/background augmentation using the masks provided with MPI-INF-3DHP is possible (see supplementary document for more details). Even though the synthesized composites may not simulate all the nuances of human-human interaction fully, we observe that our approach trained on this data generalizes well to real world scenes in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">MuPoTS-3D: Diverse Multi-Person 3D Test Set</head><p>We also present a new filmed (not composited) multi-person test set comprising 20 general real world scenes with ground-truth 3D pose for up to three subjects obtained with a multi-view marker-less motion capture system <ref type="bibr" target="#b60">[61]</ref>. Additionally, per joint occlusion annotations are available. The set covers 5 indoor and 15 outdoor settings, with trees, office buildings, road, people, vehicles, and other stationary and moving distractors in the background. Some of the outdoor footage also has challenging elements like drastic illumination changes, and lens flare. The indoor sequences use 2048 ? 2048px footage at 30fps, and outdoor sequences use 1920 ? 1080px GoPro footage at 60fps. The test set consists of &gt;8000 frames, split among the 20 sequences, with 8 subjects, in a variety of clothing styles, poses, interactions, and activities. Notably, the test sequences do not resemble the training data, and include real interaction scenarios. We call our new test set Multiperson Pose Test Set in 3D (MuPoTS-3D).</p><p>Evaluation Metric: We use the robust 3DPCK evaluation metric proposed in <ref type="bibr" target="#b32">[33]</ref>. It treats a joint's prediction as correct if it lies within a 15cm ball centered at the ground-truth joint location, and is evaluated for the common minimum set of 14 joints marked in green in <ref type="figure" target="#fig_3">Fig. 5</ref>. We report the 3DPCK numbers per sequence, averaged over the subjects for which GT reference is available, and additionally report the performance breakdown for occluded and un-occluded joints.</p><p>The relative robustness of 3DPCK over MPJPE <ref type="bibr" target="#b17">[18]</ref> is also useful to offset the effect of jitter that arises in all non-synthetic annotations, including ours. For completeness, we also report the MPJPE error for predictions matched to an annotated subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>At the core of our approach is a novel formulation which allows us to estimate the pose of multiple people in a scene even under strong occlusions with a single forward pass of a fully convolutional network. Our method builds upon the location-maps formulation <ref type="bibr" target="#b33">[34]</ref> that links 3D pose inference more strongly to image evidence by inferring 3D joint positions at the respective 2D joint pixel locations.We first recap the location-map formulation before describing our approach.</p><p>Location-Maps <ref type="bibr" target="#b33">[34]</ref>: A location-map is a joint specific feature channel storing the 3D coordinate x, y, or z at the joint 2D pixel location. For every joint, three location-maps, as well as a 2D pixel location heatmap are estimated. The latter encodes the 2D pixel location of the joint as a confidence map in the image plane. The 3D position of a joint can be read out from its location-map at the 2D pixel location of the joint, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. For an image of size W ? H, 3n location-maps of size W/k ? H/k are used to store the 3D location of all n joints, where k is a down-sampling factor. During training, the L 2 loss between the ground truth and the estimated location-map is minimized in the area around the joint's 2D pixel location. Although this simple location-map formulation enables full 3D pose inference, it has several shortcomings. First, it assumes that all joints of a person are fully visible, and breaks down under partial occlusion, which is common in general scenes. Second, efficient extension to multiple people is not straightforward. Introducing separate location-maps per person requires dynamically changing the number of outputs.</p><p>Occlusion-Robust Pose-Maps (ORPMs): We propose a novel occlusion-robust formulation that has a fixed number of outputs regardless of the number of people in the scene, while enabling pose read-outs for strongly occluded people. Our key insight is the incorporation of redundancy into the location-maps. We represent the body by decomposing it into torso, four limbs, and head (see <ref type="figure" target="#fig_3">Fig. 5</ref>). Our occlusion-robust pose-maps (ORPMs) support multiple levels of redundancy: (1) they allow the read-out of the complete base pose P ? R 3?n at one of the torso joint locations (neck or pelvis), (2) the base pose (which may not capture the full extent of articulation) can be further refined by reading out the head and individual limb poses where 2D detections are available, and (3) the complete limb pose can be read out at any 2D joint location of that limb. Together, these ensure that a complete and as articulate as possible pose estimate is available even in the presence of heavy occlusions of the body (see <ref type="figure" target="#fig_3">Fig. 5</ref>, and <ref type="figure" target="#fig_0">Fig. 2,3</ref> in the supplementary document). In addition, the redundancy in ORPMs allows to encode the pose of multiple partially overlapping persons without loss of information, thus removing the need for a variable number of output channels. See <ref type="figure" target="#fig_2">Fig. 4</ref>.</p><p>Na?ve Redundancy: The na?ve approach to introduce redundancy by allowing full pose read-out at all body joint locations breaks down for interacting and overlapping people, leading to supervision and read-out conflicts in all location-map channels. Our selective introduction of redundancy restricts these conflicts to pose-map channels of similar limbs, i.e., wrist of one person in the proximity of a knee of another person cannot cause read-out conflicts because their pose is encoded in their respective pose-maps. If the complete pose was encoded at each joint location, there would be conflicts for each pair of proximate joints across people. We now formally define ORPMs (Sec. 4.1) and explain the inference process (Sec. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Formulation</head><p>Given a monocular RGB image I of size W ? H, we seek to estimate the 3D pose P = {P i } m i=1 for each of the m persons in the image. Here, P i ? R 3?n describes the 3D locations of the n = 17 body joints of person i. The body joint locations are expressed relative to the parent joints as indicated in <ref type="figure" target="#fig_3">Fig. 5</ref> and converted to pelvis-relative locations for evaluation. We first decompose the body into pelvis, neck, head, and a set of limbs:</p><formula xml:id="formula_0">L = {{shoulder s , elbow s , wrist s }, {hip s , knee s , ankle s } | s ? {right, left}} .</formula><p>The 3D locations of the joints are then encoded in the occlusion-robust pose-maps denoted by</p><formula xml:id="formula_1">M = {M j } n j=1 , where M j ? R W ?H?3 .</formula><p>In contrast to simple location-maps, the ORPM M j stores the 3D location of joint j not only at this joint's 2D pixel</p><formula xml:id="formula_2">location (u, v) j but at a set of 2D locations ?(j) = {(u, v) neck , (u, v) pelvis } ? {(u, v) k } k?limb(j) , where: limb(j) = ? ? ? ? ? l, if ?l ? L with j ? l {head}, if j = head ?, otherwise .<label>(1)</label></formula><p>Note that-since joint j of all persons i is encoded in M j -it can happen that read-out locations coincide for different people, i.e., ? i1 (j) ? ? i2 (j) = ?. In this case, M j contains information about the person closer to the camera at the overlapping locations. However, due to our built-in redundancy in the ORPMs, a pose estimate for the partially occluded person can still be obtained at other available read-out locations. To estimate where the pose-maps can be read out, we make use of 2D joint heatmaps H = {H j ? R W ?H } n j=1 predicted by our network. Additionally, we estimate part affinity fields A = {A j ? R W ?H?2 } n j=1 which represent a 2D vector field pointing from a joint of type j to its parent <ref type="bibr" target="#b7">[8]</ref>. This facilitates association of 2D detections in the heatmaps (and hence read-out locations for the ORPMs) to person identities and enables per-person read-outs when multiple people are present. Note that we predict a fixed number of maps (n heatmaps, 3n pose-maps, and 2n part affinity fields) in a single forward pass irrespective of the number of persons in the scene, jointly encoding the 2D and 3D pose for all subjects, i.e., our network is single-shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pose Inference</head><p>Read-out of 3D pose of multiple people from ORPMs starts with inference of 2D joint locations</p><formula xml:id="formula_3">P 2D = {P 2D i } m i=1 with P 2D i = {(u, v) i j } n j=1 and joint detection confidences C 2D = {C 2D i ? R n } m i=1</formula><p>for each person i in the image. Explicit 2D joint-to-person association is done with the predicted heatmaps H and part affinity fields A using the approach of Cao et al . <ref type="bibr" target="#b7">[8]</ref>. Next, we use the 2D joint locations P 2D and the joint detection confidences C 2D in conjunction with ORPMs M to infer the 3D pose of all persons in the scene.</p><p>Read-Out Process: By virtue of the ORPMs we can read out 3D joint locations at select multiple pixel locations as described above. We define extremity joints: the wrists, the ankles, and the head. The neck and pelvis 2D detections are usually reliable, these joints are most often not occluded and lie in the middle of the body. Therefore, we start reading the full base pose at the neck location. If the neck is invalid (as defined below) then the full pose is read at the pelvis instead. If both of these joints are invalid, we consider this person as not visible in the scene and we do not predict the person's pose. While robust, full poses read at the pelvis and neck tend to be closer to the average pose in the training data. Hence, for each limb, we continue by reading out the limb pose at the extremity joint. Note again that the complete limb pose can be accessed at any of that limb's 2D joint locations. If the extremity joint is valid, the limb pose replaces the corresponding elements of the base pose. If the extremity joint is invalid however, we walk up the kinematic chain and check the other joints of this limb for validity. If all joints of the limb are invalid, the base pose cannot be further refined. This read-out procedure is illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref> and algorithmically described in the supplementary document.</p><p>2D Joint Validation: We declare a 2D joint location P 2D j i = (u, v) i j of person i as valid read-out location iff (1) it is un-occluded, i.e., has confidence value higher than a threshold t C , and (2) it is sufficiently far (? t D ) away from all read-out locations of joint j of other individuals:</p><formula xml:id="formula_4">valid(P 2D j i ) ? C 2D j i &gt; t C ? ||a ? P 2D j i || 2 ? t D ?? = [1 : m],? = i. ?a ? ??(j).<label>(2)</label></formula><p>Our ORPM formulation together with the occlusionaware inference strategy with limb refinement enables us to obtain accurate poses even for strongly occluded body parts while exploiting all available information if individual limbs are visible. We validate our performance on occluded joints and the importance of limb refinement on our new test set (see Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Network and Training Details</head><p>Our network has ResNet-50 <ref type="bibr" target="#b14">[15]</ref> as the core, after which we split it into two-a 2DPose+Affinity stream and a 3DPose stream. The core network and the first branch are trained on MS-COCO <ref type="bibr" target="#b28">[29]</ref> and the second branch is trained with MPI-INF-3DHP or MuCo-3DHP as per the scenario. Training and architectural specifics are in the supplementary document.</p><p>The 2DPose+Affinity stream predicts the 2D heatmaps H COCO for the MS-COCO body joint set, and part affinity fields A COCO . The 3DPose stream predicts 3D ORPMs M M P I as well as 2D heatmaps H M P I for the MPI-INF-3DHP <ref type="bibr" target="#b32">[33]</ref> joint set, which has some overlap with the MS-COCO joint set. For pose read-out locations as described previously, we restrict ourselves to the common minimum joint set between the two, indicated by the circles in <ref type="figure" target="#fig_3">Fig. 5</ref>.</p><p>Loss: The 2D heatmaps H COCO and H M P I are trained with per-pixel L2 loss comparing the predictions to the reference which has unit peak Gaussians with a limited support at the ground truth 2D joint locations, as is common. The part affinity fields A COCO are similarly trained with a per-pixel L2 loss, using the framework made available by Cao et al . <ref type="bibr" target="#b7">[8]</ref>. While training ORPMs with our MuCo-3DHP, per joint type j, for all subjects i in the scene, a per-pixel L2 loss is enforced in the neighborhood of all possible readout locations ? i (j). The loss is weighted by a limited support Gaussian centered at the read-out location. <ref type="table" target="#tab_3">Table 1</ref>. Sequence-wise evaluation of our method and LCR-net <ref type="bibr" target="#b48">[49]</ref> on multi-person 3D pose test set MuPoTS-3D. We report both (a) the overall accuracy (3DPCK), and (b) accuracy only for person annotations matched to a prediction <ref type="table" target="#tab_2">TS1 TS2 TS3 TS4 TS5 TS6 TS7 TS8 TS9 TS10 TS11 TS12 TS13 TS14 TS15 TS16 TS17 TS18 TS19</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head><p>The main goal of our method is multi-person 3D pose estimation in general scenes, which exhibits specific and more difficult challenges than single-person pose estimation. However, we validate the usefulness of our ORPM formulation on the single-person pose estimation task as well. To validate our approach, we perform extensive experiments on our proposed multi-person test set MuPoTS-3D, as well as two publicly available single-person benchmarks, namely Human3.6m <ref type="bibr" target="#b17">[18]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b32">[33]</ref>. <ref type="figure">Fig. 1</ref> presents qualitative results of our method showcasing the ability to handle complex in-the-wild scenes with strong inter-person occlusion. An extensive collection of qualitative results is provided in the supplementary document and video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison with Prior Art</head><p>For sequences with strong occlusion, we obtain much better results than the state-of-the-art. For unoccluded sequences our results are comparable to methods designed for single person. We outperform the only other multi-person method (LCR-net <ref type="bibr" target="#b48">[49]</ref>) quantitatively and qualitatively on both single-person and multi-person tasks. For fairness of comparison, in all evaluations, we re-target the predictions from LCRnet <ref type="bibr" target="#b48">[49]</ref> to a skeleton with bone-lengths matching the ground truth.</p><p>Multi-Person Pose Performance: We use our proposed MuPoTS-3D (see Sec. 3.2) to evaluate multiperson 3D pose performance in general scenes for our approach and LCR-net <ref type="bibr" target="#b48">[49]</ref>. In addition, we evaluate VNect <ref type="bibr" target="#b33">[34]</ref> on images cropped with the ground truth bounding box around the subject. We evaluate for all subjects that have 3D pose annotations available. If an annotated subject is missed by our method, or by LCRnet, we consider all of its joints to be incorrect in the 3DPCK metric. Table 1(a) reports the 3DPCK metric for all 20 sequences when taking all available annotations into account. Our method performs significantly better than LCR-net for most sequences, while being comparable for a few, yielding an overall improved performance of 65.0 3DPCK vs 53.8 3DPCK for LCR-net. We provide a joint-wise breakdown of the overall accuracy in the supplementary document.</p><p>Overall, our approach detects 93% of the annotated subjects, whereas LCR-net was successful for 86%. This is an additional indicator of performance. Even ignoring the undetected annotated subjects, our approach outperforms LCR-net in terms of 3D pose error (69.8 vs 62.4 3DPCK, and 132.5 vs 146 mm MPJPE).</p><p>VNect is evaluated on ground truth crops of the subjects, and therefore it operates at a 100% detection rate. In contrast, we do not use ground truth crops, and missed detections by our method count as all joints wrong. Despite this our method achieves better accuracy (65.0 vs 61.1 3DPCK, 30.1 vs 27.6 AUC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-Person Pose Performance:</head><p>On the MPI-INF-3DHP dataset (see <ref type="table" target="#tab_2">Table 2</ref> For detailed comparisons on Human3.6m <ref type="bibr" target="#b17">[18]</ref>, refer to the supplementary document. Our approach at 69.6mm MPJPE performs ?17mm better than LCR-net <ref type="bibr" target="#b48">[49]</ref>   Occlusion evaluation: To demonstrate the occlusion robustness of our method, we create synthetic random occlusions on the MPI-INF-3DHP test set. The synthetic occlusions cover about 14% of the joints. Our single-person and multi-person variants both outperform VNect for occluded joints (62.8 vs 64.0 vs 53.2 3DPCK) by a large margin, and are comparable for un-occluded joints (67.0 vs 71.0 vs 69.4 3DPCK). Note again that the single-person variant is trained on similar data as VNect and the occlusion robustness is inherent to the formulation. See the supplemental document for a more detailed breakdown by test sequence.</p><p>We use the per-joint occlusion annotations from MuPoTS-3D to further assess LCR-net and our approach under occlusion. Considering both self-and inter-personal occlusions, ?23.7 % of the joints of all subjects are occluded. Our method is more robust than LCR-net on both occluded (48.7 vs 42 3DPCK) and un-occluded (70.0 vs 57.5 3DPCK) joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablative Analysis</head><p>We validate the improvement provided by our limb refinement strategy on MPI-INF-3DHP test set. We empirically found that the base pose read out at one of the torso joints tends towards the mean pose of the training data. Hence, for poses with significant articulation of the limbs, the base pose does not provide an accurate pose estimate, especially for the end effectors. On the other side, the limb poses read out further down in the kinematic chain, i.e., closer to the extremities, include more detailed articulation information for that limb. Our proposed read-out process exploits this fact, significantly improving overall pose quality by limb refinement when limbs are available. <ref type="table" target="#tab_2">Table 2</ref> shows that the benefit of the full read-out is consistent over all metrics and valid for our method independent of whether it is trained on single-person (MPI-INF-3DHP) or multi-person data (MuCu-3DHP), with a ?10 3DPCK advantage over torso read-out. See <ref type="figure" target="#fig_0">Fig. 2,3</ref> in the supplementary document. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations and Future Work</head><p>As discussed in Section 4, we handle overlapping joints of the same type by only supervising the one closest to the camera. However, when joints of the same type are in close proximity (but not overlapping) the ground-truth ORPM for those may transition sharply from one person to the other, which are hard to regress and may lead to inaccurate predictions. One possible way to alleviate the issue is to increase the resolution of the output maps. Another source of failures is when 2D joints are mis-predicted or mis-associated. Furthermore, we have shown accurate root-relative 3D pose estimation, but estimating the relative sizes of people is challenging and remains an open problem for future work. While the compositing based MuCo-3DHP covers many plausible scenarios, further investigation into capturing/generating true person-person interactions at scale would be an important next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Multi-person 3D pose estimation from monocular RGB is a challenging problem which has not been fully addressed by previous work. Experiments on singleperson and multi-person benchmarks show that our method, relying on a novel occlusion robust pose formulation (ORPM), works well to estimate the 3D pose even under strong inter-person occlusions and humanhuman interactions better than previous approaches. The method has been trained on our new multi-person dataset (MuCo-3DHP ) synthesized at scale from existing single-person images with 3D pose annotations. Our method trained on this dataset generalizes well to real world scenes shown in our MuPOTS-3D evaluation set. We hope further investigation into monocular multi-person pose estimation would be spurred by the proposed training and evaluation data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Document:</head><p>Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Read-out Process</head><p>An algorithmic description of the read-out process is provided in Alg. 1.</p><formula xml:id="formula_5">Algorithm 1 3D Pose Inference 1: Given: P 2D , C 2D , M 2: for all i ? (1..m) do 3: if C 2D i [k] &gt; thresh, k ? {pelvis, neck} then 4:</formula><p>Person i is detected <ref type="bibr">5:</ref> for all joints j ? (1..n) do 6:</p><formula xml:id="formula_6">rloc = P 2D i [k] 7: P i [:, j] = ReadLocMap(j, rloc) 8:</formula><p>for all limbs l ? {arm l , arm r , leg l , leg r , head} do <ref type="bibr">9:</ref> for j = getExtremity(l); j / ? {pelvis, neck}; j = parent(j) do <ref type="bibr">10:</ref> if isValidReadoutLoc(i, j) then <ref type="bibr">11:</ref> refineLimb(l, P 2D i [j])</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>break 13:</p><formula xml:id="formula_7">else 14:</formula><p>No person detected <ref type="bibr">15:</ref> function getExtremity(limb l) <ref type="bibr">16:</ref> if l = leg s then return ankle s</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>else <ref type="bibr">18:</ref> if l = arm s then return wrist s</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>else return head 20: function ReadLocMap(joint j, 2DLocation rloc) <ref type="bibr">21:</ref> rloc = rloc/locM ap scale f actor <ref type="bibr">22:</ref> return M j [rloc] <ref type="bibr">23:</ref> function refineLimb(limb l, 2DLocation rloc) <ref type="bibr">24:</ref> for all joints b ? limb l do 25: <ref type="bibr">26:</ref> function isValidReadoutLoc(person i, joint j) <ref type="bibr">27:</ref> if (C 2D i [j] &gt; 0) then <ref type="bibr">28:</ref> return isIsolated(i,j)</p><formula xml:id="formula_8">P i [:, b]=ReadLocMap(b, rloc)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>29:</head><p>else <ref type="bibr">30:</ref> return 0 31: function isIsolated(person i, joint j) <ref type="bibr">32:</ref> isol = 1 <ref type="bibr">33:</ref> for all persons? ? (1..m),? = i do <ref type="bibr">34:</ref> for all 2DLocations a ? ??(j) do <ref type="bibr">35:</ref> if ||a ? P 2D i [j]|| 2 &lt; isoT hresh then <ref type="bibr">36:</ref> isol = 0 37: break 38:</p><p>return isol <ref type="figure">Figure 1</ref>. The network architecture with 2DPose+Affinity branch predicting the 2D heatmaps HCOCO and part affinity maps ACOCO with a spatial resolution of (W/8, H/8), and 3DPose branch predicting 2D heatmaps HMP I and ORPMs MMP I with a spatial resolution of (W/4, H/4), for an input image with resolution (W, H).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Network Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Architecture</head><p>A visualization if our network architecture using the web-based visualization tool Netscope can be found at: http://ethereon.github.io/netscope/#/gist/ 069a592125c78fbdd6eb11fd45306fa0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Data</head><p>We use 12 out of the 14 available camera viewpoints (using only 1 of the 3 available top down views) in MPI-INF-3DHP <ref type="bibr" target="#b32">[33]</ref> training set, and create 400k composite frames of MuCo-3DHP, of which half are without appearance augmentation. For training, we crop around the subject closest to the camera, and apply rotation, scale, and bounding-box jitter augmentation. Since the data was originally captured in a relatively restricted space, the likelihood of there being multiple people visible in the crop around the main person is high. The combination of scale augmentation, bounding-box jitter, and cropping around the subject closest to the camera results in many examples with truncation from the frame boundary, in addition to the inter-person occlusions occurring naturally due to the compositing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training</head><p>We train our network using the Caffe <ref type="bibr" target="#b20">[21]</ref> framework. The core network's weights were initialized with those trained for 2D body pose estimation on MPI <ref type="bibr" target="#b2">[3]</ref> and LSP <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> datasets as done in <ref type="bibr" target="#b32">[33]</ref>. The core network and the 2DPose + Affinity branch are trained for multi-person 2D pose estimation using the framework provided by Cao et al. <ref type="bibr" target="#b7">[8]</ref>. We use the AdaDelta solver, with a momentum of 0.9 and weight decay multiplier of 0.005, and a batch size of 8. We train for 640k iterations with a cyclical learning rate ranging from 0.1 to 0.000005. The 3DPose branch is trained with the core network and 2DPose + Affinity branch weights frozen. We use a batch size of 6 and train for 360k iterations with a cyclical learning rate ranging from 0.1 to 0.000001. We empirically found that training the part affinity fields and occlusion-robust pose-maps at lower resolution (see <ref type="figure">Fig. 1</ref>) leads to better results. <ref type="figure" target="#fig_0">Figure 2</ref> shows joint-wise accuracy comparison of our approach with LCR-net <ref type="bibr" target="#b48">[49]</ref> on the single person MPI-INF-3DHP test set. For limb joints (elbow, wrist, knee, ankle) LCR-net performs comparably or better than our torso-only readout, but our full readout performs significantly better. See <ref type="figure" target="#fig_1">Figure 3</ref>. <ref type="figure" target="#fig_3">Figure 5</ref> shows joint-wise accuracy comparison of our approach with LCR-net on our proposed multiperson 3D pose test set. We see that our approach obtains a better accuracy for all joint types for most sequences, only performing worse than LCR-net for a select few joint types on certain sequences (Test-Seq18,19,20).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Joint-wise Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation on Single-person Test Sets</head><p>Here we provide a detailed comparison against other methods for single-person 3D pose estimation. Evaluation on Human3.6m is in <ref type="table" target="#tab_3">Table 1</ref>, and on MPI-INF-3DHP test set in <ref type="table" target="#tab_2">Table 2</ref>. We additionally provide comparisons with the VNect location-maps trained on our training setup, which includes the 2D pretraining, and the 3D pose samples. <ref type="table" target="#tab_5">Table 3</ref> provides a sequencewise breakdown for the synthetic occlusion experiment on MPI-INF-3DHP test set wherein through randomly placed occlusions ?14% of the joints are occluded. This doesn't account for self-occlusions.    <ref type="figure" target="#fig_3">Figure 5</ref>. Comparison of our method and LCR-net <ref type="bibr" target="#b48">[49]</ref> on our proposed multi-person test set, here visualized as joint-wise breakdown of PCK for all 20 sequences, as well as the difference in accuracy between our method and LCR-net. LCR-net predictions were mapped to the ground truth bone lengths for fairness of comparison.   <ref type="figure" target="#fig_5">Figure 6</ref>. More qualitative results of our approach on MPI 2D pose dataset <ref type="bibr" target="#b2">[3]</ref> and our proposed MuPoTS-3D test set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Examples from our MuCo-3DHP dataset, created through compositing MPI-INF-3DHP [33] data. (Top) composited examples without appearance augmentation, (bottom) with BG and clothing augmentation. The last two columns show rotation and scale augmentation, and truncation with the frame boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Examples from our MuPoTS-3D evaluation set. Ground truth 3D pose reference and joint occlusion annotations are available for up to 3 subjects in the scene. The set covers a variety of scene settings, activities and clothing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Multiple levels of selective redundancy in our Occlusion-robust Pose-map (ORPM) formulation. VNect Location-maps<ref type="bibr" target="#b33">[34]</ref> (left) only support readout at a single pixel location per joint type. ORPMs (middle) allow the complete body pose to be read out at torso joint pixel locations (neck, pelvis). Further, each individual limb's pose can be read out at all 2D joint pixel locations of the respective limb. This translates to read-out of each joint's location being possible at multiple pixel locations in the joint's location map. The example at the bottom shows how 3D locations of multiple people are encoded into the same map per joint and no additional channels are required.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Example of the choice of read-out pixel location for right elbow pose under various scenarios. First the complete body pose is read out at one of the torso locations. a.) If the limb extremity is un-occluded, the pose for the entire limb is read out at the extremity (wrist), b.) If the limb extremity is occluded, the pose for the limb is read out at the joint location further up in the joint hierarchy (elbow), c.) If the entire limb is occluded, we retain the base pose read out at one of the torso locations (neck), d.) Read-out locations indicated for inter-person interaction, e.) If two joints of the same type (right wrist here) overlap or are in close proximity, limb pose read-out is done at a safer isolated joint further up in the hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) we compare our method trained on MPI-INF-3DHP (single-person) and MuCo-3DHP (multi-person) to three single-person methods-VNect, Zhou et al . [68], Mehta et al . [33]and LCR-net as the only other multi-person approach. Our method trained on multi-person data (73.4 3DPCK) performs marginally worse than our single-person version (75.2 3DPCK) due to the effective loss in network capacity when training on harder data. Nevertheless, both our versions consistently outperform Zhou et al . (69.2 3DPCK) and LCR-net (59.7 3DPCK) over all metrics. LCR-net predictions have a tendency to be conservative about the extent of articulation of limbs as shown in Fig. 6. In comparison to VNect (76.6 3DPCK) and Mehta et al . (75.7 3DPCK), our method (75.2 3DPCK) achieves an average accuracy that is on par. Our approach outperforms existing methods by ?3-4 3DPCK for activities which exhibit significant self-and object-occlusion like Sit on Chair and Crouch/Reach. For the full activity-wise breakdown, see the supplemental document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative comparison of LCR-net<ref type="bibr" target="#b48">[49]</ref> and our method. LCR-net output is limited in the extent of articulation of limbs, tending towards neutral poses. LCR-net also has more detection failures under significant occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 .</head><label>2</label><figDesc>Joint-wise accuracy comparison of our method and LCR-net<ref type="bibr" target="#b48">[49]</ref> on the single person MPI-INF-3DHP test set. 3D Percentage of Correct Keypoints (@150mm) as the vertical axis. LCR-net predictions were mapped to the ground truth bone lengths for fairness of comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative comparison of LCR-net<ref type="bibr" target="#b48">[49]</ref> and our method. LCR-net predictions are limited in terms of the extent of articulation of limbs, tending towards neutral poses. For our method, the base pose read out at the torso is similarly limited in terms of degree of articulation of limbs, and our full read-out addresses the issue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Examples from our MuPoTS-3D evaluation set. Ground truth 3D pose reference and joint occlusion annotations are available for up to 3 subjects in the scene (shown here for the frame on the top right). The set covers a variety of scene settings, activities and clothing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TS20</head><label></label><figDesc>Total a.) LCR-net 67.7 49.8 53.4 59.1 67.5 22.8 43.7 49.9 31.1 78.1 50.2 51.0 51.6 49.3 56.2 66.5 65.2 62.9 66.1 59.1 53.8 Ours 81.0 59.9 64.4 62.8 68.0 30.3 65.0 59.2 64.1 83.9 67.2 68.3 60.6 56.5 69.9 79.4 79.6 66.1 66.3 63.5 65.0 b.) LCR-net 69.1 67.3 54.6 61.7 74.5 25.2 48.4 63.3 69.0 78.1 53.8 52.2 60.5 60.9 59.1 70.5 76.0 70.0 77.1 81.4 62.4 Ours 81.0 64.3 64.6 63.7 73.8 30.3 65.1 60.7 64.1 83.9 71.5 69.6 69.0 69.6 71.1 82.9 79.6 72.2 76.2 85.9 69.8</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>(87.7mm), and outperforms the VNect location-map [34] (80.5mm) formulation by ?10mm. Our results are comparable to the recent state-of-theart results of Pavlakos et al . [41] (67.1mm), Martinez et al . [32] (62.9mm), Zhou et al . [68] (64.9mm), Mehta et al . [33] (68.6mm) and Tekin et al . [60] (70.81mm), and better than the recent results from Nie et al . [38] (79.5mm) and Tome et al . [62] (88.39mm).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of results on MPI-INF-3DHP [33] test set. We report the Percentage of Correct Keypoints measure in 3D (@150mm) for select activities, and the total 3DPCK and the Area Under the Curve for all activities. Complete activity-wise breakdown in the supplementary document</figDesc><table><row><cell></cell><cell cols="2">Sit Crouch</cell><cell>Total</cell></row><row><cell>Method</cell><cell cols="3">PCK PCK PCK AUC</cell></row><row><cell>VNect [34]</cell><cell>74.7</cell><cell>72.9</cell><cell>76.6 40.4</cell></row><row><cell>LCR-net [49]</cell><cell>58.5</cell><cell>69.4</cell><cell>59.7 27.6</cell></row><row><cell>Zhou et al.[68]</cell><cell>60.7</cell><cell>71.4</cell><cell>69.2 32.5</cell></row><row><cell>Mehta et al.[33]</cell><cell>74.8</cell><cell>73.7</cell><cell>75.7 39.3</cell></row><row><cell cols="2">Our Single-Person (Torso) 69.1</cell><cell>68.7</cell><cell>65.6 32.6</cell></row><row><cell>Our Single-Person (Full)</cell><cell cols="2">77.8 77.5</cell><cell>75.2 37.8</cell></row><row><cell>Our Multi-Person (Torso)</cell><cell>64.6</cell><cell>65.8</cell><cell>63.6 31.1</cell></row><row><cell>Our Multi-Person (Full)</cell><cell>75.9</cell><cell>73.9</cell><cell>73.4 36.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Comparison of results on Human3.6m<ref type="bibr" target="#b17">[18]</ref>, for single un-occluded person. Human3.6m, subjects 1,5,6,7,8 used for training. Subjects 9 and 11, all cameras used for testing. Mean Per Joint Postion Error reported in mm</figDesc><table><row><cell></cell><cell cols="2">Direct Disc.</cell><cell>Eat</cell><cell cols="4">Greet Phone Pose Purch.</cell><cell>Sit.</cell></row><row><cell>Pavlakos et al [41]</cell><cell>60.9</cell><cell>67.1</cell><cell>61.8</cell><cell>62.8</cell><cell>67.5</cell><cell>58.8</cell><cell>64.4</cell><cell>79.8</cell></row><row><cell>Mehta et al [33]</cell><cell>52.5</cell><cell>63.8</cell><cell>55.4</cell><cell>62.3</cell><cell>71.8</cell><cell>52.6</cell><cell>72.2</cell><cell>86.2</cell></row><row><cell>Tome et al [62]</cell><cell>65.0</cell><cell>73.5</cell><cell>76.8</cell><cell>86.4</cell><cell>86.3</cell><cell>69.0</cell><cell>74.8</cell><cell>110.2</cell></row><row><cell>Chen et al [9]</cell><cell>89.9</cell><cell>97.6</cell><cell>90.0</cell><cell>107.9</cell><cell>107.3</cell><cell>93.6</cell><cell>136.1</cell><cell>133.1</cell></row><row><cell>Moreno et al [35]</cell><cell>67.5</cell><cell>79.0</cell><cell>76.5</cell><cell>83.1</cell><cell>97.4</cell><cell>74.6</cell><cell>72.0</cell><cell>102.4</cell></row><row><cell>Zhou et al [68]</cell><cell>54.8</cell><cell>60.7</cell><cell>58.2</cell><cell>71.4</cell><cell>62.0</cell><cell>53.8</cell><cell>55.6</cell><cell>75.2</cell></row><row><cell>Martinez et al [32]</cell><cell>51.8</cell><cell>56.2</cell><cell>58.1</cell><cell>59.0</cell><cell>69.5</cell><cell>55.2</cell><cell>58.1</cell><cell>74.0</cell></row><row><cell>Tekin et al [60]</cell><cell>53.9</cell><cell>62.2</cell><cell>61.5</cell><cell>66.2</cell><cell>80.1</cell><cell>64.6</cell><cell>83.2</cell><cell>70.9</cell></row><row><cell>Nie et al [38]</cell><cell>62.8</cell><cell>69.2</cell><cell>79.6</cell><cell>78.8</cell><cell>80.8</cell><cell>72.5</cell><cell>73.9</cell><cell>96.1</cell></row><row><cell>VNect [34]</cell><cell>62.6</cell><cell>78.1</cell><cell>63.4</cell><cell>72.5</cell><cell>88.3</cell><cell>63.1</cell><cell>74.8</cell><cell>106.6</cell></row><row><cell>LCR-net [49]</cell><cell>76.2</cell><cell>80.2</cell><cell>75.8</cell><cell>83.3</cell><cell>92.2</cell><cell>79.9</cell><cell>71.7</cell><cell>105.9</cell></row><row><cell>VNect (with our setup)</cell><cell>65.52</cell><cell>78.8</cell><cell>64.8</cell><cell>75.0</cell><cell>85.2</cell><cell>66.4</cell><cell>88.1</cell><cell>110.2</cell></row><row><cell>Our Single-Person</cell><cell>58.2</cell><cell>67.3</cell><cell>61.2</cell><cell>65.7</cell><cell>75.82</cell><cell>62.2</cell><cell>64.6</cell><cell>82.0</cell></row><row><cell></cell><cell>Sit</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Walk Walk</cell><cell></cell></row><row><cell></cell><cell cols="4">Down Smk. Photo Wait</cell><cell>Walk</cell><cell>Dog</cell><cell>Pair</cell><cell>Avg.</cell></row><row><cell>Pavlakos et al [41]</cell><cell>92.9</cell><cell>67.0</cell><cell>72.3</cell><cell>70.0</cell><cell>54.0</cell><cell>71.0</cell><cell>57.6</cell><cell>67.1</cell></row><row><cell>Mehta et al [33]</cell><cell>120.6</cell><cell>66.0</cell><cell>79.8</cell><cell>64.0</cell><cell>48.9</cell><cell>76.8</cell><cell>53.7</cell><cell>68.6</cell></row><row><cell>Tome et al [62]</cell><cell>173.9</cell><cell>84.9</cell><cell>110.7</cell><cell>85.8</cell><cell>71.4</cell><cell>86.3</cell><cell>73.1</cell><cell>88.4</cell></row><row><cell>Chen et al [9]</cell><cell>240.1</cell><cell>106.6</cell><cell>139.2</cell><cell>106.2</cell><cell>87.0</cell><cell>114.0</cell><cell>90.5</cell><cell>114.2</cell></row><row><cell>Moreno et al [35]</cell><cell>116.7</cell><cell>87.7</cell><cell>100.4</cell><cell>94.6</cell><cell>75.2</cell><cell>87.8</cell><cell>74.9</cell><cell>85.6</cell></row><row><cell>Zhou et al [68]</cell><cell>111.6</cell><cell>64.1</cell><cell>65.5</cell><cell>66.0</cell><cell>63.2</cell><cell>51.4</cell><cell>55.3</cell><cell>64.9</cell></row><row><cell>Martinez et al [32]</cell><cell>94.6</cell><cell>62.3</cell><cell>78.4</cell><cell>59.1</cell><cell>49.5</cell><cell>65.1</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Tekin et al [60]</cell><cell>107.9</cell><cell>70.4</cell><cell>79.4</cell><cell>68.0</cell><cell>52.8</cell><cell>77.8</cell><cell>63.1</cell><cell>70.8</cell></row><row><cell>Nie et al [38]</cell><cell>106.9</cell><cell>88.0</cell><cell>86.9</cell><cell>70.7</cell><cell>71.9</cell><cell>76.5</cell><cell>73.2</cell><cell>79.5</cell></row><row><cell>VNect [34]</cell><cell>138.7</cell><cell>78.8</cell><cell>93.8</cell><cell>73.9</cell><cell>55.8</cell><cell>82.0</cell><cell>59.6</cell><cell>80.5</cell></row><row><cell>LCR-net [49]</cell><cell>127.1</cell><cell>88.0</cell><cell>105.7</cell><cell>83.7</cell><cell>64.9</cell><cell>86.6</cell><cell>84.0</cell><cell>87.7</cell></row><row><cell>VNect (with our setup)</cell><cell>155.9</cell><cell>82.0</cell><cell>95.2</cell><cell>76.8</cell><cell>59.7</cell><cell>94.1</cell><cell>64.3</cell><cell>84.3</cell></row><row><cell>Our Single-Person</cell><cell>93.0</cell><cell>68.8</cell><cell>84.5</cell><cell>65.1</cell><cell>57.6</cell><cell>72.0</cell><cell>63.6</cell><cell>69.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Comparison of our method against the state of the art on single person MPI-INF-3DHP test set. All evaluations use ground-truth bounding box crops around the subject. We report the Percentage of Correct Keypoints measure in 3D (@150mm), and the Area Under the Curve for the same, as proposed by MPI-INF-3DHP. We additionally report the Mean Per Joint Position Error in mm. Higher PCK and AUC is better, and lower MPJPE is better.</figDesc><table><row><cell></cell><cell>Stand/</cell><cell></cell><cell cols="3">Sit On Crouch/ On the</cell><cell></cell></row><row><cell>Network</cell><cell cols="5">Walk Exercise Chair Reach Floor Sports Misc.</cell><cell>Total</cell></row><row><cell></cell><cell cols="7">PCK PCK PCK PCK PCK PCK PCK PCK AUC MPJPE(mm)</cell></row><row><cell>VNect [34]</cell><cell>87.7</cell><cell>77.4</cell><cell>74.7</cell><cell>72.9</cell><cell cols="2">51.3 83.3 80.1 76.6 40.4</cell><cell>124.7</cell></row><row><cell>LCR-net [49]</cell><cell>70.5</cell><cell>56.3</cell><cell>58.5</cell><cell>69.4</cell><cell cols="2">39.6 57.7 57.6 59.7 27.6</cell><cell>158.4</cell></row><row><cell>Zhou et al.[68]</cell><cell>85.4</cell><cell>71.0</cell><cell>60.7</cell><cell>71.4</cell><cell cols="2">37.8 70.9 74.4 69.2 32.5</cell><cell>137.1</cell></row><row><cell>Mehta et al.[33]</cell><cell>86.6</cell><cell>75.3</cell><cell>74.8</cell><cell>73.7</cell><cell cols="2">52.2 82.1 77.5 75.7 39.3</cell><cell>117.6</cell></row><row><cell cols="2">Ours Single-Person (Torso) 75.0</cell><cell>64.8</cell><cell>69.1</cell><cell>68.7</cell><cell cols="2">48.6 70.0 60.6 65.6 32.6</cell><cell>142.8</cell></row><row><cell>Ours Single-Person (Full)</cell><cell>83.8</cell><cell>75.0</cell><cell>77.8</cell><cell>77.5</cell><cell cols="2">55.1 80.4 72.5 75.2 37.8</cell><cell>122.2</cell></row><row><cell>Ours Multi-Person (Torso)</cell><cell>73.7</cell><cell>63.7</cell><cell>64.6</cell><cell>65.8</cell><cell cols="2">44.7 69.5 60.2 63.6 31.1</cell><cell>146.8</cell></row><row><cell>Ours Multi-Person (Full)</cell><cell>82.0</cell><cell>74.5</cell><cell>75.9</cell><cell>73.9</cell><cell cols="2">51.6 79.0 71.8 73.4 36.2</cell><cell>126.3</cell></row><row><cell>VNect (our train. setup)</cell><cell>85.7</cell><cell>75.4</cell><cell>78.6</cell><cell>72.3</cell><cell cols="2">60.2 81.8 73.4 75.8 38.9</cell><cell>120.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Testing occlusion robustness of our method through synthetic occlusions on MPI-INF-3DHP single person test set. The synthetic occlusions cover about 14% of the evaluated joints overall. We report the Percentage of Correct Keypoints measure in 3D (@150mm) overall, as well as split by occlusion. Higher PCK.</figDesc><table><row><cell></cell><cell cols="7">Seq1 Seq2 Seq3 Seq4 Seq5 Seq6 Total</cell></row><row><cell></cell><cell cols="7">PCK PCK PCK PCK PCK PCK PCK</cell></row><row><cell>Overall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours Multi-Person</cell><cell>78.7</cell><cell>70.0</cell><cell>71.9</cell><cell>65.2</cell><cell>61.4</cell><cell>60.7</cell><cell>69.0</cell></row><row><cell>Ours Single-Person</cell><cell>80.9</cell><cell>72.8</cell><cell>72.6</cell><cell>65.7</cell><cell>62.5</cell><cell>65.8</cell><cell>71.1</cell></row><row><cell>VNect [34]</cell><cell>80.1</cell><cell>72.4</cell><cell>72.4</cell><cell>61.5</cell><cell>50.2</cell><cell>69.8</cell><cell>69.4</cell></row><row><cell>VNect (our train. setup)</cell><cell>79.3</cell><cell>74.4</cell><cell>72.2</cell><cell>67.2</cell><cell>55.7</cell><cell>64.6</cell><cell>70.4</cell></row><row><cell cols="2">Occluded Subset of Joints</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours Multi-Person</cell><cell>73.3</cell><cell>66.5</cell><cell>55.0</cell><cell>56.5</cell><cell>45.1</cell><cell>64.9</cell><cell>62.8</cell></row><row><cell>Ours Single-Person</cell><cell>74.9</cell><cell>63.2</cell><cell>59.0</cell><cell>54.2</cell><cell>48.0</cell><cell>68.4</cell><cell>64.0</cell></row><row><cell>VNect [34]</cell><cell>61.4</cell><cell>54.5</cell><cell>47.6</cell><cell>36.4</cell><cell>30.5</cell><cell>66.2</cell><cell>53.2</cell></row><row><cell>VNect (our train. setup)</cell><cell>69.6</cell><cell>61.9</cell><cell>49.0</cell><cell>50.8</cell><cell>43.5</cell><cell>63.4</cell><cell>59.2</cell></row><row><cell cols="2">Un-occluded Subset of Joints</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours Multi-Person</cell><cell>79.9</cell><cell>70.5</cell><cell>73.7</cell><cell>66.2</cell><cell>64.6</cell><cell>59.5</cell><cell>70.0</cell></row><row><cell>Ours Single-Person</cell><cell>82.1</cell><cell>74.0</cell><cell>74.1</cell><cell>67.0</cell><cell>65.3</cell><cell>65.1</cell><cell>72.2</cell></row><row><cell>VNect [34]</cell><cell>83.9</cell><cell>74.6</cell><cell>75.0</cell><cell>64.4</cell><cell>54.0</cell><cell>70.9</cell><cell>72.1</cell></row><row><cell>VNect (our train. setup)</cell><cell>81.3</cell><cell>76.0</cell><cell>74.6</cell><cell>69.0</cell><cell>58.1</cell><cell>64.8</cell><cell>72.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video based reconstruction of 3d people models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2D Human Pose Estimation: New Benchmark and State of the Art Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Twin gaussian processes for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017-IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1736" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MARCOnI -ConvNet-based MARker-less Motion Capture in Outdoor and Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using k-poselets for detecting people and localizing their keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3582" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Campus. Arttrack: Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with local joint-to-person associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="627" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating multiple diverse hypotheses for human 3d pose consistent with 2d joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jahangiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV) Workshops (PeopleCap)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Multimedia</title>
		<meeting>the 22nd ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.24.12</idno>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3334" to="3342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Regognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2848" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>248:1-248:16</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2017 Fifth International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017-IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Associative embedding: Endto-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on 3D Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01779</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for singleimage 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017-IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Articulated people detection and pose estimation: Reshaping the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thorm?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3178" to="3185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Posebits for monocular human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2337" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep multitask architecture for integrated 2d and 3d human sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Faster rcnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3108" to="3116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017-IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Kakadiaris. 3d human pose estimation: A review of the literature and analysis of covariates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A joint model for 2d and 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3634" to="3641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hand keypoint detection in single images using multiview bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Kinematic jump processes for monocular 3d human tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Articulated part-based model for joint object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00159</idno>
		<title level="m">Compositional human pose regression</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3d human body shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K V</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Reconstruction of articulated objects from point correspondences in a single uncalibrated image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="677" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Structured Prediction of 3D Human Pose with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>M?rquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">The Captury</title>
		<ptr target="http://www.thecaptury.com/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Total capture: 3d human pose estimation fusing video and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 28th British Machine Vision Conference</title>
		<meeting>28th British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Human pose estimation from video and imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016-01" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1533" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A Dual-Source Approach for 3D Pose Estimation from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kr?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
