<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Fast and Robust Target Models for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Robinson</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVL</orgName>
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">J?remo</forename><surname>Lawin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVL</orgName>
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">CVL</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Fahad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVL</orgName>
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">IIAI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVL</orgName>
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Fast and Robust Target Models for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video object segmentation (VOS) is a highly challenging problem since the initial mask, defining the target object, is only given at test-time. The main difficulty is to effectively handle appearance changes and similar background objects, while maintaining accurate segmentation. Most previous approaches fine-tune segmentation networks on the first frame, resulting in impractical frame-rates and risk of overfitting. More recent methods integrate generative target appearance models, but either achieve limited robustness or require large amounts of training data.</p><p>We propose a novel VOS architecture consisting of two network components. The target appearance model consists of a light-weight module, which is learned during the inference stage using fast optimization techniques to predict a coarse but robust target segmentation. The segmentation model is exclusively trained offline, designed to process the coarse scores into high quality segmentation masks. Our method is fast, easily trainable and remains highly effective in cases of limited training data. We perform extensive experiments on the challenging YouTube-VOS and DAVIS datasets. Our network achieves favorable performance, while operating at higher frame-rates compared to state-of-the-art. Code and trained models are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The problem of video object segmentation (VOS) has a variety of important applications, including object boundary estimation for grasping <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref>, autonomous driving <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref>, surveillance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref> and video editing <ref type="bibr" target="#b32">[33]</ref>. The task is to predict pixel-accurate masks of the region occupied by a specific target object, in every frame of a given video sequence. This work focuses on the semi-supervised setting, where a target ground truth mask is provided in the first frame. Challenges arise in dynamic environments with similar background objects and when the target undergoes * Authors contributed equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Target model output Segmentation <ref type="figure">Figure 1</ref>. During inference, our target model learns to produce segmentation score maps (center) of the target. As demonstrated in these examples, the target scores remain robust, despite difficult challenges, including distracting objects and appearance changes. Our segmentation network is trained to refine these coarse target score maps into a high-quality final object segmentation (right).</p><p>considerable appearance changes or occlusions. Successful video object segmentation therefore requires both robust and accurate target pixel classification. Aiming to achieve a robust target-specific segmentation, several methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48]</ref> fine-tune a generic segmentation network on the first frame, given the ground-truth mask. Although capable of generating accurate segmentation masks under favorable circumstances, these methods suffer from low frame-rates, impractical for many real world applications. Moreover, fine-tuning is prone to overfit to a single view of the scene, while degrading generic segmentation functionality learned during offline training. This limits performance in more challenging videos involving drastic appearance changes, occlusions and distractor objects <ref type="bibr" target="#b48">[49]</ref>. Further, the crucial fine-tuning step is not included in the offline training stage, which therefore does not simulate the full inference procedure.</p><p>Recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref> address these limitations by employing internal models of the target and back-ground appearance. They are based on, e.g., feature concatenation <ref type="bibr" target="#b32">[33]</ref>, feature matching <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref> or Gaussian models <ref type="bibr" target="#b21">[22]</ref>. Such generative models have the advantage of facilitating efficient closed-form solutions that are easily integrated into neural networks. A drawback to these methods is the demand for large amounts of data in order to learn representations applicable for the internal models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b46">47]</ref>. Due to the limited availability of annotated video data, these methods rely heavily on pre-training on image segmentation and synthesized VOS data via augmentation techniques. On the other hand, discriminative methods generally yield superior predictive power <ref type="bibr" target="#b31">[32]</ref> and have thus been preferred in many vision tasks, including image recognition <ref type="bibr" target="#b38">[39]</ref>, object detection <ref type="bibr" target="#b27">[28]</ref> and tracking <ref type="bibr" target="#b25">[26]</ref>. In this work, we therefore tackle the problem of integrating a discriminative model of the target appearance into a VOS architecture.</p><p>Our approach integrates a light-weight discriminative target model and a segmentation network, for modeling the target appearance and generating accurate segmentation masks. Operating on deep features, the proposed target model learns during inference to provide robust segmentation scores. The segmentation network is designed to process features with the segmentation scores as guidance. During offline training the network learns to accurately adhere to object edges and to suppress erroneous classification scores from the target model, see <ref type="figure">Figure 1</ref>. To learn the network parameters, we propose a training strategy that simulates the inference stage. This is realized by optimizing the target model on reference frames in each batch, and back-propagating the segmentation errors on corresponding validation frames. Contrary to fine-tuning based methods, the target adaption process is thus fully simulated during the offline training stage. During inference we keep the segmentation network fixed, while the target-specific learning is entirely performed by the target appearance model. Consequently, the segmentation network is target agnostic, retaining generic object segmentation functionality.</p><p>Unlike previous state-of-the-art methods, our discriminative target model requires no pre-training for image and synthetic video segmentation data. Our final approach, consisting of a single network architecture, is trained on VOS data in a single phase. Further, the employment of Gauss-Newton based optimization enables real-time video segmentation. We perform experiments on the DAVIS <ref type="bibr" target="#b36">[37]</ref> and YouTube-VOS 2018 <ref type="bibr" target="#b48">[49]</ref> datasets and demonstrate the impact of the components of our proposed approach in an ablative analysis. We further compare our approach to several state-of-the-art methods. Despite its simplicity, our approach achieves an overall score of 76.7 on DAVIS 2017 and 72.1 on YouTube-VOS, while operating at 22 frames per second (FPS). We also evaluate a faster version of our approach that achieves a speed of 41 FPS, with only a slight degradation in segmentation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The task of video object segmentation has seen extensive study and rapid development in recent years, largely driven by the introduction and evolution of benchmarks such as DAVIS <ref type="bibr" target="#b36">[37]</ref> and YouTube-VOS <ref type="bibr" target="#b48">[49]</ref>. First-frame fine-tuning: Most state-of-the-art approaches train a segmentation network offline, and then fine-tune it on the first frame <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48]</ref> to learn the target-specific appearance. This philosophy was extended <ref type="bibr" target="#b43">[44]</ref> by additionally fine-tuning on subsequent video frames. Other approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref> further integrate optical flow as an additional cue. While obtaining impressive results on the DAVIS 2016 dataset, the extensive fine-tuning leads to impractically long run-times. Furthermore, such extensive fine-tuning is prone to overfitting, a problem only partially addressed by heavy data augmentation <ref type="bibr" target="#b22">[23]</ref>. Non-causal methods: Another line of research approaches the VOS problem by allowing non-causal processing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>. In this work, we focus on the causal setting in order to accommodate real-time applications. Mask propagation: Several recent methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50</ref>] employ a mask-propagation module to improve spatio-temporal consistency of the segmentation. In <ref type="bibr" target="#b35">[36]</ref>, the model is learned offline to predict the target mask through refinement of the previous frame's segmentation output. To further avoid first-frame fine-tuning, some approaches <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50]</ref> concatenate the current frame features with the previous mask and a target representation generated in the first frame. Unlike these methods, we do not explicitly enforce spatio-temporal consistency through maskpropagation. Instead, we use previous segmentation masks as training data for the discriminative model. Feature matching: Recent methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> incorporate feature matching to locate the target object. Rather than fine-tuning the network on the first frame, these methods first construct appearance models from features corresponding to the initial target labels. Features from incoming frames are then classified using techniques inspired by classical clustering methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref> or feature matching <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>. In <ref type="bibr" target="#b33">[34]</ref>, a dynamic memory is used to combine feature matching from multiple previous frames. Tracking: Efficient online learning of discriminative target-specific appearance models has been explored in visual tracking <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>. Recently, optimization-based trackers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> have achieved impressive results on benchmarks. These methods train convolution filters using efficient optimization to discriminate between target and background. The close relation between the two problem domains is made explicit in <ref type="bibr" target="#b6">[7]</ref>, where object trackers are used as external components to locate the target. Gauss-Newton has previously been used in object segmentation <ref type="bibr" target="#b41">[42]</ref> for pose estimation of known object shapes. In contrast, we do not employ off-the-shelf trackers to predict the target or rely on target pose estimation. Instead we take inspiration from the optimization-based learning of a discriminative model, in order to capture the target object appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this work, we tackle the problem of predicting accurate segmentation masks of a target object, defined in the first frame of the video. This is addressed by constructing two network modules, D and S, specifically designed for target modeling and segmentation respectively. The target model D(x; w) takes features x as input and generates a coarse, but robust, segmentation output s = D(x; w) of the target object. It is parametrized by the weights w, which are solely learned during inference using the firstframe ground-truth, in order to capture the appearance of the target object.</p><p>The coarse segmentation scores s, generated by the target model D, is passed to the segmentation network S(s, x; ?), additionally taking backbone features x = F (I).</p><p>The parameters ? of the segmentation network is only trained during the offline training stage to predict the final high-resolution segmentation of the target. The coarse segmentation s thus serves as a robust guide, indicating the target location. Crucially, this allows the segmentation network to remain target agnostic, and learn generic segmentation functionality. Since S is trained with coarse segmentation inputs s generated by the target model, it learns to enhance its prediction and correct mistakes.</p><p>During inference, we update the target model using the segmentation masks generated by S. Specifically, the mask and associated features are stored in a memory M. Before the next incoming frame, we further adapt our model to the target appearance by re-optimizing D over all samples in M. In contrast to simply re-training on the latest frame, adding more training data to M over time, reduces the risk for model drifting. Our full VOS architecture is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Target model</head><p>We aim to develop a powerful and discriminative target appearance model, capable of differentiating between the target and background image regions. To successfully accommodate the VOS problem, the model must be robust to appearance changes and distractor objects. Moreover, it needs to be easily updated with new data and efficiently trainable. To this end, we employ a light-weight linear model D(x; w) realized as two convolutional layers,</p><formula xml:id="formula_0">s = D(x; w) = w 2 * (w 1 * x) ,<label>(1)</label></formula><p>with parameters w. These are trained exclusively during inference with image features x and the target segmentation mask y given in the first video frame. It then takes input feature maps x from subsequent video frames and outputs coarse segmentation scores s. The factorized formulation (1) is used for efficiency, where the first layer w 1 reduces the feature dimensionality and the second layer w 2 computes the actual segmentation scores. Fundamental to our approach, the target model parameters w must be learned with minimal computational impact. To enable the deployment of fast converging optimization techniques, we adopt an L 2 loss given by,</p><formula xml:id="formula_1">L D (w;M) = k ? k v k ?(y k ?U (D(x k )) 2 + j ? j w j 2 .</formula><p>(2) Here, the parameters ? j control the regularization term and v k are weight masks balancing the impact of target and background pixels. U denotes bilinear up-sampling of the output from the target model to the spatial resolution of the labels y k . The memory M = {(x k , y k , ? k )} K k=1 consists of sample feature maps x k , target labels y k and sample weights ? k . During inference, M is updated with new samples from the video sequence. To add more variety in the initial frame, we generate additional augmented samples {(x k ,? k , ? k )} k using the initial image I 0 and labels y 0 , see supplement for more details. Compared to blindly updating on the latest frame <ref type="bibr" target="#b43">[44]</ref>, M provides a controlled means of adding new samples while keeping past frames in memory by setting appropriate sample weights ? k . Optimization: We employ the Gauss-Newton (GN) based strategy from <ref type="bibr" target="#b10">[11]</ref> to optimize the parameters w. In comparison to the commonly used gradient descent based approaches, this strategy has significantly faster convergence properties <ref type="bibr" target="#b30">[31]</ref>. In each iteration, the optimal increment ?w is found using a quadratic approximation of the loss in (2)</p><formula xml:id="formula_2">L D (w + ?w) ? ?w T J T w J w ?w + 2?w T J T w r w + r T w r w . (3) Here, r w contains the residuals (2) as ? ? k v k ? (y k ? U (D(x k ))</formula><p>) and ? j w j and J w is the Jacobian of the residuals r w at w and. The objective (3) results in a positive definite quadratic problem, which we minimize over ?w with Conjugate Gradient (CG) descent <ref type="bibr" target="#b17">[18]</ref>. We then update w ? w + ?w and execute the next GN iteration. Pixel weighting: To address the imbalance between target and background, we employ a weight mask v in <ref type="formula">(2)</ref> to ensure that the target influence is not too small relative to the usually much larger background region. We define the target influence as the fraction of target pixels in the imag?</p><formula xml:id="formula_3">? k = N ?1 n y k (n),</formula><p>where n is the pixel index and N the total number of pixels. The weight mask is then defined as</p><formula xml:id="formula_4">v k = ?/? k , (y k ) n = 1 (1 ? ?)/(1 ?? k ), (y k ) n = 0<label>(4)</label></formula><p>where ? = max(? min ,? k ) is the desired and? k the actual target influence. We set ? min = 0.1 in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Segmentation Network</head><p>While the target model provides robust but coarse segmentation scores, the final aim is to generate an accurate segmentation mask of the target at the original image resolution. To this end, we introduce a segmentation network, that processes the coarse score s along with backbone features. The network consists of two types of building blocks: a target segmentation encoder (TSE) and a refinement module (see <ref type="figure" target="#fig_1">Figure 3</ref>). From these we construct a U-Net based architecture for object segmentation as in <ref type="bibr" target="#b50">[51]</ref>. Unlike most state-of-the-art methods for semantic segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b51">52]</ref>, the U-Net structure does not rely on dilated convolutions, but effectively integrates low-resolution deep feature maps. This is crucial for reducing the computational complexity of our target model during inference.</p><p>The segmentation network takes features maps x d as input from multiple depths in the backbone feature extractor network, with decreased resolution at each depth d. For each layer, x d along with the coarse scores s are first processed by a TSE block T d . The refinement module R d then inputs the resulting segmentation encoding generated by T d and the refined outputs z d+1 from the preceding deeper</p><formula xml:id="formula_5">layer z d = R d (T d (x d , s), z d+1 ).</formula><p>The refinement modules are comprised of two residual blocks and a channel attention block (CAB), as in <ref type="bibr" target="#b50">[51]</ref>. For the deepest block we set z d+1 to an intermediate projection of x d inside T d . The output z 1 at the shallowest layer is processed by two convolutional layers, providing the final refined segmentation output?. Target scores s are resized to matching stride and merged with the backbone feature x d , in the target segmentation encoder (TSE). The output from the TSE is further processed by a residual block before it is combined with the output from the deeper segmentation block in the CAB module. Finally, the output from the CAB module is processed by another residual block and sent to the next segmentation block. The complete network has four such layers.</p><p>Target segmentation encoder: Seeking to integrate features and scores, we introduce the target segmentation encoder (TSE). It processes features in two steps, as visualized in <ref type="figure" target="#fig_1">Figure 3</ref> (right). First, we project the backbone features to 64 channels to reduce the subsequent computational complexity. We maintain 64 channels throughout the segmentation network, keeping the number of parameters low. After projection, the features are concatenated with the segmentation score s and encoded by three convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Offline Training</head><p>We learn the parameters in our segmentation network offline by training on VOS training data. To this end, we propose a training scheme to simulate the inference stage. The network is trained on samples consisting of one reference frame and one or more validation frames. These are all randomly selected from the same video sequence. A training iteration is then performed as follows: We first optimize the target model weights w, described in Section 3.1, based on the reference frame. We then apply our full network, along with the learned target model, on the validation frames to predict the target segmentations. The parameters in the network are learned by back-propagating through the binary cross-entropy loss with respect to the ground-truth masks.</p><p>During offline training, we only learn the parameters of the segmentation network, and freeze the weights of the feature extractor. Since the target model only receives backbone features, we can pre-learn and store the target model weights for each sequence. The offline training time is therefore not significantly affected by the learning of D.</p><p>The network is trained in a single phase on VOS data. We select one reference frame and two validation frames per sample and train the segmentation network with the ADAM optimizer <ref type="bibr" target="#b23">[24]</ref>. We start with the learning rate ? = 10 ?3 , moment decay rates ? 1 = 0.9, ? 2 = 0.999 and weight decay 10 ?5 , and train for about 10 6 iterations, split into 120 epochs. The learning rate is then reduced to ? = 10 ?4 , and we train for another 60 epochs. With pre-learned target model weights, the training is completed in less than a day. </p><formula xml:id="formula_6">Algorithm 1 Inference Input: Images I i , target y 0 1: M 0 (I 0 , y 0 ) = {(x k ,? k , ? k )} K k=1 # Init dataset., sec 3.1 2: w 0 = optimize(L D (w; M 0 )) # Init D,</formula><formula xml:id="formula_7">x i = F (I i )</formula><p># Extract features 5:</p><formula xml:id="formula_8">s i = D(x i ; w i?1 ) # Predict target, sec 3.1 6:? i = S(x i , s i ) # Segment target, sec 3.2 7: M i = extend(x i ,? i ? i ; M i?1 ) # Extend dataset 8:</formula><p>if i mod t s = 0 then # Update D every ts frame 9:</p><formula xml:id="formula_9">w i = optimize(L D (w; , M i ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inference</head><p>During inference, we apply our video segmentation procedure as summarized in Algorithm 1. We first generate the augmented dataset M 0 , as described in Section 3.1, given the initial image I 0 and the corresponding target mask y 0 . We then optimize the target model D on this dataset. In the consecutive frames, we first predict the coarse segmentation scores s using D. Next, we refine s with the network S (Section 3.2). The resulting segmentation output? i along with the input features x i are added to the dataset. Each new sample (x i ,? i , ? i ) is first given a weight</p><formula xml:id="formula_10">? i = (1 ? ?) ?1 ? i?1 , with ? 0 = ?.</formula><p>We then normalize the sample weights to sum to unity. The parameter ? &lt; 1 controls the update rate, such that the most recent samples in the sequence are prioritized in the re-optimization of the target model D. For practical purposes, we limit the maximum capacity K max of the dataset. When the maximum capacity is reached, we remove the sample with the smallest weight from M i?1 before inserting a new one. In all our experiments we set ? = 0.1 and K max = 80.</p><p>During inference, we optimize the target model parameters w 1 and w 2 on the current dataset M i every t s -th frame. For efficiency, we keep the first layer of the target model w 1 fixed during updates. Setting t s to a large value reduces the inference time and regularizes the update of the target model. On the other hand, it is important that the target model is updated frequently, for objects that undergo rapid appearance changes. In our approach we set t s = 8. The framework supports multi object segmentation by employing a target model for each object and fuse the final refined predictions with softmax aggregation as in <ref type="bibr" target="#b32">[33]</ref>. We only require one feature extraction per image, since the features x i are common for all target objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation details</head><p>We implement our method in the PyTorch framework <ref type="bibr" target="#b34">[35]</ref> and use a ResNet <ref type="bibr" target="#b15">[16]</ref>, pre-trained on Ima-geNet <ref type="bibr" target="#b38">[39]</ref>, as the feature extractor F . Following the naming convention in <ref type="table" target="#tab_1">Table 1</ref> of <ref type="bibr" target="#b15">[16]</ref>, we extract four feature maps from the outputs of the blocks conv2_x through conv5_x. The target model D accepts features from conv4_x and produces 1-channel score maps. Both the input features and output scores have a spatial resolution 1/16th of the input image. Target model: The first layer w 1 has 1?1 kernels reducing input features to c = 96 channels while w 2 has a 3 ? 3 kernel with one output channel. During first-frame optimization, w 1 and w 2 are randomly initialized. Using the data augmentation (see the supplementary material), we generate a initial dataset M 0 of 5 image and label pairs. We then optimize w 1 and w 2 with the Gauss-Newton algorithm outlined in Section 3.1 with N GN = 5 GN steps. We apply N CG = 10 CG iterations in all GN steps but the first one. Since the initialization is random, we reduce the number of iterations to N CGi = 5 in the first step. In the target model update step we use N CGu = 10 CG iterations, updating w 2 every t s = 8 frame, while keeping w 1 fixed. We employ the aforementioned settings with a ResNet-101 backbone in our final approach, denoted Ours in the following sections.</p><p>We additionally develop a fast version, named Oursfast, with a ResNet-18 backbone and fewer optimization steps. Specifically, we set N GN = 4, N CGi = 5, N CG = 10 and N CGu = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We perform experiments on three benchmarks: DAVIS 2016 <ref type="bibr" target="#b36">[37]</ref>, DAVIS 2017 <ref type="bibr" target="#b36">[37]</ref> and YouTube-VOS <ref type="bibr" target="#b48">[49]</ref>. For YouTube-VOS, we compare on the official validation set, with withheld ground-truth. For ablative experiments, we also show results on a separate validation split of the YouTube-VOS train set, consisting of 300 videos not used for training. Following the standard DAVIS protocol, we report both the mean Jaccard J index and mean boundary F scores, along with the overall score J &amp;F, which is the mean of the two. For comparisons on YouTube-VOS, we report J and F scores for classes included in the training set (seen) and the ones that are not (unseen). The overall score G is computed as the average over all four scores, defined in YouTube-VOS. In addition, we compare the computational speed of the methods in terms of frames per second (FPS), computed by taking the average over the DAVIS 2016 validation set. For our approach, we used a V100 GPU and included all steps in Algorithm 1 to compute the frame rates. Further results and analysis are provided in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation study</head><p>We analyze the contribution of the key components in our approach. All compared approaches are trained using the YouTube-VOS training split. Base net: We construct a baseline network to analyze the impact of our target model D. This is performed by replacing D with an offline-trained target encoder, and retraining the segmentation network S. As for our proposed network we keep the backbone F parameters fixed. The target encoder is comprised of two convolutional layers, taking reference frame features from ResNet blocks conv4_x and the corresponding target mask as input. Features (conv4_x) extracted from the test frame are concatenated with the output from the target encoder and processed with two additional convolutional layers. The output is then passed to the segmentation network S in the same manner as for the coarse segmentation score s (see Section 3.2). We train this model with the same methodology as for our network. F.-T: We integrate a first-frame fine-tuning strategy into our network to compare this to our discriminative target model. For this purpose, we create an initial dataset M 0 with 20 samples using the same sample generation procedure employed for our approach (section 3.1). We then fine-tune all components of the network, except for the feature extractor, with supervision on the target model (loss in <ref type="formula">(2)</ref>) and the pre-trained segmentation network (binary cross-entropy loss) using the ADAM optimizer with 100 iterations and a batch size of four. In this setting we omitted the proposed optimization strategy of the target model and instead initialize the parameters randomly before fine-tuning. D-only -no update: To analyze the impact of the segmentation network S, we remove it from our architecture and instead let the target-specific model D output the final segmentations. The coarse target model predictions are upsampled to full image resolution through bilinear interpolation. In this version, we only train the target model D on the first frame, and refrain from subsequent updates. D-only: We further enable target model updates (as described in Section 3.4) using the raw target predictions. Ours -no update: For a fair comparison, we evaluate a variant of our approach with the segmentation network, but without any update of the target model D during inference. Ours: Finally, we include target model updates with segmentation network predictions to obtain our final approach.</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, we present the results in terms of the J score on a separate validation split of the YouTube-VOS training dataset. The base network, not employing the target model D, achieves a score of 49.8%. Employing fine-tuning on the first frame leads to an absolute improvement of 6%. Remarkably, using only the linear target model D is on par with online fine-tuning. While fine-tuning an entire segmentation network is prone to severe overfitting to the initial frame, our shallow target model has limited capacity, acting as an implicit regularization mechanism that benefits robustness and generalization to unseen aspects of the target and background appearance. Including updates results in an absolute improvement of 1.6%, demonstrates that we benefit from online updates despite the coarseness of the target mode generated labels. Further adding the segmentation network S (Ours -no update) leads to a major absolute gain of 8.3%. This improvement stems from the offline-learned processing of the coarse segmentations, yielding more accurate mask predictions. Finally, the proposed online updating strategy additionally improves the score to 71.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to state-of-the-art</head><p>We compare our method to recent approaches on the YouTube-VOS, DAVIS 2017 and DAVIS 2016 benchmarks. We provide results for two versions of our approach: Ours and Ours (fast) (see Section 3.5). Many compared methods include additional training data or employ models that have been pre-trained on segmentation data. For fair comparison we classify methods into two categories: "seg" for methods employing segmentation networks, pre-trained on e.g PASCAL <ref type="bibr" target="#b13">[14]</ref> or MS-COCO <ref type="bibr" target="#b27">[28]</ref> and "synth" for methods that perform additional training on synthetic VOS data generated from image segmentation datasets. YouTube-VOS <ref type="bibr" target="#b48">[49]</ref>: The official YouTube-VOS validation dataset has 474 sequences with objects from 91 classes. Out of these, 26 classes are not present in the training set. We provide results for Ours and Ours (fast), both trained on the YouTube-VOS 2018 training set. We compare our method with the results reported in <ref type="bibr" target="#b47">[48]</ref>, that were obtained by retraining the methods on YouTube-VOS. Additionally, we compare to PReMVOS <ref type="bibr" target="#b28">[29]</ref>, AGAME <ref type="bibr" target="#b21">[22]</ref>, RVOS <ref type="bibr" target="#b42">[43]</ref> and STM <ref type="bibr" target="#b33">[34]</ref>. The results are reported in <ref type="table">Table 2</ref>.</p><p>Among the methods using additional training data, OS-VOS <ref type="bibr" target="#b3">[4]</ref>, OnAVOS <ref type="bibr" target="#b43">[44]</ref> and PReMVOS employ first-frame fine-tuning, leading to inferior frame-rates below 0.3 FPS. In addition to fine-tuning, PReMVOS constitutes a highly complex framework, encompassing multiple components and cues: mask-region proposals, optical flow based mask predictions, re-identification, merging and tracking modules. In contrast, our approach is simple, consisting of a single network together with a light-weight target model. Remarkably, our approach significantly outperforms PRe-MVOS by a relative margin of 5.2%, yielding a final G-  <ref type="table">Table 2</ref>. State-of-the-art comparison on the large-scale YouTube-VOS validation dataset, containing 474 videos. The results of our approach were obtained through the official evaluation server. We report the mean Jaccard (J ) and boundary (F) scores for object classes that are seen and unseen in the training set, along with the overall mean (G). "seg" and "synth" indicate whether pre-trained segmentation models or additional data has been used during training. Our approaches achieve superior performance to methods that only trains on the YouTube-VOS train split, while operating at high frame-rates. Furthermore, Ours-fast obtains the highest frame-rates while performing comparable to state-of-the-art. score of 72.1. The recent STM method has the highest performance, employing feature matching with a dynamic memory to predict the target. RVOS is trained only on YouTube-VOS, achieving a Gscore of 56.8 by employing recurrent networks. In addition to recurrent networks, S2S employs first-frame finetuning, achieving a G-score of 64.4 with a significantly slower frame-rate compared to RVOS. In AGAME a generative appearance model is employed, resulting in a Gscore of 66.1. We further report results from a version of STM (YV18), where training has been performed solely on YouTube-VOS. This significantly degrades the performance to a G-score of 68.2. Ours outperforms all previous methods when only video data from YouTube-VOS has been used for training. We believe that, since our target model already provides robust predictions of the target on its own, our approach can achieve high performance without extensive training on additional data. Notably, Oursfast, maintains an impressive G-score of 65.7, while being significantly faster than all previous methods at 41.3 FPS. DAVIS 2017 <ref type="bibr" target="#b36">[37]</ref>: The validation set for DAVIS 2017 contains 30 sequences. We provide results for Ours and Ours-fast, trained a combination of the YouTube-VOS and DAVIS 2017 train splits, such that DAVIS 2017 is traversed eight times per epoch, and YouTube-VOS once. We report the results on DAVIS 2017 in <ref type="table">Table 3</ref>. As in the YouTube-VOS comparison above, we categorize the methods with respect to usage of training data. Since our approaches (Ours and Ours-fast) and AGAME <ref type="bibr" target="#b21">[22]</ref>, RVOS <ref type="bibr" target="#b42">[43]</ref>, STM <ref type="bibr" target="#b33">[34]</ref> and FEELVOS <ref type="bibr" target="#b44">[45]</ref> all include the YouTube-VOS during training, we add a third category denoted "yv".</p><p>OnAVOS <ref type="bibr" target="#b43">[44]</ref>, OSVOS-S <ref type="bibr" target="#b29">[30]</ref>, MGCRN <ref type="bibr" target="#b18">[19]</ref>, PRe-MVOS <ref type="bibr" target="#b28">[29]</ref>   <ref type="table">Table 3</ref>. State-of-the-art comparison on DAVIS 2017 and DAVIS 2016 validation sets. The columns with "yv", "seg", and "synth" indicate whether YouTube-VOS, pre-trained segmentation models or additional synthetic data has been used during training. The best and second best entries are shown in red and blue respectively. In addition to Ours and Ours-fast, we report the results of our approach when trained on only DAVIS 2017, in Ours (DV17). Our approach outperform compared methods with practical framerates. Furthermore, we achieve competitive results when trained with only DAVIS 2017, owing to our discriminative target model.</p><p>RGMP <ref type="bibr" target="#b32">[33]</ref>, AGAME <ref type="bibr" target="#b21">[22]</ref>, RANet <ref type="bibr" target="#b46">[47]</ref> and FEELVOS <ref type="bibr" target="#b44">[45]</ref> all employ mask-propagation, which is combined with feature matching in the latter three methods. Ours outperforms these methods with an J &amp;F score of 76.7. In addition, Ours-fast is significantly faster than all previous approaches, maintaining a J &amp;F score of 70.2. Our method is only outperformed by PReMVOS and the recent STM, achieving J &amp;F scores of 77.8 and 81.8 respectively. PRe-MVOS, however, suffer from extremely slow frame rates: approximately 500 times lower than ours. Moreover, our approach outperforms PReMVOS on the more challenging and large scale YouTube-VOS <ref type="table">(Table 2</ref>) by a large margin. We also evaluate our approach when only trained on DAVIS 2017, denoted Ours (DV17). We compare this approach to the methods FAVOS <ref type="bibr" target="#b6">[7]</ref>, AGAME (DV17) <ref type="bibr" target="#b21">[22]</ref> and STM (DV17) <ref type="bibr" target="#b33">[34]</ref>, which have also only been trained on the DAVIS 2017 train split. Our method significantly outperform all these methods with a J &amp;F score of 68.8. Moreover, this result is superior to OnAVOS, OSVOS-S, RANet, RVOS, RGMP and comparable to AGAME and FEELVOS despite their use of additional training data. DAVIS 2016 <ref type="bibr" target="#b36">[37]</ref>: Finally, we evaluate our method on the 20 validation sequences in DAVIS 2016, corresponding to a subset of DAVIS 2017 and report the results in <ref type="table">Table 3</ref>. Our methods perform comparable to the fine-tuning based approaches PReMVOS, MGCRN <ref type="bibr" target="#b18">[19]</ref>, OnAVOS and OSVOS-S. Further, Ours outperforms AGAME, RGMP, OSNM, FAVOS and FEELVOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>OSVOS-S AGAME FEELVOS RGMP <ref type="figure">Figure 4</ref>. Examples of three sequences from DAVIS 2017, demonstrating how our method performs under significant appearance changes compared to ground truth, OSVOS-S <ref type="bibr" target="#b29">[30]</ref>, AGAME <ref type="bibr" target="#b21">[22]</ref>, FEELVOS <ref type="bibr" target="#b44">[45]</ref> and RGMP <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground truth Coarse scores Output <ref type="figure">Figure 5</ref>. Qualitative results of our method, showing both target model score maps and the output segmentation masks. The top three rows are success cases and the last two represents failures when objects are too thin or appear too similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative Analysis</head><p>State-of-the-art: We compare our approach to some stateof-the-art methods in <ref type="figure">Figure 4</ref>. By using an early and a late frame from each video sequence, we study how the methods cope with large target deformations over time. In the first sequence (first and second row), the fine-tuning based OSVOS-S struggles as the target pose changes. While the mask-propagation in RGMP and generative appearance model in AGAME are accurate on fine details, they both fail to segment the red target, possibly due to occlusions.</p><p>In the second and third sequences (rows three to six), all of the above methods fail to robustly segment the different tar-gets. In contrast, our method accurately segments all targets in these challenging video sequences.</p><p>Target model: Some examples of the coarse segmentation scores and the final segmentation output are visualized in <ref type="figure">Figures 1 and 5</ref>. In most cases, the target model provides robust segmentation scores of the target object. It however struggles is some cases where the target object contains thin or small structures or details. An example is the challenging kite lines in the kite-surfing sequence, which are not accurately segmented. This is likely due to the coarse feature maps the target model is operating on. It also have problems separating almost identical targets such as the sheep. On the other hand, the model successfully handles very similar targets as in the gold-fish sequence (row 2 in <ref type="figure">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose video object segmentation approach, integrating a light-weight but highly discriminative target appearance model and a segmentation network. We find that despite its simplicity, a linear discriminative model is capable of generating robust target predictions. The segmentation network converts the predictions into high-quality object segmentations. The target model is efficiently trained during inference. Our method operates at high frame-rates and achieves state-of-the-art performance on the YouTube-VOS dataset and competitive results on DAVIS 2017 despite trained on limited data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Initial sample generation</head><p>In the first frame of the sequence, we train the target model on the initial dataset M 0 , created from the given target mask y 0 and the extracted features x 0 . To add more variety in the initial frame, we generate additional augmented samples. Based on the initial label y 0 , we cut out the target object and apply a fast inpainting method <ref type="bibr" target="#b40">[41]</ref> to restore the background. We then apply a random affine warp and blur before pasting the target object back onto the image, creating a set of augmented images? k and corresponding label masks? k . After feature extraction, we insert the unmodified first frame and the augmented frames into the dataset M 0 = {(x k ,? k , ? k )} K?1 k=0 and set the sample weights ? k such that the original sample carries twice the weight of the other samples. Example augmentations performed in the initial frame are shown in <ref type="figure" target="#fig_2">Figure 6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Detailed Quantitative Results</head><p>In this section we report some additional quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Training data</head><p>We analyze how the amount of training data impacts the performance of our approach. For this purpose we train our model on subsets of the YouTube-VOS training set containing 100%, 50%, 25% and 0% of the YouTube-VOS 2018 training split (excluding the validation split used to analyzing our approach as in Section 4 in the paper). For the version using 0% of the data, called "Ours D-only", we only apply or target appearance model, which is trained during inference, thus requiring no offline training. As shown in <ref type="table" target="#tab_4">Table 4</ref>, the performance improves as we increase the amount of training data from 0 to 100 percent of the YouTubeVOS training split. Already at 25 percent our approach outperforms recent methods such as AGAME <ref type="bibr" target="#b21">[22]</ref>   <ref type="table">Table 2</ref> in the main paper. The Ours D-only is our approach without the segmentation network as described in Section 5.1 in the main paper. It thus requires no training data at all.</p><p>(see <ref type="table">Table 2</ref> in the paper). At 50 percent, our approach surpasses all compared methods in <ref type="table">Table 2</ref> in the paper, that are trained only on the full YouTube-VOS training set. Remarkably, our target model without the segmentation (Ours D-only), consisting of a linear filter that requires no pretraining, obtains a G-score superior to the methods OS-VOS <ref type="bibr" target="#b3">[4]</ref>, OnAVOS <ref type="bibr" target="#b43">[44]</ref> and the recent RVOS <ref type="bibr" target="#b42">[43]</ref> (see Table 2 in the paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Algorithm runtime analysis</head><p>We investigate the runtime for the different steps in our proposed VOS approach in Algorithm 1 in the main paper. All runtimes have been computed by averaging over the DAVIS 2016 evaluation split. <ref type="figure" target="#fig_3">Figure 7</ref> shows how execution in frame 1 (the init phase, steps 1 and 2 in Algorithm 1) changes vs the size of the initial sample memory (or dataset) M 0 , when segmenting a single object. We present relative runtimes of the maximum time spent using all steps with |M 0 | = 20 The time spent during data augmentation is dominated by the inpainting which is performed only once, on the first frame, and hence it is appears constant. <ref type="figure" target="#fig_5">Figure 8</ref> shows how execution in frames 2 and onward (the forward phase, steps 4-9 in Algorithm 1) when fixating |M 0 | = 20 and varying the maximum dataset size K max . Again, we present timings in relation to the full execution time using |M| = 80. Note that most DAVIS 2016 videos are only a few seconds long and will never fill M entirely. This will reduce the apparent runtimes for large dataset sizes.</p><p>In addition, <ref type="table">Table 5</ref> shows the distribution of average time spent on each step in one frame in the forward phase. This is in the steady-state situation, after the sample memory is filled (here |M i | = 80), averaged over the last t s = 8 frames of the sequence.</p><p>Since the DAVIS videos are quite short, the init phase accounts for 41 percent of the total runtime when evaluating the Ours variant on DAVIS2016. On a per video basis, the initialization requires between 31 (for "cows" with 104 frames) and 60 percent (for "car-shadow" with 40 frames)    <ref type="table">Table 5</ref>. Distribution of time spent on steps in the frame loop of algorithm 1. The target prediction (step 5) is wrapped into "Other".</p><p>From <ref type="figure" target="#fig_3">figure 7</ref>, we conclude that the first-frame initialization (algorithm steps 1-2) scales approximately linearly with |M 0 |. The per-frame (forward phase) processing (Algorithm 1 steps 3-9) is dominated by the model update training and feature extraction. Theoretically, the complexity of both phases scale linearly with the number of iterations in their respective optimization steps (step 2 and 9) as well as linearly with the number of targets. <ref type="figure" target="#fig_7">Figure 9</ref> reports the mean J as functions of the memory learning rate ? and target model update interval t s (defined in Section 3.4 in the paper). The experiments are performed on the YouTubeVOS validation split, defined in Section 4 in the paper. It is apparent that the method is rather insensitive to either parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Parameter sensitivity</head><p>In addition, <ref type="table">Table 6</ref> shows the mean J as functions of the size of the initial training dataset M 0 . We test two variants of our method, one trained on YouTubeVOS data and one trained on both YouTubeVOS and DAVIS data. We evaluate on our own YouTubeVOS validation split and the DAVIS validation set. We observe that the YouTubeVOS evaluation is insensitive to the choice of |M 0 |. While still achieving a competitive J -score without initial data augmentation, our approach obtains the best performance using four additional augmented samples in M 0 .   <ref type="table">Table 6</ref>. The influence on mean J with varying |M0| during inference. We test two variants, trained on either YouTubeVOS only (yt) or both YouTubeVOS and DAVIS2017 (yt+dv17). Results shown are from evaluating on our YoutubeVOS validation split (ytv) and the DAVS2017 validation split (dvv).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our video segmentation architecture. Top left: Feature maps are extracted from the first frame. Top center:The features and given ground-truth mask are then stored in memory, and used by the optimizer to train the target model. Bottom left-center: In subsequent frames, features are extracted and then classified as foreground or background by the target model, forming low-resolution score maps. Bottom right: The score maps are refined to high-resolution segmentation masks in the segmentation network. The high resolution masks and associated features are continuously added to the memory, and the target model is periodically updated by the optimizer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of one block in our segmentation network S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Example of data augmentation for the initial sample generation with the image (top row) and corresponding label mask (bottom row). The original first frame sample is shown to the left and the augmented samples follows from left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Initialization runtime (frame 1), relative to the initial dataset size |M0| = 20 with a fixed maximum dataset size Kmax = 80 (section 3.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>at Kmax = 80</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Per-frame (forward phase) runtime relative to the maximum dataset size Kmax = 80 with a fixed initial size |M0| = 20.of the total runtime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Mean intersection-over-union results on our YouTubeVOS validation split (defined in Section 4), as a function of the memory update rate ? and retraining interval ts hyper parameters, detailed in section 3.4. ) dvv 69.4 73.8 72.6 73.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>sec 3.1 3: for i = 1, 2, . . . do</figDesc><table /><note>4:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ablative study on a validation split of 300 sequences from the YouTube-VOS train set. We analyze the different components of our approach, where D and S denote the target model and segmentation network respectively. Further, "Update" indicates if the target model update is enabled. Our target model D outperforms the Base net and is comparable to first-frame fine-tuning ("F.-T.") even with updates are disabled. Further, the segmentation network significantly improves the raw predictions from the target model D. Finally, the best performance is obtained when additionally updating target model D.</figDesc><table><row><cell>Version</cell><cell>D S Update.</cell><cell>J</cell></row><row><cell>Base net</cell><cell></cell><cell>49.8</cell></row><row><cell>F.-T.</cell><cell></cell><cell>58.9</cell></row><row><cell>D-only -no update</cell><cell></cell><cell>58.3</cell></row><row><cell>D-only</cell><cell></cell><cell>59.6</cell></row><row><cell>Ours -no update</cell><cell></cell><cell>67.9</cell></row><row><cell>Ours</cell><cell></cell><cell>71.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>employ extensive fine-tuning on the first-frame, experiencing impractical segmentation speed. The methods</figDesc><table><row><cell></cell><cell cols="4">Training Data DAVIS17 DAVIS16</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">yv seg synth J &amp;F</cell><cell cols="2">J &amp;F FPS</cell></row><row><cell>Ours (DV17)</cell><cell>--</cell><cell>-</cell><cell>68.8</cell><cell>81.7</cell><cell>21.9</cell></row><row><cell cols="2">AGAME (DV17) [22] --</cell><cell>-</cell><cell>63.2</cell><cell>-</cell><cell>14.3</cell></row><row><cell>FAVOS [7]</cell><cell>--</cell><cell>-</cell><cell>58.2</cell><cell>80.8</cell><cell>0.56</cell></row><row><cell>STM (DV17) [34]</cell><cell>--</cell><cell>-</cell><cell>43.0</cell><cell>-</cell><cell>6.25</cell></row><row><cell>Ours</cell><cell>-</cell><cell>-</cell><cell>76.7</cell><cell>83.5</cell><cell>21.9</cell></row><row><cell>Ours-fast</cell><cell>-</cell><cell>-</cell><cell>70.2</cell><cell>78.5</cell><cell>41.3</cell></row><row><cell>RVOS [43]</cell><cell>-</cell><cell>-</cell><cell>50.3</cell><cell>-</cell><cell>22.7</cell></row><row><cell>RANet [47]</cell><cell>--</cell><cell></cell><cell>65.7</cell><cell>85.5</cell><cell>30.3</cell></row><row><cell>AGAME [22]</cell><cell>-</cell><cell></cell><cell>70.0</cell><cell>-</cell><cell>14.3</cell></row><row><cell>STM [34]</cell><cell>-</cell><cell></cell><cell>81.8</cell><cell>89.3</cell><cell>6.25</cell></row><row><cell>RGMP [33]</cell><cell>--</cell><cell></cell><cell>66.7</cell><cell>81.8</cell><cell>7.7</cell></row><row><cell>FEELVOS [45]</cell><cell></cell><cell>-</cell><cell>71.5</cell><cell>81.7</cell><cell>2.22</cell></row><row><cell>OSNM [50]</cell><cell>--</cell><cell></cell><cell>54.8</cell><cell>-</cell><cell>7.14</cell></row><row><cell>PReMVOS [29]</cell><cell>-</cell><cell></cell><cell>77.8</cell><cell>86.8</cell><cell>0.03</cell></row><row><cell>OSVOS-S [30]</cell><cell>-</cell><cell>-</cell><cell>68.0</cell><cell>86.5</cell><cell>0.22</cell></row><row><cell>OnAVOS [44]</cell><cell>-</cell><cell>-</cell><cell>67.9</cell><cell>85.5</cell><cell>0.08</cell></row><row><cell>MGCRN [19]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>85.1</cell><cell>1.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>YouTubeVos 2018 test-dev results for different amount of training data, sample. Ours with 100% data is the same instance as in the comparison in</figDesc><table><row><cell></cell><cell>G</cell><cell>J</cell><cell>F</cell><cell></cell></row><row><cell>Method</cell><cell>overall</cell><cell>seen | unseen</cell><cell>seen | unseen</cell><cell>data</cell></row><row><cell>Ours</cell><cell>72.1</cell><cell>72.3 | 65.9</cell><cell>76.2 | 74.1</cell><cell>100%</cell></row><row><cell>Ours</cell><cell>70.6</cell><cell>71.4 | 63.7</cell><cell>75.5 | 71.8</cell><cell>50%</cell></row><row><cell>Ours</cell><cell>66.7</cell><cell>69.7 | 58.5</cell><cell>73.0 | 65.6</cell><cell>25%</cell></row><row><cell cols="2">Ours D-only 59.9</cell><cell>60.1 | 57.0</cell><cell>58.6 | 63.8</cell><cell>0%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledments: This work was supported by the ELLIIT Excellence Center at Link?ping-Lund for Information Technology, Autonomous Systems and Software Program (WASP) and the SSF project Symbicloud.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary material we provide a description of the initial sample generation, referred in section 3.1 in the paper. We also provide more detailed quantitative results and ablative analysis of parameters.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated tracking and grasping of a moving object with a robotic hand-eye system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Peter K Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billibon</forename><surname>Timcenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Yoshimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="152" to="165" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cnn in mrf: Video object segmentation via inference in a cnn-based higherorder spatio-temporal mrf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5977" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6181" to="6190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-K</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="686" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video object segmentation by learning location-sensitive embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="524" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detecting and tracking moving objects for video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)</title>
		<meeting>1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="319" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ATOM: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ECO: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6638" to="6646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive cartooning for privacy protection in camera networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ad?m</forename><surname>Erd?lyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tibor</forename><surname>Bar?t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Valet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rinner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<title level="m">Structured output tracking with kernels. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2096" to="2109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="583" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Methods of conjugate gradients for solving linear systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><forename type="middle">Rudolph</forename><surname>Hestenes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Stiefel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1952" />
			<biblScope unit="volume">49</biblScope>
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Motion-guided cascaded refinement network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yap-Peng</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="56" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5849" to="5858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A generative appearance model for end-to-end video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Brissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1175" to="1197" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page" from="12" to="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual recognition of grasps for human-to-robot mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Kjellstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The sixth visual object tracking vot2018 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pfugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Luka Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="90" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An algorithm for least-squares estimation of nonlinear parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marquardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the society for Industrial and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="441" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="841" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by referenceguided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2663" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visionbased offline-online perception paradigm for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Bakhtiary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Kangaroo vehicle collision detection using deep semantic segmentation convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Hossny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An image inpainting technique based on the fast marching method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of graphics tools</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A region-based gauss-newton approach to real-time monocular multiple object tracking. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Henning Tjaden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elmar</forename><surname>Schwanecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sch?mer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rvos: Endto-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreu</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-toend embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ranet: Ranking attention network for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Youtube-vos: Sequence-to-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1857" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
