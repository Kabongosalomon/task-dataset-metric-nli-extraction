<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
							<email>xucong.zhang@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonwook</forename><surname>Park</surname></persName>
							<email>spark@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
							<email>tbeeler@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Bradley</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
							<email>siyu.tang@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
							<email>otmar.hilliges@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gaze estimation is a fundamental task in many applications of computer vision, human computer interaction and robotics. Many state-of-the-art methods are trained and tested on custom datasets, making comparison across methods challenging. Furthermore, existing gaze estimation datasets have limited head pose and gaze variations, and the evaluations are conducted using different protocols and metrics. In this paper, we propose a new gaze estimation dataset called ETH-XGaze, consisting of over one million high-resolution images of varying gaze under extreme head poses. We collect this dataset from 110 participants with a custom hardware setup including 18 digital SLR cameras and adjustable illumination conditions, and a calibrated system to record ground truth gaze targets. We show that our dataset can significantly improve the robustness of gaze estimation methods across different head poses and gaze angles. Additionally, we define a standardized experimental protocol and evaluation metric on ETH-XGaze, to better unify gaze estimation research going forward. The dataset and benchmark website are available at https://ait.ethz.ch/projects/2020/ETH-XGaze</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Estimating eye-gaze from monocular images alone has recently received significant interest in computer vision <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b8">9]</ref> due to its significance in many application domains ranging from the cognitive sciences and HCI to robotics and semi-autonomous driving <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref>. Many arising computing paradigms such as smart-home appliances, autonomous cars and robots, as well as body-worn cameras will rely on understanding the attention and intent of humans without directly interacting with the observed person. We argue that in order to be more robust to a larger variety of environmental conditions, future methods should be able to accurately estimate the gaze of humans in a broader range of settings, including variation of viewpoint, extreme gaze angles, lighting variation, input image resolutions, and in the presence of occluders such as glasses.</p><p>Unfortunately, existing gaze datasets do not cater to such use-cases and are mostly limited to the frontal setting, covering a relatively narrow range of head poses and gaze directions. These are typically collected via laptops <ref type="bibr" target="#b41">[42]</ref>, mobile devices <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref> or in stationary settings <ref type="bibr" target="#b9">[10]</ref>. Recent work has moved towards more unconstrained environmental conditions in particular with respect to lighting but the coverage of head pose and gaze direction ranges remains limited <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>In this paper we detail a new dataset, dubbed ETH-XGaze, to facilitate research into robust gaze estimation methods. The dataset exhaustively samples large variations in head poses, up to the limit of where both eyes are still visible (maximum ?70 ? from directly facing the camera) as well as comprehensive gaze directions (maximum ?50 ? in the head coordinate system) <ref type="bibr" target="#b26">[27]</ref>. The dataset will allow for the development of new methods that can robustly estimate gaze direction without requiring a quasi-frontal camera placement. We show experimentally that i) the data distribution of ETH-XGaze is more comprehensive than other datasets (e.g., our dataset broadens the scope for eye-gaze research), and ii) that training on our dataset significantly improves robustness towards head pose and gaze direction variations. Beyond extending the gaze and head-pose ranges, the proposed dataset allocates considerably more pixels to the periocular region compared to existing datasets (e.g. refer to <ref type="figure">Fig. 3</ref>). This allows to train gaze estimators that can take advantage of the high-resolution imagery of modern camera hardware to improve gaze prediction. We collect data from 110 participants with different ethnicity, age, and gender -some with glasses and some without -in order to provide a rich and diverse dataset. For each of the participants we capture over 500 gaze directions with full-on illumination, plus an additional 90 samples under 15 different illumination conditions. This results in a total of over 1 million labeled samples. For all samples, the ground-truth gaze direction is known since the gaze is guided by stimuli displayed on a large screen in front of the participant, ensuring good label quality even under extreme view angles. The capture setup is depicted in <ref type="figure">Fig. 1</ref> (left).</p><p>To ensure fair and systematic comparisons between future methods that leverage this new large-scale dataset, we also propose a standardized evaluation protocol. Unlike other fields in computer vision that have benefited from such benchmark frameworks (i.e. image classification <ref type="bibr" target="#b27">[28]</ref>, face recognition <ref type="bibr" target="#b23">[24]</ref>, fullbody <ref type="bibr" target="#b14">[15]</ref>, hand pose estimation <ref type="bibr" target="#b42">[43]</ref> and multiview stereo reconstruction <ref type="bibr" target="#b28">[29]</ref>), the gaze estimation community has so far relied on a heterogeneous environment where many papers employ custom data pre-processing and evaluation protocols, rendering direct comparisons challenging. Motivated by the benchmarking approaches in adjacent areas we create a website open to the public to submit, evaluate and compare gaze estimation methods based on ETH-XGaze.</p><p>Finally, in order to provide initial insights into the value of our dataset, we provide results from a simple gaze estimation method that can serve as a baseline. Our estimation approach leverages a standard CNN architecture (i.e., ResNet-50 <ref type="bibr" target="#b10">[11]</ref>), trained with the task of estimating gaze from a monocular face patch. We present the estimation results as well as an ablation study of training </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gaze Estimation Algorithms</head><p>Initial learning-based gaze estimation methods often assume a static head pose <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>, with later works allowing for gradually more head pose freedom <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33]</ref>. In parallel, gaze estimation errors on public datasets have improved rapidly in recent years, through the use of domain adaptation <ref type="bibr" target="#b29">[30]</ref>, Bayesian networks <ref type="bibr" target="#b33">[34]</ref>, adversarial approaches <ref type="bibr" target="#b34">[35]</ref>, coarse-to-fine <ref type="bibr" target="#b5">[6]</ref>, and multi-region CNNs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9]</ref>. Recent development in the person-specific adaptation of gaze estimators <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20]</ref> are quickly reducing error metrics on public datasets even further. However, gaze-estimation is studied mostly in the frontal setting which does not apply to many emerging application domains. There is hence a need for a systematic method to understanding the robustness of a model with regards to gaze direction and head orientation ranges. We thus propose our gaze estimation dataset to cover these factors and propose concrete tasks for their evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gaze Datasets</head><p>Newly introduced datasets in any area of research tend to push the limits of the data distribution represented in existing datasets. Multi-view cameras have been used to cover head poses in previous works. However, there are limited range of head poses <ref type="bibr" target="#b30">[31]</ref>, or limited effective resolution on face region using machine vision cameras <ref type="bibr" target="#b32">[33]</ref> or wide-angle cameras <ref type="bibr" target="#b39">[40]</ref>. The Columbia dataset uses five high-resolution camera while only 5,880 samples with discrete gaze directions are recorded <ref type="bibr" target="#b30">[31]</ref>. UT Multi-view (UTMV) <ref type="bibr" target="#b32">[33]</ref> is recorded with eight machine version cameras, and HUMBI is recorded with multiple wide-angle cameras, however, their resolution of eye region is small in the captured image. Capturing different head poses with a single camera can be achieved by asking participants to explicitly move their head during recording as in EYEDIAP <ref type="bibr" target="#b9">[10]</ref>, moving the camera and gaze target around the participant as in Gaze360 <ref type="bibr" target="#b15">[16]</ref>, or both as in RT-GENE <ref type="bibr" target="#b8">[9]</ref>. Some of these approaches do result in lower resolution images, and as such are not informative in the development of generative models <ref type="bibr" target="#b29">[30]</ref> or gaze redirection methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38]</ref>. Therefore, these methods had to revert to the synthetic data from UnityEye <ref type="bibr" target="#b36">[37]</ref> or the relatively small Columbia datasets <ref type="bibr" target="#b30">[31]</ref>. In addition, it is more challenging to aim for the acquisition of a balanced dataset in terms of head pose and gaze estimation ranges when capturing in the wild (cf. <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16]</ref>), as is later shown in this paper in parameter range comparisons between our proposed dataset and existing ones. Our high resolution dataset tackles the mentioned challenge of limited head pose and gaze direction ranges in existing datasets, taking meaningful steps towards constructing a balanced set of training data for learning high performance and robust gaze estimation models. Furthermore, we see potential in leveraging the high quality imagery to enable future work in areas adjacent to gaze-estimation such as generative modeling of the eye-region, Computer Graphics and facial reconstruction.</p><p>A comprehensive summary of current gaze estimation datasets in relationship to ours is shown in Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Protocols</head><p>Having public benchmark frameworks for evaluation of popular algorithms is common for many computer vision tasks such as image classification <ref type="bibr" target="#b27">[28]</ref>, face recognition <ref type="bibr" target="#b16">[17]</ref>, pedestrian detection <ref type="bibr" target="#b7">[8]</ref> and hand pose estimation <ref type="bibr" target="#b42">[43]</ref>. Unfortunately, there is neither a unified evaluation protocol for gaze estimation nor an existing dataset that can serve as a general evaluation platform. Despite existing best practices, most previous work relies on their own data pre-processing and sometimes uses different training-test splits for evaluation. To provide a platform for gaze estimation evaluation, we share our dataset ETH-XGaze and define a set of clearly defined evaluation procedures. Furthermore, an online evaluation system and public leader-board are released along with the dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Peo. Maximum Head Pose</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum Gaze # Data Resolution</head><formula xml:id="formula_0">Columbia [31] 56 0 ? , ?30 ? ?15 ? , ?10 ? 5,880 5184?3456 UTMV [33] 50 ?36 ? , ?36 ? ?50 ? , ?36 ? 64,000 1280 ? 1024 EYEDIAP [10] 16 ?15 ? , 30 ? ?25 ? , 20 ? 237 min HD &amp; VGA MPIIGaze [42] 15 ?15 ? , 30 ? ?20 ? , ?20 ? 213,659 1280 ? 720 GazeCapture [19] 1,474 ?30 ? , 40 ? ?20 ? , ?20 ? 2,445,504 640 ? 480 RT-GENE [9] 15 ?40 ? , ?40 ? ?40 ? , ?40 ? 122,531</formula><p>1920 ? 1080 Gaze360 <ref type="bibr" target="#b15">[16]</ref> 238 </p><formula xml:id="formula_1">?90 ? , unknown ?140 ? , ?50 ? 172,000 4096 ? 3382 ETH-XGaze 110 ?80 ? , ?80 ? ?120 ? , ?70 ? 1,083,492 6000 ? 4000</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ETH-XGaze Dataset</head><p>There are several parameters that define a comprehensive gaze estimation dataset, including: head pose, gaze direction, subject appearance, illumination condition, and image resolution. We design the ETH-XGaze data collection procedure with the main objective to maximize the parameter range along each of those dimensions as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Acquisition Setup</head><p>The setup used for data collection is shown in the left of <ref type="figure">Fig. 1</ref>. We capture the subject with 18 Canon 250D digital SLR cameras from different viewpoints to cover a large range of head poses. There are five paired cameras for geometry capture and eight cameras for texture acquisition, such as to enable 3D face reconstruction in the future. The resolution of the captured images is very high (6000 ? 4000 pixels). All cameras are connected via ESPER trigger boxes 3 to a Raspberry Pi, and a wireless mouse is used to send the triggering signal to the Raspberry Pi. The delay between mouse click and triggering the camera is below 0.05 seconds. A large screen (120 ? 100 cm) is placed in the center of the cameras to show the stimuli controlled by the Raspberry Pi and projected by a projector. Since some cameras are placed behind the screen, we create cutout holes for their lenses. There are four light boxes (Walimex Daylight 250) surrounding the screen and each of them is equipped with a light bulb that emits ?4500lm. The Raspberry Pi can turn each of the light boxes on or off to simulate different illumination conditions. We mount polarization filters in front of both the light box and camera and carefully adjust the filter angle to attenuate specular reflection off the face of the participants. During recording, the participants are sitting at approximately one meter distance in front of the screen, with the head placed in a head rest to reduce unintentional head motion. </p><formula xml:id="formula_2">XGaze GazeCapture MPIIGaze RT-GENE EYEDIAP Gaze360</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Collection Procedure</head><p>During data collection, the participant focuses on a shrinking circle and clicks the mouse when the circle becomes a dot, providing the gaze point. The position of gaze points are randomly distributed on the screen. We have three methods to ensure the participant is looking at the dot when clicking the mouse. First, the participant has a short time window of 0.5 second to click the mouse to successfully collect one sample. Second, the shrinking time of the circle is random such that the participant has to focus on the shrinking circle to avoid missing the triggering time window. Third, the participant is told to collect a fixed amount of samples and any missing mouse click will increase the collection time. For most of the data collection, all four light boxes are fully on, in order to provide the maximum brightness, but we additionally simulate 15 illumination conditions by switching on and off the four light boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Characteristics</head><p>In total, we collect data from 110 participants (47 female and 63 male), aged between 19 and 41 years. 17 of them wore contact lenses and 17 of them wore eye glasses during recording. The ethnicities of the participants includes Caucasian, Middle Eastern, East Asian, South Asian and African. Each participant collected 525 gaze points under the full-lighting condition, and 90 gaze points under the varying lighting conditions -six gaze points for each of the 15 lighting conditions. For each gaze point, a total of 18 images was collected by the 18 different cameras. We manually removed samples for which the participant was not looking at the ground-truth point-of-regard due to blinking, motion blur etc. This results in total 1,083,492 images samples for whole ETH-XGaze dataset.</p><p>A comparison between the proposed and existing datasets can be found in Tab. 1. Our dataset surpasses existing datasets regarding the following aspects.</p><p>Head pose. ETH-XGaze has the largest range of head poses compared to existing datasets, as shown in the first row of <ref type="figure">Fig. 2</ref>. Examples from ETH-XGaze with different head poses are shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. In <ref type="bibr" target="#b15">[16]</ref>, it is stated that the effective head pose range of Gaze360 is ?90 ? in horizontal direction and limited head poses in vertical direction. However, head pose annotations are not provided in their dataset and hence we cannot visualize it here.</p><p>Gaze direction. ETH-XGaze has the largest range of gaze directions compared to existing datasets. The second row of <ref type="figure">Fig. 2</ref> compares the gaze direction distributions. Although Gaze360 reports ?140 ? coverage on the horizontal gaze direction, the dataset contains only very few samples beyond ?70 ? . ETH-XGaze is evenly sampled across a large range of horizontal and vertical gaze directions.</p><p>Image resolution. ETH-XGaze has the highest image resolution compared to existing datasets, especially the effective resolution on the face region. We show some examples and corresponding cropped eye images from different datasets in <ref type="figure">Fig 3.</ref> The Columbia dataset also has high image resolution, however, the dataset is comprised of only 5,880 samples. While EYEDIAP, MPIIGaze, RT-GENE and Gaze360 have fairly high resolution imagery as well, the participant is far away from the camera which results in low effective eye region resolution.</p><p>Controlled illumination conditions. ETH-XGaze provides a set of controlled illumination conditions. Although uncontrolled in-the-wild illumination conditions are important for gaze estimation <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b18">19]</ref>, controlled illumination conditions provide complementary information to better understand illumination impact and enable light synthesis. As shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, we record 16 different illumination conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">ETH-XGaze Utility</head><p>ETH-XGaze makes it possible to train gaze estimators that cover large ranges of head poses and gaze directions. This allows to better estimate gaze from oblique viewpoints, such as overhead cameras. ETH-XGaze can also be used to evaluate the robustness of a gaze estimation method with respect to these factors. In our dataset the head pose remains fixed and thus does not follow the traditional head-pose-following-gaze pattern. However, by imaging from 18 viewpoints we densely sample all natural pose-gaze combinations with respect to the camera, suitable for varied applications like gaze estimation from a personal laptop or attention measurement inside a smart home.</p><p>Our dataset allows future gaze prediction methods to train on high-resolution imagery, which is critical for generative methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b33">34]</ref>. Since the generated image quality highly depends on the training image quality, Columbia and the synthetic UnityEYE dataset have been used during training in the past. Our ETH-XGaze provides high-resolution images (6000 ? 4000 pixels), and more importantly the face region occupies a large portion of the image.</p><p>Since the data in ETH-XGaze has been captured to allow for 3D geometry reconstruction using multi-view photogrammetry methods (i.e. <ref type="bibr" target="#b1">[2]</ref>), it provides the potential of synthesizing high-quality gaze estimation data in the future. Parametric eye models <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b2">3]</ref> can be fit to the data to build a controllable rig of the eye <ref type="bibr" target="#b3">[4]</ref>. Such a rig can then be used to re-render novel images of different lighting conditions, gaze directions, and head poses with state-of-the-art rendering techniques, providing additional training data for gaze estimation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Data Pre-processing</head><p>We crop the face patch out of the original image as input for gaze estimation model training. For each input image sample, we first perform face and facial landmark detection using a state-of-the-art method <ref type="bibr" target="#b4">[5]</ref>. We then fit a 3D morphable model of the face to the detected landmarks to estimate the 3D head pose <ref type="bibr" target="#b13">[14]</ref>. The 3D head pose along with camera calibration information is used to perform data normalization <ref type="bibr" target="#b40">[41]</ref>. In a nutshell, the data normalization method maps the input image to a normalized space where a virtual camera is used to warp the face patch out of the original input image according to 3D head pose. It rotates a virtual camera to cancel the head rotation around the row axis, and moves the virtual camera to a fixed distance from the face center to warp the face patch of fixed size. More details can be found in the original paper <ref type="bibr" target="#b40">[41]</ref>. During data normalization, we define the face center as the center of the four eye corners and two nose corners, we set the focal length of the virtual camera to be 960 mm, the normalized distance to be 300 mm, and the cropped face image is 448 ? 448 pixels. Examples of face patches after data normalization are shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. The processed data along with original imagery are released to public. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Protocol</head><p>One goal of this paper is to establish a benchmark to evaluate gaze estimation algorithms. For this purpose, we define four evaluations on ETH-XGaze. The first three evaluations -cross-dataset, within-dataset, and person-specific evaluations -are popular evaluations found in the current gaze estimation literature. In addition, we propose to also assess robustness over head poses and gaze directions as a fourth evaluation criteria, which is made possible by ETH-XGaze.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline Method</head><p>We provide a baseline gaze estimation method using an off-the-shelf ResNet-50 network <ref type="bibr" target="#b10">[11]</ref>. This baseline takes the full-face patch covering 224 ? 224 pixels as input and outputs the horizontal and vertical gaze angles. We used the ADAM <ref type="bibr" target="#b17">[18]</ref> optimizer with an initial learning rate of 0.0001, and the batch size is set to be 50. We trained the baseline model for 25 epochs and decay the learning rate by a factor of 0.1 every 10 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset Preparation</head><p>We split ETH-XGaze into three parts: a training set TR comprised of 80 participants, a test set for within-dataset evaluation TE containing 15 participants, and a test set for person-specific evaluation TES consisting of another 15 participants. Splitting the test data into two disjoint sets allows us to release ground truth gaze required for the person-specific evaluation (Sec. 4.5). We ensured that the subjects in both training and test sets exhibit diverse gender, age, and ethnicity, some with and some without glasses. While we release both ground-truth gaze and imagery for the training set, we withhold the ground-truth gaze for the test sets. Authors are encouraged to submit gaze predictions on test samples to the benchmark website, and the performance will be evaluated and reported. This enables future research to compare to existing methods on neutral grounds.</p><p>Aside from the proposed ETH-XGaze dataset, we also evaluated other existing datasets with our baseline method. These datasets were pre-processed as we described in Sec. 3.5. For the EYEDIAP dataset, we used both screen sequence and floating target sequences and sampled the video sequences every 15 frames. For the GazeCapture dataset, we used the pre-processing pipeline from <ref type="bibr" target="#b24">[25]</ref> to obtain 3D head poses since the dataset does not provide camera parameters. For  the Gaze360 dataset, we used the face bounding box provided by the dataset to crop the face patch, alongside the 3D gaze ground-truth. We will ask authors of these datasets for permission to release the processed data such that the community can use it for evaluations on ETH-XGaze.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cross-dataset Evaluation</head><p>Cross-dataset evaluation has gained popularity since it indicates the generalization capabilities of a gaze estimation method. We define the cross-dataset evaluation as training the model on ETH-XGaze and testing on other datasets, as well as training on other datasets and testing on ETH-XGaze. We conducted the pair-wise cross-dataset evaluations on different datasets and show results achieved by the baseline in Tab. 2. The results exhibit rather large gaze estimation errors when testing on our ETH-XGaze, indicating that there is a big domain gap between ETH-XGaze and previous datasets. This stems from the fact that ETH-XGaze exhibits much larger variation in head pose and gaze direction compared to other datasets. Therefore, the gaze estimator has to extrapolate to those unseen head poses and gaze directions which is known to be a difficult machine learning task.</p><p>Training on GazeCapture achieves the best overall ranking since it contains similar head pose and gaze ranges compared to MPIIGaze, RT-GENE and EYE-DIAP. However, it performs poorly on test datasets that exhibit large variation in head pose and gaze direction such as Gaze360 and our ETH-XGaze. In contrast, ETH-XGaze enables thorough benchmarking of generalization capabilities of future gaze estimation approaches.</p><p>The model trained on Gaze360 achieves the best cross-dataset performance on ETH-XGaze since they contain similar head pose and gaze direction ranges. However, Gaze360 has been collected "in the wild" setting and can suffer from low-quality images and gaze labels (see <ref type="figure">Fig. 6</ref>). Our dataset, despite the lab setting, still allows for good performance (the best on EYEDIAP and Gaze360) without any data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EHT-XGaze</head><p>Gaze360 RT-GENE  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Within-dataset Evaluation</head><p>Within-dataset evaluation is another popular means of evaluating gaze estimation methods. Here the method is trained on TR and evaluated on TE. Tab. 3 shows performances of the baseline alongside comparisons to recent state-of-theart methods. The baseline achieves an error of 4.7 degrees on average on ETH-XGaze, which is reasonably low given the large ranges of head poses and gaze directions. On other datasets, the baseline exhibits an accuracy comparable to current state-of-the-art methods, indicating that it is a strong baseline. The results of the other methods are taken from the respective publications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Person-specific Evaluation</head><p>Person-specific gaze estimation has gained a lot of attention in recent years <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20]</ref> due to the huge improvements that can be achieved from even just a few personal calibration samples. We randomly selected 200 samples from each participant in TES as the personal calibration samples. The protocol is to train the model with TR and up to 200 personal calibration samples, and to test on the remaining samples of TES -for each of the 15 test subjects. We pre-trained the model on TR and then fine-tune it using the 200 samples with 25 epochs. Results from the baseline in <ref type="figure">Fig. 7</ref> show that personal calibration improves the gaze estimates by a large margin. The goal of this evaluation is not only to achieve good results but also to rely on as few calibration samples as possible. <ref type="figure">Fig. 7</ref>: Gaze estimation errors for person-specific evaluation of our baseline. We show the gaze estimation errors with and without training with 200 calibration samples. The number above each bar is the gaze estimation error in degrees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Robustness Evaluation</head><p>Previous gaze estimation works usually only report the mean gaze estimation errors without detailed analysis across head poses and gaze directions. This is partly due to the lack of sufficient data samples to cover a wide range. Knowing the performance of an algorithm with respect to these factors is important, since a method with a higher overall error might have lower error within a specific range of interest. Hence we introduce a detailed evaluation to show the robustness across head poses and gaze directions. <ref type="figure" target="#fig_3">Fig. 8</ref> shows the performance of the baseline on TE over horizontal and vertical axes of the head pose and gaze direction. The different colors represent the different training sets. While these plots evaluate the performance of the different training sets, the benchmark will compare different algorithms instead. A flat curve across the entire graph, as in the case of training on ETH-XGaze, indicates robustness to head pose and gaze direction variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Demonstration of ETH-XGaze</head><p>In this section, we evaluate the importance of different factors during training. Previous gaze estimation datasets cannot serve as the evaluation set for an ablation study of different factors such as head poses, gaze directions and illumination conditions due to the limited coverage. In contrast, the proposed ETH-XGaze is an ideal dataset for these evaluations.</p><p>Head Pose and Gaze Direction. We created several training subsets from TR by constraining the head poses and/or gaze directions angle ranges to be ?80, ?60, ?40, and ?20 in both horizontal and vertical directions. To keep the same amount of training samples for each subsets, we randomly re-sampled each training subset to have the same amount of samples as the minimal training set, i.e. the training set of ?20 in both head poses and gaze directions. The results of testing on TE are shown in the left of <ref type="figure" target="#fig_4">Fig. 9</ref>. As we can see from the figure, constraining the head pose and gaze direction results in worse performance in general, especially when we constrain both head pose and gaze direction ranges. Constraining the gaze directions achieves worse results than constraining head poses, which indicates gaze directions have more impact than the head poses. Specifically, when we constrained the angle range to be ?40 degrees, the performance decrease caused by constraining head poses is 34.6%, constraining gaze directions is 82.1%, and constraining both head poses and gaze directions is 206.4%.</p><p>Illumination condition. In the center of <ref type="figure" target="#fig_4">Fig. 9</ref>, we show results by training the baseline with all lighting conditions or only with the full-lighting condition. The performance drop (9% from 7.8 degrees to 8.5 degrees) indicates the impact of lighting conditions on gaze estimation performance.</p><p>Personal appearance. In <ref type="bibr" target="#b18">[19]</ref>, the authors show gaze estimation performance with different numbers of participants. Our repeated experiment with our baseline on ETH-XGaze shows the same trend as increasing number of participants improves the performance (see <ref type="figure" target="#fig_4">Fig. 9, right)</ref>.</p><p>Input resolution. The image resolution analysis in <ref type="bibr" target="#b41">[42]</ref> was only for eye images and the highest resolution was 60?36. The default input face patch image size to ResNet is 224 ? 224 which we used in our baseline. We resized the input image to be 112 ? 112 and 448 ? 448 and then fed them into the baseline. Since there is an average pooling layer at the end of the ResNet convolutional layers, we do not need to modify the architecture with respect to different resolutions.</p><p>The results of resolution variation are shown in Tab. 4. The performance is improved when training and testing on higher resolutions, which indicates the potential of high-resolution gaze estimation. However, different with results in <ref type="bibr" target="#b41">[42]</ref>, the model trained on one size achieves much worse results on other sizes. This can be caused by the much higher image resolution in ETH-XGaze with large appearance differences compared to the MPIIGaze in <ref type="bibr" target="#b41">[42]</ref>. We did not specifically develop the method to handle cross-resolution input images and expect future works can properly deal with cross-resolution training.   <ref type="table">Table 4</ref>: Gaze estimation errors in degrees generated by models trained with different input image sizes in pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present a new large-scale gaze estimation dataset ETH-XGaze, featuring large variation in head pose and gaze direction, high-resolution imagery, varied subject appearance, systematic illumination conditions, as well as accurate ground-truth gaze vectors. Evaluation using a baseline method shows that training on ETH-XGaze significantly improves robustness towards variation in head pose and gaze direction compared to existing datasets, adding a very valuable resource for future work on gaze estimation. In addition, we propose a standardized experimental protocol and evaluation framework that will be made available via the benchmark website alongside the dataset, allowing for fair comparison of gaze estimation algorithms on neutral ground.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Head pose (top row) and gaze direction (bottom row) distributions of different datasets. The head pose of Gaze360 is not shown here since it is not provided by the dataset. Data examples and corresponding cropped eye images from different gaze estimation datasets. ETH-XGaze has the highest image resolution and quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Samples of the 16 illumination conditions created by switching on and off the four light boxes. The first row are the original samples, and the second row employs histogram equalization. The first column is the full-lighting setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Data examples captured by 18 different camera views. The red arrow is the gaze direction. The face patch images shown are after data normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 :</head><label>8</label><figDesc>Gaze estimation error distribution across head poses (first row) and gaze directions (second row) in horizontal and vertical directions respectively. The colored curves represent results with different training sets tested on TE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 :</head><label>9</label><figDesc>Gaze estimation error distribution by constraining head poses and gaze directions (left), lighting conditions (middle), and number of people (right) during training. The number above each bar is the gaze estimation error in degrees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Overview of popular gaze estimation datasets showing the number of participants, the maximum head poses and gaze in horizontal (around yaw axis) and vertical (around pitch axis) directions in the camera coordinate system, amount of data (number of images or duration of video), and image resolution.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Gaze estimation errors in degrees on cross-dataset evaluations. The last column shows the average ranking on each test sets, and all other numbers are gaze estimation error in degrees.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the baseline with current state-of-the-art on within dataset evaluations. Numbers are gaze estimation errors in degrees.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.esperhq.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the participants of our dataset for their contributions, our reviewers for helping us improve the paper, and Jan Wezel for helping with the hardware setup. This project has received funding from the European Research Council (ERC) under the European Union???s Horizon 2020 research and innovation programme grant agreement No. StG-2016-717054.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Non-intrusive gaze tracking using artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="753" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High-quality singleshot capture of facial geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lightweight eye capture using a parametric model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>B?rard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Practical person-specific eye rigging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>B?rard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="441" to="454" />
			<date type="published" when="2019" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A coarse-to-fine adaptive network for appearance-based gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10623" to="10630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Prediction of intent in robotics and multi-agent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="158" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="743" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rt-gene: Real-time eye gaze estimation in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="334" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Eyediap: A database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Funes</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Monay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Eye Tracking Research &amp; Applications</title>
		<meeting>the ACM Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="255" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Photo-realistic monocular gaze redirection using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spurr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6932" to="6941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tabletgaze: dataset and analysis for unconstrained appearance-based gaze estimation in mobile tablets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="445" to="461" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A multiresolution 3d morphable face model and fitting framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mortazavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koppen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ratsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</title>
		<meeting>the 11th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human3.6m: large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gaze360: physically unconstrained gaze estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6912" to="6921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4873" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Eye tracking for everyone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krafka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2176" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A differential approach for gaze estimation with calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A F</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inferring human gaze from appearance via adaptive linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive linear regression for appearancebased gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2033" to="2046" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Eye tracking and eye-based human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Majaranta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in physiological computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="39" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Level playing field for million scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7044" to="7053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Few-shot adaptive gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9368" to="9377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep pictorial gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spurr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Medical physiology and biophysics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Ruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Fulton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academic Medicine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1067</biblScope>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A comparison and evaluation of multi-view stereo reconstruction algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2107" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gaze locking: passive eye contact detection for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual ACM symposium on User interface software and technology</title>
		<meeting>the 26th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Predicting primary gaze behavior using social saliency fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3503" to="3510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning-by-synthesis for appearance-based 3d gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1821" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A hierarchical generative model for eye image synthesis and eye gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="440" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generalizing eye tracking with bayesian adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11907" to="11916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A 3d morphable eye region model for gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="297" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning an appearance-based gaze estimator from one million synthesised images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Eye Tracking Research &amp; Applications</title>
		<meeting>the ACM Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving few-shot user-specific gaze adaptation via gaze redirection synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11937" to="11946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning for gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7314" to="7324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<title level="m">Humbi 1.0: Human multiview behavioral imaging dataset</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Revisiting data normalization for appearancebased gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Eye Tracking Research &amp; Applications</title>
		<meeting>the ACM Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mpiigaze: Real-world dataset and deep appearance-based gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="162" to="175" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Freihand: A dataset for markerless capture of hand pose and shape from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Argus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="813" to="822" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
