<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Remote Sensing Image Change Detection with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zipeng</forename><surname>Qi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
						</author>
						<title level="a" type="main">Remote Sensing Image Change Detection with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Change detection (CD)</term>
					<term>high-resolution optical remote sensing (RS) image</term>
					<term>transformers</term>
					<term>attention mechanism</term>
					<term>convolutional neural networks (CNNs)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern change detection (CD) has achieved remarkable success by the powerful discriminative ability of deep convolutions. However, high-resolution remote sensing CD remains challenging due to the complexity of objects in the scene. Objects with the same semantic concept may show distinct spectral characteristics at different times and spatial locations. Most recent CD pipelines using pure convolutions are still struggling to relate long-range concepts in space-time. Nonlocal self-attention approaches show promising performance via modeling dense relations among pixels, yet are computationally inefficient. Here, we propose a bitemporal image transformer (BIT) to efficiently and effectively model contexts within the spatial-temporal domain. Our intuition is that the high-level concepts of the change of interest can be represented by a few visual words, i.e., semantic tokens. To achieve this, we express the bitemporal image into a few tokens, and use a transformer encoder to model contexts in the compact tokenbased space-time. The learned context-rich tokens are then feedback to the pixel-space for refining the original features via a transformer decoder. We incorporate BIT in a deep feature differencing-based CD framework. Extensive experiments on three CD datasets demonstrate the effectiveness and efficiency of the proposed method. Notably, our BIT-based model significantly outperforms the purely convolutional baseline using only 3 times lower computational costs and model parameters. Based on a naive backbone (ResNet18) without sophisticated structures (e.g., FPN, UNet), our model surpasses several state-of-the-art CD methods, including better than four recent attention-based methods in terms of efficiency and accuracy. Our code is available at https://github.com/justchenhao/BIT CD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic CD technology can reduce abundant labor costs and time consumption, thus has raised increasing attention <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>.</p><p>The availability of high-resolution (HR) satellite data and aerial data is opening up new avenues for monitoring landcover and land-use at a fine scale. CD based on HR optical RS images remains a challenging task for two aspects: 1) complexity of the objects present in the scene, 2) different imaging conditions. Both contribute to the fact that the objects with the same semantic concept show distinct spectral characteristics at different times and different spatial locations (space-time). For example, as shown in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>, the building objects in a scene have varying shapes and appearance (in yellow boxes), and the same building object at different times may have distinct colors (in red boxes) due to illumination variations and appearance alteration. To identify the change of interest in the complex scene, a strong CD model needs to, 1) recognize high-level semantic information of the change of interest in a scene, 2) distinguish the real change from the complex irrelevant changes.</p><p>Nowadays, due to its powerful discriminative ability, deep Convolutional Neural Networks (CNN) have been successfully applied in RS image analysis and have shown good performance in the CD task <ref type="bibr" target="#b4">[5]</ref>. Most recent supervised CD methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> rely on a CNN-based structure to extract from each temporal image, high-level semantic features that reveal the change of interest.</p><p>Since context modeling within the spatial and temporal scope is critical to identify the change of interest in highresolution remote sensing images, the latest efforts have been focusing on increasing the reception field (RF) of the model, through stacking more convolution layers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, using dilated convolution <ref type="bibr" target="#b6">[7]</ref>, and applying attention mechanisms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. Different from the purely convolution-based approach that is inherently limited to the size of the RF, the attention-based approach (channel attention <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>, spatial attention <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, and self-attention <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref>) is effective in modeling global information. However, most existing methods are still struggling to relate long-range concepts in space-time, because they either apply attention separately to each temporal image for enhancing its features <ref type="bibr" target="#b8">[9]</ref>, or simply use attention to re-weight the fused bitemporal features/images in the channel or spatial dimension <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b13">14]</ref>. Some recent work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref> has achieved promising performance by utilizing self-attention to model the semantic relations between any pairs of pixels in space-time. However, they are computationally inefficient and need high computational complexity that grows quadratically with the number of pixels. To tackle the above challenge, in this work, we introduce the Bitemporal Image Transformer (BIT) to model longrange context within the bitemporal image in an efficient and effective manner. Our intuition is that the high-level concepts of the change of interest could be represented by a few visual words, i.e., semantic tokens. Instead of modeling dense relations among pixels in pixel-space, our BIT expresses the input images into a few high-level semantic tokens, and models the context in a compact token-based space-time. Moreover, we enhance the feature representation of the original pixelspace by leveraging relations between each pixel and semantic tokens. <ref type="figure" target="#fig_0">Fig 1 gives</ref> an example to show the effect of our BIT on image features. Given the original image features related to the building concept (see <ref type="figure" target="#fig_0">Fig 1 (b)</ref>), our BIT learns to further consistently highlight the building areas (see <ref type="figure" target="#fig_0">Fig 1 (c)</ref>) by considering the global contexts in space-time. Note that we show the differencing image between the enhanced features and the original features to better demonstrate the role of the proposed BIT.</p><p>We incorporate BIT in a deep feature differencing-based CD framework. The overall procedure of our BIT-based model is illustrated in <ref type="figure">Fig. 2</ref>. A CNN backbone (ResNet) is used to extract high-level semantic features from the input image pair. We employ spatial attention to convert each temporal feature map into a compact set of semantic tokens. Then we use a transformer <ref type="bibr" target="#b14">[15]</ref> encoder to model the context within the two token sets. The resulting context-rich tokens are reprojected to the pixel-space by a Siamese transformer decoder for enhancing the original pixel-level features. Finally, we compute the Feature Difference Images (FDI) from the two refined feature maps, and then fed them into a shallow CNN to produce pixel-level change predictions.</p><p>The contribution of our work can be summarised as follows:</p><p>? An efficient transformer-based method is proposed for remote sensing image change detection. We introduce transformers into the CD task to better model contexts within the bitemporal image, which benefits to identify the change of interest and exclude irrelevant changes. ? Instead of modeling dense relations among any pairs of elements in pixel-space, our BIT expresses the input images into a few visual words, i.e., tokens, and models the context in the compact token-based space-time. ? Extensive experiments on three CD datasets validate the effectiveness and efficiency of the proposed method. We replace the last convolutional stage of ResNet18 with BIT, and the resulting BIT-based model outperforms the purely convolutional counterpart with a significant margin using only 3 times lower computational costs and model parameters. Based on a naive CNN backbone without sophisticated structures (e.g., FPN, UNet), ours shows better performance in terms of efficiency and accuracy than several recent attention-based CD methods.</p><p>The rest of this paper is organized as follows. Section II describes the related work of deep learning-based CD methods and the recent transformer-based models in RS. Section III gives the details of our proposed method. Some experimental results are reported in section IV. The discussion is given in section V and conclusion is drawn in section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Learning based Remote Sensing Image Change detection</head><p>Deep learning-based supervised CD methods for optical RS images can be generally divided into two main streams <ref type="bibr" target="#b7">[8]</ref>.</p><p>One is the two-stage solution <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>, where a CNN/FCN is trained to separately classify the bitemporal images, and then their classification results are compared for change decision. This kind of approach is only practical when both the change label and the bitemporal semantic labels are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs</head><p>Transformer Encoder Another is the single-stage solution, which directly produces the change result from the bitemporal images. The patchlevel approach <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> models the CD task as a similarity detection process by grouping bitemporal images into pairs of patches and employing a CNN on each pair to obtain its center prediction. The pixel-level approach <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref> uses FCNs to directly generate a high-resolution change map from the two inputs, which is usually more efficient and effective than the patch-level approach. Since the CD task needs to handle two inputs, how to fuse the bitemporal information is an important topic. Existing FCN-based methods can be roughly divided into two groups according to the stage of fusion of bitemporal information. The image-level approach <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b28">29]</ref> concatenates the bitemporal images as a single input to a semantic segmentation network. The feature-level approach <ref type="bibr">[2, 6, 7, 9-12, 22, 25-28, 30]</ref> combines the bitemporal features extracted from the neural networks and makes change decisions based on fused features.</p><p>Much recent work aims to improve the feature discriminative power of the neural networks, by designing multi-level feature fusion structures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>, combining GAN-based optimization objectives <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>, and increasing the reception field (RF) of the model for better context modeling in terms of the spatial and temporal scope <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>.</p><p>Context modeling is critical to identify the change of interest in high-resolution remote sensing images due to the complexity of the objects in a scene and the variation of image conditions. To increase the RF size, existing methods include employing a deeper CNN model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, using dilated convolution <ref type="bibr" target="#b6">[7]</ref>, and applying attention mechanisms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. For example, Zhang et al. <ref type="bibr" target="#b6">[7]</ref> apply a deep CNN backbone (ResNet101 <ref type="bibr" target="#b31">[32]</ref>) to extract image features and use dilated convolution to enlarge the RF size of the model.</p><p>Considering that purely convolutional networks are inherently limited to the size of the RF for each pixel, many latest efforts are focusing on introducing attention mechanisms to further enlarge the RF of the model, such as channel attention <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>, spatial attention <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, self-attention <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref>. However, most of them still struggling to fully exploit the time-related context, because they either treat the attention as a feature enhancing module separately for each temporal image <ref type="bibr" target="#b8">[9]</ref>, or simply use attention to re-weight the fused bitemporal features/images in the channel or spatial dimension <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. Non-local self-attention <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> shows promising performance due to its ability to exploit global relations among pixels in space-time. However, they are computationally inefficient and need high computational complexity that grows quadratically with the number of pixels.</p><p>The main purpose of our paper is to learn and exploit the global semantic information within the bitemporal images in an efficient and effective manner for enhancing CD performance. Different from existing attention-based CD methods that directly model dense relations among any pairs of elements in pixel-based space, we extract a few semantic tokens from images and model the context in token-based space-time. The resulting context-rich tokens are then utilized to enhance the original features in pixel-space. Our intuition is that the change of interest within the scene can be described by a few visual words (tokens) and the high-level features of each pixel can be represented by the combination of these semantic tokens. As a result, our method exhibits high efficiency and high performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transformer-based Model</head><p>The transformer, firstly introduced in 2017 <ref type="bibr" target="#b14">[15]</ref>, has been widely used in the field of natural language processing (NLP) to solve sequence-to-sequence tasks while handling long-range dependencies with ease. A recent trend is the adoption of transformers in the computer vision (CV) field. Due to the strong representation ability of the transformer, transformerbased models show comparable or even better performance as the convolutional counterparts in various visual tasks, including image classification <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref>, segmentation <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>, object detection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>, image generation <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>, image captioning <ref type="bibr" target="#b41">[42]</ref>, and super-resolution <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>The astounding performance of transformer models on NLP/CV tasks has intrigued the remote sensing community to study their applications in remote sensing tasks, such as image time-series classification <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>, hyperspectral image classification <ref type="bibr" target="#b46">[47]</ref>, scene classification <ref type="bibr" target="#b47">[48]</ref>, and remote sensing image captioning <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>. For example, Li et al. <ref type="bibr" target="#b45">[46]</ref> proposed a CNN-transformer approach to perform the crop classification of time-series images, where the transformer was used to learn the pattern related to land cover semantics from the sequence of multitemporal features extracted via CNN. He et al. <ref type="bibr" target="#b46">[47]</ref> applied a variant of the transformer (BERT <ref type="bibr" target="#b50">[51]</ref>) to capture global dependencies among pixels in hyperspectral image classification. Moreover, Wang et al. <ref type="bibr" target="#b49">[50]</ref> employed the transformer to translate the disordered words extracted by CNN from the given RS image into a well-formed sentence.</p><p>In this paper, we explore the potential of transformers in the binary CD task. Our proposed BIT-based method is efficient and effective in modeling global semantic relations in spacetime to benefit the feature representation of the change of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EFFICIENT TRANSFORMER BASED CHANGE DETECTION MODEL</head><p>The overall procedure of our BIT-based model is illustrated in <ref type="figure">Fig. 2</ref>. We incorporate the BIT into a normal change detection pipeline because we want to leverage the strengths of both convolutions and transformers. Our model starts with several convolution blocks to obtain the feature map for each input image, then fed them into BIT to generate enhanced bitemporal features. Finally, the resulting feature maps are fed to a prediction head to produce pixel-level predictions. Our key insight is that BIT learns and relates the global context of high-level semantic concepts, and feedback to benefit the original bitemporal features.</p><p>Our BIT has three main components: 1) a Siamese semantic tokenizer, which groups pixels into concepts to generate a compact set of semantic tokens for each temporal input, 2) a transformer encoder, which models context of semantic concepts in token-based space-time, and 3) a Siamese transformer decoder, which projects the corresponding semantic tokens back to pixel-space to obtain the refined feature map for each temporal.</p><p>The inference detail of our BIT-based model for change detection is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic Tokenizer</head><p>Our intuition is that the change of interest in input images could be described by a few high-level concepts, namely semantic tokens. And the semantic concepts can be shared by the bitemporal images. To this end, we employ a Siamese tokenizer to extract compact semantic tokens from the feature map of each temporal. Similar to the tokenizer in NLP, which splits the input sentence into several elements (i.e., word or phrase) and represents each element with a token vector, our semantic tokenizer splits the whole image into a few visual words, each corresponds to one token vector. As shown in <ref type="figure">Fig. 3</ref>, to obtain the compact tokens, our tokenizer learns a set of spatial attention maps to spatially pool the feature map to a set of features, i.e., the token set. Let X 1 , X 2 ? R HW ?C be the input bitemporal feature maps, where H, W, C is height, width, and channel dimension of the feature map. Let T 1 , T 2 ? R L?C be the two sets of tokens, where L is the size of the vocabulary set of tokens.</p><p>For each pixel X i p on the feature map X i (i = 1, 2), we use a point-wise convolution to obtain L semantic groups, each group denotes one semantic concept. Then we compute spatial attention maps by a softmax function operated on the HW dimension of each semantic group. Finally, we use the attention maps to compute the weighted average sum of pixels in X i to obtain a compact vocabulary set of size L, i.e., semantic tokens T i . Formally,</p><formula xml:id="formula_0">T i = (A i ) T X i = (?(?(X i ; W))) T X i ,<label>(1)</label></formula><p>where ?(?) denotes the point-wise convolution with a learnable kernel W ? R C?L , ?(?) is the softmax function to normalize each semantic group to obtain the attention maps</p><formula xml:id="formula_1">A i ? R HW ?L . T i is computed by the multiplication of A i and X i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transformer Encoder</head><p>After obtaining two semantic token sets T 1 , T 2 for the input bitemporal image, we then model the context between these tokens with a transformer encoder <ref type="bibr" target="#b14">[15]</ref>. Our motivation is that the global semantic relations in the token-based space-time can be fully exploited by the transformer, thus producing contextrich token representation for each temporal. As shown in <ref type="figure" target="#fig_2">Fig.  4</ref> (a), we first concatenate the two sets of tokens into one token set T ? R 2L?C , and fed it into the transformer encoder to obtain a new token set T new . Finally, we split the tokens into two sets T i new (i = 1, 2). The transformer encoder consists of N E layers of multihead self-attention (MSA) and multilayer perceptron (MLP) blocks ( <ref type="figure" target="#fig_2">Fig. 4 (a)</ref>). Different from the original transformer that uses the post-norm residual unit, we follow ViT <ref type="bibr" target="#b32">[33]</ref> to adopt the pre-norm residual unit (PreNorm), i.e., the layer normalization occurs immediately before the MSA/MLP. PreNorm has been shown more stable and competent than the counterpart <ref type="bibr" target="#b51">[52]</ref>.</p><p>At each layer l, the input to self-attention is a triple (query Q, key K, value V) computed from the input T (l?1) ? R 2L?C as:</p><formula xml:id="formula_2">Q = T (l?1) W q , K = T (l?1) W k , V = T (l?1) W v ,<label>(2)</label></formula><p>where W l?1 q , W l?1 k , W l?1 v ? R C?d are the learnable parameters of three linear projection layers and d is the channel dimension of the triple. One attention head is formulated as:</p><formula xml:id="formula_3">Att(Q, K, V) = ? QK T ? d V,<label>(3)</label></formula><p>where ?(?) denotes the softmax function operated on the channel dimension. The core idea of the transformer encoder is multi-head selfattention. MSA performs multiple independent attention heads in parallel, and the outputs are concatenated and then projected to result in the final values. The advantage of MSA is that it can jointly attend to information from different representation subspaces at different positions. Formally,</p><formula xml:id="formula_4">MSA(T (l?1) ) = Concat(head 1 , .., head h )W O , where head j = Att(T (l?1) W q j , T (l?1) W k j , T (l?1) W v j ),<label>(4)</label></formula><formula xml:id="formula_5">where W q j , W k j , W v j ? R C?d , W O ? R hd?C are the linear projection matrices, h is the number of attention heads.</formula><p>The MLP block consists of two linear transformation layers with a GELU <ref type="bibr" target="#b52">[53]</ref> activation in between. The dimensionality of input and output is C, and the inner-layer has dimensionality 2C. Formally,</p><formula xml:id="formula_6">MLP(T (l?1) ) = GELU(T (l?1) W 1 )W 2 (5) where W 1 ? R C?2C , W 2 ? R 2C?C are the linear projection matrices.</formula><p>Note that we add the learnable positional embedding (PE) W P E ? R 2L?C to the token sequence T before feeding it to the transformer layers. Our empirical evidence (Sec. IV-D) indicates it is necessary to supplement PE to tokens. PE encodes the information about the relative or absolute position of elements in the token-based space-time. Such position information may benefit context modeling. For example, temporal positional information can guide transformers to exploit temporal-related contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Transformer Decoder</head><p>Till now, we have obtained two sets of context-rich tokens T i new (i = 1, 2) for each temporal image. These contextrich tokens contain compact high-level semantic information that well reveals the change of interest. Now, we need to project the representation of concepts back to pixel-space to obtain pixel-level features. To achieve this, we use a modified Siamese transformer decoder <ref type="bibr" target="#b14">[15]</ref> to refine image features of each temporal. As shown in <ref type="figure" target="#fig_2">Fig. 4 (b)</ref>, given a sequence of features X i , the transformer decoder exploits the relation between each pixel and the token set T i new to obtain refined features X i new . We treat pixels in X i as queries and tokens as keys. Our intuition is that each pixel can be represented by the combination of the compact semantic tokens.</p><p>Our transformer decoder consists of N D layers of multihead cross attention (MA) and MLP blocks. Different from the original implementation in <ref type="bibr" target="#b14">[15]</ref>, we remove the MSA block to avoid abundant computation of dense relations among pixels <ref type="bibr" target="#b5">6</ref> in X i . We adopt PerNorm and the same configuration of MLP as the transformer encoder. In MSA, the query, key, and value are derived from the same input sequence, while in MA, the query is from the image features X i , and the key and value are from the tokens T i new . Formally, at each layer l, MA is defined as:</p><formula xml:id="formula_7">MA(X i,(l?1) , T i new ) = Concat(head 1 , ..., head h )W O , where head j = Att(X i,(l?1) W q j , T i new W k j , T i new W v j ),<label>(6)</label></formula><p>where W q j , W k j , W v j ? R C?d , W O ? R hd?C are the linear projection matrices, h is the number of attention heads.</p><p>Note that we do not add PE to the input queries, because our empirical evidence (Sec. IV-D) shows no considerable gains when adding PE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network Details</head><p>CNN backbone. We use a modified ResNet18 <ref type="bibr" target="#b31">[32]</ref> to extract bitemporal image feature maps. The original ResNet18 has 5 stages, each with downsampling by 2. We replace the stride of the last two stages to 1 and add a point-wise convolution (output channel C = 32) behind ResNet to reduce the feature dimension, followed by a bilinear interpolation layer, thus obtaining the output feature maps with a downsampling factor of 4 to reduce the loss of spatial details. We name this backbone ResNet18 S5. To validate the effectiveness of the proposed method, we also use two lighter backbone, namely ResNet18 S4/ResNet18 S3, which only uses the first four/three stages of the ResNet18.</p><p>Bitemporal image transformer. According to parameter experiments in Sec. IV-E, we set token length L = 4. We set the layer numbers of the transformer encoder to 1 and that of the transformer decoder to 8. The number of heads h in MSA and MA is set to 8 and the channel dimension d for each head is set to 8.</p><p>Prediction head. Benefiting from the high-level semantic features extracted by CNN backbone and BIT, a very shallow FCN is employed for change discrimination. Given two upsampled feature maps X 1 * , X 2 * ? R H0?W0?C from the output of BIT (H 0 , W 0 is the height, width of the original image, respectively), the prediction head is to generate the predicted change probability maps P ? R H0?W0?2 , which is given by</p><formula xml:id="formula_8">P = ?(g(D)) = ?(g(|X 1 * ? X 2 * |)),<label>(7)</label></formula><p>where Feature Difference Images (FDI) D ? R H0?W0?C is the element-wise absolute of the subtraction of the two feature maps, g : R H0?W0?C ? R H0?W0?2 is the change classifier and ?(?) denotes a softmax function pixel-wisely operated on the channel dimension of the output of the classifier. The configuration of our change classifier is two 3?3 convolutional layers with BatchNorm. The output channel of each convolution is "32, 2". In the inference phase, the prediction mask M ? R H0?W0 is computed by a pixel-wise Argmax operation on the channel dimension of P .</p><p>Loss function. In the training stage, we minimize the crossentropy loss to optimize the network parameters. Formally, the loss function is defined as:</p><formula xml:id="formula_9">L = 1 H 0 ? W 0 H,W h=1,w=1 l(P hw , Y hw ),<label>(8)</label></formula><p>where l(P hw , y) = ?log(P hwy ) is the cross-entropy loss, and Y hw is the label for the pixel at location (h, w).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental setup</head><p>We conduct experiments on three change detection datasets. LEVIR-CD <ref type="bibr" target="#b1">[2]</ref> is a public large scale building CD dataset. It contains 637 pairs of high-resolution (0.5m) RS images of size 1024 ? 1024. We follow its default dataset split (training/validation/test). For the limitation of GPU memory capacity, we cut images into small patches of size 256 ? 256 with no overlap. Therefore, we obtain 7120/1024/2048 pairs of patches for training/validation/test respectively.</p><p>WHU-CD [54] is a public building CD dataset. It contains one pair of high-resolution (0.075m) aerial images of size 32507 ? 15354. As no data split solution is provided in <ref type="bibr" target="#b53">[54]</ref>, we crop the images into small patches of size 256 ? 256 with no overlap and randomly split it into three parts: 6096/762/762 for training/validation/test respectively. DSIFN-CD [10] is a public binary CD dataset. It includes six large pairs of high-resolution (2m) satellite images from six major cities in China, respectively. The dataset contains the change of multiple kinds of land-cover objects, such as roads, buildings, croplands, and water bodies. We follow the default cropped samples of size 512 ? 512 provided by the authors. We have 3600/340/48 samples for training/validation/test respectively.</p><p>To validate the effectiveness of our BIT-based model, we set the following models for comparison:</p><p>? Base: our baseline model that consists of the CNN backbone (ResNet18 S5) and the prediction head. ? BIT: our BIT-based model with a light backbone (ResNet18 S4). To further evaluate the efficiency of the proposed method, we additionally set the following models:</p><p>? Base S4: a light CNN backbone (ResNet18 S4) + the prediction head. ? Base S3: a much light CNN backbone (ResNet18 S3) + the prediction head. ? BIT S3: our BIT-based model with a much light backbone (ResNet18 S3). Implementation details. Our models are implemented on PyTorch and trained using a single NVIDIA Tesla V100 GPU. We apply normal data augmentation to the input image patches, including flip, rescale, crop, and gaussian blur. We use stochastic gradient descent (SGD) with momentum to optimize the model. We set the momentum to 0.99 and the weight decay to 0.0005. The learning rate is initially set to 0.01 and linearly decay to 0 until trained 200 epochs. Validation is performed after each training epoch, and the best model on the validation set is used for evaluation on the test set.</p><p>Evaluation Metrics. We use the F1-score with regard to the change category as the main evaluation indices. F1-score is calculated by the precision and recall of the test as follows:</p><formula xml:id="formula_10">F 1 = 2 recall ?1 + precision ?1 ,<label>(9)</label></formula><p>Additionally, precision, recall, Intersection over Union (IoU) of the change category, and overall accuracy (OA) are also reported. The above metrics are defined as follows: </p><p>where TP, FP, FN represent the number of true positive, false positive, and false negative respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison to state-of-the-art</head><p>We make a comparison with several state-of-the-art methods, including three purely convolutional-based methods (FC-EF <ref type="bibr" target="#b21">[22]</ref>, FC-Siam-Di <ref type="bibr" target="#b21">[22]</ref>, FC-Siam-Conc <ref type="bibr" target="#b21">[22]</ref>) and four attention-based methods (DTCDSCN <ref type="bibr" target="#b8">[9]</ref>, STANet <ref type="bibr" target="#b1">[2]</ref>, IFNet <ref type="bibr" target="#b9">[10]</ref> and SNUNet <ref type="bibr" target="#b13">[14]</ref>).</p><p>? FC-EF <ref type="bibr" target="#b21">[22]</ref>: Image-level fusion method, where the bitemporal images are concatenated as a single input to a fully convolutional network. ? FC-Siam-Di <ref type="bibr" target="#b21">[22]</ref>: Feature-level fusion method, which employs a Siamese FCN to extract multi-level features and use feature difference to fuse the bitemporal information. ? FC-Siam-Conc <ref type="bibr" target="#b21">[22]</ref>: Feature-level fusion method, which employs a Siamese FCN to extract multi-level features and use feature concatenation to fuse the bitemporal information. ? DTCDSCN <ref type="bibr" target="#b8">[9]</ref>: Multi-scale feature concatenation method, which adds channel attention and spatial attention to a deep Siamese FCN, thus obtaining more discriminative features. Note that they also trained two additional semantic segmentation decoders under the supervision of the label maps of each temporal. We omit the semantic segmentation decoders for a fair comparison. Tab. I reports the overall comparison results on LEVIR-CD, WHU-CD and DSIFN-CD test sets. The quantitative results show our BIT-based model consistently outperforms the other methods across these datasets with a significant margin. For example, the F1-score of our BIT exceeds the recent STANet by 2/1.6/4.7 points on the three datasets, respectively. Please note that our CNN backbone is only the pure ResNet and we do not apply the sophisticated structures such as FPN in <ref type="bibr" target="#b1">[2]</ref> or UNet in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>, which are powerful for pixelwise prediction tasks by fusing the low-level features with high spatial accuracy and high-level semantic features. We can conclude that even using a simple backbone, our BIT-based model can achieve superior performance. It may attribute to the ability of our BIT to model the context within the global highly abstract spatial-temporal scope and utilize the context for enhancing the feature representation in pixel-space.</p><p>The visualization comparison of the methods on the three datasets is displayed in <ref type="figure" target="#fig_4">Fig. 5</ref>. For a better view, different colors are used to denote TP (white), TN (black), FP (red), FN (green). We can observe that the BIT-based model achieves better results than others. First, our BIT-based model can better avoid the false positive (e.g., <ref type="figure" target="#fig_4">Fig 5 (a)</ref>, (e), (g), (i)) due to the similar appearance of the object as that of the interest change. For example, as shown in <ref type="figure" target="#fig_4">Fig. 5 (a)</ref>, most comparison methods incorrectly classify the swimming pool area as the building change (view as red), while based on the enhanced discriminant features via global context modeling, the STANet and our BIT can reduce such false detection. In <ref type="figure" target="#fig_4">Fig. 5 (c)</ref>, the roads are mistaken as building changes by conventional methods because the roads have similar color behaviors as buildings and these methods fail to exclude these pseudo changes due to their limited reception field. Second, our BIT can also well handle the irrelevant changes caused by seasonal differences or appearance alteration of land-cover elements (e.g., <ref type="figure" target="#fig_4">Fig 5 (b)</ref>, (f) and (l)). An example of the non-semantic change of building in <ref type="figure" target="#fig_4">Fig 5 (f)</ref> illustrates the effectiveness of our BIT that learns the effective context within the spatial-temporal domain to better express the real semantic change and exclude the irrelevant change. Lastly, our BIT can generate relatively intact prediction results (e.g., <ref type="figure" target="#fig_4">Fig 5 (c)</ref>, (h) and (j)) for large areas of change. For instance, in <ref type="figure" target="#fig_4">Fig. 5 (j)</ref>, the large building area in image 2 can not be detected entirely (view as green) by some comparison methods due to their limited reception field, while our BIT-based model renders more complete results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model efficiency and effectiveness</head><p>To fairly compare the model efficiency, we test all the methods on a computing server equipped with an Intel Xeon Silver 4214 CPU and an NVIDIA Tesla V100 GPU. Tab. II reports the number of parameters (Params.), floating-point operations per second (FLOPs), and F1/IoU scores of different methods on LEVIR-CD, WHU-CD, and DSIFN-CD test sets.  First, we verify the efficiency of our proposed BIT by comparing the convolutional counterparts. Tab. II shows that built on Base S3/Base S4, the model added the BIT (BIT S3/BIT S4) is more effective and efficient than that (Base S4/Base S5) with more convolutional layers. For example, BIT S4 outperforms the Base S5 by 1.7/2.4/10.8 points of the F1-score on the three test sets while with 3 times smaller numbers of model parameters and 3 times lower computational costs. Moreover, we can observe that compared to Base S4, adding more convolutional layers only introduce trivial improvements (i.e., 0.16/0.75/0.18 points of the F1score on the three test sets) while the improvements by BIT is much more (i.e., 4?60 times) than that of the CNN.</p><p>Second, we make a comparison with four attention-based methods (DTCDSCN, STANet, IFNet and SNUNet). As shown in Tab. II, our BIT S4 outperforms the four counterparts in the F1/IoU scores with a significant margin with much small computational complexity and model parameters. Interestingly, even with a much lighter backbone (about 10 times smaller), our BIT-based model (BIT S3) is still superior to the four compared methods on most datasets. The comparison results further prove the effectiveness and efficiency of our BIT-based model.</p><p>Training visualization. <ref type="figure" target="#fig_6">Fig. 6</ref> illustrates the mean F1-score on the training/validation sets for each training epoch. We can observe that although the Base and BIT models have similar performance on training accuracy, BIT outperforms Base with regard to the validation accuracy in terms of stability and effectiveness. It indicates that the training of BIT is more stable and efficient, and our BIT-based model has more generalization ability. It may due to its ability to learn compact context-rich concepts, which effectively represent the change of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation studies</head><p>Context modeling. We perform ablation on the Transformer Encoder (TE) to validate its effectiveness in context modeling, where multi-head self-attention is the core component in TE for modeling context. From Tab. III, we can observe consistent and significant drops in F1-score on the LEVIR-CD, WHU-CD, and DSIFN-CD datasets when removing TE from BIT. It indicates the vital importance of self-attention in TE to model  relations within token-based space-time. Moreover, we replace our BIT with a Non-local <ref type="bibr" target="#b55">[56]</ref> self-attention layer, which is able to model relations within the pixel-based space-time. The comparison results in Tab. III show our BIT outperforms Nonlocal on the three test sets with a significant margin. It may because our BIT learns the context in a tokens-based space, which is more compact and has higher information density than that of Non-local, thus facilitating the effective extraction of relations. Ablation on tokenizer. We perform ablation on the tokenizer by removing it from the BIT. The resulting model can be considered to use dense tokens, which are sequences of features extracted by the CNN backbone. As shown in Tab. III, the BIT-based model (w.o. tokenizer) receives significant drops in the F1-score. It indicates that the tokenizer module is critical in our transformer-based framework. We can see that the model (w.o. tokenizer) only slightly better than Base S4. It may because that the dense features contain too much redundancy information that makes the training of the transformer-based model a tough task. On the contrary, our proposed tokenizer spatially pool the dense features to aggregate the semantic information, thus obtaining compact tokens of concepts.</p><p>Ablation on transformer decoder. To verify the effectiveness of our Transformer Decoder (TD), we replace it with a simple module to fuse the tokens T i new from TE and the original features X i from the CNN backbone. In the simple module, we expand the spatial dimension of each token in T i new (containing L tokens) to a shape of R HW . And the L expanded tokens and X i are summed to produce the updated features that are then fed to the prediction head. Tab. III indicates consistent performance declines of the BIT model without TD on the three test sets. It may because crossattention (the core part of TD) provides an elegant way to enhance the original features with the context-rich tokens by modeling their relations. Furthermore, the BIT (w.o. both TE and TD) is much inferior to the normal BIT model.</p><p>Effect of position embedding. The Transformer architecture is permutation-invariant, while the CD task requires both spatial and temporal position information. To this end, we add the learned position embedding (PE) to the feature sequence fed to the transformer. We perform ablations on PE in TE and TD. We set the BIT model containing no PE as the baseline. As shown in Tab. IV, our BIT model achieves consistent improvements in the F1-score on the three test sets when adding PE to the tokens fed into TE. It indicates that the position information within the bitemporal token sets is critical for context modeling in TE. Compared to the baseline, there are no significant improvements in the F1-score to the BIT model when adding PE to queries fed into TD. The positional information may be unnecessary to the queries into TD because the keys (i.e., tokens) into TD are highly abstract and contain no spatial structure. Therefore, we only add PE in TE, but not in TD in our BIT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Parameter analysis</head><p>Token length. Our tokenizer spatially pools the dense features of the image into a compact token set. Our intuition is that the change of interest within the bitemporal images can be described by a few visual concepts, i.e., semantic tokens. The length of the token set L is an important hyperparameter. We test different L ? {2, 4, 8, 16, 32} to analyze its effect on the performance of our model on the LEVIR-CD, WHU-CD, and DSIFN-CD dataset, respectively. Tab. V shows a significant improvement in the F1-score of the model when reducing the token length from 32 to 4. It indicates that a compact token set is sufficient to denote semantic concepts of interest changes and redundant tokens may hinder the model performance. We can also observe a slight drop in F1-score when further decreasing L from 4 to 2. It is because the model may lose some useful information related to change concepts when L is too short. Therefore, we set L to 4.</p><p>Depth of transformer. The number of transformer layers is one important hyperparameter. We test different configurations of the BIT model that contains varying numbers of transformer layers in TE and TD. Tab. VI shows no significant improvements to the F1/IoU scores of BIT on the three datasets when increasing the depth of the transformer encoder. It indicates that relations between the bitemporal tokens can be well learned by a single layer TE. Tab. VI also shows the model performance is roughly positively correlated with the decoder depth. It may because image features are refined after each layer of the transformer decoder by considering the contextrich tokens. The best result is obtained when the decoder depth is 8. Although there may be performance gains by further increasing the decoder depth, for the tradeoff between efficiency and precision, we set the encoder depth to 1 and the decoder depth to 8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Token visualization</head><p>We hypothesize that our tokenizer can extract high-level semantic concepts that reveal the change of interest. For better understanding the semantic tokens, we visualize the attention maps A i ? R HW ?L that the tokenizer extracted from the bitemporal feature maps. Each token T i l in the token set T i is corresponding to one attention map A i l ? R HW . <ref type="figure" target="#fig_7">Fig. 7</ref> shows the visualization results of tokens for some bitemporal images from the LEVIR-CD, WHU-CD, and DSIFN-CD datasets. We display the attention maps of two selected tokens from T i for each input image. Red denotes higher attention values and blue denotes lower values.</p><p>From <ref type="figure" target="#fig_7">Fig. 7</ref>, we can see that the extracted token can attend to the region that belongs to the semantic concept of the change of interest. Different tokens may relate to objects of different semantic meanings. For example, as the LEVIR-CD and WHU-CD datasets only describe the building changes, the learned tokens in these datasets mainly attend to the pixels belongs to buildings. While because the DSIFN-CD dataset   <ref type="figure">Fig. 8</ref>. An example of network visualization. (a) input images, (b) selected high-level feature maps X i , (c) selected attention maps A i by tokenizer, (d) refined feature maps X i new , (e) differencing between X i new and X i , (f) bitemporal feature differencing image, (g) change probability map P. The sample is from the LEVIR-CD data set. We use the same normalization (min-max) to visualize each activation map. contains various kinds of changes, these tokens can highlight different semantic areas, such as buildings, croplands, and water bodies. Interestingly, as shown in <ref type="figure" target="#fig_7">Fig. 7</ref> (c) and (f), our tokenizer can also highlight the pixels surrounding the building (e.g., shadow), even though no explicit supervision of such areas is provided when training our model. It is not surprising because the context surrounding the building is a critical cue for object recognition. It indicates that our model can implicitly learn some additional concepts to promote change recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Network visualization</head><p>To better understand our model, we provide an example to visualize the activation maps at different stages of the BIT model. Given the bitemporal image ( <ref type="figure">Fig. 8 (a)</ref>), a Siamese FCN generates the high-level feature maps X i <ref type="figure">(Fig. 8 (b)</ref>). Then the tokenizer spatially pools the feature maps into several token vectors using the learned attention maps A i <ref type="figure">(Fig. 8 (c)</ref>). The context-rich tokens generated by the transformer encoder are then projected back to the pixel-space via the transformer decoder, resulting in the refined feature maps X i new ( <ref type="figure">Fig. 8  (d)</ref>). We show four corresponding representative feature maps from the original features X i , and from the refined features X i new . From <ref type="figure">Fig. 8 (b)</ref> and (d), we can observe that our model can extract high-level features related to the change of interest for each temporal image, such as concepts of buildings and their edges. To better illustrate the effect of the BIT module, the differencing images between the refined and the original features are shown in <ref type="figure">Fig. 8 (e</ref>). It indicates that our BIT can further highlight the regions of semantic concepts related to the change category. Lastly, the prediction head calculates feature differencing images ( <ref type="figure">Fig. 8 (f)</ref>) between X i new and X i , and generates the change probability map P ( <ref type="figure">Fig. 8 (g)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>We provide an efficient and effective method to perform change detection in high-resolution remote sensing images. The high reflectance variation for pixels of the same category in whole space-time brings difficulties to the model in recognizing objects of interest and distinguishing real changes from irrelevant changes. Context modeling in space-time is critical for enhancing feature discrimination power. Our proposed BIT module can efficiently model the context information in the token-based space-time and use the context-rich tokens to enhance the original features. Compared to the Base model, our BIT-base model can generate more accurate predictions with fewer false alarms and higher recalls (see <ref type="figure" target="#fig_4">Fig. 5</ref> and <ref type="table" target="#tab_2">Table I</ref>). Furthermore, the BIT can enhance the efficiency and stability of the training of the model (see <ref type="figure" target="#fig_6">Fig. 6</ref>). It is because that our BIT expresses the images into a small number of visual words (token vectors), such high-density information may improve the training efficiency. Our BIT can also be viewed as an efficient attention-base way to increase the reception field of the model, thus benefit feature representation power for change recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose an efficient transformer-based model for change detection in remote sensing images. Our BIT learns a compact set of tokens to represent high-level concepts that reveal the change of interest existing in the bitemporal images. We leverage the transformer to relate semantic concepts in the token-based space-time. Extensive experiments have validated the effectiveness of our method. We replace the last convolutional stage of ResNet18 with BIT, obtaining significant accuracy improvements (1.7/2.4/10.8 points of the F1-score on the LEVIR-CD/WHU-CD/DSIFN-CD test sets) with 3 times lower computational complexity and 3 times smaller model parameters. Our empirical evidence indicates BIT is more efficient and effective than purely convolutional modules. Only using a simple CNN backbone (ResNet18), our method outperforms several other CD methods that employ more sophisticated structures, such as FPN and UNet. We also show better performance in terms of efficiency and accuracy than four recent attention-based methods on the three CD datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>complexity of objects within a scene in bitemporal high-resolution images (above: image 1, below: image 2Illustration of the necessity of context modeling and the effect of our BIT module. (a) An example of a complex scene in bitemporal high-resolution images. Building objects show different spectral characteristics at different times (red boxes) and different spatial locations (yellow boxes). A strong building CD model needs to recognize the building objects and distinguish real changes from irrelevant changes by leveraging context information. Based on the high-level image features (b), our BIT module exploits global contexts in space-time to enhance the original features. The differencing image (c) between the enhanced features and the original one shows the consistent improvement in features of building areas across space-time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 : 3 X 8 TFig. 3 .</head><label>1383</label><figDesc>Inference of BIT-based Model for Change Detection. Input: I = {(I 1 , I 2 )} (a pair of registered images) Output: M (a prediction change mask) 1 // step1: extract high-level features by a CNN backbone 2 for i in {1, 2} do i = CNN Backbone(I i ) 4 end 5 // step2: use BIT to refine bitemporal image features 6 // compute the token set for each temporal feature 7 for i in {1, 2} do i = Semantic Tokenizer(X i ) 9 end 10 T=Concat(T 1 , T 2 ) 11 // use encoder to generate context-rich tokens 12 T new =Transformer Encoder(T) 13 T 1 new , T 2 new =Split(T new ) 14 // use decoder to refine the original features 15 for i in {1, 2} do 16 X i new = Transformer Decoder(X i , T i new ) 17 end 18 // step3: obtain change mask by the prediction head 19 M = Prediction Head(X 1 new , X 2 new ) Illustration of our semantic tokenizer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of our transformer encoder and transformer decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>precision = TP / (TP + FP) recall = TP / (TP+FN) IoU = TP / (TP+FN+FP) OA = (TP+TN) / (TP+TN+FN+FP)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization results of different methods on the LEVIR-CD, WHU-CD, and DSIFN-CD test sets. Different colors are used for a better view, i.e., white for true positive, black for true negative, red for false positive, and green for false negative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Validation accuracy on LEVIR-CD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Accuracy of models for each training epoch. The mean F1-score is reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Token visualization on the LEVIR-CD, WHU-CD, and DSIFN-CD test sets. Red denotes higher attention values and blue denotes lower values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Deep supervision is also employed to enhance the discrimination ability of intermediate features. We implement the above CD networks using their public codes with default hyperparameters.</figDesc><table><row><cell>? STANet [2]: Metric-based Siamese FCN based method,</cell></row><row><cell>which integrates the spatial-temporal attention mecha-</cell></row><row><cell>nism to obtain more discriminative features.</cell></row><row><cell>? IFNet [10]: Multi-scale feature concatenation method,</cell></row><row><cell>which applies channel attention and spatial attention to</cell></row><row><cell>the concatenated bitemporal features at each level of the</cell></row></table><note>decoder. Deep supervision (i.e., computing supervised loss at each level of the decoder) is used to better train the intermediate layers.? SNUNet [14]: Multi-scale feature concatenation method, which combines the Siamese network and NestedUNet[55] to extract high-resolution high-level features. Channel attention is applied to the features at each level of the decoder.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I COMPARISON</head><label>I</label><figDesc>RESULTS ON THE THREE CD TEST SETS. THE HIGHEST SCORE IS MARKED IN BOLD. ALL THE SCORES ARE DESCRIBED IN PERCENTAGE (%). Rec. / F1 / IoU / OA Pre. / Rec. / F1 / IoU / OA Pre. / Rec. / F1 / IoU / OA FC-EF [22] 86.91 / 80.17 / 83.40 / 71.53 / 98.39 71.63 / 67.25 / 69.37 / 53.11 / 97.61 72.61 / 52.73 / 61.09 / 43.98 / 88.59 89.31 / 80.68 / 98.92 86.64 / 81.48 / 83.98 / 72.39 / 98.75 68.36 / 70.18 / 69.26 / 52.97 / 89.41</figDesc><table><row><cell></cell><cell>LEVIR-CD</cell><cell>WHU-CD</cell><cell>DSIFN-CD</cell></row><row><cell cols="2">Pre. / FC-Siam-Di [22] 89.53 / 83.31 / 86.31 / 75.92 / 98.67</cell><cell>47.33 / 77.66 / 58.81 / 41.66 / 95.63</cell><cell>59.67 / 65.71 / 62.54 / 45.50 / 86.63</cell></row><row><cell>FC-Siam-Conc [22]</cell><cell>91.99 / 76.77 / 83.69 / 71.96 / 98.49</cell><cell>60.88 / 73.58 / 66.63 / 49.95 / 97.04</cell><cell>66.45 / 54.21 / 59.71 / 42.56 / 87.57</cell></row><row><cell>DTCDSCN [9]</cell><cell>88.53 / 86.83 / 87.67 / 78.05 / 98.77</cell><cell>63.92 / 82.30 / 71.95 / 56.19 / 97.42</cell><cell>53.87 / 77.99 / 63.72 / 46.76 / 84.91</cell></row><row><cell>STANet [2]</cell><cell>83.81 / 91.00 / 87.26 / 77.40 / 98.66</cell><cell cols="2">79.37 / 85.50 / 82.32 / 69.95 / 98.52 67.71 / 61.68 / 64.56 / 47.66 / 88.49</cell></row><row><cell>IFNet [10]</cell><cell>94.02 / 82.93 / 88.13 / 78.77 / 98.87</cell><cell>96.91 / 73.19 / 83.40 / 71.52 / 98.83</cell><cell>67.86 / 53.94 / 60.10 / 42.96 / 87.83</cell></row><row><cell>SNUNet [14]</cell><cell>89.18 / 87.17 / 88.16 / 78.83 / 98.82</cell><cell>85.60 / 81.49 / 83.50 / 71.67 / 98.71</cell><cell>60.60 / 72.89 / 66.18 / 49.45 / 87.34</cell></row><row><cell>Base</cell><cell>88.24 / 86.91 / 87.57 / 77.89 / 98.76</cell><cell>81.80 / 81.42 / 81.61 / 68.93 / 98.53</cell><cell>73.30 / 48.65 / 58.48 / 41.32 / 88.26</cell></row><row><cell>BIT</cell><cell>89.24 / 89.37 /</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II ABLATION</head><label>II</label><figDesc>STUDY ON MODEL EFFICIENCY. WE REPORT THE NUMBER OF PARAMETERS (PARAMS.), FLOATING-POINT OPERATIONS PER SECOND (FLOPS), AS WELL AS THE F1 AND IOU SCORES ON THE THREE CD TEST SETS. THE INPUT IMAGE TO THE MODEL HAS A RESIZE OF 256 ? 256 ? 3 TO CALCULATE THE FLOPS. PE) ON THREE CD DATASETS. WE PERFORM ABLATIONS ON PE IN TRANSFORMER ENCODER (TE) AND TRANSFORMER DECODER (TD). THE F1-SCORE IS REPORTED. NOTE THAT THE DEPTH OF TE AND TD ARE SET TO 1. EFFECT OF THE TOKEN LENGTH. THE F1/IOU SCORES OF THE BIT ARE EVALUATED ON THE LEVIR-CD, WHU-CD, AND DSIFN-CD TEST SETS. NOTE THAT THE DEPTH OF TE AND TD ARE SET TO 1. THE DEPTH OF THE TRANSFORMER. WE PERFORM ANALYSIS ON THE ENCODER DEPTH (E.D.) AND DECODER DEPTH (D.D.) OF THE BIT, AND REPORT THE F1/IOU SCORES FOR EACH CONFIGURATION ON THE LEVIR-CD, WHU-CD, AND DSIFN-CD TEST SETS.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LEVIR-CD</cell><cell cols="2">WHU-CD</cell><cell></cell><cell cols="2">DSIFN-CD</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Model</cell><cell></cell><cell cols="3">Params.(M) FLOPs (G)</cell><cell>F1</cell><cell>IoU</cell><cell>F1</cell><cell>IoU</cell><cell></cell><cell>F1</cell><cell>IoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">DTCDSCN [9]</cell><cell></cell><cell>41.07</cell><cell>7.21</cell><cell cols="2">87.67 78.05</cell><cell cols="3">71.95 56.19</cell><cell cols="2">63.72 46.76</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">STANet [2]</cell><cell></cell><cell>16.93</cell><cell>6.58</cell><cell cols="2">87.26 77.40</cell><cell cols="3">82.32 69.95</cell><cell cols="2">64.56 47.66</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">IFNet [10]</cell><cell></cell><cell>50.71</cell><cell>41.18</cell><cell cols="2">88.13 78.77</cell><cell cols="3">83.40 71.52</cell><cell cols="2">60.10 42.96</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SNUNet [14]</cell><cell></cell><cell>12.03</cell><cell>27.44</cell><cell cols="2">88.16 78.83</cell><cell cols="3">83.50 71.67</cell><cell cols="2">66.18 49.45</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Base S3</cell><cell></cell><cell>1.28</cell><cell>1.78</cell><cell cols="2">82.23 76.24</cell><cell cols="3">79.52 66.00</cell><cell cols="2">56.00 38.88</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">+ CNN (Base S4)</cell><cell>3.38</cell><cell>4.09</cell><cell cols="2">87.41 77.64</cell><cell cols="3">80.86 67.87</cell><cell cols="2">58.30 41.15</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">+ BIT (BIT S3)</cell><cell>1.45</cell><cell>2.05</cell><cell cols="2">88.51 79.39</cell><cell cols="3">81.38 68.60</cell><cell cols="2">69.00 52.67</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Base S4</cell><cell></cell><cell>3.38</cell><cell>4.09</cell><cell cols="2">87.41 77.64</cell><cell cols="3">80.86 67.87</cell><cell cols="2">58.30 41.15</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">+CNN (Base S5)</cell><cell>11.85</cell><cell>12.99</cell><cell cols="2">87.57 77.89</cell><cell cols="3">81.61 68.93</cell><cell cols="2">58.48 41.32</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">+BIT (BIT S4)</cell><cell></cell><cell>3.55</cell><cell>4.35</cell><cell cols="2">89.31 80.68</cell><cell cols="3">83.98 72.39</cell><cell cols="2">69.26 52.97</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VI</cell></row><row><cell cols="8">TABLE III ABLATION STUDY OF OUR BIT ON THREE CD DATASETS. ABLATIONS ARE PERFORMED ON TOKENIZER (T), TRANSFORMER ENCODER (TE), AND TRANSFORMER DECODER (TD). WE ALSO ADD THE NON-LOCAL TO THE BASELINE FOR COMPARISON. THE F1-SCORE IS REPORTED. NOTE THAT THE DEPTH OF TE AND TD ARE SET TO 1.</cell><cell cols="6">EFFECT OF LEVIR-CD</cell><cell>WHU-CD</cell><cell>DSIFN-CD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">E.D. D.D.</cell><cell>F1</cell><cell></cell><cell>IoU</cell><cell>F1</cell><cell>IoU</cell><cell>F1</cell><cell>IoU</cell></row><row><cell>Model</cell><cell></cell><cell>T</cell><cell cols="5">TE TD LEVIR WHU DSIFN</cell><cell>1</cell><cell>1</cell><cell></cell><cell>88.93</cell><cell cols="2">80.07</cell><cell>82.34</cell><cell>70.00</cell><cell>67.38 50.80</cell></row><row><cell cols="2">Base S4 +Non-local</cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>87.41 87.56</cell><cell>80.86 80.93</cell><cell>58.30 59.94</cell><cell>2 4</cell><cell>1 1</cell><cell></cell><cell>89.13 88.97</cell><cell cols="2">80.39 80.13</cell><cell>81.83 82.15</cell><cell>69.24 69.70</cell><cell>66.96 50.34 66.95 50.32</cell></row><row><cell>BIT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>88.93</cell><cell>82.34</cell><cell>67.38</cell><cell>8</cell><cell>1</cell><cell></cell><cell>88.93</cell><cell cols="2">80.06</cell><cell>80.73</cell><cell>67.68</cell><cell>67.11 50.50</cell></row><row><cell>BIT BIT</cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>87.58 87.35</cell><cell>81.68 81.05</cell><cell>61.76 62.93</cell><cell>1</cell><cell>2</cell><cell></cell><cell>88.91</cell><cell cols="2">80.03</cell><cell>82.99</cell><cell>70.92</cell><cell>67.17 50.57</cell></row><row><cell>BIT</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>88.07</cell><cell>79.16</cell><cell>64.47</cell><cell>1</cell><cell>4</cell><cell></cell><cell>89.26</cell><cell cols="2">80.59</cell><cell>83.69</cell><cell>71.95</cell><cell>69.05 52.73</cell></row><row><cell>BIT</cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell>87.38</cell><cell>80.82</cell><cell>59.54</cell><cell>1</cell><cell>8</cell><cell></cell><cell>89.31</cell><cell cols="2">80.68</cell><cell>83.98</cell><cell>72.39</cell><cell>69.26 52.97</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">TABLE IV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">ABLATION STUDY OF POSITION EMBEDDING (Model PE in TE PE in TD LEVIR WHU DSIFN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BIT</cell><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell>87.77</cell><cell>82.06</cell><cell>60.81</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BIT</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>88.93</cell><cell>82.34</cell><cell>67.38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BIT</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>87.87</cell><cell>81.40</cell><cell>60.23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BIT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89.07</cell><cell>82.01</cell><cell>65.68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">LEVIR-CD</cell><cell cols="2">WHU-CD</cell><cell cols="2">DSIFN-CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Length</cell><cell>F1</cell><cell></cell><cell>IoU</cell><cell>F1</cell><cell>IoU</cell><cell>F1</cell><cell>IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>32</cell><cell cols="3">87.76 78.18</cell><cell cols="2">81.53 68.82</cell><cell cols="2">62.40 45.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>16</cell><cell cols="3">88.45 79.74</cell><cell cols="2">81.79 69.19</cell><cell cols="2">63.07 46.06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8</cell><cell cols="3">88.19 78.88</cell><cell cols="2">81.83 69.27</cell><cell cols="2">64.28 47.36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell cols="3">88.93 80.07</cell><cell cols="2">82.34 70.00</cell><cell cols="2">67.38 50.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell cols="3">88.90 80.02</cell><cell cols="2">82.02 69.53</cell><cell cols="2">65.13 48.29</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Review article digital change detection techniques using remotely-sensed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="989" to="1003" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A spatial-temporal attention-based method and a new dataset for remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote. Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1662</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Change detection of deforestation in the brazilian amazon using landsat data and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>De Bem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>De Carvalho Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Guimar?es</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A T</forename><surname>Gomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">901</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Building damage detection in satellite imagery using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zaytseva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Change detection based on artificial intelligence: State-of-the-art and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1688</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dasnet: Dual attentive fully convolutional siamese networks for change detection of high resolution satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tripletbased semantic relation learning for aerial remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote. Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="270" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A feature difference convolutional neural network-based change detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TGRS</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building change detection for remote sensing images using a dual-task constrained deep siamese convolutional network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A deeply supervised image fusion network for change detection in high resolution bi-temporal remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tapete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="183" to="200" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optical remote sensing image change detection based on attention mechanism and image difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pga-siamnet: Pyramid feature-based attention-guided siamese network for remote sensing orthoimagery building change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">484</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Looking for change? roll the dice and demand attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">I</forename><surname>Diakogiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Waldner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Caccetta</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Snunet-cd: A densely connected siamese network for change detection of vhr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Building change detection via a combination of cnns using only rgb aerial imageries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nemoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Imaizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hikosaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building instance change detection from large-scale aerial images using convolutional neural networks and simulated samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1343</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The temporal dynamics of slums employing a cnn-based change detection approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kuffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Persello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote. Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page">2844</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Urban change detection for multispectral earth observation using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IGARSS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Siamese network with multi-level features for patch-based change detection in satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">U</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Cor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kerekes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Savakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Global Conference on Signal and Information Processing</title>
		<meeting><address><addrLine>Anaheim, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-11-26" />
			<biblScope unit="page" from="958" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A deep siamese network with hybrid convolutional feature extraction module for change detection based on multi-sensor remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">205</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional siamese networks for change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Change detection in remote sensing images using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">V</forename><surname>Vizilter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">V</forename><surname>Vygolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Knyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Rubis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end change detection for high resolution satellite images using improved unet++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1382</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Ppcnet: A combined patch-level and pixel-level end-to-end deep network for highresolution remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From w-net to cdgan: Bitemporal change detection via deep learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1790" to="1802" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Change detection based on deep siamese convolutional network for optical aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1845" to="1849" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dual learning-based siamese framework for change detection using bi-temporal vhr optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1292</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using adversarial network for multiple change detection in bitemporal remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial instance augmentation for building change detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Incorporating metric learning and adversarial network for seasonal invariant change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Emery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote. Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2720" to="2731" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual transformers: Token-based image representation and processing for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature pyramid transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="323" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Proceedings, Part I, ser. Lecture Notes in Computer Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<editor>A. Vedaldi, H. Bischof, T. Brox, and J. Frahm</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
	<note>End-to-end object detection with transformers</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deformable {detr}: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=gZ9hCDWe6ke" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Cptr: Full transformer network for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-supervised pre-training of transformers for satellite image time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A cnn-transformer hybrid approach for crop classification using multitemporal multisensor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="847" to="858" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hsi-bert: Hyperspectral image classification using the bidirectional encoder representation from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="165" to="178" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Vision transformers for remote sensing image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bashmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M A</forename><surname>Rahhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Dayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Ajlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Remote sensing image caption generation via transformer and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multim. Tools Appl</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="26" to="661" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Word-sentence framework for remote sensing image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Transformers without tears: Improving the normalization of self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<idno>abs/1910.05895</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unet++: A nested u-net architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-51" />
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis -and -Multimodal Learning for Clinical Decision Support -4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-20" />
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
