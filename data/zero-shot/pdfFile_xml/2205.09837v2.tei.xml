<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Summarization as Indirect Supervision for Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keming</forename><surname>Lu</surname></persName>
							<email>keminglu@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California ? University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Hung</forename><surname>Hsu</surname></persName>
							<email>ihunghsu@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California ? University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California ? University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Mingyu</roleName><forename type="first">Derek</forename><surname>Ma</surname></persName>
							<email>ma@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California ? University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
							<email>muhaoche@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California ? University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Summarization as Indirect Supervision for Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relation extraction (RE) models have been challenged by their reliance on training data with expensive annotations. Considering that summarization tasks aim at acquiring concise expressions of synoptical information from the longer context, these tasks naturally align with the objective of RE, i.e., extracting a kind of synoptical information that describes the relation of entity mentions. We present SURE, which converts RE into a summarization formulation. SURE leads to more precise and resource-efficient RE based on indirect supervision from summarization tasks. To achieve this goal, we develop sentence and relation conversion techniques that essentially bridge the formulation of summarization and RE tasks. We also incorporate constraint decoding techniques with Trie scoring to further enhance summarization-based RE with robust inference. Experiments on three RE datasets demonstrate the effectiveness of SURE in both full-dataset and low-resource settings, showing that summarization is a promising source of indirect supervision signals to improve RE models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) aims at extracting relations between entity mentions from their textual context. For example, given a sentence "Steve Jobs is the founder of Apple", an RE model would identify the relation "founded" between mentioned entities "Steve Jobs" and "Apple". RE is a fundamental natural language understanding task and is also the essential step of structural knowledge acquisition for constructing knowledge bases. Hence, advanced RE models is crucial for various knowledgedriven downstream tasks, such as dialogue system , narrative prediction <ref type="bibr" target="#b5">(Chen et al., 2019)</ref>, and question answering <ref type="bibr" target="#b43">(Yasunaga et al., 2021;</ref><ref type="bibr" target="#b12">Hao et al., 2017)</ref>.</p><p>Given sentences with detected pairs of entity mentions, most recent studies formulate RE as multi-class classification <ref type="bibr" target="#b55">(Zhou and Chen, 2022;</ref><ref type="bibr" target="#b42">Yamada et al., 2020;</ref><ref type="bibr" target="#b2">Baldini Soares et al., 2019)</ref>. Models presented in these studies employ pretrained language models (PLM) equipped with classification heads and are finetuned with a cross-entropy loss. Although such methods have achieved enhanced performances on several benchmarks <ref type="bibr" target="#b38">Stoica et al., 2021;</ref><ref type="bibr" target="#b0">Alt et al., 2020)</ref>, they fall short of capturing the semantic meanings of the relations. This shortage hinders PLMs from effectively matching the sentential context with the relations that are merely converted as logits. On the other hand, obtaining high-quality annotations for RE is often costly due to the difficulty for annotators to recognize and mutually agree on such structural information. This represents another challenge for RE models that have relied on direct supervision from sufficient end-task training data. Existing literature finds that classification models have drastically degraded performance under low-resource scenarios <ref type="bibr" target="#b37">(Sainz et al., 2021)</ref>, showing that label efficiency is a vital issue when adopting prior methods in real application scenarios. To combat this issue, we aim at investigating an indirectly supervised method for RE, which allows the use additional supervision signals that are not specific to RE without solely relying on direct RE annotations.</p><p>This study proposes SURE (Summarization as Relation Extraction), which reformulates and addresses RE as a summarization task. 2 Summarization seeks to acquire concise expressions of synoptical information from longer context <ref type="bibr" target="#b10">(El-Kassas et al., 2021)</ref>, which aligns well with the objective of RE if we consider the relation between entities as  <ref type="figure">Figure 1</ref>: Overview of SURE inference with an example. SURE constructs input sequences and verbalizes candidate relations with semantic templates (Relation Verbalization subfigure). Then we use a summarization model to find the best prediction by Trie scoring technique (Trie scoring subfigure). This model is pretrained on summarization tasks and then simply finetuned with input sequence target verbalized ground-truth relation. For each common prefix (CP), we will calculate the probability of each relation candidate, which is obtained using a trained summarization model, as shown in the Next Token Prediction subfigure. The "{subj}" and "{obj}" are two placeholders representing subject and object entity in each sample.</p><p>one aspect of synoptical information in the sentential context. Such an affinity of task objectives naturally motivates us to leverage indirect supervision from summarization tasks to improve RE models. In comparison to a multi-class classifier, summarizing the relation information also allows generating a semantically rich representation of the relation. Furthermore, unlike existing RE models that rely on costly manual annotations of structural information on training sentences, summarization tasks allow training with considerably richer and unannotated parallel text corpora. 3 Hence, summarization tasks can bring in abundant indirect supervision signals, and can potentially lead to label-efficient models under scenarios without much task-specific annotation. <ref type="figure">Fig. 1</ref> illustrates the structure of SURE. Specifically, SURE transforms RE to a summarization task with relation and sentence conversion techniques ( ?3.2), and applies constrained inference for relation prediction ( ?3.4). We deploy an entity information verbalization technique to highlight the sentential contexts with entity information, and verbalize relations into template-style short summaries. In that way, the converted inputs and outputs of RE naturally suit a summarization model. Then, we adapt a summarization model to the RE task by finetuning it on the converted RE data ( ?3.3). During inference, a Trie scoring technique is designed to infer the relations ( ?3.4). In this way, SURE fully utilizes indirect supervision from summarization, allowing a precise RE model to be obtained even in low-resource scenarios.</p><p>The contributions of this work are two-folds. First, to the best of our knowledge, this is the first study on using indirect supervision from summarization for RE. Since the objective of summarization naturally aligns with RE, it allows precise RE models to be trained without solely relying on direct task annotations, and benefits with robust RE under low-resource scenarios. Second, we investigate input conversion techniques that effectively bridge the formulation of summarization and RE tasks, as well as constraint techniques that further enhance the inference of summarization-based RE. Our contributions are verified with experiments on three widely used sentence-level RE datasets, TA-CRED, TACREV, and SemEval, as well as three low-resource settings of TACRED. We observe that SURE outperforms various baselines, especially in the low-resource setting with 10% TACRED train-ing data. SURE also achieves SOTA performance with 75.1% and 83.5% in micro F1 on TACRED and TACREV, respectively. We also perform comprehensive ablation studies to show the effectiveness of indirect supervision from summarization and the best options of input conversion for SURE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Relation extraction. Recent studies on (sentencelevel) RE typically formulate the task as multiclass classification tasks by finetuning pretrained language models <ref type="bibr" target="#b40">(Wu and He, 2019;</ref><ref type="bibr" target="#b16">Hsu et al., 2022a;</ref><ref type="bibr" target="#b29">Lyu and Chen, 2021)</ref> or developing pretraining objectives for RE <ref type="bibr" target="#b2">(Baldini Soares et al., 2019;</ref><ref type="bibr" target="#b35">Peng et al., 2020)</ref>. For example, <ref type="bibr" target="#b40">Wu and He (2019)</ref> enrich the contextual representation of the PLM with marked-out subject and object entity mentions. <ref type="bibr" target="#b29">Lyu and Chen (2021)</ref> propose a model-agnostic paradigm that introduces mutual restrictions of relations and entity types into relation classifiers. On the basis of PLMs, some studies further improve RE with external knowledge from knowledge bases (KBs) <ref type="bibr" target="#b42">(Yamada et al., 2020;</ref><ref type="bibr" target="#b36">Peters et al., 2019;</ref><ref type="bibr" target="#b52">Zhang et al., 2019)</ref>. More studies have been introduced to improve entity pair representations and classifiers for RE such that we cannot exhaust them in this short summary. We refer readers to the recent benchmarking study <ref type="bibr" target="#b55">(Zhou and Chen, 2022)</ref>.</p><p>Another threads of recent effort in RE introduce several reformulations of RE with prompt learning <ref type="bibr" target="#b11">(Han et al., 2021;</ref>. Specifically, <ref type="bibr" target="#b11">Han et al. (2021)</ref> propose prompt tuning methods for RE by applying logic rules to construct hierarchical prompts.  leverage prompt tuning for RE by injecting semantics of relations and entity types. Instead of leveraging pretrained masked language models, we use generative approaches to solve RE.</p><p>Indirect supervision. Indirect supervision <ref type="bibr" target="#b13">(He et al., 2021)</ref> methods often modify the training and inference processes on a task into a different formation, hence allowing the use of additional supervision signals that is not specific to this task. <ref type="bibr" target="#b23">Levy et al. (2017)</ref> show that RE can be addressed as answering reading comprehension questions and improved by the training process of a machine reading comprehensive task. Similarly,  also employ QA data to improve model generalization abilities in coreference resolution. <ref type="bibr" target="#b46">Yin et al. (2020)</ref> propose a few-shot NLI-based framework to address different tasks, such as question answering and coreference resolution. <ref type="bibr" target="#b25">Li et al. (2022)</ref> further improve this strategy by incorporating NLI with learning-to-rank, leading to a robust system for ultra-fine entity typing <ref type="bibr" target="#b7">(Choi et al., 2018)</ref>. Similar idea of leveraging NLI as indirect supervision signal is applied by <ref type="bibr" target="#b37">Sainz et al. (2021)</ref>, which focuses on low-resource RE task. As discussed, the objective of RE aligns well with that of a summarization task. While there is no prior study that investigates indirect supervision from summarization, this is exactly the focus of our study.</p><p>Generative approaches for discriminative tasks. Formulating discriminative tasks as generation tasks can be an efficient way to guide PLMs to leverage semantics of decision labels <ref type="bibr">Hsu et al., 2022b;</ref><ref type="bibr" target="#b47">Yuan et al., 2022)</ref>. Instead of predicting classification logits, a common paradigm for these models is to represent the class as a concise structure and employ controlled decoding for generation. Several studies <ref type="bibr" target="#b49">(Zeng et al., 2018</ref><ref type="bibr" target="#b48">(Zeng et al., , 2020</ref><ref type="bibr" target="#b44">Ye et al., 2020;</ref><ref type="bibr" target="#b3">Cao and Ananiadou, 2021)</ref> use sequence-tosequence-based models to generate relations written in a triplet format. <ref type="bibr" target="#b34">Paolini et al. (2020)</ref> incorporate many structured prediction tasks, including RE, into machine translation. Huguet <ref type="bibr" target="#b21">Cabot and Navigli (2021)</ref> simplify RE as expressing relations as a sequence of text to perform end-to-end generation of relations. These works mostly formulate RE as text-to-structure learning instead of generating natural language sentences that is a more natural target to exploit the power of pretrained generative models <ref type="bibr">(Hsu et al., 2022b)</ref>. Additionally, they do not include indirect supervision of summarization, which is naturally close to the objective of RE and has the potential to benefit RE performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we describe SURE, a model for addressing RE with summarization. We introduce preliminaries ( ?3.1), how RE data are converted to suit summarization tasks ( ?3.2), training ( ?3.3), and constrained inference of SURE ( ?3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Problem Definition. The input to the sentencelevel RE is a sentence s with entity mentions e 1 and e 2 4 , where their auxiliary entity type information t 1 , t 2 is given. An RE model aims at inferring the relation r between the subject and object entities e 1 and e 2 from a set of candidate relations R = R P ?{?}, which include positive relations R P and a Not-Available (NA) relation ?. We also involve type-related candidate relations R(t 1 , t 2 ), which is a subset of R which has specific types of head and tail entities.</p><p>Overview. <ref type="figure">Fig. 1</ref> demonstrates the overview of SURE. The summarization task takes a context as the input sequence and a summary target is expected to be generated. To formulate RE as summarization, we first need to hint the summarization model which entity pair is targeted for summarization. To do so, we process the input sentence such that entity mentions and their type information will be highlighted ( ?3.2). We explore existing entity marking tricks <ref type="bibr" target="#b55">(Zhou and Chen, 2022</ref>) and also develop entity information verbalization technique that directly augments entity information as part of the context. The processed sentence will then be fed into SURE. The summary targets for SURE is created via verbalizing existing RE labels to templates, such as the Relation Verbalization subfigure in <ref type="figure">Fig. 1</ref>. In the training process ( ?3.3), SURE uses pretrained summarization models as a start point, and finetunes them with processed sentences as the input and verbalized relation descriptions as the targets. During inference, we incorporate several constrained inference techniques to help SURE decide the inferred relation ( ?3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation and Sentence Conversion</head><p>Summarization takes text sequences as inputs and outputs. We hereby describe the input sequence construction and relation verbalization, representing two essential techniques for converting RE data to suit the summarization task.</p><p>Input sequence construction. Relation extraction focuses on analyzing the interaction between two specific entities, so we need to further process source sentences so that additional information can be involved and captured by summarization models. SURE explores a series of sentence processing techniques that highlight and incorporate entity information, aiming for identifying a technique that suits the summarization task well. Entity information includes entity names, types, and spans, which is useful for inferring the relation. We explore with two strategies for processing the source sentence.</p><p>? Entity typed marker. Various entity marking techniques are widely adopted in previous multiclass classification RE systems <ref type="bibr" target="#b52">(Zhang et al., , 2019</ref><ref type="bibr">Wang et al., 2021;</ref><ref type="bibr" target="#b56">Zhou et al., 2021;</ref><ref type="bibr" target="#b54">Zhong and Chen, 2021;</ref><ref type="bibr" target="#b55">Zhou and Chen, 2022)</ref>. We list all the techniques in Appx. Tab. 9.</p><p>Our preliminary experiments (Appx. Tab. 10) find that the following typed entity marker technique with punctuation works the best for SURE among these marking methods (inserted typed markers are in blue, while the original text is in black): @ * person * Mandelbrot @ was born in Poland but as a child moved to # ? country ? France #.</p><p>? Entity information verbalization. We develop a simple sentence rewriting technique that directly describes entity information as an augmented part of the linguistic context (in blue):</p><p>The subject entity is Mandelbrot. The object entity is France. The type of Mandelbrot is person. The type of France is country. Mandelbrot was born in Poland but as a child moved to France.</p><p>Although this technique cannot encode entity span information, it keeps the input data close to natural language instead of adding special tokens. This aligns well with the indirect supervision from summarization. Thus, it shows better performance to the entity typed marker technique, as shown in the ablation study ( ?4.3).</p><p>We hereby list all input conversion techniques we experiment in this work in Tab. 9. Tab. 10 shows additional results on the bart-large-cnn model, which provides the same conclusion as results on pegasuslarge. We also compare this mixing technique in the ablation study ( ?4.3) and find it achieves the best performance in the full training setting.</p><p>for NLI. We adapt minimal additional updates to their templates so the templates can better fit summarization and less human effort is involved. We let the subject entities always appear in the head of sentences while the objects are in the tail. For example, "org:parents" relation is verbalized with templates "{subj} has the parent company {obj}". Detailed semantic templates are demonstrates in Appx. Tab. 11 and Tab. 14. Notice that semantic meaning of a relation can be verbalized in various ways, so we also construct alternative semantic templates and discuss how different templates influence model performance in ?4.3. For comparison, we also experiment with structural templates that are widely used in existing sequence-to-structure methods <ref type="bibr" target="#b49">(Zeng et al., 2018</ref><ref type="bibr" target="#b48">(Zeng et al., , 2020</ref>. As listed in Appx. Tab. 13, these templates directly concatenate entity names and relations, which are shown by our ablation study ( ?4.3) to be less effective than the semantic templates.</p><p>Discussion. SURE requires manually designed templates of relations for both training and inference. To minimize manual effort and give a fair comparison to prior work, we adopt the same relation verbalization templates from <ref type="bibr" target="#b37">Sainz et al. (2021)</ref>. They restrict the influence of human effort by limiting the time for creating the templates and build 2 templates in average for each relation. For simplicity, we adopt one template for each relation from their templates, which suggests SURE will need less manual effort for template design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Process</head><p>The aforementioned rewriting and verbalization techniques ( ?3.2) highlight the sentential contexts with entity information, and convert the extraction as summary. Hence, the converted inputs and outputs of RE naturally suit the summarization task. This allows us to train a summarization model first using large parallel training corpora for abstractive summarization such as XSum <ref type="bibr" target="#b32">(Narayan et al., 2018)</ref> or <ref type="bibr">CNN/Dailymail (Hermann et al., 2015)</ref>, and further adapt it to learn to "summarize" relation. In our experiments, SURE adopts checkpoints of pretrained generative models <ref type="bibr" target="#b24">(Lewis et al., 2020;</ref> that are pretrained on summarization tasks as starting points. Then, we follow the same finetuning process of seq-to-seq training with the cross entropy loss to finetune the model on converted RE data. In this way, SURE can leverage indirect supervision obtained from the summariza-tion task to enhance RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference</head><p>The inference process of SURE involves first applying Trie scoring to rank the possibility of each relations, and setting entity type constraints. The score is further calibrated to make selective predictions between known and NA relations.</p><p>Trie scoring. Summarization models employ beam search techniques to generate sequential outputs , while RE seeks to find out the relation described the input. To support relation prediction using a summarization model, we develop an inference method that will rank each relation candidate by using the summarization model as proxies for scoring. Inspiring by the Trie constrained decoding , we develop a Trie scoring technique, allowing efficient ranking for candidate relation verbalizations. Instead of calculating the probability of whole relation templates for ranking, our method conducts a traverse on the Trie and estimates the probability of each relation candidate as a path probability on the Trie.</p><p>Given the set of tokenized templates of all candidate relations as T = {T i } nr i=1 , we build a Trie <ref type="bibr" target="#b1">(Aoe et al., 1992)</ref> by combining the prefixes of all templates, as an example in the Trie Scoring subfigure in <ref type="figure">Fig. 1</ref>. A path of a relation template can be described as a sequence of decision processes, which goes from the root to a leaf node. If we denote N f as the set of forky nodes (the nodes with more than one child), then the probability of a path can be estimated by continued producting the probability of choosing a specific child in a forky node n f i ? N f . Specifically, we denote the path from root to n f i as CP i , which is the common prefix for all templates in the sub-tree with n f i as the root. For example, CP 2 = "{subj} was" in <ref type="figure">Fig. 1</ref> is the common prefix of templates T 1 and T 2 . If we denote the children of n f i as a set C(n f i ), the prediction probability of relation r i can be calculated by</p><formula xml:id="formula_0">p(r i ) = ? ?n f j ,c?C(n f j ) p(c ? T i |CP j , S).</formula><p>p(c ? T i |CP j , S) thereof is the probability for the model to generate the next token c given the previous common prefix CP j , and c is selected from C(n f i ) on T i . This probability is calculated using a seq-to-seq summarization model with input sentence S as encoder input and CP i as de-coder input prefix, such as the illustration in the Next Token Prediction(CP2) subfigure in <ref type="figure">Fig. 1</ref>.</p><p>Type constrained inference. Type constrained inference emerges in many recent works <ref type="bibr" target="#b29">(Lyu and Chen, 2021;</ref><ref type="bibr" target="#b37">Sainz et al., 2021;</ref><ref type="bibr" target="#b9">Cohen et al., 2020)</ref>. By constructing a type-relation mapping from the training set, models can only predict valid relations given entity types, which significantly shrinkages the size of the candidate relation set. Type constrained inference can be easily incorporated into SURE by pruning the templates of invalid entity types from Trie scoring. The merit of type constrained inference will be discussed in Appx. ?A.2.</p><p>Calibration for NA relation. Considering that it is not possible for a relation ontology to exhaust all possible relations between any entities, the inference of a trained RE model can naturally be exposed to many instances where not a positive relation from R P is expressed. Hence, it is particularly important to enforce the model to selectively make a decision between positive relations or predicting abstention. This is realized by a calibration technique in SURE, where a score threshold s is set for NA and is calibrated as below:</p><formula xml:id="formula_1">r = arg max r i ?R P p(r i ) , p(NA) s NA , p(NA) &gt; s</formula><p>The best threshold s is found on the development set and is used as a fixed threshold in inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present the experimental evaluation of SURE for RE under both high-and lowresource setups ( ?4.1- ?4.2). In addition, we also conduct comprehensive ablation studies to investigate the effectiveness of the incorporated techniques in SURE ( ?4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We conduct experiments on three widely used sentence-level RE benchmarks: SemEval 2010 Task 8 (SemEval; <ref type="bibr" target="#b14">Hendrickx et al. 2010</ref>), TACRED , and TACRED-Revisited (TACREV; <ref type="bibr" target="#b0">Alt et al. 2020)</ref>. SemEval is an RE dataset which does not provide entity types, so we simply remove the processing of entity types in sentence conversion ( ?3.2) to adapt SURE on this dataset. TACRED contains entity pairs drawn from the yearly TAC-KBP challenge. We list our templates for SemEval and TA-CRED in Appx. Tab. 14 and Tab. 11, respectively.</p><p>TACREV relabeled develop and test sets of TA-CRED to correct mislabeled entity types and relations. TACREV shares the same templates with TACRED since they have exactly the same relations. Statistics of these datasets are showed in Appx. Tab. 6. We report macro F1 on SemEval with the official grading script for this benchmark 5 , and micro F1 on TACRED and TACREV to keep consistency with previous works <ref type="bibr" target="#b42">(Yamada et al., 2020;</ref><ref type="bibr" target="#b55">Zhou and Chen, 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines.</head><p>We compare SURE with 8 recent classification-based RE methods: (1) Span-BERT <ref type="bibr" target="#b22">(Joshi et al., 2020</ref>) is a pre-training method designed to better represent and predict spans of text; <ref type="formula">(2)</ref>   <ref type="bibr" target="#b42">(Yamada et al., 2020)</ref> modifies the PLM with an entity-aware selfattention mechanism; (7) IRE RoBERTa-large <ref type="bibr" target="#b55">(Zhou and Chen, 2022)</ref> is an improved baseline model incorporated with typed entity markers; (8) RE-CENT (Lyu and Chen, 2021) introduces type constraint ( ?3.4) and achieve state of the art performance on TACRED. We also compare SURE with two indirect supervision methods, i.e. (9) NLI DeBERTa <ref type="bibr" target="#b37">(Sainz et al., 2021)</ref> that formulates RE as NLI, and (10) KnowPrompt  that formulates RE as prompt tuning.</p><p>Low resource setting. We evaluate the performance of SURE under low-resource scenarios. To do so, we use the same splits of <ref type="bibr" target="#b37">Sainz et al. (2021)</ref> to build the TACRED datasets with 1/5/10 percent of training and development samples.</p><p>Model configurations. We develop SURE based on two widely pretrained generative models BART <ref type="bibr" target="#b24">(Lewis et al., 2020)</ref> and PEGASUS . BART is a denoising autoencoder for pretraining sequence-to-sequence models. We use two summarization models BARTlarge-cnn and BART-large-xsum that are finetuned with CNN/Dailymail <ref type="bibr" target="#b15">(Hermann et al., 2015)</ref> and XSum <ref type="bibr" target="#b33">(Narayan et al., 2021)</ref>, respectively. We also consider BART-large as a baseline without indi-  <ref type="bibr" target="#b37">Sainz et al. (2021)</ref>. ? indicates we reproduce the baseline results (Appx. ?A.4). <ref type="table">Table 1</ref>: Result of SURE compared with existing methods under both low resource on TACRED and full training scenarios on TACRED, TACREV and SemEval. We report micro F1 on TACRED and TACREV, and report macro F1 on SemEval. Baseline F1 scores on the table without special tags are reported by their original studies. Hyphens indicate unavailable results in prior studies. We report SURE performance with entity information verbalization for consistency. However, SURE achieves better performance (83.5%) with mix technique of entity information verbalization and entity typed marker on TACREV (Tab. 3). We run our models with three different seeds and report the median. The best scores are identified with bold and the second best scores are underlined.  rect supervision of summarization. PEGASUS is a sequence-to-sequence model pretrained with a gap sentences generation task, which significantly benefits various summarization downstream tasks. Similarly, we use PEGASUS-large as a stronger initial checkpoint than BARTs. We use grid search to find optimal hyperparameters for finetuning summarization models. The best hyperparameters of our experiments and re-implementation of baselines are shown in Appx. ?A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We present our main results on both full training and low-resource settings in Tab. 1. We report the performance of SURE with entity information verbalization technique, which is proved to be the best way of input sequence construction ( ?3.2) in most settings as shown in our ablation study ( ?4.3).  Effectiveness of indirect supervision. We further evaluate SURE based on different pretrained models, as shown in Tab. 1. We observe that models finetuned on summarization tasks (CNN and XSum) generally lead to better performance, especially in the low-resource setting. For example, SURE BART-large-cnn outperforms SURE BART-large by 6.8% on the 1% split of TACRED, while this improvement diminishes to 0.3% on the full dataset.</p><p>Besides, pretrained models that perform better on summarization tasks also indicate better performance on RE. Particularly, SURE based on PEGASUS-large, which outperforms BART-large on summarization tasks, outperforms all other versions under both low-resource and full-dataset setting. Both observations show a strong correlation between the performance in summarization and RE, indicating that indirect supervision from summarization is beneficial for RE models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We provide the following analyses to further evaluate core components of SURE, including different template designs, sentence conversion techniques and Trie scoring. We also report the ablation study on type constrained inference and calibration of NA relation in Appx. ?A.2 and A.3, respectively.</p><p>Relation template design. Template design is a manual part of SURE. The semantic meaning of a relation can be verbalized in different ways, leading to varied performance. In Tab. 2, we compare three representative types of templates with pegasus-large in this ablation study to show how verbalization templates influence the performance of SURE. SEMANTIC1 thereof denotes semantic templates beginning with subject entities and ending with object entities, which is showed in Appx. Tab. 11. SEMANTIC2 are also semantic templates that intuitively describe the relation between two entiteis with a pattern "The relation between {subj} and {obj} is {relation}", which is showed in Appx.</p><p>Tab. 12. STRUCTURAL marks structural templates forming in a triplet structure "{subj} {relation} {obj}", which is showed in Appx. Tab. 13. Furthermore, we also set up a zero-shot setting where the model directly infers on the test set of TACRED after calibration on the development set. The results from different templates are reported on both low-resource and full-training scenarios. First of all, we observe that the two semantic templates consistently outperform structural templates, indicating that semantic templates are more suitable for acquiring indirect supervision from summarization. Besides, comparing two semantic templates, we find that Semantic1 works better with pegasuslarge, which suggests that the optimal verbalization may vary. And this difference is more obvious under low resource scenarios. Consequently, zeroshot inference is an effective and efficient method for evaluating manual-designed templates. In future work, we can investigate how to improve this part by prompt tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input conversion.</head><p>We conduct experiments to evaluate various input sentence conversion techniques for injecting entity information into source sentences ( ?3.2). We first conduct experiments on six different input formulations on bart-large-cnn, which is listed in Tab. 9 and results are shown in Appx. Tab. 10. This experiment indicates entity typed marker with punctuation is the best technique for SURE among all entity marker techniques. Then, we further evaluate three techniques on pegasus-large under both full training and low resource scenarios. Tab. 3 shows entity information verbalization achieves significantly better performance under low resource scenarios compared with marker and mix techniques. This is because entity information verbalization transforms input to better fit the input of summarization, while additional markers need more data to learn their representations. In the full training setting, the mixing technique marginally outperforms entity information verbalization.</p><p>Trie scoring. Trie scoring uses teacher forcing  <ref type="bibr" target="#b52">(Zhang et al., 2019)</ref> ... &lt;e1&gt; {subj} &lt;/e1&gt; ... &lt;e2&gt; {obj} &lt;/e2&gt; ...</p><p>Entity typed marker <ref type="bibr" target="#b54">(Zhong and Chen, 2021)</ref> .   <ref type="table">Table 5</ref>: Analysis of different input formulation techniques on bart-large-cnn. We report micro F1 scores on both datasets. The best F1 score is identified with bold.</p><p>to constraint models focusing on candidate templates and have the advantage of efficiency compared with directly comparing likelihoods of all templates. Furthermore, we also make comparisons between Trie scoring and two basic scoring methods on full TACRED with SURE pegasus-large . The first one is to generate the summary of an example and uses ROUGE-L <ref type="bibr" target="#b27">(Lin, 2004)</ref> scores between summary and candidate templates as prediction scores. This method achieves 74.7% on TACRED, which is 0.4% less than that of Trie scoring. Another method is adding a copy mechanism <ref type="bibr" target="#b49">(Zeng et al., 2018)</ref> to ensure summaries begin with head entities, which achieves a comparable performance of the previous method (74.8%), which further proves the advantages of Trie scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a new method SURE that leverages indirect supervision from summarization tasks to improve RE. To do so, we verbalize relations with semantic templates, and augments entity information as parts of the linguistic context in the inputs to allow them to suit the formation of summarization. We also incorporate SURE with constrained inference based on Trie scoring, as well as infer-ence with abstention and entity type constraints. Extensive experiments show that such indirectly supervised RE by SURE lead to more precise and resource-efficient RE. Future work includes further developing our model on document-level RE tasks and minimizing manual effort in template design with prompt tuning <ref type="bibr" target="#b26">(Li and Liang, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We appreciate the reviewers for their insightful comments and suggestions. This work is partly supported by the National Science Foundation of United States Grant IIS 2105329, and a Cisco Faculty Research Award.</p><p>Limitations SURE assumes that summarization data and manual-designed verbalization templates of relations are easy to obtain. This assumption is hold in the general domain. However, obtaining such data and templates can still be difficult in specific lower-resource domains. For example, summarization data in other languages are not as rich as those in English. Hence, SURE may benefit less from indirect supervision signals when it is adapted to multilingual scenarios. Besides, designing templates in specific domains, such as biomedical relation extraction, may require extra involvement of expert knowledge. Although we put certain manual efforts in template design, automatically optimizing templates are also feasible for SURE and can be explored in future work, as described in the Conclusion section.    The best hyperparameters for low-resource setting with pegasus-large are listed below: With the 1/5/10% indices of TACRED provided by <ref type="bibr">(Sainz et al., 2021) 6</ref> , we re-implement RE-CENT <ref type="bibr" target="#b29">(Lyu and Chen, 2021)</ref> and test it under both low-resource and full-training scenarios 7 . However, we find the origin evaluation scripts provided by the author has a serious issue which wrongly corrects all false positive samples of the binary classifier as true negative samples. So the recall of NA is always 100% and precision of positive relations is unreasonably high. We correct this issue and the test results significantly differ from origin results reported by previous study. We also test IRE <ref type="bibr" target="#b55">(Zhou and Chen, 2022)</ref> and KnowPrompt  on low resource datasets and search the best hyperparameter with grid searching 89 . The previous work of KnowPrompt reports the micro F1 score on SemEval. We train KnowPrompt on Se-mEval with codes and hyper-parameters provided by authors and re-evaluate it with official macro-F1 scoring method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Manual-constructed templates</head><p>In this subsection, we display our manualconstructed templates for SemEval (Tab. 14), and three templates designed for TACRED, which are Semantic1 (Tab. 11), Semantic2(Tab. 12), and Structural(Tab. 13).  <ref type="bibr" target="#b52">(Zhang et al., 2019)</ref> ... &lt;e1&gt; {subj} &lt;/e1&gt; ... &lt;e2&gt; {obj} &lt;/e2&gt; ...</p><p>Entity typed marker <ref type="bibr" target="#b54">(Zhong and Chen, 2021)</ref> .   <ref type="table">Table 10</ref>: Analysis of different input formulation techniques on bart-large-cnn. We report micro F1 scores on both datasets. The best F1 score is identified with bold.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>KnowBERT (Peters et al., 2019) is a PLM embedded multiple KBs; (3) R-BERT<ref type="bibr" target="#b40">(Wu and He, 2019)</ref> uses a PLM to encode processed sentences where subject and object entities are marked out; (4) MTB (Baldini Soares et al., 2019) builds task-agnostic relation representations solely from entity-linked text; (5) K-Adapter(Wang et al.,  2021)  infuses knowledge into pretrained language models with adapters. (6) LUKE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Analysis of different template designs. The highest scores are highlighted with bold formation. These experiments are conducted with the entity information verbalization technique.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Analysis of different input formulation techniques on PEGASUS-large based models on SemEval, while its performance is comparable to KnowPrompt.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Context 2 . ... {subj} ... {obj} ...</figDesc><table><row><cell>Technique</cell><cell>Example</cell><cell>M 1 S 1 T 1 TS 1</cell></row><row><cell>Entity information verbalization</cell><cell></cell><cell></cell></row><row><cell>Entity marker</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>.. &lt;e1-{subj-type}&gt; {subj} &lt;/e1-{subj-type}&gt; ... &lt;e2-{obj-type}&gt; {obj} &lt;/e2-{obj-type}&gt; ... Entity typed marker (punct) (Zhou and Chen, 2022) ... @ * {subj-type} * {subj} @ ... # ? {obj-type} ? {obj} # ... Column names are short for mentions, spans, types and type semantics, respectively. 2 Augmented context are generated with the template: "The {subj} entity is {subj} . The {obj} entity is {obj} . The type of {subj} is {subj-type} . The type of {obj} is {obj-type} "</figDesc><table><row><cell>Entity information verbalization + Entity typed marker</cell><cell>Context 2 ... @ * {subj-type} * {subj} @ ... # ? {obj-type} ? {obj} # ...</cell></row><row><cell>Entity information verbalization + Entity typed marker (punct)</cell><cell>Context 2 ... @ * {subj-type} * {subj} @ ... # ? {obj-type} ? {obj} # ...</cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Sentence processing techniques for incorporating entity information.</figDesc><table><row><cell>Technique</cell><cell>P</cell><cell>TACRED R</cell><cell>F1</cell><cell>P</cell><cell>TACREV R</cell><cell>F1</cell></row><row><cell>Entity information verbalization</cell><cell cols="6">71.8 75.1 73.4 78.4 83.8 81.0</cell></row><row><cell>Entity tag</cell><cell cols="6">71.3 71.0 71.1 81.0 75.2 78.0</cell></row><row><cell>Entity typed tag</cell><cell cols="6">73.5 69.7 71.5 79.9 79.6 79.8</cell></row><row><cell>Entity typed tag(punct)</cell><cell cols="6">70.3 74.0 72.1 81.1 79.3 80.2</cell></row><row><cell>Entity infromation verbalization + Entity tag</cell><cell cols="6">73.4 71.8 72.6 78.7 81.7 80.2</cell></row><row><cell>Entity infromation verbalization + Entity typed tag</cell><cell cols="6">70.6 73.5 72.1 82.8 80.0 81.4</cell></row><row><cell cols="7">Entity infromation verbalization + Entity typed tag(punct) 72.6 74.5 73.6 82.1 79.8 81.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="3">: Statistics of datasets</cell></row><row><cell cols="3">A.2 Analysis of type constrained decoding</cell></row><row><cell cols="3">In this ablation study, we make comparisons be-</cell></row><row><cell cols="3">tween SURE with and without typed constrained</cell></row><row><cell cols="3">decoding. The results is demonstrated in Tab. 7.</cell></row><row><cell cols="3">Type constraint brings improvement for 0.2% in</cell></row><row><cell cols="3">average in TACRED and has comparable perfor-</cell></row><row><cell cols="3">mance on TACREV. Type-relation mapping is in-</cell></row><row><cell cols="3">herently involved in training data, so this ablation</cell></row><row><cell cols="3">study proves SURE can learn type-relation map-</cell></row><row><cell>ping from data.</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="2">TACRED TACREV</cell></row><row><cell>bart-large-cnn</cell><cell>73.6</cell><cell>81.0</cell></row><row><cell>-type constraint</cell><cell>73.4</cell><cell>81.0</cell></row><row><cell>bart-large-xsum</cell><cell>73.3</cell><cell>81.0</cell></row><row><cell>-type constraint</cell><cell>73.1</cell><cell>81.0</cell></row><row><cell>pegasus-large</cell><cell>75.1</cell><cell>83.3</cell></row><row><cell>-type constraint</cell><cell>75.0</cell><cell>83.3</cell></row><row><cell>average gap</cell><cell>-0.2</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Comparison between SURE with and without type constraint decoding. We report micro F1 on TA-CRED and TACREV.</figDesc><table><row><cell>A.3 Analysis of calibration for NA relation</cell></row><row><cell>In this ablation study, we make comparisons be-</cell></row><row><cell>tween SURE with and without calibration for NA</cell></row><row><cell>relation. The results is demonstrated in Tab. 8. Cal-</cell></row><row><cell>ibration brings improvement for 0.3% in average</cell></row><row><cell>on TACRED and 0.1% in average on TACREV.</cell></row><row><cell>A.4 Hyper-parameters and</cell></row><row><cell>reimplementation</cell></row><row><cell>This section details the training and inference pro-</cell></row><row><cell>cesses of baselines and our models. We train and in-</cell></row><row><cell>ference all models with PyTorch and Huggingface</cell></row><row><cell>Transformers on GeForce RTX 2080 or NVIDIA</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Comparison between SURE with and without</cell></row><row><cell>calibration for NA relation We report micro F1 on TA-</cell></row><row><cell>CRED and TACREV.</cell></row><row><cell>RTX A5000 GPUs. All optimization uses Adam</cell></row><row><cell>and linear scheduler. A weight decay is used for</cell></row><row><cell>regularization. We run all experiments on three</cell></row><row><cell>seeds [0, 100, 500] and report the median. Specif-</cell></row><row><cell>ically, the best hyperparameters for full training</cell></row><row><cell>setting with pegasus-large are listed below:</cell></row><row><cell>? learning rate: 1e-4</cell></row><row><cell>? weight decay: 5e-6</cell></row><row><cell>? epoch number: 20</cell></row><row><cell>? max source length: 256</cell></row><row><cell>? max target length: 64</cell></row><row><cell>? gradient accumulation steps: 2</cell></row><row><cell>? warm up steps: 1000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Context 2 . ... {subj} ... {obj} ...</figDesc><table><row><cell>Example</cell><cell>M 1 S 1 T 1 TS 1</cell></row><row><cell>Entity information verbalization</cell><cell></cell></row><row><cell>Entity marker</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>.. &lt;e1-{subj-type}&gt; {subj} &lt;/e1-{subj-type}&gt; ... &lt;e2-{obj-type}&gt; {obj} &lt;/e2-{obj-type}&gt; ... Entity typed marker (punct) (Zhou and Chen, 2022) ... @ * {subj-type} * {subj} @ ... # ? {obj-type} ? {obj} # ... Entity information verbalization + Entity typed marker Context 2 ... @ * {subj-type} * {subj} @ ... # ? {obj-type} ? {obj} # ... Entity information verbalization + Entity typed marker (punct) Context 2 ... @ * {subj-type} * {subj} @ ... # ? {obj-type} ? {obj} # ... Column names are short for mentions, spans, types and type semantics, respectively. 2 Augmented context are generated with the template: "The {subj} entity is {subj} . The {obj} entity is {obj} . The type of {subj} is {subj-type} . The type of {obj} is {obj-type} "</figDesc><table /><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Sentence processing techniques for incorporating entity information. Entity infromation verbalization + Entity typed tag(punct) 72.6 74.5 73.6 82.1 79.8 81.0</figDesc><table><row><cell>Technique</cell><cell>P</cell><cell>TACRED R</cell><cell>F1</cell><cell>P</cell><cell>TACREV R</cell><cell>F1</cell></row><row><cell>Entity information verbalization</cell><cell cols="6">71.8 75.1 73.4 78.4 83.8 81.0</cell></row><row><cell>Entity tag</cell><cell cols="6">71.3 71.0 71.1 81.0 75.2 78.0</cell></row><row><cell>Entity typed tag</cell><cell cols="6">73.5 69.7 71.5 79.9 79.6 79.8</cell></row><row><cell>Entity typed tag(punct)</cell><cell cols="6">70.3 74.0 72.1 81.1 79.3 80.2</cell></row><row><cell>Entity infromation verbalization + Entity tag</cell><cell cols="6">73.4 71.8 72.6 78.7 81.7 80.2</cell></row><row><cell>Entity infromation verbalization + Entity typed tag</cell><cell cols="6">70.6 73.5 72.1 82.8 80.0 81.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is public available at https://github. com/luka-group/SuRE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In this paper, we specifically consider abstractive summarization instead of extractive summarization. Since relation labels are often not directly expressed in sentences, extractive summarization does not always support the inference of RE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Particularly, summarization corpora are constructed in the scales from hundred thousands<ref type="bibr" target="#b31">(Nallapati et al., 2016;</ref><ref type="bibr" target="#b32">Narayan et al., 2018)</ref> to million scales<ref type="bibr" target="#b45">(Yin et al., 2021)</ref>, and may be rapidly augmented in large scales from easy-to-consume data sources (e.g., community Q&amp;A platforms<ref type="bibr" target="#b30">(Mishra et al., 2021)</ref> and scientific paper abstracts<ref type="bibr" target="#b8">(Cohan et al., 2018)</ref>).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">An entity mention is presented as an entity name in text, and is structured as a mention span with position information.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Relation verbalization. The target of summarization is verbalized by a set of simple semantic templates, as shown in the Relation Verbalization subfigure ofFig. 1. Each template contains {subj} and {obj} placeholders to be filled with subject and object entity mentions in the sentence. The templates seek to form short summaries that describe the relations between two entities, and will be used for models' training and inference. Semantic templates are also leveraged in<ref type="bibr" target="#b37">Sainz et al. (2021)</ref>, where the templates are used as hypotheses for NLI-based RE. However, their templates are specifically designed</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The metric calculated by the script is the macro F1 on (9+1)-way classification taking directionality into account.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Github repository of low-resource indices: https:// github.com/osainz59/Ask2Transformers 7 Github repository of RECENT: https://github. com/Saintfe/RECENT</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Github repository of IRE: https://github.com/ wzhouad/RE_improved_baseline 9 Github repository of KnowPrompt: https://github. com/zjunlp/KnowPrompt</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tacred revisited: A thorough evaluation of the tacred relation extraction task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Gabryszak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1558" to="1569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An efficient implementation of trie structures. Software: Practice and Experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsushi</forename><surname>Jun-Ichi Aoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="695" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1279</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generati-veRE: Incorporating a novel copy mechanism and pretrained model for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.182</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2119" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autoregressive entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incorporating structured commonsense knowledge in story completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page" from="6244" to="6251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">KnowPrompt: Knowledgeaware Prompt-tuning with Synergistic Optimization for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International World Wide Web Conferences</title>
		<imprint>
			<date type="published" when="2022" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ultra-fine entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhwan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goharian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2097</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="615" to="621" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Relation classification as two-way spanprediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachar</forename><surname>Amir Dn Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Rosenman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04829</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic text summarization: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wafaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>El-Kassas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cherif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A</forename><surname>Salama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoda K</forename><surname>Rafea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page">113679</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11259</idno>
		<title level="m">Ptr: Prompt tuning with rules for text classification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An endto-end model for question answering over knowledge base with cross-attention combining global knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1021</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Foreseeing the benefits of incidental supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.134</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1782" to="1800" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Dominican Republic</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>S?aghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pad?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discourse-level relation extraction via graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Hung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Sixth AAAI Conference On Artificial Intelligence Workshop on Deep Learning on Graphs: Method and Applications (DLG-AAAI)</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Hung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Hao</forename><surname>Huang</surname></persName>
		</author>
		<editor>Natarajan, Kai-Wei Chang, and</editor>
		<imprint>
			<pubPlace>Elizabeth Boschee, Scott Miller, Prem</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Degree: A data-efficient generative event extraction model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multilingual generative language models for zero-shot crosslingual event argument extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Hung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Document-level entity-based extraction as template generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kung-Hsiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.426</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5257" to="5269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">REBEL: Relation extraction by end-to-end language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pere-Llu?s Huguet</forename><surname>Cabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.204</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2370" to="2381" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00300</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zero-shot relation extraction via reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-1034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ultra-fine entity typing with indirect supervision from natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangzheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.353</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge diffusion for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuman</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1138</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1489" to="1498" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relation classification with entity type restriction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanhuan</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.34</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="390" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Looking beyond sentencelevel natural language inference for question answering and text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshuman</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruvesh</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparna</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Talamadupula</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.104</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1322" to="1336" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?aglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Gul?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1028</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Planning with learned entity prompts for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gon?alo</forename><surname>Sim?es</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00438</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1475" to="1492" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Structured prediction as translation between augmented natural languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning from Context or Names? An Empirical Study on Neural Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.298</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3661" to="3672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Label verbalization and entailment for effective zero and few-shot relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Sainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oier</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ander</forename><surname>Barrena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.92</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1199" to="1212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Re-tacred: Addressing shortcomings of the tacred dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-fifth AAAI Conference on Aritificial Intelligence</title>
		<meeting>the Thirty-fifth AAAI Conference on Aritificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Emmanouil Antonios Platanios, and Barnab?s P?czos</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">2021. K-adapter: Infusing knowledge into pre-trained models with adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<biblScope unit="page" from="1405" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Enriching pretrained language model with entity information for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanchan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM international conference on information and knowledge management</title>
		<meeting>the 28th ACM international conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2361" to="2364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">CorefQA: Coreference resolution as query-based span prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.622</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6953" to="6963" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">LUKE: Deep contextualized entity representations with entityaware self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.523</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">QA-GNN: Reasoning with language models and knowledge graphs for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.45</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="535" to="546" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06207</idno>
		<title level="m">Contrastive triple extraction with generative transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">DocNLI: A large-scale dataset for documentlevel natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.435</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4913" to="4922" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Universal natural language processing with limited annotations: Try few-shot textual entailment as a start</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Nazneen Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02584</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Generative biomedical entity linking via knowledge base-guided pre-training and synonyms-aware finetuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.296</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="4038" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Copymtl: Copy mechanism for joint extraction of entities and relations with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9507" to="9514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1047</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced language representation with informative entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1139</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Knowledgegrounded dialogue generation with pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.272</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3377" to="3390" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A frustratingly easy approach for entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="50" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">An improved baseline for sentence-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (AACL)</title>
		<meeting>the Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (AACL)</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14612" to="14620" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
