<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SEMI-FND: Stacked Ensemble Based Multimodal Inference For Faster Fake News Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhav</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Instrumentation and Control Engineering Department</orgName>
								<orgName type="institution">Netaji Subhas University of Technology</orgName>
								<address>
									<addrLine>Sector-3</addrLine>
									<postCode>Delhi -110078</postCode>
									<settlement>Dwarka</settlement>
									<region>New</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ridam</forename><surname>Srivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Instrumentation and Control Engineering Department</orgName>
								<orgName type="institution">Netaji Subhas University of Technology</orgName>
								<address>
									<addrLine>Sector-3</addrLine>
									<postCode>Delhi -110078</postCode>
									<settlement>Dwarka</settlement>
									<region>New</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P S</forename><surname>Rana</surname></persName>
							<email>kpsrana@nsut.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Instrumentation and Control Engineering Department</orgName>
								<orgName type="institution">Netaji Subhas University of Technology</orgName>
								<address>
									<addrLine>Sector-3</addrLine>
									<postCode>Delhi -110078</postCode>
									<settlement>Dwarka</settlement>
									<region>New</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
							<email>vineet.kumar@nsut.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Instrumentation and Control Engineering Department</orgName>
								<orgName type="institution">Netaji Subhas University of Technology</orgName>
								<address>
									<addrLine>Sector-3</addrLine>
									<postCode>Delhi -110078</postCode>
									<settlement>Dwarka</settlement>
									<region>New</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SEMI-FND: Stacked Ensemble Based Multimodal Inference For Faster Fake News Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multimodal Learning</term>
					<term>Stacked Ensemble</term>
					<term>Fake News Detection</term>
					<term>NASNet Mobile</term>
					<term>Embeddings * Denotes those authors have equal contribution 2</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fake News Detection (FND) is an essential field in natural language processing that aims to identify and check the truthfulness of major claims in a news article to decide the news veracity. FND finds its uses in preventing social, political and national damage caused due to misrepresentation of facts which may harm a certain section of society. Further, with the explosive rise in fake news dissemination over social media, including images and text, it has become imperative to identify fake news faster and more accurately. To solve this problem, this work investigates a novel multimodal stacked ensemble-based approach (SEMI-FND) to fake news detection. Focus is also kept on ensuring faster performance with fewer parameters. Moreover, to improve multimodal performance, a deep unimodal analysis is done on the image modality to identify NasNet Mobile as the most appropriate model for the task. For text, an ensemble of BERT and ELECTRA is used. The approach was evaluated on two datasets -Twitter MediaEval and Weibo Corpus. The suggested framework offered accuracies of 85.80% and 86.83% on the Twitter and Weibo datasets respectively. These reported metrics are found to be superior when compared to similar recent works. Further, we also report a reduction in the number of parameters used in training when compared to recent relevant works. SEMI-FND offers an overall parameter reduction of at least 20% with unimodal parametric reduction on text being 60%. Therefore, based on the investigations presented, it is concluded that the application of a stacked ensembling significantly improves FND over other approaches while also improving speed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The advent of the World Wide Web in the 20th century has given a new meaning to the term "Fake News". One might assume that the term has always carried the same meaning in society. However, fake news today is not what it was before the arrival of the internet. Before the mid-1990s, fake news either referred to satire and intentional sensationalism, which was aimed at increasing readership and gathering attention or referred to the news propagandas aimed at discrediting organizations or ideologies <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref>. Today, the most common definition of fake news <ref type="bibr" target="#b2">[3]</ref> is -"news articles that are intentionally and verifiably false." Modern fake news is aimed at spreading false information with harmful intent and is sometimes generated and propagated by hostile foreign actorsas seen in multiple cases in recent times. According to Zhang et al. <ref type="bibr" target="#b3">[4]</ref>, modern fake news is characterized by differences in three features -Volume, Veracity and, Velocity. The Volume of fake news has seen a substantial increase in recent times due to a decrease in the verification process. While newspapers and print media involved layers of verification, the internet has allowed a larger user base to publish fake news and stories <ref type="bibr" target="#b5">[5]</ref>. This is shown in <ref type="figure">Fig. 1</ref>, which displays the interest in the term 'fake news' as obtained from Google Trends <ref type="bibr" target="#b6">[6]</ref>. Second, the Variety of fake news has increased to incorporate fake reviews, satires, false advertisements, misinformation, and disinformation. Finally, the Velocity of the dissemination of fake news has increased manifolds due to the advent of social media <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Interest in the term -'Fake News'</head><p>This exponential rise in the amount of fake news on the internet, has invariably led to an increase in major events that have triggered the recent interest of researchers on this problem. According to Balmas <ref type="bibr" target="#b7">[7]</ref>, the major contributors of this problem have been the rise of social media platforms like WhatsApp, Facebook, Instagram and, most majorly, Twitter. A major example of the same is the effect of fake news during the 2016 US Presidential Elections. According to <ref type="bibr" target="#b9">[8]</ref>, this was linked to an increase in polarization and partisan conflict during the election process. Furthermore, their analysis of 171 million Tweets during the election process showed that 25% of the tweets were extremely biased or fake. This issue of misinformation has also become a pivotal factor in the status quo due to the COVID-19 pandemic. On social media, false COVID-19 cures such as using bleach as an injection have proliferated <ref type="bibr" target="#b10">[9]</ref>, as have false conspiracy theories that the virus was created in a Wuhan lab <ref type="bibr" target="#b11">[10]</ref>, amongst many other theories. This dissemination of misinformation becomes a larger issue due to its impact on human life, especially in third-world countries with inadequate knowledge and resources required for dealing with such events. In Bangladesh for instance, rumours related to Ramu Violence (2012) <ref type="bibr" target="#b12">[11]</ref> lead to the disruption of social stability, while in India fake news spread by extremists has resulted in violence against minorities, specifically through incidents related to cow e-vigilantes. An example of tweets related to these incidents is shown in <ref type="figure">Fig. 2.   Fig. 2</ref>. In clockwise order: (i) Fake news during the US Presidential Elections (2016), (ii) Fake news aimed at inciting communal riots, (iii) Fake news regarding the social presence of the Indian President.</p><p>From the above discussion, it can be concluded that it is imperative to research more robust and accurate models for fake news detection. Ideally, a fake news detection system should be one that can fulfill the following needs sufficiently well:</p><p>1. Accurately detect fake news while also being robust enough to counter the false-positive trap which can lead to fake news being classified as real news. 2. Increase processing speed to counter the surge in the amount of fake news on social media platforms. <ref type="bibr" target="#b2">3</ref>. Ensure performance on all types of data circulated onlineranging from text to images, GIFs, and videos.</p><p>Since the surge of fake news in 2014, multiple works have aimed to solve this issue by employing a variety of methods. Early methods <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b14">13]</ref> attempted to focus only on statistical and word-based features for FND. For example, Rashkin et al. <ref type="bibr" target="#b15">[14]</ref> employed the use of adverbs and their types to identify fake news from text. On the other hand, Nakashole and Mitchell <ref type="bibr" target="#b13">[12]</ref> attempted to train traditional models using the text and its source as the input features. However, recent works <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b17">16]</ref> have recognized the importance of including more modalities while increasing the focus on utilizing context-aware techniques. This is because today, purely textual based methods have become redundant due to three major reasons: 1. The primary factor remains the short length of most modern news mediums. According to a survey by <ref type="bibr" target="#b18">[17]</ref>, Twitter remains one of the major sources of fake news, which has a constraint of 150 characters. This can lead to difficulties in fine-tuning machine learning models to verify the veracity of tweets. 2. Moreover, the increase in intentional and targeted fake news has caused the writing style of most news articles to be very similar to genuine news. This reduced separation between the two also increases the complexity of the task. 3. Finally, with an increase in the popularity of interactive content (GIFs, videos, and images), most fake news is disseminated majorly through other modalities. While text remains an essential component, without the support of other modalities, it has become almost impossible to make the distinction with acceptable accuracies <ref type="bibr" target="#b18">[17]</ref>.</p><p>Due to these reasons, multimodal approaches to FND have seen an increased interest in recent times. Shim et al. <ref type="bibr" target="#b18">[17]</ref> employed the use of links present in tweets and articles as a separate modality to increase the contextual knowledge of their classifier. They also showed an increase in performance when compared to conventional text-only methods applied to the same data. Further, Singhal et al. <ref type="bibr" target="#b20">[18]</ref> reported a substantial increase in performance with the inclusion of images as a modality for FND tasks. However, none of these works have attempted to establish a comparative analysis of image models for FND tasks. Further, most works have only explored the use of the popular BERT text model for the processing of lexical features. Furthermore, none of the works have explored the application of ensemble-based decision-making for the text modality. It has been previously shown that the ensembling of text models can lead to improved performance <ref type="bibr" target="#b21">[19]</ref><ref type="bibr" target="#b22">[20]</ref><ref type="bibr" target="#b23">[21]</ref>. Though some works have recently started investigating methods to reduce the processing speed of FND systems <ref type="bibr" target="#b21">[19,</ref><ref type="bibr" target="#b22">20]</ref> there has overall been very little focus and no concrete conclusions as of now.</p><p>Drawing motivation from the above background, the proposed work presents a novel stacked ensemblebased architecture for multimodal fake news detection by employing decision level fusion of modalities. Further the work presents a comparative analysis of 18 pre-trained image models to select the most accurate model for image processing while keeping the number of parameters as low as possible to improve speed. Furthermore, the work introduces an ensemble of two deep learning architectures -ELECTRA and BERT for textual features. The ensemble is merged at the decision level using equal weightage averaging. Finally, image and text modalities are also merged at the decision level with the use of equal weightage averaging. This paper thus proposes a novel extractive FND method that focuses on both images and text modalities while aiming to improve the overall speed of the task. To evaluate its performance, the proposed method is tested on two datasets: Twitter MediaEval Dataset (2016) <ref type="bibr" target="#b24">[22]</ref> and Weibo Fake News Corpus. The significant contributions of the paper can be summarized as follows: 1. A deep learning based stacked ensemble multimodal architecture for FND is successfully presented and tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A comparative study of pretrained image models is carried out on two datasets to select the most accurate yet fast model. 3. The use of an ensemble of two pretrained text models: BERT and ELECTRA are proposed and successfully tested. 4. The work also reports a 20% reduction in image-based training parameters and a 122% reduction in text-based training parameters when compared to recent works [23] <ref type="bibr" target="#b26">[24]</ref>. Overall, the presented work is shown to be faster than recent counterparts while achieving superior performance.</p><p>The paper is organized as follows. Following the introduction in Section 1, Section 2 presents a brief survey of related works in the field. Section 3 describes the developed approach and proposed pipeline along with the above-mentioned comparative analysis of image models. It also includes a detailed overview of the datasets used and a brief study of the evaluation metrics. Section 4 summarizes the findings and provides a comparison with recent similar works. A comparative analysis with respect to the runtime of proposed and recent models is also presented. Finally, Section 5 brings the study to a close with some potential future ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head><p>This section is further divided into two subsections. Subsection 2.1 presents works that utilize a traditional approach to FND systems. These include both statistical methods and supervised machine learning based methods. Subsection 2.2 presents works that utilize deep learning to classify news as fake or real. These also include multimodal systems for FND. Finally, inferences and research gaps are drawn and presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Traditional Methods</head><p>Since the formulation of the problem of fake news detection, one of the most common approaches proposed has been that of exploiting linguistic features that capture different writing styles and sensational headlines. These features are extracted from the text in terms of document organizations from different levels, such as characters, words, sentences, and documents, and can uncover various relationships that aid in the detection of fake news <ref type="bibr" target="#b27">[25]</ref>. Suggesting one such correlation between the objectivity of language and trustworthiness of sources, Nakashole et al. <ref type="bibr" target="#b28">[26]</ref> introduced a believability computational model, FactChecker. Based on their initial analysis, the use of neutral and impartial language was found to be more closely related to trustworthy sources than a speculative and opinionated style of writing. This hypothesis formed the basis for calculating believability scores, along with the incorporation of the influence of co-mentions. Corroborating on these findings and investigating further, Rashkin et al. <ref type="bibr" target="#b15">[14]</ref> performed a deeper linguistic comparison of different types of fake news, i.e., propaganda, satire and hoaxes and for varying levels of truth. They reported uncertainty and vagueness to be a characteristic of false information, along with the use of subjective, superlatives, and modal adverbs. Khurana <ref type="bibr" target="#b29">[27]</ref> on the other hand, not only took into consideration the kind of words used in the text but also employed sentiment analysis to capture the entire emotion of the text while using POS Tagging and n-grams to obtain insights into the semantics and syntax of a statement. These extracted features were tested on several classifiers, with logistic regression yielding the best accuracy of 50.16% on the LIAR dataset. Leveraging logistic regression further, Tacchini et al. <ref type="bibr" target="#b30">[28]</ref> tested whether the set of users interacting with news posts on social networking sites can be used to ascertain if the news is fake. While the study reported promising results with logistic regression, the Boolean label crowdsourcing algorithm achieved superior results with more than 99% accuracy on a self-created dataset. Traditional classifiers have also been tested, with results varying primarily in the overall framework and feature extracted. Granik et al. <ref type="bibr" target="#b32">[29]</ref> for instance, reported a 74% accuracy with the Naive Bayes classifier on the Buzzfeed News dataset, employing a simple classification approach without any additional attributes.</p><p>In <ref type="bibr" target="#b33">[30]</ref>, Katsaros et al. evaluated representatives from eight well-known families of classification algorithms and concluded a space with hundred dimensions to be of adequate dimensionality to capture the needed text features with high accuracy. Moreover, Ahmad et al. in <ref type="bibr" target="#b34">[31]</ref>, evaluated individual learning classifiers against various ensemble models used as voting classifiers. These ensemble models included a combination of logistic regression, random forest, and KNN, and another of logistic regression, linear SVM, and classification and regression trees. The study reported that ensemble learners show an overall better score on all performance metrics as compared to the individual learners, serving as a motivation to adopt the ensemble approach in this work. Lastly, the use of Rhetorical Structure Theory in an analytic framework to identify systematic differences between false and truthful stories was explored by Rubin et al. <ref type="bibr" target="#b35">[32]</ref>. The authors utilized a Vector Space Model (VSM) to assess each story's position in a multi-dimensional RST space with respect to its distance to truth and treated deceptive centers as a measure of the story's degree of truthfulness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep-Learning and Multimodal Methods</head><p>Most recent works on fake news detection have concentrated on leveraging neural networks for this task and have reported significant improvements over the baseline results. Singhania et al. <ref type="bibr" target="#b37">[33]</ref> proposed a threelevel hierarchical attention network, one each for words, sentences, and the headline to construct a news vector, by processing an article in a hierarchical bottom-up manner. Because of the three layers of attention, the model gave differential importance to all parts of an article, achieving an accuracy of 96.77% on a large real-world dataset. Using a deep convolutional neural network (CNN), Kaliyar et al. <ref type="bibr" target="#b38">[34]</ref> used the model to automatically learn the discriminatory features through multiple hidden layers built in the network. By extracting several features at each layer and employing GloVe as a pre-trained word embedding, they reported an improvement of 5% over state-of-the-art results on the Kaggle news dataset. GloVe embeddings have also been found to be more useful in providing vector projections that reflect relations, similarities, dissimilarities with other words than the traditional Bag of Words, as reported by Aggarwal et al. in <ref type="bibr" target="#b39">[35]</ref>.</p><p>The study concentrated on solely relying on text processing without the context of the history and credibility of the author or source and achieved benchmark results due to word embeddings complementing the CNN and recurrent neural networks (RNN) based model. The utility of CNN and RNN in fake news prediction has further been investigated by Bahad et al <ref type="bibr" target="#b40">[36]</ref>. In <ref type="bibr" target="#b40">[36]</ref>, the authors evaluated and compared the accuracy of a Bi-directional LSTM-RNN model with CNN, vanilla RNN, and unidirectional LSTM-RNN. While CNN was found to perform better in extracting local and position-invariant features, LSTM-RNN was reported to be more useful for a long-range semantic dependency-based classification. Overall, Bi-directional LSTM-RNN model were reported to be significantly more effective than unidirectional models. Exploring the efficacy of semi-supervised approaches, Benamira et al. <ref type="bibr" target="#b42">[37]</ref> tested a simple nearest neighbor graph among articles based on word embedding similarities, accompanied by graph neural networks for classification. They achieved a performance improvement of up to 3% with only 10% of the labelled data, while also reducing the standard deviation in results and taking less time for processing. Their work provided a basis for semi-supervised content-based detection methods.</p><p>Recently there has been an increased interest in exploiting visual features along with textual context, as is explored in this work. Attempting to capture the hidden patterns in the words and images used in fake news, authors in <ref type="bibr" target="#b43">[38]</ref> extracted latent features via multiple convolutional layers. Their framework projected explicit and latent features into a unified feature space and was trained on both text and image inputs simultaneously. They achieved a precision of 92.20% on a dataset focused on news regarding the American presidential elections. Unlike the event-specific features learnt in <ref type="bibr" target="#b43">[38]</ref>, Wang et al. <ref type="bibr" target="#b25">[23]</ref> proposed an endto-end framework of adversarial neural networks to derive event-invariant features and thus facilitate the detection of fake news on newly arrived events. The work combines a multi-modal feature extractor for linguistic and visual features, a fake news detector to learn the discriminable representations and an event discriminator for removing features specific to an event while keeping the shared features among different events.</p><p>Comparable to the methodology adopted in this work, SpotFake <ref type="bibr" target="#b44">[39]</ref> is a multimodal framework leveraging language models to learn text features and pre-trained ImageNet models for extracting features from images. Combining word embeddings from BERT and feature representations from VGG-19, the two modalities are concatenated to form the news feature vector. Although the work employs computationally expensive models, it outperforms the baselines by a margin of 6% accuracy on average. Besides images, behavior of contributing parties in fake news propagation has also been explored <ref type="bibr" target="#b45">[40]</ref>. Ruchansky et al. <ref type="bibr" target="#b45">[40]</ref> integrated the temporal pattern of user activity on a given article with the source characteristic based on the behavior of users to perform classification and reported an accuracy of 89.20% on the Twitter Fake News dataset. Alrubaian et al. <ref type="bibr" target="#b46">[41]</ref> proposed a framework for credibility analysis to assess information credibility on Twitter as a means of preventing the proliferation of malicious information. Their proposed framework integrated four components -a reputation-based component to filter neglected information, a credibility classifier engine that distinguishes between credible and non-credible content, a user experience component that yields ratings of Twitter-user expertise on a specific topic, and a feature-ranking algorithm for selecting the best features based on their relative importance. Using conventional classifiers like random forests and naive Bayes for these components, the work achieved a significant balance between recall and precision on two datasets from 489,330 unique Twitter accounts. Along similar lines, Shu et al. <ref type="bibr" target="#b47">[42]</ref> investigated the importance of modelling user-user relations and news-user relations to capture effective feature representations. The authors explored the correlations of publisher bias, news stance, and relevant user engagements, in a Tri-Relationship Fake News detection framework. The framework extracted effective features from news publishers and user engagements separately, while also capturing the interrelationship, leading to good detection performance in the early stages of news dissemination. In <ref type="bibr" target="#b48">[43]</ref>, Aphiwongsophon et el. combined the text from tweets with metadata attributes such as verification status, followers count, number of hashtags and more. Naive Bayes and SVM classifiers were trained on the above feature set and the results were found comparable to that of simple neural networks, indicating that combining linguistic features with raw data attributes can be beneficial. Moving forward on this path <ref type="bibr" target="#b50">[44]</ref>, Gupta et al. made use of sub-word level embeddings from FastText, contextual embeddings from BERT and TF-IDF based ranking for the construction of a Bag of Words feature vector, all of which were combined with the user metadata to form a single feature vector. Their study reported an F1 score of 0.93 on the English Fake news dataset, in agreement with the findings of <ref type="bibr" target="#b50">[44]</ref>. Based on the above survey, the following three inferences were drawn. 1. The benefit of ensembling is clearly established when implemented in supervised ML techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Deep learning networks and specifically, pre-trained word embeddings have been shown to be superior when compared to feature-based classifiers. 3. Ensembling of pre-trained word embeddings has not been utilized for FND tasks. 4. Decision level fusion of modalities has been shown to be better than feature level fusion for FND tasks. 5. Very few works have focused on parameter reduction and on improving training time.</p><p>Motivated by the above inferences, this work introduces a novel stacked ensemble based multimodal architecture for faster fake news detection. To improve performance on image modality while keeping processing time low, unimodal analysis is carried out on 18 models. Pre-trained word embedding models are selected to create the ensemble for text analysis such that the number of parameters remain low while maintaining good accuracy. Decision level fusion is used to combine modalities and to combine stacked text models. The detailed methodology is further elaborated upon in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BRIEF DESCRIPTION OF USED DATASETS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Twitter MediaEval Dataset[22]</head><p>The Twitter MediaEval Dataset <ref type="bibr" target="#b24">[22]</ref> was curated as part of the '2015 Verifying Multimedia Use Challenge' conducted by MediaEval. The task involved identifying whether the multimedia items accompanying a tweet reflect the reality in the way purported by the tweet. The dataset comprises 17,000 unique tweets and their associated images, collected around several widely known events or news stories. The dataset is divided into two parts: the development set (9000 false news tweets, 6000 actual news tweets) and the test set (9000 fake news tweets, 6000 real news tweets) (2000 tweets). By cross-checking online sources such as articles and blogs, these tweets have been manually verified and classified into 'real' and 'fake' data points. A tweet is marked to be real if the corresponding image represents the event that the tweet refers to, while a fake tweet contains images that are not relevant to the event described in the tweet. For usage in this work, all datapoints with only unimodal information were removed from the dataset. Further, all datapoints with videos and GIFs were not considered. After these modifications, details of the corpus are shown in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="bibr" target="#b51">[45]</ref> The Weibo NER dataset is a Chinese Named Entity Recognition dataset containing false rumor posts from May 2012 to January 2016 collected from the social network, Sina Weibo. This multi-media dataset containing images, Graphics Interchange Format (GIF) and other media has all its posts verified by the official rumour debunking system of Weibo <ref type="bibr" target="#b52">[46]</ref>. Though Weibo follows the microblogging model of Twitter, there are some significant distinctions between the two <ref type="bibr" target="#b51">[45]</ref>. Firstly, some linguistic aspects usually studied for English tweets, such as case sensitivity of English words, repeated letters, and word lengthening, do not apply to the Chinese language used in Weibo. Moreover, the types of microblogs retweeted differ from those on Twitter. The datasets used in this work are thus varied and contribute to testing the robustness of the proposed framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Weibo Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PROPOSED METHODOLOGY</head><p>This section describes the methodology adopted in the proposed work. This section begins by providing a detailed overview of the suggested framework in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preprocessing of Images and Text</head><p>Prior to unimodal model selection for images and application of the pre-trained text embeddings on the text, various preprocessing steps were applied to the data to improve results. The preprocessing of text was kept as minimal as possible due to the already low character count of input data. The preprocessing steps applied to the images were:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Removal of GIFs and Videos:</head><p>In line with recent proposed works <ref type="bibr" target="#b25">[23,</ref><ref type="bibr" target="#b44">39]</ref>, both datasets were scanned and all datapoints including GIFs and videos as the only supporting modality were removed. This was done to keep the required processing power at a minimum and to generalize the training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Resizing of Image:</head><p>Since all pre-trained models provided by Keras <ref type="bibr" target="#b53">[47]</ref> have a standard input image dimensionality of (224, 254, 3), all images from both datasets were resized to these dimensions and stored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Image Normalization &amp; Noise Addition: Post resizing, normalization of all images was carried out. This was done by applying Min-Max Normalization on the inter-cubic distances of all images.</head><p>To improve training and to prevent overfitting, given the low number of unique images in both datasets, Gaussian Noise <ref type="bibr" target="#b54">[48]</ref> was added to all images artificially. The probability density function ( ) of a Gaussian random variable ( ) is given in Eq. 1.</p><formula xml:id="formula_0">( ) = 1 ?2 ? ? ( ? ) 2 2 2 (1)</formula><p>where, is the mean grey value; is the standard deviation.</p><p>As mentioned before, minimal preprocessing steps were applied to the text. The only applied step included removal of URLs and unwanted characters. Both datasets, specifically the Twitter MediaEval corpus, contain a large number of tweets. These usually have a high frequency of unwanted text such as URLs, email addresses, and hyperlinks. To prevent the effect of such characters on the output, regex expressions were employed to remove URLs and hyperlinks. Post the application of these preprocessing steps, the data was fed to the proposed architecture for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Selection for the Image Modality</head><p>After preprocessing the images, the training and validation splits of the Twitter MediaEval Dataset and the Weibo Fake News Corpus were used to select the optimal model for image modality inference. For the Twitter dataset, only the development (training) dataset was used with a 75-25% split to ensure no overlap with the test set. However, for the Weibo dataset, no such split is provided. Hence, a 7:1:2 split was used in line with the most popular works on the dataset <ref type="bibr" target="#b25">[23,</ref><ref type="bibr" target="#b26">24]</ref>. Since the focus of the work is to devise a multimodal method for FND, all datapoints with only one modality present were removed from consideration. Post these steps, the total number of images on which analysis was carried out was 10,024: 512 images from the Twitter Dataset and 9,528 images from the Weibo Corpus. The analysis was carried out on both datasets and the results from both were averaged to arrive at the final results. For the selection of the most suited image processing architecture, 17 pre-trained models were obtained from the Keras library. <ref type="table" target="#tab_1">Table 2</ref> summarizes the parameter count and other features of the 17 models. It should be noted that NasNet Large <ref type="bibr" target="#b56">[49]</ref> was not used due to a different input shape than all other models. Further, only two variations of the EfficientNet Model <ref type="bibr" target="#b57">[50]</ref> were used due to processing limitations. It should also be noted that prior to this work, VGG16 and VGG19 <ref type="bibr" target="#b58">[51]</ref> remained the most used models. Training and validation methodologies were kept the same for all models. Input dimensions were kept as (224, 224, 3) as discussed in subsection 4.1 All models were trained and validated on Google Collaboratory Pro [52] on an NVIDIA Tesla T4 GPU. The pooling layers were kept as 'avg' for all models and all layers were frozen. Training was done in a batch size of 8 for 3 epochs and categorical cross-entropy was used as the loss function. For a comparative analysis, the validation accuracy after 5 epochs was considered against the number of parameters in the model. Finally, these results were averaged for both datasets and were used to select the most appropriate model for image inference in FND. <ref type="figure">Fig 4.</ref> shows the average results for both datasets while <ref type="table" target="#tab_2">Table 3</ref> presents the results of the top 5 models. <ref type="figure">Fig. 4</ref>. Results of image model analysis on two datasets. It can be clearly seen from the obtained results that NasNet Mobile outperform all other models by a very large margin. While the validation accuracy of the top 5 models remains within 2% of each other, using NasNet Mobile results in a parameter reduction of 76.44% when compared to the second-best model. It is further noted that the model achieves an improvement of 4% over the more commonly used VGG16 architecture while reducing parameters by 70%. For these reasons, NasNet Mobile was finalized as the architecture used for image inference in SEMI-FND. The NasNet Mobile architecture is based on the Neural Architecture Search (NAS) framework proposed by <ref type="bibr" target="#b59">[53]</ref>. In this model, the overall architectures of the convolutional nets are manually predetermined. The model is characterized by repeating convolutional cells where each cell has the same architecture, but different weights. These convolutional cells are in turn of two types, namely, Normal Cells and Reduction Cells. Normal cells return a feature map of the same dimensions as the input feature map, while Reduction Cells return a feature map with the height and width reduced by a factor of two. The structures of these Normal and Reduction cells are searched by a controller RNN which recursively predicts the structure based on the provided hidden initial states. This is formulated as a reinforcement learning problem, with the resulting accuracies used to update the controller so that it generates better architectures over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Text Embedding Models: BERT and ELECTRA</head><p>In order to leverage information from textual data, vector representations of words were computed using BERT and ELECTRA. Bidirectional Encoder Representations from Transformers, or BERT is a deeply bidirectional model that provides contextualized word embeddings by effectively capturing information from both the right and left context of the input tokens. This is achieved through Masked Language Model (MLM), wherein instead of predicting the next word in a sequence like traditional language models, the discriminator in the model is trained to predict a missing word from within the sequence itself <ref type="bibr" target="#b60">[54]</ref>. Around 15% of the words are randomly masked i.e., replaced by <ref type="bibr">[MASK]</ref> tokens to indicate the missing word that has to be predicted, enabling the model to understand the relationship between words. In addition, BERT is also trained on the task of Next Sentence Prediction (NSP) to gain an understanding of the relationship between sentences. This effective pre-training approach of MLM and NSP has proved to be superlative in comparison to traditional NLP models with several recent works leveraging the model for NLP tasks. The improved performance, however, comes at great computational costs, with even the small BERT version used in this work requiring 30 million parameters to be trained. To improve the training time required and capture the context of the sentences from a different perspective, the lesser investigated ELCTRA model is also explored in this work. While the underlying architecture and most hyperparameters of ELECTRA are same as BERT, a different training approach is followed for the discriminator. Instead of masking a random selection of input tokens, ELECTRA replaces random tokens with plausible alternatives produced by a generator <ref type="bibr" target="#b62">[55]</ref>. The task of the discriminator is then to distinguish between the real and false data, allowing the model to learn from all the input tokens instead of focusing on solely the [MASK] tokens. Both the discriminator and generator consist of an encoder which maps the sequence on input tokens ( ) = [ 1 , ? , ] into a sequence of contextualized vector representations ?( ) = [? 1 , ? , ? ]. For a given position t, where ( ) = [ ], the generator outputs a probability for generating a particular token with a SoftMax layer given by:</p><formula xml:id="formula_1">( : ) = exp( ( ) ? ( ) ) ? exp( ( ? ) ? ( ) ) ? (2)</formula><p>where, e denotes token embeddings.</p><p>In essence, for a given position t, the discriminator predicts whether the token x(t) comes from the data or the generator distribution i.e., whether the token is "real" or not. This approach to pre-training makes the model computationally more efficient, outperforming BERT base by 5 points on the GLUE benchmark dataset with only 1/20th the parameters of the latter. While ELECTRA-small further reduces the training time, its accuracy reported on benchmark datasets such as SQuAD and GLUE is lesser than that of BERT. Therefore, to leverage the benefits of both models and take into account both training approaches, the presented work utilizes the small BERT model V2 <ref type="bibr" target="#b64">[56]</ref> for embedding textual information into a 768dimensional vector, along with the ELECTRA-small ++ model V2 <ref type="bibr" target="#b65">[57]</ref> that produces embeddings of 256 dimensions. By ensembling the outputs of these two models, the overall accuracy for lexical features is greatly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ensembling of Text Models and NasNet Mobile</head><p>As mentioned before, the proposed method employs stacked ensembling of the two text models to capture relevant information from both sets of pooled outputs. <ref type="figure" target="#fig_1">Fig. 5</ref> shows the detailed procedure by which this step is achieved. Per subsection 4.3, BERT (Small) and ELECTRA (Small++) were selected as the two text embedding models for the task. Specifically, the small BERT has 12 hidden layers, a hidden size of 768 and 12 attention heads. Both models are obtained via the TFHub library <ref type="bibr" target="#b66">[58]</ref>. To obtain the class probabilities from text representation, first the preprocessed text was fed through the text embedding models to obtain the pooled and sequence outputs. For this implementation, only the pooled outputs were used to reduce computation power and increase the speed of the architecture. The outputs were then run through a dense layer of 128 units. This was done for two reasons:</p><p>1. The output of the BERT architecture is of the shape (Batch_Size * 768) while that of the ELECTRA architecture is (Batch_Size * 256). To ensure uniform contribution of both text representations, the method reduces both outputs to 128 dimensions. 2. This further contributes to improving the speed of the architecture as well.</p><p>After obtaining the final text representation of both models, stacking was performed by concatenating outputs along the length. This resulted in a text representation of shape (Batch_Size * 256). Finally, to arrive at the class probabilities, the concatenated output was fed through a deep neural architecture composed of dense and dropout layers as shown in <ref type="figure" target="#fig_1">Fig. 5</ref>. A sigmoid layer was used to arrive at the output which was stored for further concatenation with the image modality. After obtaining the class probabilities for text, the same procedure is applied to images with the help of NasNet Mobile. As shown in <ref type="figure" target="#fig_2">Fig. 6</ref>, the preprocessed images were fed through the NasNet Mobile architecture to obtain the output representation. These output representations were then flattened and fed through a neural architecture to arrive at the image modality class probabilities. Finally, decision level fusion of both modalities was done by averaging the probabilities to arrive at the final result for each class <ref type="figure" target="#fig_2">(Fig. 6</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation Metrics</head><p>The proposed method is evaluated on two datasets as mentioned before. To keep the work consistent with other counterparts, the method is evaluated on the basis of class wise precision, recall and F1 score. The overall prediction accuracy is also calculated and presented. Formulas (2-5) represent the mathematical formulation of these metrics. It should be noted that not all relevant works make use of all the metrics mentioned above. Due to this limitation, comparisons have been made with works that make use of at least two of the four mentioned metrics. Further, to evaluate the speed of the architecture, the number of parameters is also used as a point of comparison. Further, the overall multimodal training time is also provided but not compared due to a lack of publications citing the same. </p><p>where, TP and TN stand for count of True Positives and True Negatives respectively, FP and FN stand for count of False Positives and False Negatives respectively, P, R and F1 stand for Precision, Recall and F1 Score respectively, i represents the i th class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS AND DISCUSSION</head><p>The efficacy of the suggested framework was assessed on the datasets presented in Section 3. All neural networks were implemented using TensorFlow and the pretrained models for images and text were extracted from Keras <ref type="bibr" target="#b67">[59]</ref> and TFHub <ref type="bibr" target="#b66">[58]</ref> respectively. It should be noted that Keras LR was used to hyper tune the learning rate for both the neural networks shown in <ref type="figure" target="#fig_1">Fig. 5 and 6</ref>. The Adam optimizer was used on both networks with categorical cross-entropy as the loss function. All tests were run on Google Collaboratory (GPU) on a 16GB Tesla T4 GPU and 32GB of RAM. For evaluation, four metrics were used, as mentioned previously. <ref type="table" target="#tab_3">Tables 4 and 5</ref> present the results obtained for the two datasets and compare the same with recent relevant works, while <ref type="table">Table 6</ref> presents the training time results and parameter comparison. <ref type="figure">Figures 7 and 8</ref> provide examples of generated outputs from both datasets. The section concludes by highlighting the advantages of the proposed model. Wang et al. <ref type="bibr" target="#b25">[23]</ref> were the first to propose an Event Adversarial Neural Network (EANN) which used an event discriminator to measure the dissimilarities among different events in order to generalize for new and emerging events. The work reported both event adversarial scores (EANN) and the event variant scores (EANN-), with the EANN achieving higher accuracy of 71.50%. However, they utilize a TextCNN <ref type="bibr" target="#b71">[62]</ref> which is surpassed in performance by deep embeddings utilized in this work. Further, the usage of NasNet Mobile in place of VGG-19 in this paper results in the proposed model outperforming their model by 14.30%. To tackle the issue of shared representation of textual and image features, Khattar et al. <ref type="bibr" target="#b26">[24]</ref> proposed an encoder-decoder architecture for multimodal FND (MVAE). The encoder module used by <ref type="bibr" target="#b26">[24]</ref> included a Bi-LSTM network to encode the textual data while using a VGG-19 architecture for images. Using a decoder to train the data against the encoder, they obtained an accuracy of 74.50%. Their work, however, fell short in class wise recall as shown in <ref type="table" target="#tab_3">Table 4</ref>. The proposed work outperforms MVAE by 11.30% while also showing a considerable increase in the class-wise recall and F1 scores. Singhal et al. <ref type="bibr" target="#b44">[39]</ref> utilized a multimodal architecture of BERT and VGG-19 (SpotFake) with decision level fusion to achieve 77.80% accuracy. Their architecture also utilized a reduced representation of only 32 dimensions for both text and images to improve processing speed. On the other hand, the proposed framework in this work utilizes a larger dimensional representation for images and text while reducing the number of parameters by opting for smaller models. This, along with the stacked ensemble approach allows for an increase of 8% over SpotFake. In a departure from deep learning approaches, Shah et al. <ref type="bibr" target="#b68">[60]</ref> utilized a cultural genetic algorithm to arrive at an accuracy of 79.8% which is surpassed by 6% by SEMI-FND. Finally, Singh et al. <ref type="bibr" target="#b69">[61]</ref> used a combination of RoBERTa <ref type="bibr" target="#b72">[63]</ref> and EfficientNetB0 <ref type="bibr" target="#b57">[50]</ref> to improve the accuracy while reducing processing time. They reported a high accuracy of 85.30% while also reporting high F1 scores. indicating the good performance of the model on the imbalanced nature of FND. While the proposed model provides only a minor improvement of 0.5% over <ref type="bibr" target="#b69">[61]</ref>, it succeeds in significantly reducing the computational requirements as shown in <ref type="table">Table 6</ref>. While the above presented results in <ref type="table" target="#tab_3">Table 4</ref> and 5 show that SEMI-FND performs better than other recently proposed works, <ref type="table">Table 6</ref> compares the modality wise parameter count of these proposed works against that of SEMI-FND. It can be clearly seen that in the case of image modality, only <ref type="bibr" target="#b69">[61]</ref> comes close to the low parameter count of SEMI-FND. The proposed work offers a minimum parameter reduction of 19.44% and a maximum reduction of 78.67%. Even in the text modality, the ensemble of ELECTRA and BERT reduces parameter count by 60%. This shows that the proposed work not only performs well on state-of-the-art datasets, but does so in a much more efficient and fast manner. However, none of the compared works take into account the training time metric and thus a comparison could not be made with this metric. The authors hope that this work will allow for more FND based models to provide their training times as a metric since the speed and efficiency of the model is essential in real life use cases.</p><p>From the above-presented results and discussions, the merits of the proposed method can be summarized as follows: 1. As is evident from Tables 4 and 5, the SEMI-FND model presents superior accuracy and F1 scores to notable recent works. The evaluation of the model of two datasets in different languages further reinforces its credibility and adaptability. 2. As mentioned before, the developed approach is computationally less expensive than most current works utilizing resource-heavy models such as VGG19 and LSTM-RNNs. Further, the model still maintains better accuracy than the mentioned papers. 3. As a direct consequence of the previous point, the training time of the proposed pipeline is considerably lower and offers a significant advantage for real-time applications, as early detection is especially useful in containing the spread of false information. 4. Lastly, in addition to improved accuracy, the presented framework achieves balanced scores between the real and fake classes, indicating that the model is not biased towards one particular class preventing systematic prejudice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION AND FUTURE WORK</head><p>This work successfully presented an efficient multimodal framework for fake news detection utilizing BERT and ELECTRA models for extracting contextual features from given text and NasNet Mobile for processing of the associated images. It also illustrated the viability of stacking textual embeddings and decision level fusion of modalities. The proposed framework greatly reduces the compute requirements for the task along with minimizing the total time required to make the decision, while maintaining respectable accuracy. The performance of the suggested approach was evaluated using accuracy, precision, recall and F1 score on two publicly available datasets, namely Twitter and Weibo. The proposed pipeline offered an accuracy of 85.80% on the Twitter MediaEval 2015 dataset and 86.83% on the Weibo dataset. These results, being superior to their counterparts, clearly highlight the effectiveness of a multimodal approach and use of simple DNN networks in conjugation with specific processing models for each modality as opposed to conventional classifiers, adversarial networks or compute heavy DNN models such as LSTMs and RNNs. Though the presented framework achieves superior performance over recent works, a natural progression of this work would be to incorporate user metadata in the proposed pipeline to both improve the efficacy of distinguishing between real and false information and to identify accounts that are likely to contribute to the dissemination of misinformation. The same has been difficult to accomplish due to the dearth of datasets that include resources to extract all relevant features. A promising direction would thus be to create a comprehensive and large-scale fake news benchmark dataset covering linguistic and visual features with associated user profiles and metadata. Further, leveraging concepts such as POS tagging and topic modeling would be worth investigating as they take into account textual semantics and can be used to learn the latent stance from topics, thereby for aiding models in feature extraction and fine-tuned classification. Lastly, these NLP techniques can also be combined with information retrieval and reinforcement learning as most existing research focuses on supervised learning approaches and the accuracy of semi-supervised, unsupervised, and hybrid approaches has not yet been adequately tested.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig 3 .Fig. 3 .</head><label>33</label><figDesc>Subsection 4.1 introduces the preprocessing steps applied to visual and textual features. Following that, Subsection 4.2 explains the procedure used for unimodal model analysis for the image modality. It also includes the results of the analysis which in turn represent the final model used for images. Subsection 4.3 introduces the text embedding models employed, namely BERT and ELECTRA. The ensembling of the text models along with the decision level fusion of text and image modalities is explained in Subsection 4.4. Finally, Subsection 4.5 introduces the metrics used for evaluating the efficacy of the approach Detailed description of the proposed methodology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Stacked Ensembling of BERT and ELECTRA models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Fusion of Modalities to arrive at final output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Summary of datasets used</figDesc><table><row><cell>Dataset</cell><cell>Real</cell><cell>Training</cell><cell>Fake</cell><cell>Real</cell><cell>Testing</cell><cell>Fake</cell></row><row><cell>Twitter MediaEval</cell><cell>5008</cell><cell></cell><cell>7032</cell><cell>1217</cell><cell></cell><cell>2564</cell></row><row><cell>Weibo</cell><cell>4274</cell><cell></cell><cell>4274</cell><cell>475</cell><cell></cell><cell>475</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Summary of models used for Image Analysis and Model Selection</figDesc><table><row><cell>Name</cell><cell>Size (MB)</cell><cell>No. Of Parameters</cell><cell>Depth</cell></row><row><cell></cell><cell></cell><cell>(Millions)</cell><cell></cell></row><row><cell>Xception</cell><cell>88</cell><cell>22.90</cell><cell>81</cell></row><row><cell>MobileNet</cell><cell>16</cell><cell>4.30</cell><cell>55</cell></row><row><cell>MobileNet V2</cell><cell>14</cell><cell>3.50</cell><cell>105</cell></row><row><cell>MobileNet V3 Large</cell><cell>NA</cell><cell>5.40</cell><cell>217</cell></row><row><cell>ResNet101</cell><cell>171</cell><cell>44.70</cell><cell>209</cell></row><row><cell>ResNet 101 V2</cell><cell>171</cell><cell>44.70</cell><cell>205</cell></row><row><cell>ResNet 152 V2</cell><cell>232</cell><cell>60.40</cell><cell>307</cell></row><row><cell>ResNet50 V2</cell><cell>98</cell><cell>25.60</cell><cell>103</cell></row><row><cell>InceptionResNetV2</cell><cell>215</cell><cell>55.90</cell><cell>449</cell></row><row><cell>EfficientNetB0</cell><cell>29</cell><cell>5.30</cell><cell>132</cell></row><row><cell>EfficientNetB7</cell><cell>256</cell><cell>66.70</cell><cell>438</cell></row><row><cell>VGG 16</cell><cell>528</cell><cell>138.40</cell><cell>16</cell></row><row><cell>VGG 19</cell><cell>549</cell><cell>143.70</cell><cell>19</cell></row><row><cell>NASNet Mobile</cell><cell>23</cell><cell>4.30</cell><cell>389</cell></row><row><cell>DenseNet 121</cell><cell>33</cell><cell>8.10</cell><cell>242</cell></row><row><cell>DenseNet 169</cell><cell>57</cell><cell>14.30</cell><cell>338</cell></row><row><cell>DenseNet 201</cell><cell>88</cell><cell>20.20</cell><cell>402</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results of top 5 models</figDesc><table><row><cell>Name of Model</cell><cell>Number of Parameters</cell><cell>Validation Accuracy (%)</cell></row><row><cell></cell><cell></cell><cell>After 3 Epochs</cell></row><row><cell>NasNet Mobile</cell><cell>4,269,716</cell><cell>74.31</cell></row><row><cell>DenseNet 201</cell><cell>18,321,984</cell><cell>73.67</cell></row><row><cell>ResNet 101 V2</cell><cell>42,626,560</cell><cell>73.13</cell></row><row><cell>ResNet 152 V2</cell><cell>58,331,648</cell><cell>72.67</cell></row><row><cell>DenseNet 121</cell><cell>7,037,504</cell><cell>72.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Performance Comparison Summary: Twitter MediaEval Dataset</figDesc><table><row><cell>Method</cell><cell>Accuracy (%)</cell><cell cols="2">Class: Fake (%) Precision Recall</cell><cell>F1</cell><cell cols="2">Class: Real (%) Precision Recall</cell><cell>F1</cell></row><row><cell>EANN [23]</cell><cell>71.50</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>EANN (-) [23]</cell><cell>64.80</cell><cell>81.00</cell><cell>49.80</cell><cell>61.70</cell><cell>58.40</cell><cell>75.90</cell><cell>66.00</cell></row><row><cell>MVAE [24]</cell><cell>74.50</cell><cell>80.10</cell><cell>71.90</cell><cell>75.80</cell><cell>68.90</cell><cell>77.70</cell><cell>73.00</cell></row><row><cell>SpotFake [39]</cell><cell>77.80</cell><cell>75.10</cell><cell>90.00</cell><cell>82.00</cell><cell>83.20</cell><cell>60.06</cell><cell>70.10</cell></row><row><cell>Cultural Algo [60]</cell><cell>79.80</cell><cell>79.10</cell><cell>83.30</cell><cell>76.00</cell><cell>79.10</cell><cell>83.30</cell><cell>76.00</cell></row><row><cell>Efficient Roberta [61]</cell><cell>85.30</cell><cell>82.10</cell><cell>94.30</cell><cell>82.70</cell><cell>91.30</cell><cell>74.50</cell><cell>82.00</cell></row><row><cell>Proposed Method</cell><cell>85.80</cell><cell>73.70</cell><cell>95.50</cell><cell>83.20</cell><cell>95.30</cell><cell>72.90</cell><cell>82.60</cell></row><row><cell>(SEMI-FND)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Performance Comparison Summary: Weibo DatasetFor the Weibo dataset, the proposed method is compared with four works, all of which are similar to the ones discussed inTable 4. Wang et al.'s<ref type="bibr" target="#b25">[23]</ref> EANN architecture achieved a score of 82.70% on the Weibo dataset using their event invariant version and a score of 79.50% using the event variant one. In comparison, the proposed work achieves a score of 86.83%: an improvement of 4.13%. MVAE, proposed by Khattar et al.<ref type="bibr" target="#b26">[24]</ref> also achieved scores similar to Wang et al.<ref type="bibr" target="#b25">[23]</ref> with the highest reported accuracy being 82.40%. Once again, an improvement of Y% was seen when compared to SEMI-FND. Prior to the presented work, SpotFake<ref type="bibr" target="#b44">[39]</ref> had the highest scores on the Weibo dataset with an accuracy of 89.23%. This could be attributed to the low dimensionality of the text features used by Singhal et al.<ref type="bibr" target="#b44">[39]</ref> which reduced the errors introduced by the translated nature of the Weibo dataset (Chinese to English). While Singh et al.<ref type="bibr" target="#b69">[61]</ref> had the highest scores on the Twitter MediaEval dataset prior to the proposed work, they obtain relatively low scores on the Weibo dataset. They attributed this to two major reasons:1. The Weibo dataset contains a large frequency of images with human faces in comparison to the Twitter MediaEval dataset. 2. Due to the complexity of the Chinese dataset, the translation is not very exact. In addition, the posts in the Weibo dataset were longer than the shorter tweets on Twitter. SEMI-FND, however, achieves an improvement over both these works by increasing performance over<ref type="bibr" target="#b69">[61]</ref> by 5.63%. Comparison of Parameter Count</figDesc><table><row><cell>Method</cell><cell>Accuracy (%)</cell><cell cols="2">Class: Fake (%) Precision Recall</cell><cell>F1</cell><cell cols="2">Class: Real (%) Precision Recall</cell><cell>F1</cell></row><row><cell>EANN</cell><cell>82.70</cell><cell>82.70</cell><cell>69.70</cell><cell>75.60</cell><cell>75.20</cell><cell>86.30</cell><cell>80.40</cell></row><row><cell>EANN (-)</cell><cell>79.50</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>MVAE</cell><cell>82.40</cell><cell>85.40</cell><cell>76.90</cell><cell>80.90</cell><cell>80.20</cell><cell>87.50</cell><cell>83.70</cell></row><row><cell>Efficient Roberta</cell><cell>81.20</cell><cell>85.10</cell><cell>78.40</cell><cell>81.60</cell><cell>74.40</cell><cell>82.60</cell><cell>78.20</cell></row><row><cell>Proposed Method</cell><cell>86.83</cell><cell>87.37</cell><cell>80.20</cell><cell>83.63</cell><cell>86.29</cell><cell>87.10</cell><cell>80.70</cell></row><row><cell>(SEMI-FND)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://www.cits.ucsb.edu/fake-news/brief-history" />
		<title level="m">A Brief History of Fake News | Center for Information Technology and Society -UC</title>
		<meeting><address><addrLine>Santa Barbara</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-02-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Misleading online content: Recognizing clickbait as &quot;false news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2823465.2823467</idno>
		<ptr target="https://doi.org/10.1145/2823465.2823467" />
	</analytic>
	<monogr>
		<title level="m">WMDD 2015 -Proceedings of the ACM Workshop on Multimodal Deception Detection</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="15" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Allcott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gentzkow</surname></persName>
		</author>
		<idno type="DOI">10.1257/JEP.31.2.211</idno>
		<ptr target="https://doi.org/10.1257/JEP.31.2.211" />
	</analytic>
	<monogr>
		<title level="m">Social Media and Fake News in the 2016 Election</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="211" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An overview of online fake news: Characterization, detection, and discussion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ghorbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">102025</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/J.IPM.2019.03.004</idno>
		<ptr target="https://doi.org/10.1016/J.IPM.2019.03.004" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Traore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saad</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-69155-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-69155-8_9" />
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<ptr target="https://trends.google.com/trends/?geo=IN" />
		<title level="m">Google Trends</title>
		<imprint>
			<date type="published" when="2022-02-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balmas</surname></persName>
		</author>
		<idno type="DOI">10.1177/0093650212453600</idno>
		<ptr target="Http://Dx.Doi.Org/10.1177/0093650212453600.41" />
		<title level="m">When Fake News Becomes Real: Combined Exposure to Multiple News Sources and Political Attitudes of Inefficacy, Alienation, and Cynicism</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="430" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="DOI">10.1177/0093650212453600</idno>
		<ptr target="https://doi.org/10.1177/0093650212453600" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Influence of fake news in Twitter during the 2016 US presidential election</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Makse</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-018-07761-2</idno>
		<ptr target="https://doi.org/10.1038/s41467-018-07761-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Novel Coronavirus (2019-nCoV): situation report</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The proximal origin of SARS-CoV-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rambaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">I</forename><surname>Lipkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Garry</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-020-0820-9</idno>
		<ptr target="https://doi.org/10.1038/s41591-020-0820-9" />
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="page" from="450" to="452" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>26:4. 26 (2020</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Social Media Fake News in India</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Al-Zaman</surname></persName>
		</author>
		<idno type="DOI">10.15206/AJPOR.2021.9.1.25</idno>
		<ptr target="https://doi.org/10.15206/AJPOR.2021.9.1.25" />
	</analytic>
	<monogr>
		<title level="j">Asian Journal for Public Opinion Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="25" to="47" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Language-Aware Truth Assessment of Fact Candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.3115/V1/P14-1095</idno>
		<ptr target="https://doi.org/10.3115/V1/P14-1095" />
	</analytic>
	<monogr>
		<title level="m">52nd Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1009" to="1019" />
		</imprint>
	</monogr>
	<note>ACL 2014 -Proceedings of the Conference</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fake News Detection Using Machine Learning Ensemble Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yousaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yousaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Ahmad</surname></persName>
		</author>
		<idno type="DOI">10.1155/2020/8885861</idno>
		<ptr target="https://doi.org/10.1155/2020/8885861" />
	</analytic>
	<monogr>
		<title level="j">Complexity</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/D17-1317</idno>
		<ptr target="https://doi.org/10.18653/V1/D17-1317" />
	</analytic>
	<monogr>
		<title level="m">EMNLP 2017 -Conference on Empirical Methods in Natural Language Processing, Proceedings</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2931" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Credibility Analysis System for Assessing Information on Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alrubaian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Qurishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alamri</surname></persName>
		</author>
		<idno type="DOI">10.1109/TDSC.2016.2602338</idno>
		<ptr target="https://doi.org/10.1109/TDSC.2016.2602338" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Dependable and Secure Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="661" to="674" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hostility Detection and Covid-19 Fake News Detection in Social Media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teki</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2101.05953</idno>
		<ptr target="https://doi.org/10.48550/arxiv.2101.05953" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A link2vec-based fake news detection model using web search results, Expert Systems with Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page">115491</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/J.ESWA.2021.115491</idno>
		<ptr target="https://doi.org/10.1016/J.ESWA.2021.115491" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SpotFake: A multi-modal framework for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumaraguru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
		<idno type="DOI">10.1109/BIGMM.2019.00-44</idno>
		<ptr target="https://doi.org/10.1109/BIGMM.2019.00-44" />
	</analytic>
	<monogr>
		<title level="m">Proceedings -2019 IEEE 5th International Conference on Multimedia Big Data</title>
		<meeting>-2019 IEEE 5th International Conference on Multimedia Big Data</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="39" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<ptr target="https://aclanthology.org/2020.smm4h-1.5/" />
		<title level="m">Ensemble BERT for Classifying Medication-mentioning Tweets -ACL Anthology</title>
		<imprint>
			<date type="published" when="2022-04-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ensemble machine learning model for classification of spam product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">U</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Uddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alouffi</surname></persName>
		</author>
		<idno type="DOI">10.1155/2020/8857570</idno>
		<ptr target="https://doi.org/10.1155/2020/8857570" />
	</analytic>
	<monogr>
		<title level="j">Complexity</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2002.10345</idno>
		<ptr target="https://doi.org/10.48550/arxiv.2002.10345" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<ptr target="http://www.multimediaeval.org/mediaeval2016/" />
	</analytic>
	<monogr>
		<title level="j">MediaEval</title>
		<imprint>
			<date type="published" when="2016-04-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">EANN: Event adversarial neural networks for multi-modal fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219903</idno>
		<ptr target="https://doi.org/10.1145/3219819.3219903" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="849" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimodal variational autoencoder for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Khattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Goud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mvae</forename></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313552</idno>
		<ptr target="https://doi.org/10.1145/3308558.3313552" />
	</analytic>
	<monogr>
		<title level="m">The Web Conference 2019 -Proceedings of the World Wide Web Conference</title>
		<imprint>
			<publisher>WWW 2019</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2915" to="2921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fake News Detection on Social Media: A Data Mining Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.1708.01967</idno>
		<ptr target="https://doi.org/10.48550/arxiv.1708.01967" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Language-Aware Truth Assessment of Fact Candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.3115/V1/P14-1095</idno>
		<ptr target="https://doi.org/10.3115/V1/P14-1095" />
	</analytic>
	<monogr>
		<title level="m">52nd Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1009" to="1019" />
		</imprint>
	</monogr>
	<note>ACL 2014 -Proceedings of the Conference</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The Linguistic Features of Fake News Headlines and Statements, University of Amsterdam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Khurana</surname></persName>
		</author>
		<ptr target="https://vdocument.in/the-linguistic-features-of-fake-news-headlines-and-statements-sify-fake-news-the.html" />
		<imprint>
			<date type="published" when="2017-04-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Some Like it Hoax: Automated Fake News Detection in Social Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tacchini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ballarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Della Vedova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Alfaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<idno type="DOI">10.48550/arxiv.1704.07506</idno>
		<ptr target="https://doi.org/10.48550/arxiv.1704.07506" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fake news detection using naive Bayes classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mesyura</surname></persName>
		</author>
		<idno type="DOI">10.1109/UKRCON.2017.8100379</idno>
		<ptr target="https://doi.org/10.1109/UKRCON.2017.8100379" />
	</analytic>
	<monogr>
		<title level="m">IEEE 1st Ukraine Conference on Electrical and Computer Engineering</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="900" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Which machine learning paradigm for fake news detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Katsaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stavropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papakostas</surname></persName>
		</author>
		<idno type="DOI">10.1145/3350546.3352552</idno>
		<ptr target="https://doi.org/10.1145/3350546.3352552" />
	</analytic>
	<monogr>
		<title level="m">Proceedings -2019 IEEE/WIC/ACM International Conference on Web Intelligence</title>
		<meeting>-2019 IEEE/WIC/ACM International Conference on Web Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="383" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fake News Detection Using Machine Learning Ensemble Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yousaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yousaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Ahmad</surname></persName>
		</author>
		<idno type="DOI">10.1155/2020/8885861</idno>
		<ptr target="https://doi.org/10.1155/2020/8885861" />
	</analytic>
	<monogr>
		<title level="j">Complexity</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Identification of Truth and Deception in Text: Application of Vector Space Model to Rhetorical Structure Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vashchilko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<idno type="DOI">10.5555/2388616.2388631</idno>
		<ptr target="https://doi.org/10.5555/2388616.2388631" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3HAN: A Deep Neural Network for Fake News Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-70096-0_59</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-70096-0_59" />
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="572" to="581" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">FNDNet -A deep convolutional neural network for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Kaliyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.COGSYS.2019.12.005</idno>
		<ptr target="https://doi.org/10.1016/J.COGSYS.2019.12.005" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="32" to="44" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fake News Detection Using a Blend of Neural Networks: An Application of Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="DOI">10.1007/S42979-020-00165-4</idno>
		<ptr target="https://doi.org/10.1007/S42979-020-00165-4" />
	</analytic>
	<monogr>
		<title level="j">SN Computer Science</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fake News Detection using Bi-directional LSTM-Recurrent Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bahad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kamal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="74" to="82" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/J.PROCS.2020.01.072</idno>
		<ptr target="https://doi.org/10.1016/J.PROCS.2020.01.072" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semi-supervised learning and graph neural networks for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benamira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lesot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Malliaros</surname></persName>
		</author>
		<idno type="DOI">10.1145/3341161.3342958</idno>
		<ptr target="https://doi.org/10.1145/3341161.3342958" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2019</title>
		<meeting>the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="568" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ti-Cnn</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.1806.00749</idno>
		<ptr target="https://doi.org/10.48550/arxiv.1806.00749" />
		<title level="m">Convolutional Neural Networks for Fake News Detection</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SpotFake: A multi-modal framework for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumaraguru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
		<idno type="DOI">10.1109/BIGMM.2019.00-44</idno>
		<ptr target="https://doi.org/10.1109/BIGMM.2019.00-44" />
	</analytic>
	<monogr>
		<title level="m">Proceedings -2019 IEEE 5th International Conference on Multimedia Big Data</title>
		<meeting>-2019 IEEE 5th International Conference on Multimedia Big Data</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="39" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ruchansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3132847.3132877</idno>
		<ptr target="https://doi.org/10.1145/3132847.3132877" />
		<title level="m">CSI: A Hybrid Deep Model for Fake News Detection, International Conference on Information and Knowledge Management, Proceedings. Part F131841</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Credibility Analysis System for Assessing Information on Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alrubaian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Qurishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alamri</surname></persName>
		</author>
		<idno type="DOI">10.1109/TDSC.2016.2602338</idno>
		<ptr target="https://doi.org/10.1109/TDSC.2016.2602338" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Dependable and Secure Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="661" to="674" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Beyond News Contents: The Role of Social Context for Fake News Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3289600.3290994</idno>
		<ptr target="https://doi.org/10.1145/3289600.3290994" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Detecting fake news with machine learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aphiwongsophon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chongstitvatana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECTI-CON 2018 -15th International Conference on Electrical Engineering/Electronics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="528" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ECTICON.2018.8620051</idno>
		<ptr target="https://doi.org/10.1109/ECTICON.2018.8620051" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Hostility Detection and Covid-19 Fake News Detection in Social Media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teki</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2101.05953</idno>
		<ptr target="https://doi.org/10.48550/arxiv.2101.05953" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multimodal fusion with recurrent neural networks for rumor detection on microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3123266.3123454</idno>
		<ptr target="https://doi.org/10.1145/3123266.3123454" />
	</analytic>
	<monogr>
		<title level="m">MM 2017 -Proceedings of the 2017 ACM Multimedia Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="795" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Automatic detection of rumor on Sina Weibo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2350190.2350203</idno>
		<ptr target="https://doi.org/10.1145/2350190.2350203" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title/>
		<ptr target="https://keras.io/api/applications/(accessedApril9" />
	</analytic>
	<monogr>
		<title level="j">Keras Applications</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Boyat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Review Paper: Noise Models in Digital Image Processing, Signal &amp; Image Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="63" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<idno type="DOI">10.48550/arxiv.1505.03489</idno>
		<ptr target="https://doi.org/10.48550/arxiv.1505.03489" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning Transferable Architectures for Scalable Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.1707.07012</idno>
		<ptr target="https://doi.org/10.48550/arxiv.1707.07012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efficientnet</forename></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.1905.11946</idno>
		<ptr target="https://doi.org/10.48550/arxiv.1905.11946" />
		<title level="m">Rethinking Model Scaling for Convolutional Neural Networks, 36th International Conference on Machine Learning, ICML 2019</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="10691" to="10700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.1409.1556</idno>
		<ptr target="https://doi.org/10.48550/arxiv.1409.1556" />
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition, 3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.1611.01578</idno>
		<ptr target="https://doi.org/10.48550/arxiv.1611.01578" />
		<title level="m">Neural Architecture Search with Reinforcement Learning, 5th International Conference on Learning Representations, ICLR 2017 -Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<title level="m">Pre-training of Deep Bidirectional Transformers for Language Understanding, NAACL HLT 2019 -2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<idno type="DOI">10.48550/arxiv.1810.04805</idno>
		<ptr target="https://doi.org/10.48550/arxiv.1810.04805" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le Google Brain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<idno type="DOI">10.48550/arxiv.2003.10555</idno>
		<ptr target="https://doi.org/10.48550/arxiv.2003.10555" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tensorflow</forename><surname>Hub</surname></persName>
		</author>
		<ptr target="https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/2" />
		<imprint>
			<date type="published" when="2022-05-17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tensorflow</forename><surname>Hub</surname></persName>
		</author>
		<ptr target="https://tfhub.dev/google/electra_small/2" />
		<imprint>
			<date type="published" when="2022-05-17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Home | Tensorflow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hub</surname></persName>
		</author>
		<ptr target="https://tfhub.dev/" />
		<imprint>
			<date type="published" when="2022-05-17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Keras: the Python deep learning API</title>
		<ptr target="https://keras.io/" />
		<imprint>
			<date type="published" when="2022-05-17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Multimodal fake news detection using a Cultural Algorithm with situational and normative knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kobti</surname></persName>
		</author>
		<idno type="DOI">10.1109/CEC48606.2020.9185643</idno>
		<ptr target="https://doi.org/10.1109/CEC48606.2020.9185643" />
	</analytic>
	<monogr>
		<title level="m">CEC 2020 -Conference Proceedings</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Predicting image credibility in fake news over social media using multimodal approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/S00521-021-06086-4/FIGURES/10</idno>
		<ptr target="https://doi.org/10.1007/S00521-021-06086-4/FIGURES/10" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">TextCNN with Attention for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alshubaily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2108.01921</idno>
		<ptr target="https://doi.org/10.48550/arxiv.2108.01921" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<idno type="DOI">10.48550/arxiv.1907.11692</idno>
		<ptr target="https://doi.org/10.48550/arxiv.1907.11692" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
