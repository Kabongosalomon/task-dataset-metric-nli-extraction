<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Enhanced Span-based Decomposition Method for Few-Shot Sequence Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
							<email>wangpeiyi9979@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxin</forename><surname>Xu</surname></persName>
							<email>runxinxu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Cloud Xiaowei</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
							<email>qingyuzhou@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent Cloud Xiaowei</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
							<email>yunbocao@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent Cloud Xiaowei</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Enhanced Span-based Decomposition Method for Few-Shot Sequence Labeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-Shot Sequence Labeling (FSSL) is a canonical paradigm for the tagging models, e.g., named entity recognition and slot filling, to generalize on an emerging, resourcescarce domain. Recently, the metric-based meta-learning framework has been recognized as a promising approach for FSSL. However, most prior works assign a label to each token based on the token-level similarities, which ignores the integrality of named entities or slots. To this end, in this paper, we propose ESD, an Enhanced Span-based Decomposition method for FSSL. ESD formulates FSSL as a spanlevel matching problem between test query and supporting instances. Specifically, ESD decomposes the span matching problem into a series of span-level procedures, mainly including enhanced span representation, class prototype aggregation and span conflicts resolution. Extensive experiments show that ESD achieves the new state-of-the-art results on two popular FSSL benchmarks, FewNERD and SNIPS, and is proven to be more robust in the nested and noisy tagging scenarios. Our code is available at https://github. com/Wangpeiyi9979/ESD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many natural language understanding tasks can be formulated as sequence labeling tasks, such as Named Entity Recognition (NER) and Slot Filling (SF) tasks. Most prior works on sequence labeling follow the supervised learning paradigm, which requires large-scale annotated data and is limited to pre-defined classes. In order to generalize on the emerging, resource-scare domains, Few-Shot Sequence Labeling (FSSL) has been proposed <ref type="bibr" target="#b15">(Hou et al., 2020;</ref><ref type="bibr" target="#b30">Yang and Katiyar, 2020</ref>  <ref type="bibr">(never accesses "PER" and "ORG" data)</ref> query : [Albert Einstein]PER was born in Germany data) needs to solve the N -way (N unseen classes) K-shot (only K annotated examples for each class) task in the target domain. <ref type="figure" target="#fig_0">Figure 1</ref> shows a 2-way 2-shot target domain few-shot NER task, where 'PER' and 'ORG' are 2 unseen entity types, and in the training set S = {S 1 , S 2 }, both of them have only 2 annotated entities. The tagging models should annotate 'Albert Einstein' in the test sentence q as a 'PER' according to S.</p><p>Recently, metric-based meta-learning (MBML) methods have become the mainstream and state-ofthe-art methods in FSSL <ref type="bibr" target="#b15">(Hou et al., 2020;</ref><ref type="bibr" target="#b6">Ding et al., 2021)</ref>, which train the models on the tasks sampled from the source domain sentences in order to mimic and solve the target task. In each task of training and testing, they make the prediction through modeling the similarity between the training set (support set) and the test sentence (query). Specifically, previous MBML methods <ref type="bibr" target="#b6">(Ding et al., 2021;</ref><ref type="bibr" target="#b15">Hou et al., 2020;</ref><ref type="bibr" target="#b30">Yang and Katiyar, 2020)</ref> mainly formulate FSSL as a token-level matching problem, which assigns a label to each token based on the token-level similarities. For example, the token 'Albert' would be labeled as 'B-PER' due to its resemblance to 'Steve' and 'Isaac'.</p><p>However, for the MBML methods, selecting the appropriate metric granularity would be fundamental to the success. We argue that prior works that focus on modeling the token-level similarity are sub-optimal in FSSL: 1) they ignore the integrality of named entities or dialog slots that are composed of a text span instead of a single token. 2) in FSSL, the conditional random fields (CRF) is an important component for the token-level sequence labeling models <ref type="bibr" target="#b30">(Yang and Katiyar, 2020;</ref><ref type="bibr" target="#b15">Hou et al., 2020)</ref>, but the transition probability between classes in the target domain task can not be sufficiently learned with very few examples <ref type="bibr" target="#b32">(Yu et al., 2021)</ref>. The previous methods resort to estimate the values with the abundant source domain data, which may suffer from domain shift problem <ref type="bibr" target="#b15">(Hou et al., 2020)</ref>. To overcome these drawbacks of the token-level models, in this paper, we propose ESD, an Enhanced Span-based Decomposition model that formulates FSSL as a span-level matching problem between test query and support set instances.</p><p>Specifically, ESD decomposes the span matching problem into three main subsequent span-level procedures. 1) Span representation. We find the span representation can be enhanced by information from other spans in the same sentence and the interactions between query and support set. Thus we propose a span-enhancing module to reinforce the span representation by intra-span and interspan interactions. 2) Span prototype aggregation. MBML methods usually aggregate the span vectors that belongs to the same class in the support set to form the class prototype representation. Among all class prototypes, the O-type serves as negative examples and covers all the miscellaneous spans that are not entities, which poses new challenges to identify the entity boundaries. To this end, we propose a span-prototypical module to divide O-type spans into 3 sub-types to distinguish the miscellaneous semantics by their relative position with recognized entities, together with a dynamically aggregation mechanism to form the specific prototype representation for each query span. 3) Span conflicts resolution. In the span-level matching paradigm, the predicted spans may conflict with each other. For example, a model may annotate both "Albert Einstein" and "Albert" as "PER". Therefore, we propose a span refining module that incorporates the Soft-NMS <ref type="bibr" target="#b2">(Bodla et al., 2017;</ref><ref type="bibr" target="#b21">Shen et al., 2021)</ref> algorithm into the beam search to alleviate this conflict problem. With Beam Soft-NMS, ESD can also handle nested tagging cases without any extra training, which previous methods are incapable of.</p><p>We summarize our contribution as follows: (1) We propose ESD, an enhanced span-based decomposition model, which formulates FSSL as a spanlevel matching problem. (2) We decompose the span matching problem into 3 main subsequent procedures, which firstly produce enhanced span representation, then distinguish miscellaneous semantics of O-types, and achieve the specific prototype representation for each query, and finally resolve span conflicts . (3) Extensive experiments show that ESD achieves new state-of-the-art performance on both few-shot NER and slot filling benchmarks and that ESD is more robust than other methods in nested and noisy scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Few-Shot Learning</head><p>Few-Shot Learning (FSL) aims to enable the model to solve the target domain task with a very small training set D train <ref type="bibr" target="#b29">(Wang et al., 2020)</ref>. In FSL, people usually consider the N -way K-shot task, where the D train has N classes, and each class has only K annotated examples (K is very small, e.g., 5). Training a model only based on D train from scratch will inevitably lead to over-fitting. Therefore, researchers usually introduce the source domain annotated data to help train models, e.g., Few-Shot Relation Classification (FSRC) <ref type="bibr" target="#b10">(Han et al., 2018)</ref>, Few-Shot Text Classification (FSTC) <ref type="bibr" target="#b9">(Geng et al., 2019)</ref> and Few-Shot Event Classification (FSEC) <ref type="bibr" target="#b27">(Wang et al., 2021a)</ref>. The source domain data do not contain examples belonging to classes in D train , and thus the FSL setting can be guaranteed. Specifically, FSSL tasks in our paper also have the source domain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Metric-Based Meta-Learning</head><p>Meta-learning <ref type="bibr" target="#b13">(Hochreiter, Younger, and Conwell, 2001a</ref>) is a popular method to deal with Few-Shot Learning (FSL). Meta-learning constructs a series of tasks sampled from the source domain data to mimic the target domain task, and trains models across these sampled tasks. Each task contains a training set (support set) and a test instance (query). The core idea of meta-learning is to help model learn the ability to quickly adapt to new tasks, i.e., learn to learn <ref type="bibr" target="#b14">(Hochreiter, Younger, and Conwell, 2001b)</ref>. Meta-learning can be combined with the metric-learning <ref type="bibr" target="#b17">(Kulis et al., 2013</ref>) (metric-based meta-learning), which makes predictions based on the similarity of the support set and the query. For example, Prototypical Network <ref type="bibr" target="#b22">(Snell, Swersky, and Zemel, 2017)</ref>   <ref type="figure">Figure 2</ref>: The architecture (5 span modules) of ESD with a 2-way ('PER' and 'ORG') 2-shot input task. We only list spans with lengths less than 2 for clarity. ESD assigns label 'PER' to the span "Albert Einstein" and 'O' to the other spans in query q based on the support set {S 1 , S 2 }.</p><p>metric-based meta-learning paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Few-Shot Sequence Labeling</head><p>Recently, few shot sequence labeling (FSSL) has been widely explored by researchers. For example, <ref type="bibr" target="#b7">(Fritzler, Logacheva, and Kretov, 2019a</ref>) leverages the prototypical network <ref type="bibr" target="#b22">(Snell, Swersky, and Zemel, 2017)</ref> in the few-shot NER. <ref type="bibr" target="#b30">(Yang and Katiyar, 2020)</ref> further proposes a cheap but effective method to capture the label dependencies between entity tags without expensive CRF training. <ref type="bibr" target="#b28">(Wang et al., 2021b</ref>) utilizes a large unlabelled dataset and proposes a distillation method. <ref type="bibr" target="#b4">(Cui et al., 2021)</ref> introduces a prompt method to tap the potential of BART <ref type="bibr" target="#b18">(Lewis et al., 2020)</ref>. <ref type="bibr" target="#b15">(Hou et al., 2020)</ref> extends the TapNet <ref type="bibr" target="#b31">(Yoon, Seo, and Moon, 2019)</ref> and proposes a collapsed CRF for few-shot slot filling.  and <ref type="bibr" target="#b0">(Athiwaratkun et al., 2020)</ref> formulate FSSL as a machine comprehension problem and a generation problem, respectively. <ref type="bibr" target="#b32">(Yu et al., 2021)</ref> proposed to retrieve the most similar exemplars in the support set for spanlevel prediction. Although their methods also involve span matching, their main focus is on the retrieval-augmented training. Our work differs from <ref type="bibr" target="#b32">(Yu et al., 2021)</ref> in that we propose an effective actionable pipeline to get enhanced span representation, handle miscellaneous semantics of O-types and resolve the potential span conflicts in both non-nested and nested situations, where the last two modules are essential to align the candidate spans with class prototypes in the support set but missing in <ref type="bibr" target="#b32">(Yu et al., 2021)</ref>. Besides, our enhanced span representation greatly improves the batched softmax objective of <ref type="bibr" target="#b32">(Yu et al., 2021)</ref> by inter-and intra-span interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Formulation</head><p>We define a sentence as x and its label as</p><formula xml:id="formula_0">y = {(s i , y i )} M i=1 , where s i is a span of x (e.g.</formula><p>, 'Steve Jobs'), y i is the label of s i ( e.g., 'PER') and M is the number of spans in the x. Following the previous FSSL setting <ref type="bibr" target="#b15">(Hou et al., 2020;</ref><ref type="bibr" target="#b6">Ding et al., 2021)</ref>, we have data in source domain D source and target domain D target , and models are evaluated on tasks from D target . Meta-learning based FSSL has two stages, meta-training and meta-testing. In the meta-training stage, the model is trained on tasks sampled from D source to mimic the test situation, and in the meta-testing stage, the model is evaluated on test tasks. A task is defined as T = {S, q}, consisting of a support set S = {(x i , y i )} I i=1 , and a query sentence q = x. S includes N types of entities or slots (N -way), and each type has K annotated examples (K-shot). For spans that do not belong to the N types, e.g., 'studied at' in <ref type="figure" target="#fig_0">Figure 1</ref>, we set their label to O. The types except O in each test task are guaranteed to not exist in the training tasks. Given a task T = {S, q}, the model needs to assign a label to each span in the query sentence q based on S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>Our ESD formulates FSSL as a span-level matching problem and decomposes it into a series of span-related procedures for a better span matching. <ref type="figure">Figure 2</ref> illustrates the architecture of ESD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Span Initialization Module</head><p>Given a task T = {S, q}, we use BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> to encode sentences in S and q, and utilize the output of the last layer to represent tokens in the sentence. Therefore, for a sentence with N tokens x = (x 1 , x 2 , ..., x N ), we can achieve representations (h 1 , h 2 , ..., h N ), where h i ? R dw is the hidden state corresponding to the token x i . Then, for a span s = (l, r), where l and r are the start index and end index of span s in the sentence x, we obtain its initial representation s (l,r) = [h l ; h r ]W s . <ref type="bibr">1</ref> We enumerate spans in the sentence with a maximum length of L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Span Enhancing Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Intra Span Attention</head><p>Intuitively, the meaning of specific spans can be inferred from other spans in the same sentence. We thus design an Intra Span Attention (ISA) mechanism. Given all the span representations of a sentence S ? R B?d (B is the number of spans). We denote the i-th row of S as s i , which represents the i-th span in the sentence. For s i , we first get its</p><formula xml:id="formula_1">ISA span representations i = B j=1 ? i j s j , where ? i = softmax(s i S T ).</formula><p>For clarity, we denote this attention aggregation operation as ?, i.e.,</p><formula xml:id="formula_2">s i = ?(s i , S)<label>(1)</label></formula><p>then, a Feed Forward Neural Networks (FFN) <ref type="bibr" target="#b24">(Vaswani et al., 2017)</ref> with residual connection <ref type="bibr" target="#b11">(He et al., 2016)</ref> and layer normalization <ref type="bibr" target="#b1">(Ba, Kiros, and Hinton, 2016)</ref> are used to get the final ISA enhanced features i ,</p><formula xml:id="formula_3">s i = LayerNorm(s i + FFN isa (s i )) (2) FFN isa (s i ) = GELU(s i W 1 isa )W 2 isa (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Cross Span Attention</head><p>After ISA, to accommodate the span matching between the test query and supporting sentences, and facilitate the inter-span interaction, we propose Cross Span Attention (CSA) to enhance query spansQ ? R Bq?d with the support set spans {S i ? R B i ?d ; i = 1, ..., I}, and vice versa. We first concatenate all span representations in the support set into one matrixS = [S 1 ,S 2 , ...,S I ] ? R Bs?d . We denote the n-th row ofS ass n and the m-th row ofQ asq m , and obtain their CSA span representations? n = ?(s n ,Q) andq m = ?(q m ,S). Then, <ref type="bibr">1</ref> We omit the bias term in this paper for clarity.</p><p>we get the final CSA enhanced representation? n andq m as follows,</p><formula xml:id="formula_4">s n = LayerNorm(s n + FFN csa (? n )) (4) q m = LayerNorm(q m + FFN csa (q m )) (5) FFN csa (x) = GELU(xW 1 csa )W 2 csa (6) 4.3 Span Prototypical Module</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Instance Span Attention</head><p>Since different support set spans play different roles for a query span, we propose multi-INstance Span Attention (INSA) to get class representations. For the i-th class that contains K annotated spans with</p><formula xml:id="formula_5">enhanced representations? i = [? 1 i , ...,? K i ]</formula><p>in the support set, given a query spanq m , INSA gets the corresponding prototypical representation z i m = ?(q m ,? i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">O Partition and Prototypical Span Attention</head><p>The O-type spans have huge quantities and miscellaneous semantics, which is hard to be represented by only one prototypical vector. In the span-based framework, considering the boundary information is essential for a span, we divide the O-type spans into 3 sub-classes according to their boundary, to alleviate their miscellaneous semantics problem. Specifically, given a sentence with I annotated spans {(l i , r i )} I i=1 , where l i and r i are the left and right boundary of the i-th annotated span. For each of the other spans (l o , r o ) , we assign it a sub-class O sub as follows,</p><formula xml:id="formula_6">O sub = ? ? ? ? ? O 1 , ?i, s.t. r o &lt; l i ? l o &gt; r i O 2 , ?i, s.t. l o ? l i ? r o ? r i O 3 , Others<label>(7)</label></formula><p>where O 1 denotes the span that does not overlap with any entities or slots in the sentence, e.g., "study at" in S 2 of <ref type="figure">Figure 2</ref>, and O 2 represents the span that is the sub-span of an entity or slot, e.g., "Isaac" in S 2 of <ref type="figure">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Span Matching Module</head><p>Given a task T = (S, q), for the m-th span in q m , we achieve its enhanced representationq m , ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beam States (size=2)</head><p>Step4: 1 3 4 1 2 , <ref type="figure">Figure 3</ref>: A demonstration for Beam Soft-NMS (BSNMS). For clarity, we set the filter threshold ? to 0.6, and suppose the span score is always decayed by the overlapped spans with a constant decayed score ?0.1 (Note that the example is simplified for demonstration). BSNMS filters false positive spans (colored in orange) in this demonstration. A more clear step by step demonstration is included in Appendix D.</p><p>and corresponding prototypical vectors Z m = (z o m , z 1 m , ..., z N m ) through previous span modules. Then, we predict q m as the type z k in the support set with probability,</p><formula xml:id="formula_7">p(y m = z k |q m ) = exp(?L 2 (q m , z m k )) k exp(?L 2 (q m , z m k ))<label>(8)</label></formula><p>where L 2 is the euclidean distance. During training, we use cross-entropy as our loss function,</p><formula xml:id="formula_8">L = ? 1 B q Bq m=1 log p(y * m |q m )<label>(9)</label></formula><p>where y * m is the gold label of q m and B q is the number of spans in the query q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Span Refining Module in Inference</head><p>During inference, spans outputted by the matching module may have conflicts, we thus propose a refining module that incorporates the SoftNMS into beam search for the conflicts resolution. For the mth span with the left index l m and the right index r m in the query, we obtain its prediction probability distribution p m , label y m = argmax(p m ) and score score m = max(p m ). <ref type="figure">Figure 3</ref> illustrates a simplified post-processing process. In each step, we first expand all beam states (e.g., states 1-3 and 1-2 in step2 of <ref type="figure">Figure 3)</ref>, and then prune new states according to the beam size. Specifically, given a beam state S containing spans {l t , r t , score t , y t } T t=1 , for each non-contained span s i = (l i , r i , score i , y i ), we first calculate its decayed score score decay i , score decay i = score i * u ? (10)</p><formula xml:id="formula_9">? = T t=1 I(IoU(s i , s t ) ? k)<label>(11)</label></formula><p>where IoU(s i , s t ) = |{l i ,...,r i }?{lt,...,rt}| |{l i ,...,r i }?{lt,...,rt}| is the overlap ratio of two spans. The decay ratio u and threshold k are hyperparameters. Then, we expand the beam state S with the non-contained span s i if score decay i &gt; ?. For example, in the step2 of <ref type="figure">Figure 3</ref>, we expand the state 1-3 to 1-3-4, while the state 1-3-2 fails to be expanded since score decay 2 = 0.58 &lt;= ?. After expanding all states, we prune available beam states with lower path scores. For example, we prune states 1-4, 3-4 and 3-2 in step2 of <ref type="figure">Figure 3</ref>. In addition, as our needed output is order-independent, we also prune duplicate states, e.g., the state 3-1 (equivalent to the state 1-3) for the diversity of beam states. When all states in the beam can not be expanded or have been expanded before but failed, we select the beam with the largest path score as the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments Setup</head><p>Datasets We evaluate ESD on FewNERD <ref type="bibr" target="#b6">(Ding et al., 2021)</ref> and SNIPS <ref type="bibr" target="#b3">(Coucke et al., 2018)</ref>. FewNERD designs an annotation schema of 8 coarse-grained (e.g., 'Person') entity types and 66 fine-grained (e.g., 'Person-Artist') entity types, and constructs two tasks. One is FewNERD-INTRA, where all the entities in the training set (source domain), validation set and test set (target domain) belong to different coarse-grained types. The other is FewNERD-INTER, where only the fine-grained entity types are mutually disjoint in different sets. For the sake of sampling diversity, FewNERD adopts the N -way K ? 2K-shot sampling method to construct tasks (each class in the support set has K ? 2K annotated entities). Both FewNERD-INTRA and FewNERD-INTER have 4 settings, 5-way 1 ? 2-shot, 5-way 5 ? 10-shot, 10-way 1 ? 2-shot and 10-way 5 ? 10-shot. SNIPS is a slot filling dataset, which contains 7 domains D = {D 1 , D 2 , ..., D 7 }. <ref type="bibr" target="#b15">(Hou et al., 2020)</ref> constructs few-shot slot filling task with the leave-oneout strategy, which means when testing on the target domain D i , they randomly chose D j (i = j)   <ref type="table" target="#tab_9">Table 1</ref>: F1 scores with standard deviations on FewNERD. The best results are in boldface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>We Mu Pl Bo Se Re Cr Avg.  as the validation domain, and train the model on source domains D ? {D i , D j }. In the sampling task of SNIPS, all classes have K annotated examples in the support set, but the number of them (N ) is not fixed. The few-shot slot filling task in each domain of SNIPS has two settings, 1-shot and 5-shot. FSSL models are trained and evaluated on tasks sampled from the source and target domain, respectively. To ensure the fairness, we use the public sampled data provided by <ref type="bibr" target="#b6">(Ding et al., 2021)</ref> for FewNERD 2 and data provided by <ref type="bibr" target="#b15">(Hou et al., 2020)</ref> for SNIPS, to train and evaluate our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-SHOT</head><p>Parameter Settings Following previous methods <ref type="bibr" target="#b15">(Hou et al., 2020;</ref><ref type="bibr" target="#b6">Ding et al., 2021)</ref>, we use uncased BERT-base as our encoder. We use Adam <ref type="bibr" target="#b16">(Kingma and Ba, 2015)</ref> as our optimizer. We set the dropout ratio <ref type="bibr" target="#b23">(Srivastava et al., 2014)</ref> to 0.1. The maximum span length L is set to 8. For BSNMS, the beam size b is 5. Because FewNERD and 2 The FewNERD data we used is from https://cloud. tsinghua.edu.cn/f/0e38bd108d7b49808cc4/ ?dl=1, which corresponds to the results reported in https://arxiv.org/pdf/2105.07464v6.pdf. SNIPS do not have nested instances, we set the threshold to filter false positive spans ? to 0.1, the threshold to decay span scores k to 1e-5 and the decay ratio u to 1e-5 to force the refining results have no nested spans. More details of our parameter settings are provided in Appendix A.</p><p>Evaluation Metrics For FewNERD, following <ref type="bibr" target="#b6">(Ding et al., 2021)</ref>, we report the micro F1 over all test tasks, and the average result of 5 different runs. For SNIPS, following <ref type="bibr" target="#b15">(Hou et al., 2020)</ref>, we first calculate micro F1 score for each test episode (an episode contains a number of test tasks), and then report the average F1 score for all test episodes as the final result. We report the average result of 10 different runs the same as <ref type="bibr" target="#b15">(Hou et al., 2020)</ref>.</p><p>Baselines For systematic comparisons, we introduce a variety of baselines, including ProtoBERT <ref type="bibr" target="#b6">(Ding et al., 2021;</ref><ref type="bibr" target="#b15">Hou et al., 2020)</ref>, NNShot <ref type="bibr" target="#b6">(Ding et al., 2021)</ref>, StructShot <ref type="bibr" target="#b6">(Ding et al., 2021)</ref>, Trans-ferBERT <ref type="bibr" target="#b15">(Hou et al., 2020)</ref>, MN+BERT <ref type="bibr" target="#b15">(Hou et al., 2020)</ref>, L-TapNet+CDT <ref type="bibr" target="#b15">(Hou et al., 2020)</ref>, Retriever <ref type="bibr" target="#b32">(Yu et al., 2021)</ref>, ConVEx (Henderson  and Vuli?, 2021) and Ma2021 . Please refer to the Appendix A for more details.  <ref type="table" target="#tab_6">Table 2</ref> shows the results on SNIPS. In the 1-shot setting, L-TapNet+CDT is the previous best method. Compared with L-TapNet+CDT, ESD achieves comparable results and 4.58 F1-scores improvement in the 1-shot and 5-shot settings, respectively. We think the reason is that the information benefits brought by our cross span attention mechanism in 1-shot setting is much less than that in 5-shot setting. In addition, compared with L-TapNet+CDT, ESD performs more stable, and also has a better model efficiency (Please refer to Section 6.3). In 5-shot setting, Ma2021 is previous best method, and ESD outperforms it 1.14 and 0.42 F1scores in 1-shot and 5-shot settings, respectively. These results prove the effectiveness of ESD in few-shot sequence labeling. 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ablation Study</head><p>To illustrate the effect of our proposed mechanisms, we conduct ablation studies by removing one component of ESD at a time. <ref type="table" target="#tab_8">Table 3</ref> shows the results on the validation set of SNIPS (1-shot, domain "We"). Firstly, we remove ISA or CSA, which means the span cannot be aware of other spans 3 Results on SNIPS are worse than that reported in our first version paper https://arxiv.org/pdf/2109. 13023v1.pdf, since we fix a data processing bug. within the same sentence or spans from other sentences. As shown in <ref type="table" target="#tab_8">Table 3</ref>, the average F1 scores drop 0.7 and 2.6 without ISA and CSA, respectively. These results suggest that our span enhancing module with ISA and CSA is effective. In addition, CSA is more effective than ISA, since CSA can enhance query spans with whole support set spans, while ISA enhances spans only with other spans in the same setence. CSA brings much more information than ISA. Secondly, when we remove INSA and achieve the prototypical representation of a class through an average operation, the average F1 score would drop 3.5. When we do not consider the sub-classes of O-type spans (r.m. OP), the average F1 score would drop 1.1. These results show that our span prototypical module is necessary. At last, the result without BSNMS suggests the importance of our post-processing algorithm in this span-level few-shot labeling framework. Moreover, with BSNMS, ESD can also easily adapt to the nested situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Robustness in the Nested Situation</head><p>Sequence labeling tasks such as NER can have nested situations. For example, in the sentence "Isaac Newton studied at Cambridge University", where "Cambridge" is a location and "Cambridge University" is an organization. However, both FewNERD and SNIPS do not annotate nested examples. To explore the robustness of ESD on such a challenging but common situation, we construct FewNERD-nested, which has the same training tasks as FewNERD-INTRA, but different test tasks with a nested ratio r nested . In FewNERD-nested, we sample each test task either from FewNERD or from GENIA <ref type="bibr" target="#b20">(Ohta et al., 2002)</ref> with the probability 1 ? r nested and r nested , respectively, and   all tasks sampled from GENIA are guaranteed to have the query with nested entities. We sample validation tasks with nested instances from ACE05 <ref type="bibr" target="#b26">(Walker et al., 2006)</ref> to tune k, u and ? in BSNMS. Please refer to the Appendix B for more details about FewNERD-nested. <ref type="figure" target="#fig_2">Figure 4</ref> shows the results of ESD and several typical baselines in FewNERD-nested with different r nested . When r nested increases, ESD is more stable than previous methods, since ESD with BSNMS can easily extend to nested tagging cases without any extra training while previous methods are incapable of. In addition, we also compare different post-processing methods when r nested = 1. As shown in <ref type="table" target="#tab_11">Table 4</ref>, the beam search method can not handle the nested situation, and thus it harms the performance. Soft-NMS <ref type="bibr" target="#b21">(Shen et al., 2021)</ref> and BSNMS can both improve the model performance. However, when incorporating beam search into SoftNMS, the model can be more flexible and avoid some local optimal post-processing results achieved by SoftNMS (e.g., spans 1,3 and 4 in <ref type="figure">Figure 3</ref>), and thus BSNMS outperforms SoftNMS 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Model Efficiency</head><p>Compared with the token-level models, ESD needs to enumerate all spans within the length L for a sentence. Therefore, the number of spans is approx-  <ref type="table">Table 6</ref>: Error analysis of 5 way 1 ? 2 shot on FewNERD-INTER. 'FP-Span' denotes extracted entities with the wrong span boundary, and 'FP-Type' represents extracted entities with the right span boundary but the wrong entity type. 'Total' denotes the total wrong prediction of two types.</p><p>imately L times that of tokens, which may bring extra computation overhead. To evaluate the efficiency of ESD, we compare the average inference time per task of ESD (including the BSNMS postprocessing process), L-TapNet+CDT (the state-ofthe-art token-level baseline model with the open codebase in the SNPIS) and ProtoBERT (an extremely simple token-level baseline). As shown in <ref type="table" target="#tab_12">Table 5</ref>, with exactly the same hardware setting, in the domain "We" of SNIPS 1-shot setting, ESD (avg. 8.53 ms per task) is nearly 3 times faster than L-TapNet+CDT (avg. 24.67 ms per task). We see a similar tendency in the 5-shot setting. Although <ref type="bibr">ESD (avg. 8.53</ref> and 18.47 ms in 1-and 5-shot per task) is slower than ProtoBERT (avg. 3.13 and 5.27 ms), it outperforms ProtoBERT by 31.53 and 16.68 F1 scores in 1-and 5-shot settings (reported in <ref type="table" target="#tab_6">Table 2</ref>) with a bearable latency. Besides the inference time, we also compare the parameter number of these models. As is shown, the added parameter scale (span enhancing and prototypical modules) is very small (2M) compared with the ProtoBERT and L-TapNet+CDT (110M). These results show that ESD has an acceptable efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Error Analysis</head><p>To further explore what types of errors the model makes in detail, we divide error of model prediction into 2 categories, 'FP-Span' and 'FP-Type'. As shown in <ref type="table">Table 6</ref>, ESD outperforms baselines and has much less false positive prediction errors. 'FP-Span' is the major prediction error for all models, showing that it is hard to locate the right span boundary in FSSL for existing models. Therefore, we should pay more attention to the span recognition in the future work. However, compared with previous methods, ESD has less ratio of the 'FPspan' error. We think the reason is that our spanlevel matching framework with a series of span-related procedures has a better perception of the entity and slot spans than that of our baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose ESD, an enhanced spanbased decomposition model for few-shot sequence labeling (FSSL). To overcome the drawbacks of previous token-level methods, ESD formulates FSSL as a span-level matching problem, and decomposes it into a series of span-related procedures, mainly including span representation, class prototype aggregation and span conflicts resolution for a better span matching. Extensive experiments show that ESD achieves the state-of-the-art performance on two popular few-shot sequence labeling benchmarks and that ESD is more robust than previous models in the noisy and nested situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Baselines</head><p>We compare ESD with a variety of baselines as follows:</p><p>? TransferBERT <ref type="figure">(Hou et al., 2020)</ref> is a finetuning based model, which is a direct application of BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> to the few-shot sequence labeling.</p><p>? ConVEx <ref type="bibr" target="#b12">(Henderson and Vuli?, 2021</ref>) is a fine-tuning based model, which is first pretrained on the Reddit corpus with the sequence labeling objective tasks and then fine-tuned on the source domain and target domain annotated data for final few shot sequence labelingm</p><p>? Ma2021  formulates sequence labeling as the machine reading comprehension problem, and proposes some questions to extract slots in the query sentence.</p><p>? ProtoBERT <ref type="bibr" target="#b8">(Fritzler, Logacheva, and Kretov, 2019b)</ref> predicts the query labels according to the similarity of BERT hidden states of support set and query tokens.</p><p>? Matching Net (MN)+BERT <ref type="bibr" target="#b15">(Hou et al., 2020)</ref> is similar to ProtoBERT. The only difference is that MN uses the matching network <ref type="bibr" target="#b25">(Vinyals et al., 2016)</ref> for token classification.</p><p>? L-TapNet-CDT <ref type="bibr" target="#b15">(Hou et al., 2020)</ref> utilizes the task-adaptive projection network <ref type="bibr" target="#b31">(Yoon, Seo, and Moon, 2019)</ref>, pair-wise embedding and collapsed dependency transfer mechanisms to do classification.</p><p>? NNShot (Yang and Katiyar, 2020) is similar to ProtoBERT, while it makes the prediction based on the nearest neighbor.</p><p>? StructShot (Yang and Katiyar, 2020) adopts an additional Viterbi decoder during the inference phase on top of NNShot.</p><p>? Retriever <ref type="bibr" target="#b32">(Yu et al., 2021</ref>) is a retrieval based method which does classification according to the most similar example in the support set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Parameter Setting</head><p>In our implementation, we utilize BERT-baseuncased as our backbone encoder the same as <ref type="bibr" target="#b15">(Hou et al., 2020;</ref><ref type="bibr" target="#b32">Yu et al., 2021;</ref><ref type="bibr" target="#b6">Ding et al., 2021)</ref>. We use Adam <ref type="bibr" target="#b16">(Kingma and Ba, 2015)</ref> as our optimizer. In FewNERD, the learning rate is 2e ? 5 for BERT encoder and 5e ? 4 for other modules. In the 1-shot setting of SNIPS, for the domain "Mu", the learning rate is 5e ? 6 for BERT encoder and 1e ? 4 for other modules. For the domain "Bo", the learning rate is 1e ? 5 for BERT encoder and 1e ? 4 for other modules. For other settings of SNIPS, the learning rate is 5e?5 for BERT encoder and 5e?4 for other modules. We set the dropout ratio <ref type="bibr" target="#b23">(Srivastava et al., 2014)</ref> to 0.1. The dimension of span representation d and the maximum span length L is set to 100 and 8, respectively. For BSNMS, the beam size b is 5. Since these is no nested instances in FewNERD and SNIPS, we set the threshold to filter false positive spans ? to 0.1, the threshold to decay span scores k to 1e ? 5 and the decay ratio u to 1e?5 to force the refining results have no nested spans. We use the grid search to search our hyperparameters, and the scope of each hyperparameter are included in <ref type="table" target="#tab_15">Table 7</ref>. We train our model on a single NVIDIA A40 GPU with 48GB memory.    We construct our FewNERD-nested via ACE05 <ref type="bibr" target="#b26">(Walker et al., 2006)</ref>, GENIA <ref type="bibr" target="#b20">(Ohta et al., 2002)</ref> and the origin FewNERD datasets. GENIA is a biological named entity recognition dataset, which contains 5 kinds of entities, 'DNA', 'Protein', 'cell_type', 'RNA' and 'cell_line', and all of these entity types are not included in the FewNERD. We partition the sentences in GENIA into two groups, G 1 and G 2 . G 1 consists of sentences with nested entities (4, 744 sentences in total), and G 2 consists of sentences without nested entities (11, 924 sentences in total). We utilize sentences in G 1 and G 2 to construct the query and support set of the GENIA task, respectively. Our FewNERDnested contains 2000 5-way 5?10-shot test tasks in total, where r nested percent tasks are from GE-NIA, and the remained tasks are from FewNERD-INTRA. We use ACE05 to construct the validation set. ACE05 is a widely used named entity recognition dataset, which contains 7 coarse-grained entity types, 'FAC', 'PER', 'LOC', 'VEH', 'GPE', 'GPE', 'WEA' and 'ORG'. The 'LOC', 'PER', 'ORG' and 'FAC' are not included in the training set of FewNERD-INTRA, and therefore we use them (28 fine-grained entity types in total) and sample 1000 nested tasks as the validation dataset to tune the k, ? and u of BSNMS. The search scope is included in the <ref type="table" target="#tab_16">Table 8</ref>. In this nested situation, we finally set the k, ? and u to 0.1, 0.1 and 0.4 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Robustness in the Noisy Situation</head><p>FSSL methods tend to be seriously influenced by the noise in the support set, since they make the prediction based on only limited annotated examples. To explore the robustness of ESD in the noisy situation, we construct FewNERD-noise. In FewNERD-noise, we disturb each 5 way 5 ? 10 shot FewNERD-INTRA task with a noisy ratio r noise , which means there are nearly r noise percent entities in the support set are mislabeled. As illustrated in the right part of <ref type="figure" target="#fig_4">Figure 5</ref>, with r noise increasing, the performance of ESD drops less than baselines, which furthuer shows the superiority of our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D A Detail Case Study of BSNMS</head><p>For span conflicts resolution, we propose a postprocessing method BSNMS. A step by step demonstration of BSNMS is shown in <ref type="figure">Figure 6</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A 2-way 2-shot few-shot NER task. The model needs to learn new entities with few examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The performance drops in nested situations when increasing r nest . ?F1% is the F1 reduction percent over r nested = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The performance drops in noisy situations when increasing r noise . ?F1% is the F1 reduction percent over r noise = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). In FSSL, the model (typically trained on the source domain Albert Einstein was born in Germany S1: [Steve Jobs]PER created [Apple]ORG S2: [Isaac Newton]PER studied at [Cambridge University]ORG</figDesc><table><row><cell>Model</cell></row></table><note>* Equal contribution.? Corresponding author.? Contribution during internship in Tencent.query :</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>76?0.84 15.04?0.44 42.54?0.94 35.40?0.13 28.44 38.83?1.49 32.45?0.79 58.79?0.44 52.92?0.37 45.75 NNShot 25.78?0.91 18.27?0.41 36.18?0.79 27.38?0.53 26.90 47.24?1.00 38.87?0.21 55.64?0.63 49.57?2.73 47.83 StructShot 30.21?0.90 21.03?1.13 38.00?1.29 26.42?0.60 28.92 51.88?0.69 43.34?0.10 57.32?0.63 49.57?3.08 50.53 ESD (Ours) 36.08?1.6 30.00?0.70 52.14?1.5 42.15?2.6 40.09 59.29?1.25 52.16?0.79 69.06?0.80 64.00?0.43 61.13</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1 ? 2 shot</cell><cell cols="2">5 ? 10 shot</cell><cell></cell></row><row><cell>5 way</cell><cell>10 way</cell><cell>5 way</cell><cell>10 way</cell><cell>5 way</cell><cell>10 way</cell><cell>5 way</cell><cell>10 way</cell><cell>Avg.</cell></row><row><cell>ProtoBERT 20.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>TapNet+CDT 71.64?3.62 67.16?2.97 75.88?1.51 84.38?2.81 82.58?2.12 70.05?1.61 73.41?2.61 75.01?2.46 ESD (Ours) 84.50?1.06 66.61?2.00 79.69?1.35 82.57?1.37 82.22?0.81 80.44?0.80 81.13?1.84 79.59?0.39</figDesc><table><row><cell></cell><cell>TransferBERT</cell><cell cols="8">55.82?2.75 38.01?1.74 45.65?2.02 31.63?5.32 21.96?3.98 41.79?3.81 38.53?7.42 39.06?3.86</cell></row><row><cell></cell><cell>MN+BERT</cell><cell cols="8">21.74?4.60 10.68?1.07 39.71?1.81 58.15?0.68 24.21?1.20 32.88?0.64 69.66?1.68 36.72?1.67</cell></row><row><cell></cell><cell>ProtoBERT</cell><cell cols="8">46.72?1.03 40.07?0.48 50.78?2.09 68.73?1.87 60.81?1.70 55.58?3.56 67.67?1.16 55.77?1.70</cell></row><row><cell></cell><cell>Ma2021</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>69.3(unk)</cell></row><row><cell></cell><cell cols="9">L-TapNet+CDT 71.53?4.04 60.56?0.77 66.27?2.71 84.54?1.08 76.27?1.72 70.79?1.60 62.89?1.88 70.41?1.97</cell></row><row><cell></cell><cell>ESD (Ours)</cell><cell cols="8">78.25?1.50 54.74?1.02 71.15?1.55 71.45?1.38 67.85?0.75 71.52?0.98 78.14?1.46 70.44?0.47</cell></row><row><cell></cell><cell>TransferBERT</cell><cell cols="8">59.41?0.30 42.00?2.83 46.07?4.32 20.74?3.36 28.20?0.29 67.75?1.28 58.61?3.67 46.11?2.29</cell></row><row><cell></cell><cell>MN+BERT</cell><cell cols="8">36.67?3.64 33.67?6.12 52.60?2.84 69.09?2.36 38.42?4.06 33.28?2.99 72.10?1.48 47.98?3.36</cell></row><row><cell>5-SHOT</cell><cell>ProtoBERT Retriever ConVEx</cell><cell cols="8">67.82?4.11 55.99?2.24 46.02?3.19 72.17?1.75 73.59?1.60 60.18?6.96 66.89?2.88 63.24?3.25 82.95(unk) 61.74(unk) 71.75(unk) 81.65(unk) 73.10(unk) 79.54(unk) 51.35(unk) 71.72(unk) 71.5(unk) 77.6(unk) 79.0(unk) 84.5(unk) 84.0(unk) 73.8(unk) 67.4(unk) 76.8(unk)</cell></row><row><cell></cell><cell>Ma2021</cell><cell cols="8">89.39(unk) 75.11(unk) 77.18(unk) 84.16(unk) 73.53(unk) 82.29(unk) 72.51(unk) 79.17(unk)</cell></row><row><cell></cell><cell>L-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>F1 scores with standard deviations on 7 domains of SNIPS. The best results are in boldface. 'unk' denotes methods that do not report deviations in their paper. Baselines of 1-shot and 5-shot settings are different since ConVEx and Retriever do not report the 1-shot results in their paper.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>The effect of our proposed mechanisms on the validation set of SNIPS (1-shot, domain "We"). We report the average result of 3 different runs with standard deviations. r.m. denotes remove.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>illustrates the results on FewNERD. As is</cell></row><row><cell>shown, among all task settings, ESD consistently</cell></row><row><cell>outperforms ProtoBERT, NNShot and StructShot</cell></row><row><cell>by a large margin. For example, compared with</cell></row><row><cell>StructShot, ESD achieves 11.17 and 10.60 average</cell></row><row><cell>F1 improvement on INTRA and INTER, respec-</cell></row><row><cell>tively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Comparison between different post-processing methods in the nested situation with r nested = 1.</figDesc><table><row><cell>Models</cell><cell>#Para.</cell><cell cols="2">Inference Time</cell></row><row><cell></cell><cell></cell><cell>1-SHOT</cell><cell>5-SHOT</cell></row><row><cell>ESD</cell><cell>112M</cell><cell>8.53 ms</cell><cell>18.47 ms</cell></row><row><cell>ProtoBERT</cell><cell>110M</cell><cell>3.13 ms</cell><cell>5.27 ms</cell></row><row><cell>L-TapNet+CDT</cell><cell>110M</cell><cell cols="2">24.67 ms 54.19 ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: The parameter number and inference time per</cell></row><row><cell>task of ESD and L-TapNet+CDT in the domain "We" of</cell></row><row><cell>SNIPS. Although ESD is slower than ProtoBERT, ESD</cell></row><row><cell>outperforms ProtoBERT by 31.53 and 16.68 F1 scores</cell></row><row><cell>in 1-and 5-shot settings with a bearable latency.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>The searching scope of hyperparameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 8 :</head><label>8</label><figDesc>The searching scope of BSNMS hyperparameters.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">ESD is also more robust than baselines in the noisy situation. Please refer to Appendix C for details.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank the anonymous reviewers for their thoughtful and constructive comments. This paper is supported by the Na- </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Step2</head><p>Step3</p><p>Step4</p><p>Step5 (a) Extend the state 1 to states 1-3, 1-2 and 1-4, and the state 3 to states 3-1, 3-4 and 3-2.</p><p>(b) Prune the state 1-4, 3-4 and 3-2 due to low path scores. Prune the state 3-1 due to there exist a equivalent state 1-3.</p><p>(a) Extend the state 1-3 to states 1-3-4 and 1-3-2, and the state 1-2 to states 1-2-3, 1-2-4 (b) Filter states 1-3-2, 1-2-3, 1-2-4 since their low DS. Keep the state 1-2 since it fail to extend.</p><p>(a) Extend the state 1-3-4 to the state 1-3-4-2.</p><p>(b) Filter states 1-3-4-2 since its low DS. Keep the state 1-3-4 since it fail to extend.</p><p>(a) Can not extend any more, since all states in the beam have been extended.</p><p>(b) Select the beam state with larger PS, i.e., span 1 and 2 as the final BSNMS post-processing results. <ref type="figure">Figure 6</ref>: A step by step processing process of BSNMS with beam size=2. For clarity, we set the filter threshold ? to 0.6, and suppose the span score is always decayed by the overlapped spans with a constant decayed score ?0.1. STEP1:create 2 (beam size) states with spans having larger predicted scores; STEP2: extend all states (i.e., states 1 and 3) in the beam to 1-3, 1-2, 1-4, 3-1, 3-4 and 3-2. Compute DS of the new added span and PS of the new state. As beam size=2, prune states 1-4, 3-4 and 3-2 according to their lower PS. The state 3-1 is dropped since it is equivalent with the state 1-3; STEP3: extend states in the beam (1-3 and 1-2) to 1-3-4, 1-3-2, 1-2-3, 1-2-4, and compute their DS and PS. Filter states 1-3-2, 1-2-3, 1-2-4 since their DS are not greater than ?, i.e., low DS; STEP4: extend states in the beam (only 1-3-4 since the state 1-2 has been extended before) to 1-3-4-2, and filter 1-3-4-2 due to its low DS; STEP5: all states in the beam can not extend any more, and select the final state (1-2 in this case) with the largest PS.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Augmented Natural Language for Generative Sequence Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="375" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Soft-NMS-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10190</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Template-Based Named Entity Recognition Using BART</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1835" to="1845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Minneapolis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Few-NERD: A Fewshot Named Entity Recognition Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3198" to="3213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Few-shot classification in named entity recognition task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fritzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kretov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing</title>
		<meeting>the 34th ACM/SIGAPP Symposium on Applied Computing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Few-shot classification in named entity recognition task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fritzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kretov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing</title>
		<meeting>the 34th ACM/SIGAPP Symposium on Applied Computing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Induction Networks for Few-Shot Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3904" to="3913" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ConVEx: Data-Efficient and Few-Shot Slot Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vuli?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3375" to="3389" />
		</imprint>
	</monogr>
	<note>Online: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1381" to="1393" />
		</imprint>
	</monogr>
	<note>Online: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Metric Learning: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="287" to="364" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Frustratingly Simple Few-Shot Slot Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1028" to="1033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The GENIA corpus: An annotated research abstract corpus in molecular biology domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the human language technology conference</title>
		<meeting>the human language technology conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="73" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Locate and Label: A Two-stage Identifier for Nested Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2782" to="2794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4080" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Matching Networks for One Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D.</editor>
		<editor>Sugiyama, M.</editor>
		<editor>Luxburg, U.</editor>
		<editor>Guyon, I.</editor>
		<editor>and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Medero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">ACE 2005 multilingual training corpus. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Behind the Scenes: An Exploration of Trigger Biases Problem in Few-Shot Event Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1969" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meta Self-training for Few-shot Neural Sequence Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Awadallah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<biblScope unit="page" from="1737" to="1747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generalizing from a few examples: A survey on fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katiyar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6365" to="6375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tapnet: Neural network augmented with task-adaptive projection for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7115" to="7123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Few-shot Intent Classification and Slot Filling with Retrieved Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="734" to="749" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
