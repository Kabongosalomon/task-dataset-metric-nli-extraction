<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decomposed Meta-Learning for Few-Shot Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Ma</surname></persName>
							<email>hittingtingma@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiqiang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianhui</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
							<email>tjzhao@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Decomposed Meta-Learning for Few-Shot Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot named entity recognition (NER) systems aim at recognizing novel-class named entities based on only a few labeled examples. In this paper, we present a decomposed metalearning approach which addresses the problem of few-shot NER by sequentially tackling few-shot span detection and few-shot entity typing using meta-learning. In particular, we take the few-shot span detection as a sequence labeling problem and train the span detector by introducing the model-agnostic meta-learning (MAML) algorithm to find a good model parameter initialization that could fast adapt to new entity classes. For few-shot entity typing, we propose MAML-ProtoNet, i.e., MAMLenhanced prototypical networks to find a good embedding space that can better distinguish text span representations from different entity classes. Extensive experiments on various benchmarks show that our approach achieves superior performance over prior methods. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER) aims at locating and classifying text spans into pre-defined entity classes such as locations, organizations, etc. Deep neural architectures have shown great success in fully supervised NER <ref type="bibr" target="#b19">(Lample et al., 2016;</ref><ref type="bibr" target="#b24">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b3">Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b26">Peters et al., 2017)</ref> with a fair amount of labeled data available for training. However, in practical applications, NER systems are usually expected to rapidly adapt to some new entity types unseen during training. It is costly while not flexible to collect a number of additional labeled data for these types. As a result, the problem of few-shot NER, which involves learning unseen entity types from only a * Equal contributions. ? Work during internship at Microsoft Research Asia. 1 Our implementation is publicly available at https: //github.com/microsoft/vert-papers/tree/ master/papers/DecomposedMetaNER few labeled examples for each class (also known as support examples), has attracted considerable attention from the research community in recent years.</p><p>Previous studies on few-shot NER are typically based on token-level metric learning, in which a model compares each query token to the prototype <ref type="bibr" target="#b30">(Snell et al., 2017)</ref> of each entity class or each token of support examples and assign the label according to their distances <ref type="bibr" target="#b12">(Fritzler et al., 2019;</ref><ref type="bibr" target="#b17">Hou et al., 2020;</ref><ref type="bibr" target="#b41">Yang and Katiyar, 2020)</ref>. Alternatively, some more recent attempts have switched to span-level metric-learning <ref type="bibr" target="#b36">Wang et al., 2021a)</ref> to bypass the issue of token-wise label dependency while explicitly utilizing phrasal representations.</p><p>However, these methods based on metric learning might be less effective when encountering large domain gap, since they just directly use the learned metric without any further adaptation to the target domain. In other words, they do not fully explore the information brought by the support examples. There also exist additional limitations in the current methods based on span-level metric learning. First, the decoding process requires careful handling of overlapping spans due to the nature of span enumeration. Second, the class prototype corresponding to non-entities (i.e., prototype of the "O" class) is usually noisy because non-entity common words in the large vocabulary rarely share anything together in common. Moreover, when targeting at a different domain, the only available information useful for domain transfer is the limited number of support examples. Unfortunately, these key examples are only used for inference-phase similarity calculation in previous methods.</p><p>To tackle these limitations, this paper presents a decomposed meta-learning framework that addresses the problem of few-shot NER by sequentially conducting few-shot entity span detection and few-shot entity typing respectively via meta-learning. Specifically, for few-shot span detection, we model it as a sequence labeling problem to avoid handling overlapping spans. Note that the detection model aims at locating named entities and is classagnostic. We only feed the detected entity spans to the typing model for entity class inference, and hence the problem of noisy "O" prototype could also be eliminated. When training the span detector, we specifically use the model-agnostic metalearning (MAML) <ref type="bibr" target="#b10">(Finn et al., 2017)</ref> algorithm to find a good model parameter initialization that could fast adapt to new entity classes with learned class-agnostic meta-knowledge of span boundaries after updating with the target-domain support examples. The boundary information of domain-specific entities from the support examples is supposed to be effectively leveraged via these update steps such that the model could better transfer to the target domain. For few-shot entity typing, we implement the typing model with standard prototypical networks <ref type="bibr">(Snell et al., 2017, ProtoNet)</ref>, and propose MAML-ProtoNet to narrow the gap between source domains and the target domain. Compared with ProtoNet which only uses support examples for inference-phase similarity calculation, the proposed MAML-Proto additionally utilizes these examples to modify the shared embedding space of spans and prototypes by clustering spans representations from the same entity class while dispersing those from different entity classes for more accurate predictions.</p><p>We evaluate our proposed framework on several benchmark datasets with different few-shot settings. Experimental results show that our framework achieves superior performance over previous state-of-the-art methods. We also conduct qualitative and quantitative analyses over how the different strategies to conduct meta-learning might affect the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>Given an input sequence x = {x i } L i=1 with L tokens, an NER system is supposed to output a label sequence y = {y i } L i=1 , where x i is the i-th token, y i ? Y ? {O} is the label of x i , Y is the pre-defined entity class set, and O denotes non-entities.</p><p>In this paper, we focus on the standard N -way K-shot setting as in <ref type="bibr" target="#b9">Ding et al. (2021)</ref>. An example of 2-way 1-shot episode is shown in Table 1. In the training phase, we consider train-</p><formula xml:id="formula_0">ing episodes E train = {(S train , Q train , Y train )} built from source-domain labeled data, where S train = {(x (i) , y (i) )} N ?K i=1 denotes the support set, Q train = {x (j) , y (j) } N ?K j=1</formula><p>denotes the query set, Y train denotes the set of entity classes, and |Y train | = N . In the testing phase, we consider novel episodes E new = {(S new , Q new , Y new )} constructed with data from target domains in a similar way. In the few-shot NER task, a model learned with training episodes E train is expected to leverage the support set</p><formula xml:id="formula_1">S new = {(x (i) , y (i) )} N ?K i=1 of a novel episode (S new , Q new , Y new ) ? E new to make predictions on the query set Q new = {x (j) } N ?K j=1 .</formula><p>Here, Y new denotes the set of entity classes with a cardinality of N . Note that,  3 Methodology <ref type="figure">Figure 1</ref> illustrates the overall framework of our decomposed meta-learning approach for few-shot named entity recognition. It is composed of two steps: entity span detection and entity typing.</p><formula xml:id="formula_2">? Y train , Y new , Y train ? Y new = ?.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Entity Span Detection</head><p>The span detection model aims at locating all the named entities in an input sequence. The model should be type-agnostic, i.e., we do not differentiate the specific entity classes. As a result, the parameters of the model can be shared across different domains and classes. With this in mind, we train the span detection model by exploiting model-agnostic meta-learning <ref type="bibr" target="#b10">(Finn et al., 2017)</ref> to promote the learning of the domain-invariant internal representations rather than domain-specific features. In this way, the meta-learned model is  <ref type="figure">Figure 1</ref>: The framework of our proposed approach is decomposed into two modules: (a) entity span detection with parameters ? and (b) entity typing with parameters ?. Two modules are trained independently using (S train , Q train ). At meta-test time, these two modules firstly are finetuned on the support set S new , then given a query sentence in Q new , the spans detected by (a) are sent to (b) for entity typing.</p><p>expected to be more sensitive to target-domain support examples, and hence only a few fine-tune steps on these examples can make rapid progress without overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Basic Detector</head><p>Model In this work, we implement a strong span detector via sequence labeling. We apply the BIOES tagging scheme instead of the standard BIO2 to provide more specific and fine-grained boundary information of entity spans. 2 Given an input sequence x = {x i } L i=1 with L tokens, we first leverage an encoder f ? to obtain contextualized representations h = {h i } L i=1 for all tokens:</p><formula xml:id="formula_3">h = f ? (x).<label>(1)</label></formula><p>With each h i derived, we then use a linear classification layer to compute the probability distribution of labels that indicate whether the token x i is inside an entity or not, using a softmax function:</p><formula xml:id="formula_4">p(x i ) = softmax(W h i + b),<label>(2)</label></formula><p>where p(x i ) ? R |C| with C = {B, I, O, E, S} being the label set. ? = {?, W, b} are trainable parameters.</p><p>Training Generally, the learning loss w.r.t. x is modeled as the averaged cross-entropy of the predicted label distribution and the ground-truth one over all tokens. Following , we add a maximum term here to mitigate the problem of insufficient learning for tokens with relatively higher losses, which can be formulated as:</p><formula xml:id="formula_5">L(?) = 1 L L i=1 CrossEntropy (y i , p(x i )) + ? max i?{1,2,...,L} CrossEntropy (y i , p(x i )) , (3) where ? ? 0 is a weighting factor.</formula><p>Inference For inference, we use the learned model to predict the label distribution for each token in a given test case. We apply the Viterbi algorithm <ref type="bibr" target="#b11">(Forney, 1973)</ref> for decoding. It is worthy to note that we do not train a transition matrix here, but simply add constraints to ensure that the predicted label sequence would not violate the BIOES tagging scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Meta-Learning Procedure</head><p>Here we elaborate on the proposed meta-learning procedure which consists of two phases: metatraining on E train and meta-testing on E new . The Appendix A.1 describes the general framework of meta-learning for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Training</head><p>In this phase, we train a mention detection model M ? by repeatedly simulating the Meta-Testing phase, where the meta-trained model is fine-tuned with the support set of a novel episode and then tested on the corresponding query set.</p><p>Specifically, we first randomly sample an episode (S</p><formula xml:id="formula_6">(i) train , Q (i) train , Y (i)</formula><p>train ) from E train and perform inner-update:</p><formula xml:id="formula_7">? i = U n (?; ?, S (i) train ),<label>(4)</label></formula><p>where U n denotes n-step gradient updates with the learning rate ? to minimize L(?; S (i) train ), i.e., the loss in Eq. (3) derived from the support set S (i) train . We then evaluate ? on the query set Q (i) train and perform meta-update by aggregating multiple episodes:</p><formula xml:id="formula_8">min ? i L(? i ; Q (i) train ).<label>(5)</label></formula><p>Since Eq. (5) involves the second order derivative, we employ its first-order approximation for computational efficiency:</p><formula xml:id="formula_9">? ? ? ? ? i ? ? i L(? i ; Q (i) train ),<label>(6)</label></formula><p>where ? denotes the learning rate used in metaupdate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Testing</head><p>In this phase, we first fine-tune the meta-trained span detection model M ? * with the loss function defined in Eq.</p><p>(3) on the support set S new from a novel episode, and then make predictions for corresponding query examples Q new with the fine-tuned model M ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity Typing</head><p>For entity typing, we aim to assign a specific entity class for each span output by the mention detection model. In the few-shot learning scenario, we take the prototypical networks (ProtoNet) <ref type="bibr" target="#b30">(Snell et al., 2017)</ref> as the backbone for entity typing. To explore the knowledge brought by support examples from a novel episode, we propose to enhance the ProtoNet with the model-agnostic meta-learning (MAML) algorithm <ref type="bibr" target="#b10">(Finn et al., 2017)</ref> for a more representative embedding space, where text spans from different entity classes are more distinguishable to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Basic Model: ProtoNet Span Representation</head><p>Given an input sequence with L tokens x = {x i } L i=1 , we use an encoder g ? to compute contextual token representations h = {h i } L i=1 in the same way as Eq. <ref type="formula" target="#formula_3">(1)</ref>:</p><formula xml:id="formula_10">h = g ? (x).<label>(7)</label></formula><p>Assume x <ref type="bibr">[i,j]</ref> being the output of the span detection model which starts at x i and ends at x j , we compute the span representation of x <ref type="bibr">[i,j]</ref> by averaging representations of all tokens inside x <ref type="bibr">[i,j]</ref> :</p><formula xml:id="formula_11">s [i,j] = 1 j ? i + 1 j k=i h k .<label>(8)</label></formula><p>Class Prototypes Let S k = {x <ref type="bibr">[i,j]</ref> } denotes the set of entity spans contained in a given support set S that belongs to the entity class y k ? Y, we compute the prototype c k for each entity class y k by averaging span representations of all</p><formula xml:id="formula_12">x [i,j] ? S k : c k (S) = 1 |S k | x [i,j] ?S k s [i,j] .<label>(9)</label></formula><p>Training Given a training episode denoted as (S train , Q train , Y train ), we first utilize the support set S train to compute prototypes for all entity classes in Y train via Eq. (9). Then, for each span x <ref type="bibr">[i,j]</ref> from the query set Q train , we calculate the probability that x <ref type="bibr">[i,j]</ref> belongs to an entity class y k ? Y based on the distance between its span representation s <ref type="bibr">[i,j]</ref> and the prototype of y k :</p><formula xml:id="formula_13">p(y k ; x [i,j] ) = exp ?d c k (S train ), s [i,j] y i ?Y exp ?d c i (S train ), s [i,j] ,<label>(10)</label></formula><formula xml:id="formula_14">where d(?, ?) denotes the distance function. Let y [i,j] ? Y denote the ground-truth entity class w.r.t.</formula><p>x <ref type="bibr">[i,j]</ref> , the parameters of the ProtoNet, i.e., ?, are trained to minimize the cross-entropy loss:</p><formula xml:id="formula_15">L(?) = x [i,j] ?Q train ? log p(y [i,j] ; x [i,j] ). (11)</formula><p>Inference During inference time, given a novel episode (S new , Q new , Y new ) for inference, we first leverage the learned model to compute prototypes for all y k ? Y new on S new . Then, upon the mention detection model, we inference the entity class for each detected entity span x <ref type="bibr">[i,j]</ref> in Q new by taking the label y k ? Y new with the highest probability in Eq. (10):?</p><formula xml:id="formula_16">[i,j] = arg max y k p(y k ; x [i,j] ).<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">MAML Enhanced ProtoNet</head><p>Here, we elaborate on the procedure to integrate the ProtoNet and the model-agnostic meta-learning.</p><p>Meta-Training Given a randomly sampled episode (S</p><formula xml:id="formula_17">(i) train , Q (i) train , Y (i)</formula><p>train ) from E train , for inner-update, we first compute prototypes for each entity class in Y train using S (i) train via Eq. (9), and then take each span</p><formula xml:id="formula_18">x [i,j] ? S (i)</formula><p>train as the query item in conventional ProtoNet for gradient update:</p><formula xml:id="formula_19">? i = U n (?; ?, S (i) train ),<label>(13)</label></formula><p>where U n denotes n-step gradient updates with the learning rate ? to minimize the cross-entropy loss L(?; S (i) train ) as in Eq. (11). As for meta-update, we first re-compute prototypes for each entity class in Y (i) train with ? , i.e., the model parameters obtained from inner-update. After that, we perform meta-update by evaluating ? on the query set Q (i) train . We employ the firstorder approximation again for computational efficiency. When aggregating gradients from multiple episodes, it could be formulated as: We conduct experiments to evaluate the proposed approach on two groups of datasets.</p><formula xml:id="formula_20">? ? ? ? ? i ? ? i L(? i ; Q (i) train ),<label>(14)</label></formula><formula xml:id="formula_21">Meta-Testing Given (S new , Q new , Y new ),</formula><p>Few-NERD <ref type="bibr" target="#b9">(Ding et al., 2021)</ref>. It is annotated with a hierarchy of 8 coarse-grained and 66 finegrained entity types.  <ref type="bibr" target="#b17">Hou et al. (2020)</ref>. For more details of these datasets, please refer to the Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation</head><p>For evaluation on Few-NERD, we employ episode evaluation as in <ref type="bibr" target="#b9">Ding et al. (2021)</ref> and calculate the precision (P), recall (R), and micro F1score (F1) over all test episodes. For evaluation on Cross-Dataset, we calculate P, R, F1 within each episode and then average over all episodes as in <ref type="bibr" target="#b17">Hou et al. (2020)</ref>. For all results, we report the mean and standard deviation based on 5 runs with different seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation Details</head><p>We implement our approach with PyTorch 1.9.0 4 . We leverage two separate BERT models for f ? in Eq.</p><p>(1) and g ? in Eq. <ref type="formula" target="#formula_10">(7)</ref>, respectively. Following previous methods <ref type="bibr" target="#b17">(Hou et al., 2020;</ref><ref type="bibr" target="#b9">Ding et al., 2021)</ref>, we use the BERT-base-uncased model <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>. The parameters of the embedding layer are frozen during optimization. We train all models for 1,000 steps and choose the best model with the validation set. We use a batch size of 32, maximum sequence length of 128, and a dropout probability of 0.2. For the optimizers, we use AdamW <ref type="bibr" target="#b23">(Loshchilov and Hutter, 2019)</ref> with a 1% linearly scheduled warmup. We perform grid search for other hyper-parameters and select the best settings with the validation set. For more details, please refer to the Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>Baselines For FewNERD, we compare the proposed approach to ESD <ref type="formula">(</ref> Results <ref type="table" target="#tab_6">Table 2</ref> and <ref type="table" target="#tab_7">Table 3</ref> report the results of our approach alongside those reported by previous 4 https://pytorch.org/ 5 To make fair comparison with CONTAINER (Das et al., 2021) and ESD <ref type="bibr" target="#b36">(Wang et al., 2021a)</ref>, we use the data from https://cloud.tsinghua.edu.cn/f/ 8483dc1a34da4a34ab58/?dl=1, which corresponds to the results reported in https://arxiv.org/pdf/ 2105.07464v5.pdf.</p><p>For results of our approach on data from https://cloud.tsinghua.edu.cn/f/ 0e38bd108d7b49808cc4/?dl=1, please refer to our Github.   state-of-the-art methods. <ref type="bibr">6</ref> It can be seen that our proposed method outperforms the prior methods with a large margin, achieving an performance improvement up to 10.60 F1 scores on Few-NERD (Intra, 5way 1?2 shot) and 19.71 F1 scores on Cross-Dataset (Wiki, 5-shot), which well demonstrates the effectiveness of the proposed approach. <ref type="table" target="#tab_6">Table 2</ref> and <ref type="table" target="#tab_7">Table 3</ref> also depict that compared with the results of Few-NERD Inter, where the training episodes and test episodes may be constructed with the data from the same domain while still focusing on different fine-grained entity classes, our approach attains more impressive performance in other settings where exists larger transfer gap, e.g., transferring across different coarse entity classes even different datasets built from different domains. This suggests that our approach is good at dealing with difficult cases, highlighting the necessity of exploring information contained in target-domain support examples and the strong adaptation ability of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To validate the contributions of different components in the proposed approach, we introduce the following variants and baselines for ablation study: 1) Ours w/o MAML, where we train both the men- <ref type="bibr">6</ref> We also provide the intermediate results, i.e., F1-scores of entity span detection in the Appendix A.5.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New-Support: Youth Sing Praise performed the show at the National Shrine of Our Lady of the Snows in Belleville .</head><p>Training-Support: He also functioned as a drama critic , allowing him free entry to Broadway and downtown shows . </p><formula xml:id="formula_22">Sup-Span-f.t. Sup-Span MAML-Span-f.t. (Ours) Sup-Span-f.t.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How does MAML promote the span detector?</head><p>To bring up insights on how MAML promotes the span detector, here we introduce two baselines and compare them to our approach by case study. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, given a query sentence from a novel episode, Sup-Span only predicts a false positive span "Broadway" while missing the golden span "New Century Theatre". Note that "Broadway" appears in training corpus as an entity span, indicating that the span detector trained in a fully supervised manner performs well on seen entity spans, but struggles to detect un-seen entity spans. <ref type="figure" target="#fig_1">Figure 2</ref> also shows that both our method and Sup-Span-f.t. can successfully detect "New Century Theatre". However, Sup-Span-f.t. still outputs "Broadway" while our method can produce more accurate predictions. This shows that though finetuning can benefit full supervised model on new entity classes to some extend, it may bias too much to the training data.</p><p>We further investigate how performances of aforementioned span detectors vary with different fine-tune steps. As shown in <ref type="figure" target="#fig_2">Figure 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra Inter</head><p>Ours (w/ MAML-ProtoNet) 52.04 68.77 Ours w/ ProtoNet 50.53 67.79 How does MAML enhance the ProtoNet? We first compare the proposed MAML-Proto to the conventional ProtoNet based on the same span detector proposed in this paper. <ref type="table" target="#tab_11">Table 5</ref> shows that our MAML-ProtoNet achieves superior performance than the conventional ProtoNet, which verifies the effectiveness of leveraging the support examples to refine the learned embedding space at test time. To further analyze how MAML adjusts the representation space of entity spans and prototypes, we utilize t-SNE (van der <ref type="bibr" target="#b34">Maaten and Hinton, 2008)</ref> to reduce the dimension of span representations obtained from ProtoNet and MAML-ProtoNet for entity typing, the visualization is shown in <ref type="figure">Figure 4</ref>. We can see that MAML enhanced Proto can cluster span representations of the same entity class while dispersing span representations of different entity classes . Therefore, compared with ProtoNet, it is easier for the proposed MAML-ProtoNet to assign an entity class for a query span by measuring similarities between its representation and the prototype of each entity class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Neural NER Modern NER systems usually formulate the NER task as a sequence labeling problem and tackle it by implementing deep neural networks and a token-level classification layer with a conditional random field <ref type="bibr" target="#b18">(Lafferty et al., 2001</ref>, CRF) layer on top <ref type="bibr" target="#b24">(Ma and Hovy, 2016;</ref><ref type="bibr" target="#b3">Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b22">Liu et al., 2019;</ref><ref type="bibr" target="#b8">Devlin et al., 2019)</ref>. Alternative approaches for NER are also proposed to handle the problem based on span classification <ref type="bibr" target="#b25">(Ouchi et al., 2020;</ref><ref type="bibr" target="#b13">Fu et al., 2021)</ref>, machine reading comprehension <ref type="bibr" target="#b21">(Li et al., 2020b)</ref>, and sequence generation <ref type="bibr" target="#b40">(Yan et al., 2021)</ref>.</p><p>Few-Shot Learning and Meta-Learning Recently, few-shot learning has received increasing attention in the NLP community <ref type="bibr" target="#b16">(Han et al., 2018;</ref><ref type="bibr" target="#b15">Geng et al., 2019;</ref><ref type="bibr" target="#b2">Chen et al., 2019;</ref><ref type="bibr" target="#b1">Brown et al., 2020;</ref><ref type="bibr" target="#b29">Schick and Sch?tze, 2021;</ref><ref type="bibr" target="#b14">Gao et al., 2021)</ref>. and meta-learning has become a popular paradigm for few-shot settings. Typical meta-learning approaches can be divided into three categories: black-box adaption based methods <ref type="bibr" target="#b28">(Santoro et al., 2016)</ref>, optimization based methods <ref type="bibr" target="#b10">(Finn et al., 2017)</ref>, and metric learning based methods <ref type="bibr" target="#b35">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b30">Snell et al., 2017)</ref>. Our work takes advantages of two popular meta-learning approaches, i.e., prototypical network <ref type="bibr" target="#b30">(Snell et al., 2017)</ref> and</p><p>MAML <ref type="bibr" target="#b10">(Finn et al., 2017)</ref>. The most related work of this paper is <ref type="bibr" target="#b33">Triantafillou et al. (2020)</ref>, which similarly implements MAML updates over prototypical networks for few-shot image classification.</p><p>Few-Shot NER Studies on few-shot NER typically adopt metric learning based approaches at either token-level <ref type="bibr" target="#b12">(Fritzler et al., 2019;</ref><ref type="bibr" target="#b17">Hou et al., 2020;</ref><ref type="bibr" target="#b41">Yang and Katiyar, 2020;</ref><ref type="bibr" target="#b32">Tong et al., 2021)</ref> or span-level <ref type="bibr" target="#b36">Wang et al., 2021a)</ref>. <ref type="bibr" target="#b0">Athiwaratkun et al. (2020)</ref> and <ref type="bibr" target="#b4">Cui et al. (2021)</ref> also propose to address the problem via sequence generation and adapt the model to a new domain within the conventional transfer learning paradigm (training plus finetuning). Differently, <ref type="bibr" target="#b37">Wang et al. (2021b)</ref> propose to decompose the problem into span detection and entity type classification to better leverage type description. They exploit a traditional span-based classifier to detect entity spans and leverage class descriptions to learn representations for each entity class. When adapting the model to new domains in the few-shot setting, they directly fine-tune the model with the support examples. In this paper, we propose a decomposed metalearning based method to handle few-shot span detection and few-shot entity typing sequentially for few-shot NER. The contribution and novelty of our work lie in that: i) Previous work transfers the metric-learning based model learned in source domains to a novel target domain either without any parameter updates <ref type="bibr" target="#b17">(Hou et al., 2020;</ref><ref type="bibr" target="#b36">Wang et al., 2021a)</ref> or by simply applying conventional fine-tuning <ref type="bibr" target="#b4">(Cui et al., 2021;</ref><ref type="bibr" target="#b5">Das et al., 2021;</ref><ref type="bibr" target="#b37">Wang et al., 2021b)</ref>, while we introduce the model-agnostic meta-learning and integrate it with the prevalent prototypical networks to leverage the information contained in support examples more effectively. ii) Existing studies depend on one <ref type="bibr" target="#b17">(Hou et al., 2020)</ref> or multiple prototypes <ref type="bibr" target="#b32">(Tong et al., 2021;</ref><ref type="bibr" target="#b36">Wang et al., 2021a)</ref> to represent text spans of non-entities ("O") for class inference, while we avoid this problem by only locating named entities during span detection. Moreover, meta-learning has also been exploited in a few recent studies <ref type="bibr" target="#b20">(Li et al., 2020a;</ref><ref type="bibr" target="#b6">de Lichy et al., 2021)</ref> for fewshot NER. However, our work substantially differs from them in that we proposed a decomposed metalearning procedure to separately optimize the span detection model and the entity typing model. This paper presents a decomposed meta-learning method for few-shot NER problem, i.e., sequentially tackle few-shot span-detection and few-shot entity typing using meta-learning. We formulate the few-shot span detection as a sequence labeling problem and employ MAML to learn a good parameter initialization, which enables the model to fast adapt to novel entity classes by fully exploring information contained in support examples. For fewshot entity typing, we propose MAML-ProtoNet, which can find a better embedding space than conventional ProtoNet to represent entity spans from different classes more distinguishably, thus making more accurate predictions. Extensive experiments on various benchmarks show that our approach achieves superior performance over prior methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Meta learning</p><p>The goal of meta-learning is to learn to fast adapt to a new few-shot task that is never-seen-before.</p><p>To train a meta-learning model, a large number of episodes T train (few-shot tasks) are constructed from training data D train , which usually follows the N -way K-shot task formulation and are used to train the meta-learning model. One episode contains a small training set S train , called support set, and a test set Q train , called query set. The metalearner generates a task-specific model for a new task T i via updating on support set S train , then the task-specific model is tested on Q train to get a test error.  For Few-NERD, we use episodes released by <ref type="bibr" target="#b9">Ding et al. (2021)</ref> 7 which contain 20,000 episodes for training, 1,000 episodes for validation, and 5,000 episodes for testing. Each episode is an N-way K?2K-shot few-shot task. As for Cross-Dataset, two datasets are used for constructing training episodes, one dataset is used for validation, and episodes from the remained dataset are 7 https://ningding97.github.io/fewnerd/ used for evaluation. We use public episodes 8 constructed by <ref type="bibr" target="#b17">Hou et al. (2020)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Additional Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head><p>Setting We use BERT-base-unca sed from Huggingface Library <ref type="bibr" target="#b38">(Wolf et al., 2020)</ref> as our base encoder following <ref type="bibr" target="#b9">Ding et al. (2021)</ref>. We use AdamW <ref type="bibr" target="#b23">(Loshchilov and Hutter, 2019)</ref> as our optimizer with a learning rate of 3e-5 and 1% linear warmup steps at both the meta-training and finetuning in meta-testing time for all experiments. The batch size is set to 32, the max sequence length is set to 128 and we keep dropout rate as 0.1. At meta-training phase, the inner update step is set to 2 for all experiments. When finetuning the span detector at meta-testing phase, the finetune step is set to 3 for all inter settings on Few-NERD dataset and 30 for other experiments. For entity typing, the finetune step at meta-testing phase is set to 3 for all experiments on Few-NERD dataset, 20 for all Cross-Dataset experiments. To further boost the performance, we only keep entities that have a similarity score with its nearest prototype greater than a threshold of 2.5. We set max-loss coefficient ? as 2 at meta-training query set evaluation phase, 5 at other phases. We validate our model on dev set every 100 steps and select the checkpoint with best f1 score performance on dev set within the max train steps 1,000. We use grid search for hyperparameter setting, the search space is shown in <ref type="table" target="#tab_13">Table A</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Baselines</head><p>We consider the following metric-learning based baselines:</p><p>SimBERT <ref type="bibr" target="#b17">(Hou et al., 2020)</ref> applies BERT without any finetuning as the embedding function, then assign each token's label by retrieving the most similar token in the support set .</p><p>ProtoBERT <ref type="bibr" target="#b12">(Fritzler et al., 2019)</ref> uses a tokenlevel prototypical network <ref type="bibr" target="#b30">(Snell et al., 2017)</ref> which represents each class by averaging token representation with the same label, then the label of each token in the query set is decided by its nearest class prototype.</p><p>MatchingBERT <ref type="bibr" target="#b35">(Vinyals et al., 2016)</ref> is similar to ProtoBERT except that it calculates the similarity between query instances and support instances instead of class prototypes.</p><p>L-TapNet+CDT <ref type="bibr" target="#b17">(Hou et al., 2020</ref><ref type="bibr">) enhances TapNet (Yoon et al., 2019</ref> with pair-wise embedding, label semantic, and CDT transition mechanism.</p><p>NNShot <ref type="bibr" target="#b41">(Yang and Katiyar, 2020)</ref> pretrains BERT for token embedding by conventional classification for training, a token-level nearest neighbor method is used at testing.</p><p>StructShot <ref type="bibr" target="#b41">(Yang and Katiyar, 2020)</ref> improves NNshot by using an abstract transition probability for Viterbi decoding at testing.</p><p>ESD <ref type="bibr" target="#b36">(Wang et al., 2021a)</ref> is a span-level metric learning based method. It enhances prototypical network by using inter-and cross-span attention for better span representation and designs multiple prototypes for O label.</p><p>Besides, we also compare with the finetunebased methods:</p><p>TransferBERT <ref type="bibr" target="#b17">(Hou et al., 2020)</ref> trains a tokenlevel BERT classifier, then finetune task-specific linear classifier on support set at test time.</p><p>CONTAINER (Das et al., 2021) uses tokenlevel contrastive learning for training BERT as token embedding function, then finetune the BERT on support set and apply a nearest neighbor method at inference time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Results of Span Detection</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc><ref type="bibr" target="#b36">Wang et al., 2021a)</ref>,CON- TAINER (Das et al., 2021), and methods from<ref type="bibr" target="#b9">Ding et al. (2021)</ref>, e.g., ProtoBERT, StructShot, etc. For Corss-Dataset, we compare our method to L-TapNet+CDT<ref type="bibr" target="#b17">(Hou et al., 2020)</ref> and other baselines from<ref type="bibr" target="#b17">Hou et al. (2020)</ref>, e.g., Transfer-BERT, Matching Network, etc. Please refer to the Appendix A.4 for more details about baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Case study of span detection. Sup-Span: train a span detector in the fully supervised manner on available data from all training episodes, and then directly use it for span detection. Sup-Span-f.t.: further fine-tune the model learned by Sup-Span as in the proposed approach. dicate that exploring information contained in support examples with the proposed meta-learning procedure does bring performance gain for few-shot transfer. 2) Ours outperforms Ours w/o Span Detector and Ours w/o MAML outperforms Ours w/o Span Detector w/o MAML demonstrate the essentiality of the decomposed framework (i.e., mention detection and entity typing) to mitigate the problem of noisy prototype for non-entities. 3) Though MAML plays an important role in learning from few-shot support examples, Ours w/o ProtoNet, which requires the model to adapt the up-most classification layer without sharing knowledge with training episodes leads to unsatisfactory results, verifying the reasonableness and the effectiveness of our decomposed meta-learning procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>F1 scores of differently trained span detectors w.r.t. fine-tune steps on Few-NERD 5-way 1?2-shot test set. The light-colored area indicates the range of results obtained from multiple random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>t-SNE visualization of span representations for entity typing on Few-NERD Intra, 5-way 5?10shot dev set. The representations are obtained from BERT trained with ProtoNet, and our MAML enhanced ProtoNet respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: An example of the simplest 2-way 1-shot set-</cell></row><row><cell>ting, which contains two entity classes and each class</cell></row><row><cell>has one example (shot) in the support set S. Different</cell></row><row><cell>colors indicate different entity classes.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>a novel episode unseen during training, conventional ProtoNet directly adopts the meta-trained model to compute prototypes with S new , and then inference on Q new . Here, we first take the support examples from S new to fine-tune the meta-learned model ? * for a few steps in a way the same as Eq.(13), however, the loss is computed on S new . Then, we leverage S new again to compute prototypes with the fine-tuned model, and further inference the entity class for each detected span in Q new as in Eq. (12).</figDesc><table><row><cell>4 Experiments</cell></row><row><cell>4.1 Settings</cell></row><row><cell>4.1.1 Datasets</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Two tasks are considered on this dataset: i) Intra, where all entities in train/dev/test splits belong to different coarsegrained types. ii) Inter, where train/dev/test splits may share coarse-grained types while keeping the fine-grained entity types mutually disjoint. 3</figDesc><table><row><cell>Cross-Dataset (Hou et al., 2020). Four datasets</cell></row><row><cell>focusing on four domains are used here:</cell></row><row><cell>CoNLL-2003 (Tjong Kim Sang, 2002) (news),</cell></row><row><cell>GUM (Zeldes, 2017) (Wiki) , WNUT-2017 (Der-</cell></row></table><note>czynski et al., 2017) (social), and Ontonotes (Prad- han et al., 2013) (mixed). We take two domains for training, one for validation, and the remaining for test. For fair comparison, we directly use sampled3 https://github.com/thunlp/Few-NERD episodes by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>45?0.92 19.76?0.59 41.93?0.55 34.61?0.59 44.44?0.11 39.09?0.87 58.80?1.42 53.97?0.38 NNShot ? 31.01?1.21 21.88?0.23 35.74?2.36 27.67?1.06 54.29?0.40 46.98?1.96 50.56?3.33 50.00?0.36 StructShot ? 35.92?0.69 25.38?0.84 38.83?1.72 26.39?2.59 57.33?0.53 49.46?0.53 57.16?2.09 49.44?1.16 32.29?1.10 50.68?0.94 42.92?0.75 66.46?0.49 59.95?0.69 74.14?0.80 67.91?1.41 Ours 52.04?0.44 43.50?0.59 63.23?0.45 56.84?0.14 68.77?0.24 63.26?0.40 71.62?0.16 68.32?0.10</figDesc><table><row><cell></cell><cell></cell><cell>Intra</cell><cell></cell><cell></cell><cell></cell><cell>Inter</cell><cell></cell></row><row><cell></cell><cell cols="2">1?2-shot</cell><cell cols="2">5?10-shot</cell><cell cols="2">1?2-shot</cell><cell cols="2">5?10-shot</cell></row><row><cell></cell><cell>5 way</cell><cell>10 way</cell><cell>5 way</cell><cell>10 way</cell><cell>5 way</cell><cell>10 way</cell><cell>5 way</cell><cell>10 way</cell></row><row><cell>ProtoBERT  ?</cell><cell cols="8">23.39?1.77</cell></row><row><cell cols="2">CONTAINER (Das et al., 2021) 40.43</cell><cell>33.84</cell><cell>53.70</cell><cell>47.49</cell><cell>55.95</cell><cell>48.35</cell><cell>61.83</cell><cell>57.12</cell></row><row><cell>ESD (Wang et al., 2021a)</cell><cell>41.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>F1 scores with standard deviations on Few-NERD for both inter and intra settings. ? denotes the results reported in<ref type="bibr" target="#b9">Ding et al. (2021)</ref>.5  The best results are in bold.<ref type="bibr" target="#b17">Hou et al., 2020)</ref> 44.30?3.15 12.04?0.65 20.80?1.06 15.17?1.25 45.35?2.67 11.65?2.34 23.30?2.80 20.95?2.81 Ours 46.09?0.44 17.54?0.98 25.14?0.24 34.13?0.92 58.18?0.87 31.36?0.91 31.02?1.28 45.55?0.90</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>1-shot</cell><cell></cell><cell></cell><cell></cell><cell>5-shot</cell><cell></cell></row><row><cell>Models</cell><cell>News</cell><cell>Wiki</cell><cell>Social</cell><cell>Mixed</cell><cell>News</cell><cell>Wiki</cell><cell>Social</cell><cell>Mixed</cell></row><row><cell>TransferBERT  ?</cell><cell cols="8">4.75?1.42 0.57?0.32 2.71?0.72 3.46?0.54 15.36?2.81 3.62?0.57 11.08?0.57 35.49?7.60</cell></row><row><cell>SimBERT  ?</cell><cell cols="8">19.22?0.00 6.91?0.00 5.18?0.00 13.99?0.00 32.01?0.00 10.63?0.00 8.20?0.00 21.14?0.00</cell></row><row><cell>Matching Network  ?</cell><cell cols="8">19.50?0.35 4.73?0.16 17.23?2.75 15.06?1.61 19.85?0.74 5.58?0.23 6.61?1.75 8.08?0.47</cell></row><row><cell>ProtoBERT  ?</cell><cell cols="8">32.49?2.01 3.89?0.24 10.68?1.40 6.67?0.46 50.06?1.57 9.54?0.44 17.26?2.65 13.59?1.61</cell></row><row><cell>L-TapNet+CDT (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>F1 scores with standard deviations on Cross-Dataset.? denotes the results reported in Hou et al. (2020). The best results are in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Ablation study: F1 scores on Few-NERD 5-</cell></row><row><cell>way 1?2-shot are reported.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4</head><label>4</label><figDesc>highlights the contributions of each component in our proposed approach. Generally speaking, removing any of them will generally lead to a performance drop. Moreover, we can draw some indepth observations as follows. 1) Ours outperforms Ours w/o MAML and Ours w/o Span Detector outperforms Ours w/o Span Detector w/o MAML in-The production opened on Broadway at the New Century Theatre where it ran from November.</figDesc><table><row><cell>New-Query:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Analysis on entity typing under Few-NERD 5-</cell></row><row><cell>way 1?2-shot setting. F1 scores are reported. Ours w/</cell></row><row><cell>ProtoNet: built upon the same span detection model as</cell></row><row><cell>Ours, directly leverage ProtoNet for inference.</cell></row><row><cell>model (MAML-Span-f.t.) consistently outperforms</cell></row><row><cell>Sup-Span-f.t., suggesting that the proposed meta-</cell></row><row><cell>learning procedure could better leverage support</cell></row><row><cell>examples from novel episodes and meanwhile, help</cell></row><row><cell>the model adapt to new episodes more effectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>The meta-learner then learns to learn new tasks by considering how to reduce the test error on Q train by updating on S train . To evaluate the task learning ability of a meta-learner, a bunch of episodes T test are constructed from the normal test data D test , and the expectation of performance on Q test from all test episodes is severed as evaluation protocol. To distinguish the training phase of meta-learner on episodes T train and training of a task-specific model on support set S, the former is called meta-training and the latter is called training. Similarly, the testing of a meta-learner on T test is called meta-testing, and the evaluating of a taskspecific model on query set Q is called testing.</figDesc><table><row><cell>A.2 Datasets</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Table A.1 shows the dataset statistics of original</cell></row><row><cell cols="3">data for constructing few-shot episodes.</cell><cell></cell></row><row><cell>Dataset</cell><cell>Domain</cell><cell cols="2"># Sentences # Classes</cell></row><row><cell cols="2">Few-NERD Wikipedia</cell><cell>188.2k</cell><cell>66</cell></row><row><cell>CoNLL03</cell><cell>News</cell><cell>20.7k</cell><cell>4</cell></row><row><cell>GUM</cell><cell>Wiki</cell><cell>3.5k</cell><cell>11</cell></row><row><cell>WNUT</cell><cell>Social</cell><cell>5.6k</cell><cell>6</cell></row><row><cell>OntoNotes</cell><cell>Mixed</cell><cell>159.6k</cell><cell>18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table A .</head><label>A</label><figDesc></figDesc><table /><note>1: Evaluation dataset statistics</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>. For 5shot, 200 episodes are used for training, 100 episodes for validation, and 100 for testing. For the 1shot experiment, 400/100/200 episodes are used for training/validation/testing, except for experiments on OntoNotes(Mixed), where 400/200/100 episodes are constructed for train/dev/test.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>.2. The total model has 196M parameters and trains in ?60min on a Tesla V100 GPU.</figDesc><table><row><cell>Learning rate</cell><cell>{1e-5, 3e-5, 1e-4}</cell></row><row><cell>Meta-test fine-tune steps</cell><cell>{3, 5, 10, 20, 30}</cell></row><row><cell>Max-loss coefficient ?</cell><cell>{0, 1, 2, 5, 10}</cell></row><row><cell>Type similarity threshold</cell><cell>{1, 2.5, 5}</cell></row><row><cell>Mini-batch size</cell><cell>{16, 32}</cell></row><row><cell cols="2">Table A.2: Hyper-parameters search space used in our</cell></row><row><cell>experiments.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table A .</head><label>A</label><figDesc>3 and Table A.4 show the performance of our span detection module on Few-NERD and Cross-Dataset. Intra 73.69?0.14 74.32?1.84 77.76?0.24 78.66?0.15 Inter 76.71?0.30 76.63?0.24 75.97?0.14 76.62?0.11 Table A.3: F1 scores of our entity span detection module on Few-NERD for both inter and intra settings. 65.06?0.91 35.63?2.17 38.89?0.55 46.52?1.24 5-shot 74.20?0.33 46.26?1.28 43.16?1.23 54.70?0.88 Table A.4: F1 scores of our entity span detection module on Cross-Dataset.</figDesc><table><row><cell>Models</cell><cell cols="2">1?2-shot</cell><cell cols="2">5?10-shot</cell></row><row><cell></cell><cell>5 way</cell><cell>10 way</cell><cell>5 way</cell><cell>10 way</cell></row><row><cell cols="2">Models News</cell><cell>Wiki</cell><cell>Social</cell><cell>Mixed</cell></row><row><cell>1-shot</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We found BIOES to be stronger than BIO for typeagnostic span detection as it explicitly encourages the model to learn more specific and fine-grained boundary information. Besides, our entity typing model aims to assign an entity type for each detected span, which does not involve any tagging scheme.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/AtmaHou/ FewShotTagging</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Augmented natural language for generative sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.27</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="375" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mc-Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam; NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>December 6-12, 2020, virtual</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meta relational learning for few-shot link prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1431</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4217" to="4226" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00104</idno>
		<title level="m">Named entity recognition with bidirectional LSTM-CNNs. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Template-based named entity recognition using BART</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.161</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1835" to="1845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Container: Fewshot named entity recognition via contrastive learning. ArXiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarkar</forename><surname>Snigdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarathi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">J</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2109.07589</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meta-learning for few-shot named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadrien</forename><surname>Cyprien De Lichy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Glaude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Campbell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.metanlp-1.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing</title>
		<meeting>the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Results of the WNUT2017 shared task on novel and emerging entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marieke</forename><surname>Van Erp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4418</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Noisy User-generated Text</title>
		<meeting>the 3rd Workshop on Noisy User-generated Text<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="140" to="147" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Few-NERD: A few-shot named entity recognition dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.248</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3198" to="3213" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08-11" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The viterbi algorithm. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Few-shot classification in named entity recognition task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fritzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kretov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing</title>
		<meeting>the 34th ACM/SIGAPP Symposium on Applied Computing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SpanNER: Named entity re-/recognition as span prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.558</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7183" to="7195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.295</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Induction networks for few-shot text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiying</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1403</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3904" to="3913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Few-shot slot tagging with collapsed dependency transfer and label-enhanced task-adaptive projection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkui</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1381" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning<address><addrLine>Williams College, Williamstown, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001-06-28" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Few-shot named entity recognition via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2020.3038670</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge &amp; Data Engineering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A unified MRC framework for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5849" to="5859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards improving neural named entity recognition with gazetteers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1524</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5301" to="5307" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note type="report_type">OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Instance-based learning of span representations: A case study through named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ouchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Yokoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuki</forename><surname>Kuribayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuto</forename><surname>Konno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.575</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6452" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1161</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1756" to="1765" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Bj?rkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.185</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-02: The 6th Conference on Natural Language Learning</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning from miscellaneous other-class words for fewshot named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meihan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.487</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6236" to="6247" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">86</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">An enhanced span-based decomposition method for few-shot sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/2109.13023</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning from language description: Low-shot named entity recognition via decomposed framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoda</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Enhanced meta-learning for cross-lingual named entity recognition with minimal resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianhui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>B?rje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqing</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9274" to="9281" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A unified generative framework for various NER subtasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.451</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5808" to="5822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simple and effective few-shot named entity recognition with structured nearest neighbor learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.516</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6365" to="6375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tapnet: Neural network augmented with taskadaptive projection for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sung Whan Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7115" to="7123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Few-shot intent classification and slot filling with retrieved examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.59</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="734" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The gum corpus: Creating multilayer resources in the classroom. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zeldes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="581" to="612" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
