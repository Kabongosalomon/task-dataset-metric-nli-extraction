<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs Steering Committee</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>5 Chemistry</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">TU Dortmund University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>5 Chemistry</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>5 Chemistry</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>5 Chemistry</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Re</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
						</author>
						<title level="a" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs Steering Committee</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the OPEN GRAPH BENCHMARK (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs are widely used for abstracting complex systems of interacting objects, such as social networks <ref type="bibr" target="#b28">(Easley et al., 2010)</ref>, knowledge graphs <ref type="bibr" target="#b60">(Nickel et al., 2015)</ref>, molecular graphs <ref type="bibr" target="#b89">(Wu et al., 2018)</ref>, and biological networks <ref type="bibr" target="#b8">(Barabasi &amp; Oltvai, 2004)</ref>, as well as for modeling 3D objects <ref type="bibr" target="#b72">(Simonovsky &amp; Komodakis, 2017)</ref>, manifolds <ref type="bibr" target="#b14">(Bronstein et al., 2017)</ref>, and source code <ref type="bibr" target="#b3">(Allamanis et al., 2017)</ref>. Machine learning (ML), especially deep learning, on graphs is an emerging field <ref type="bibr" target="#b36">(Hamilton et al., 2017b;</ref><ref type="bibr" target="#b14">Bronstein et al., 2017)</ref>. Recently, significant methodological advances have been made in graph ML <ref type="bibr" target="#b33">(Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b97">Ying et al., 2018b;</ref><ref type="bibr" target="#b81">Veli?kovi? et al., 2019;</ref>, which have produced promising results in applications from diverse domains <ref type="bibr" target="#b96">(Ying et al., 2018a;</ref><ref type="bibr" target="#b104">Zitnik et al., 2018;</ref><ref type="bibr" target="#b74">Stokes et al., 2020)</ref>.</p><p>How can we further advance research in graph ML? Historically, high-quality and largescale datasets have played significant roles in advancing research, as exemplified by IMA- <ref type="bibr">GENET</ref>  <ref type="bibr" target="#b22">(Deng et al., 2009</ref>) and MS COCO <ref type="bibr" target="#b55">(Lin et al., 2014)</ref> in computer vision, GLUE BENCHMARK <ref type="bibr" target="#b83">(Wang et al., 2018)</ref> and SQUAD <ref type="bibr" target="#b66">(Rajpurkar et al., 2016)</ref> in natural language processing, and LIBRISPEECH <ref type="bibr" target="#b61">(Panayotov et al., 2015)</ref> and CHIME <ref type="bibr" target="#b9">(Barker et al., 2015)</ref> in speech processing. However, in graph ML research, commonly-used datasets and evaluation procedures present issues that may negatively impact future progress.  <ref type="figure">Figure 1</ref>: OGB provides datasets that are diverse in scale, domains, and task categories.</p><p>Issues with current benchmarks. Most of the frequently-used graph datasets are extremely small compared to graphs found in real applications (with more than 1 million nodes or 100 thousand graphs) <ref type="bibr" target="#b84">(Wang et al., 2020;</ref><ref type="bibr" target="#b96">Ying et al., 2018a;</ref><ref type="bibr" target="#b89">Wu et al., 2018;</ref><ref type="bibr" target="#b40">Husain et al., 2019;</ref><ref type="bibr" target="#b11">Bhatia et al., 2016;</ref><ref type="bibr" target="#b82">Vrande?i? &amp; Kr?tzsch, 2014)</ref>. For example, the widely-used node classification datasets, CORA, CITESEER, and PUBMED <ref type="bibr" target="#b95">(Yang et al., 2016)</ref>, only have 2,700 to 20,000 nodes, the popular graph classification datasets from the TU collection <ref type="bibr" target="#b92">(Yanardag &amp; Vishwanathan, 2015;</ref><ref type="bibr" target="#b44">Kersting et al., 2020)</ref> only contain 200 to 5,000 graphs, and the commonly-used knowledge graph completion datasets, FB15K and WN18 <ref type="bibr" target="#b13">(Bordes et al., 2013)</ref>, only have 15,000 to 40,000 entities. As models are extensively developed on these small datasets, the majority of them turn out to be not scalable to larger graphs <ref type="bibr" target="#b80">Velickovic et al., 2018;</ref><ref type="bibr" target="#b13">Bordes et al., 2013;</ref><ref type="bibr" target="#b78">Trouillon et al., 2016)</ref>. The small datasets also make it hard to rigorously evaluate data-hungry models, such as Graph Neural Networks (GNNs) <ref type="bibr" target="#b53">(Li et al., 2016;</ref><ref type="bibr" target="#b26">Duvenaud et al., 2015;</ref><ref type="bibr" target="#b32">Gilmer et al., 2017;</ref>. In fact, the performance of GNNs on these datasets is often unstable and nearly statistically  <ref type="figure">Figure 2</ref>: Overview of the OGB pipeline: (a) OGB provides realistic graph benchmark datasets that cover different prediction tasks <ref type="bibr">(node, link, graph)</ref>, are from diverse application domains, and are at different scales. (b) OGB fully automates dataset processing and splitting. That is, the OGB data loaders automatically download and process graphs, provide graph objects (compatible with PYTORCH 1 <ref type="bibr" target="#b62">(Paszke et al., 2019)</ref> and its associated graph libraries, PYTORCH GEOMETRIC 2 <ref type="bibr" target="#b31">(Fey &amp; Lenssen, 2019)</ref> and DEEP GRAPH LIBRARY 3 <ref type="bibr" target="#b85">(Wang et al., 2019a)</ref>), and further split the datasets in a standardized manner. (c) After an ML model is developed, (d) OGB evaluates the model in a dataset-dependent manner, and outputs the model performance appropriate for the task at hand. Finally, (e) OGB provides public leaderboards to keep track of recent advances.</p><p>identical to each other, due to the small number of samples the models are trained and evaluated on <ref type="bibr" target="#b27">(Dwivedi et al., 2020;</ref><ref type="bibr" target="#b37">Hu et al., 2020a)</ref>.</p><p>Furthermore, there is no unified and commonly-followed experimental protocol. Different studies adopt their own dataset splits, evaluation metrics, and cross-validation protocols, making it challenging to compare performance reported across various studies <ref type="bibr" target="#b71">(Shchur et al., 2018;</ref><ref type="bibr" target="#b29">Errica et al., 2019;</ref><ref type="bibr" target="#b27">Dwivedi et al., 2020)</ref>. In addition, many studies follow the convention of using random splits to generate training/test sets <ref type="bibr" target="#b13">Bordes et al., 2013)</ref>, which is not realistic or useful for real-world applications and generally leads to overly optimistic performance results <ref type="bibr" target="#b56">(Lohr, 2009</ref>).</p><p>Thus, there is an urgent need for a comprehensive suite of real-world benchmarks that combine a diverse set of datasets of various sizes coming from different domains. Data splits as well as evaluation metrics are important so that progress can be measured in a consistent and reproducible way. Last but not least, benchmarks also need to provide different types of tasks, such as node classification, link prediction, and graph classification.</p><p>Present work: OGB. Here, we present the OPEN GRAPH BENCHMARK (OGB) with the goal of facilitating scalable, robust, and reproducible graph ML research. The premise of OGB is to develop a diverse set of challenging and realistic benchmark datasets that can empower the rigorous advancements in graph ML. As illustrated in <ref type="figure">Figure 1</ref>, the OGB datasets are designed to have the following three characteristics:</p><p>1. Large scale. The OGB datasets are orders-of-magnitude larger than existing benchmarks and can be categorized into three different scales (small, medium, and large). Even the "small" OGB graphs have more than 100 thousand nodes or more than 1 million edges, but are small enough to fit into the memory of a single GPU, making them suitable for testing computationally intensive algorithms. Additionally, OGB introduces "medium" (more than 1 million nodes or more than 10 million edges) and "large" (on the order of 100 million nodes or 1 billion edges) datasets, which can facilitate the development of scalable models based on mini-batching and distributed training. 2. Diverse domains. The OGB datasets aim to include graphs that are representative of a wide range of domains, as shown in <ref type="table" target="#tab_1">Table 1</ref>. The broad coverage of domains in OGB empowers the development and demonstration of general-purpose models, and can be used to distinguish them from domain-specific techniques. Furthermore, for each dataset, OGB adopts domain-specific data splits (e.g., based on time, species, molecular structure, GitHub project, etc.) that are more realistic and meaningful than conventional random splits. 3. Multiple task categories. Besides data diversity, OGB supports three categories of fundamental graph ML tasks, i.e., node, link, and graph property predictions, each of which requires the models to make predictions at different levels of graphs, i.e., at the level of a node, link, and entire graph, respectively. The currently-available OGB datasets are categorized in <ref type="table" target="#tab_1">Table 1</ref> according to their task categories, application domains, and scales. Currently, OGB includes 15 diverse graph datasets, with at least 4 datasets for each task category. All the datasets are constructed by ourselves, except for ogbn-products, ogbg-molpcba, and ogbg-molhiv, whose graphs and target labels are adopted from <ref type="bibr" target="#b16">Chiang et al. (2019)</ref> and <ref type="bibr" target="#b89">Wu et al. (2018)</ref>. For these datasets, we resolve critical issues of the existing data splits by presenting more meaningful and standardized splits.</p><p>In addition to building the graph datasets, we also perform extensive benchmark experiments for each dataset. Through the experiments and ablation studies, we highlight research challenges and opportunities provided by each dataset, especially on (1) scaling models to large graphs, and (2) improving out-of-distribution generalization performance under the realistic data split scenarios.</p><p>Finally, as illustrated in <ref type="figure">Figure 2</ref>, OGB presents an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation, in the same spirit as OpenML <ref type="bibr" target="#b79">(Vanschoren et al., 2013;</ref><ref type="bibr" target="#b30">Feurer et al., 2019)</ref>. Specifically, given the OGB datasets (a), the end-users can focus on developing their graph ML models (c) by using the OGB data loaders (b) and evaluators (d), both of which are provided by our OGB Python package 4 . OGB also hosts a public leaderboard 5 (e) for publicizing state-of-the-art, reproducible graph ML research. As a starting point, for each dataset, we include results from a suite of well-known baselines, particularly GNN-based methods, together with code to reproduce our results.</p><p>OGB is an on-going open-source, community-driven initiative. Over time we plan to release new versions of the datasets and methods, and provide updates to the leaderboard. The OGB web-site (https://ogb.stanford.edu) provides the documentation, example scripts, and public leaderboard. We also welcome inputs from the community at ogb@cs.stanford.edu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Shortcomings of Current Benchmarks</head><p>We first review commonly-used graph benchmarks and discuss the current state of the field. We organize the discussion around three categories of graph ML tasks: predictions at the level of nodes, links, and graphs.</p><p>Node property prediction. Currently, the three graphs (CORA, CITESEER and PUBMED) proposed in <ref type="bibr" target="#b95">Yang et al. (2016)</ref> have been widely used as semi-supervised node classification datasets, particularly for evaluating GNNs. The sizes of these graphs are rather small, ranging from 2,700 to 20,000 nodes. Recent studies suggest that datasets at this small scale can be solved quite well with simple GNN architectures <ref type="bibr" target="#b71">(Shchur et al., 2018;</ref>, and the performance of different GNNs on these datasets is nearly statistically identical <ref type="bibr" target="#b27">(Dwivedi et al., 2020;</ref><ref type="bibr" target="#b37">Hu et al., 2020a)</ref>. Furthermore, there is no consensus on the splitting procedures for these datasets, which makes it hard to fairly compare different model designs <ref type="bibr" target="#b71">(Shchur et al., 2018)</ref>. Finally, a recent study <ref type="bibr" target="#b106">(Zou et al., 2020)</ref> shows that these datasets have some fundamental data quality issues. For example, in CORA, 42% of the nodes leak information between their features and labels, and 1% of the nodes are duplicated. The situation for CITESEER is even worse, with leakage rates of 62% and duplication rates of 5%.</p><p>Some recent works in graph ML have proposed relatively large datasets, such as PPI (56,944 nodes), REDDIT (334,863 nodes) <ref type="bibr" target="#b36">(Hamilton et al., 2017b)</ref> and AMAZON2M (2,449,029 nodes) <ref type="bibr" target="#b16">(Chiang et al., 2019)</ref>. However, there exist some inherent issues with the proposed data splits. Specifically, 83%, 65% and 90% of the nodes are used for training in the PPI, REDDIT and AMAZON2M datasets, respectively, which results in an artificially small distribution shift across the training/validation/test sets. Consequently, as may be expected, the performance improvements on these benchmarks have quickly saturated. For example, recent GNN models <ref type="bibr" target="#b16">(Chiang et al., 2019;</ref><ref type="bibr">Zeng et al., 2020)</ref> can already yield F1 scores of 99.5 for PPI and 97.0 for REDDIT, and 90.4% accuracy for AMAZON2M, with extremely small generalization gaps between training and test accuracy. Finally, it is also practically required for graph ML models to handle web-scale graphs (beyond 100 million nodes and 1 billion edges) in industrial applications <ref type="bibr" target="#b96">(Ying et al., 2018a)</ref>. However, to date, there have been no publicly available graph datasets of such scale with label information.</p><p>In summary, several factors (e.g., size, leakage, splits, etc.) associated with the current use of existing datasets make them unsuitable as benchmark datasets for graph ML.</p><p>Link property prediction. Broadly, there are two lines of efforts for the link-level task: link prediction in homogeneous networks <ref type="bibr" target="#b54">(Liben-Nowell &amp; Kleinberg, 2007;</ref><ref type="bibr" target="#b101">Zhang &amp; Chen, 2018)</ref> and relation completion in (heterogeneous) knowledge graphs <ref type="bibr" target="#b13">(Bordes et al., 2013;</ref><ref type="bibr" target="#b60">Nickel et al., 2015;</ref><ref type="bibr">Hu et al., 2020b)</ref>. There are several problems with the current benchmark datasets in these areas.</p><p>First, representative datasets are either extremely small or do not come with input node features. For link prediction, while the well-known recommender system datasets used in <ref type="bibr" target="#b10">Berg et al. (2017)</ref> include node features, their sizes are very small, with the largest having only 6,000 nodes. On the other hand, although the Open Academic Graph (OAG) used in <ref type="bibr" target="#b65">Qiu et al. (2019)</ref> comprises tens of millions of nodes, there are no associated node features. Regarding the knowledge graph completion, the widely-used dataset, FB15K, is very small, containing only 14,951 entities, which is a tiny subset of the original Freebase knowledge graph with more than 50 million entities <ref type="bibr" target="#b12">(Bollacker et al., 2008)</ref>.</p><p>Second, similar to the node-level task, random splits are predominantly used in link-level prediction <ref type="bibr" target="#b13">(Bordes et al., 2013;</ref><ref type="bibr" target="#b33">Grover &amp; Leskovec, 2016)</ref>. The random splits are not realistic in many practical applications such as friend recommendation in social networks, in which test edges (friend relations after a certain timestamp) naturally follow a different distribution from training edges (friend relations before a certain timestamp).</p><p>Finally, the existing datasets are mostly oriented to applications in recommender systems, social media and knowledge graphs, in which the graphs are typically very sparse. This may result in techniques specialised for sparse link inference that are not generalizable to domains with dense graphs, such as the protein-protein association graphs and drug-drug interaction networks typically found in biology and medicine <ref type="bibr" target="#b77">(Szklarczyk et al., 2019;</ref><ref type="bibr" target="#b87">Wishart et al., 2018;</ref><ref type="bibr" target="#b19">Davis et al., 2019;</ref><ref type="bibr" target="#b76">Szklarczyk et al., 2016;</ref><ref type="bibr" target="#b64">Pi?ero et al., 2020)</ref>. Very recently, <ref type="bibr" target="#b73">Sinha et al. (2020)</ref> proposed a synthetic link prediction benchmark to diagnose model's logical generalization capability. Their focus is on synthetic tasks, which is complementary to OGB that focuses on realistic tasks.</p><p>Graph property prediction. Graph-level prediction tasks are found in important applications in natural sciences, such as predicting molecular properties in chemistry <ref type="bibr" target="#b26">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b32">Gilmer et al., 2017;</ref><ref type="bibr" target="#b37">Hu et al., 2020a)</ref>, where molecules are naturally represented as molecular graphs.</p><p>In graph classification, the most widely-used graph-level datasets from the TU collection <ref type="bibr" target="#b44">(Kersting et al., 2020)</ref> are known to have many issues, such as small sizes (i.e., most of the datasets only contain less than 1,000 graphs), 6 unrealistic settings (e.g., no bond features for molecules), random data splits, inconsistent evaluation protocols, and isomorphism bias <ref type="bibr" target="#b42">(Ivanov et al., 2019)</ref>. A very recent attempt <ref type="bibr" target="#b27">(Dwivedi et al., 2020)</ref> to address these issues mainly focuses on benchmarking ML models, specifically the building blocks of GNNs, rather than developing application-oriented realistic datasets. In fact, five out of the six proposed datasets are synthetic.</p><p>Recent work in graph ML <ref type="bibr" target="#b37">(Hu et al., 2020a;</ref><ref type="bibr" target="#b41">Ishiguro et al., 2019)</ref> has started to adopt MOLECU-LENET <ref type="bibr" target="#b89">(Wu et al., 2018)</ref> which contains a set of realistic and large-scale molecular property prediction datasets. However, there is limited consensus in the dataset splitting and molecular graph features, making it hard to compare different models in a fair manner. OGB adopts the MOLECULENET datasets, while providing unified dataset splits as well as the molecular graph features that are found to provide favorable performance over na?ve features.</p><p>Beyond molecular graphs, OGB also provides graphs from other domains, such as biological networks and Abstract Syntax Tree (AST) representations of source code. These types of graphs exhibit distinct characteristics from molecular graphs, enabling the evaluation of the versatility of graph ML models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OGB: Overview</head><p>The goal of OGB is to support and catalyze research in graph ML, which is a fast-growing and increasingly important area. OGB datasets cover a variety of real-world applications and span several important domains. Furthermore, OGB provides a common codebase using popular deep learning frameworks for loading, constructing, and representing graphs as well as code implementations of established performance metrics for fast model evaluation and comparison.</p><p>In the subsequent sections (Sections 4, 5, and 6), we describe in detail each of the datasets in OGB, focusing on the properties of the graph(s), the prediction task, and the dataset splitting scheme. The currently-available datasets are summarized in <ref type="table">Table 2</ref>. We also compare datasets from diverse application domains by inspecting their basic graph statistics, e.g., node degree, clustering coefficient, and diameter, as summarized in <ref type="table" target="#tab_3">Table 3</ref>. We show that OGB datasets exhibit diverse graph characteristics. The difference in graph characteristics results in the inherent difference in how information propagates in the graphs, which can significantly affect the behavior of many graph ML models such as GNNs and random-walk-based node embeddings <ref type="bibr" target="#b90">(Xu et al., 2018)</ref>. Overall, the diversity in graph characteristics originates from the diverse application domains and is crucial to evaluate the versatility of graph ML models.</p><p>In the same sections, we additionally present an extensive benchmark analysis for each dataset, using representative node embedding models, GNNs, as well as recently-introduced mini-batchbased GNNs. We discuss our initial findings, and highlight research challenges and opportunities in: (1) scaling models to large graphs, and (2) improving out-of-distribution generalization under the realistic data splits. We repeat each experiment 10 times using different random seeds, and report the mean and unbiased standard deviation of all training and test results corresponding to the best validation results. All code to reproduce our baseline experiments is publicly available at https://github.com/snap-stanford/ogb/tree/master/examples and is meant as a starting point to accelerate further research on our proposed datasets. We refer the interested reader to our code base for the details of model architectures and hyper-parameter settings.</p><p>Finally, in Section 7, we briefly explain the usage of our OGB Python package that can be readily installed via pip. We demonstrate how the OGB package makes the pipeline shown in <ref type="figure">Figure 2</ref> easily accessible by providing the automatic data loaders and evaluators. The package is publicly <ref type="table">Table 2</ref>: Summary of currently-available OGB datasets. An OGB dataset, e.g., ogbg-molhiv, is identified by its prefix (ogbg-) and its name (molhiv). The prefix specifies the category of the graph ML task, i.e., node (ogbn-), link (ogbl-), or graph (ogbg-) property prediction. A realistic split scheme is provided for each dataset, whose detail can be found in Sections 4, 5, and 6.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OGB Node Property Prediction</head><p>We currently provide 5 datasets, adopted from 3 different application domains, for predicting the properties of individual nodes. Specifically, ogbn-products is an Amazon products co-purchasing network <ref type="bibr" target="#b11">(Bhatia et al., 2016)</ref> originally developed by <ref type="bibr" target="#b16">Chiang et al. (2019)</ref> (cf. Section 4.1). The ogbn-arxiv, ogbn-mag, and ogbn-papers100M datasets are extracted from the Microsoft Academic Graph (MAG) <ref type="bibr" target="#b84">(Wang et al., 2020)</ref>, with different scales, tasks, and include both homogeneous and heterogeneous graphs. Specifically, ogbn-arxiv is a paper citation network of arXiv papers (cf. Section 4.3), ogbn-mag is a heterogeneous academic graph containing differ-ent node types (papers, authors, institutions, and topics) and their relations (cf. Section 4.5), and ogbn-papers100M is an extremely large paper citation network from the entire MAG with more than 100 million nodes and 1 billion edges (cf. Section 4.4). The ogbn-proteins dataset is a protein-protein association network <ref type="bibr" target="#b77">(Szklarczyk et al., 2019</ref>) (cf. Section 4.2).</p><p>The 5 datasets exhibit highly diverse graph statistics, as shown in <ref type="table" target="#tab_3">Table 3</ref>. Notably, the biological network, ogbn-proteins, is much denser than the social/information networks as can be observed from its large average node degree and small graph diameter. On the other hand, the co-purchasing network, ogbn-products, has more clustered graph structure than the other datasets, as can be seen from its large average clustering coefficient. Finally, the heterogeneous academic graph, ogbn-mag, exhibits rather interesting graph characteristics, simultaneously having small average node degree, clustering coefficient, and graph diameter.</p><p>Baselines. We consider the following representative models as our baselines unless otherwise specified.</p><p>? MLP: A multilayer perceptron (MLP) predictor that uses the given raw node features directly as input. Graph structure information is not utilized. ? NODE2VEC: An MLP predictor that uses as input the concatenation of the raw node features and NODE2VEC embeddings <ref type="bibr" target="#b33">(Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b63">Perozzi et al., 2014</ref>  <ref type="bibr" target="#b16">(Chiang et al., 2019)</ref> that partitions the graphs into a fixed number of subgraphs and draws mini-batches from them. ? GRAPHSAINT (optional): A mini-batch training technique of GNNs <ref type="bibr">(Zeng et al., 2020)</ref> that samples subgraphs via a random walk sampler.</p><p>The three mini-batch GNN training, NEIGHBORSAMPLING, CLUSTERGCN, and GRAPHSAINT, are explored only for graph datasets where full-batch GCN/GRAPHSAGE did not fit into the common GPU memory size of 11GB. The mini-batch GNNs are more GPU memory-efficient than the fullbatch GNNs because they first partition and sample the graph into subgraphs. Hence, in order to train the network, they require only a small amount of nodes to be loaded into the GPU memory at each mini-batch. Inference is then performed on the whole graph without GPU usage. To choose the architecture for the mini-batch GNNs, we first run full-batch GCN and GRAPHSAGE on an NVIDIA Quadro RTX 8000 with 48GB of memory, and then adopt the best performing full-batch GNN architecture for the mini-batch GNNs. All models are trained with a fixed hidden dimensionality of 256, a fixed number of two or three layers, and a tuned dropout ratio ? {0.0, 0.5}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ogbn-products: Amazon Products Co-purchasing Network</head><p>The ogbn-products dataset is an undirected and unweighted graph, representing an Amazon product co-purchasing network <ref type="bibr" target="#b11">(Bhatia et al., 2016)</ref>. Nodes represent products sold in Amazon, and edges between two products indicate that the products are purchased together. The graphs, target labels, and node features are generated following <ref type="bibr" target="#b16">Chiang et al. (2019)</ref>, where node features are dimensionality-reduced bag-of-words of the product descriptions. Our contribution, when adopting the dataset in OGB, is to resolve its critical data split issue by presenting a more realistic and challenging split (see below). Prediction task. The task is to predict the category of a product in a multi-class classification setup, where the 47 top-level categories are used for target labels.</p><p>Dataset splitting. We consider a more challenging and realistic dataset splitting that differs from the one used in <ref type="bibr" target="#b16">Chiang et al. (2019)</ref>. Instead of randomly assigning 90% of the nodes for training and 10% of the nodes for testing (without a validation set), we use the sales ranking (popularity) to split nodes into training/validation/test sets. Specifically, we sort the products according to their sales ranking and use the top 8% for training, next top 2% for validation, and the rest for testing. This is a more challenging splitting procedure that closely matches the real-world application where manual labeling is prioritized to important nodes in the network and ML models are subsequently used to make predictions on less important ones.  Discussion. Our benchmarking results in <ref type="table" target="#tab_5">Table 4</ref> show that the highest test performances are attained by GNNs, while the MLP baseline that solely relies on a product's description is not sufficient for accurately predicting the category of a product. Even with the GNNs, we observe the huge generalization gap 7 , which can be explained by differing node distributions across the splits, as visualized in <ref type="figure" target="#fig_1">Figure 3</ref>. This is in stark contrast with the conventional random split used by <ref type="bibr" target="#b16">Chiang et al. (2019)</ref>. Even with the same split ratio (8/2/90), we find GRAPHSAGE already achieves 88.20?0.08% test accuracy with only ? 1 percentage points of generalization gap. These results indicate that the realistic split is much more challenging than the random split and offer an important opportunity to improve out-of-distribution generalization. <ref type="table" target="#tab_5">Table 4</ref> also shows that the recent mini-batch-based GNNs 8 give promising results, even slightly outperforming the full-batch version of GRAPHSAGE that does not fit into ordinary GPU memory. The improved performance can be attributed to the regularization effects of mini-batch noise and edge dropout <ref type="bibr" target="#b68">(Rong et al., 2020b)</ref>. Nevertheless, the mini-batch GNNs have been much less explored compared to the full-batch GNNs due to the prevalent use of the extremely small benchmark datasets such as CORA and CITESEER. As a result, many important questions remain open, e.g., what mini-batch training methods can induce the best regularization effect, and how to allow mini-batch training for advanced GNNs that rely on large receptive-field sizes <ref type="bibr" target="#b90">(Xu et al., 2018;</ref><ref type="bibr" target="#b47">Klicpera et al., 2019;</ref>, since the current mini-batch methods are rather limited by the number of nodes from which they aggregate information. Overall, ogbn-products is an ideal benchmark dataset for the field to move beyond the extremely small graph datasets and to catalyze the development of scalable mini-batch-based graph models with improved out-of-distribution prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ogbn-proteins: Protein-Protein Association Network</head><p>The ogbn-proteins dataset is an undirected, weighted, and typed (according to species) graph. Nodes represent proteins, and edges indicate different types of biologically meaningful associations between proteins, e.g., physical interactions, co-expression or homology <ref type="bibr" target="#b77">(Szklarczyk et al., 2019;</ref><ref type="bibr"></ref>  Consortium, 2018). All edges come with 8-dimensional features, where each dimension represents the approximate confidence of a single association type and takes on values between 0 and 1 (the larger the value is, the more confident we are about the association). The proteins come from 8 species.</p><p>Prediction task. The task is to predict the presence of protein functions in a multi-label binary classification setup, where there are 112 kinds of labels to predict in total. The performance is measured by the average of ROC-AUC scores across the 112 tasks.</p><p>Dataset splitting. We split the protein nodes into training/validation/test sets according to the species which the proteins come from. This enables the evaluation of the generalization performance of the model across different species.</p><p>Discussion. The ogbn-proteins dataset does not have input node features 9 , but has edge features on more than 30 million edges. In our baseline experiments, we opt for simplicity and use the average edge features of incoming edges as node features.</p><p>Benchmarking results are shown in <ref type="table" target="#tab_6">Table 5</ref>. Surprisingly, simple MLPs 10 perform better than more sophisticated approaches like NODE2VEC and GCN. Only GRAPHSAGE is able to outperform the na?ve MLP approach, which indicates that central node information (that is not explicitly modeled in GCN in its message-passing) already contains a lot of crucial information for making correct predictions.</p><p>We further evaluate the best performing GRAPHSAGE on conventional random split with the same split ratio as the species split. On the random split, we find the generalization gap is extremely low, with 87.83?0.03% test ROC-AUC that is only 0.27 percentage points lower than the training ROC-AUC (88.10?0.01%). This is in contrast to 10.18 percentage points of generalization gap (training AUC minus test AUC) in the species split, as calculated from the GRAPHSAGE experiment in <ref type="table" target="#tab_6">Table  5</ref>. The result suggests the unique challenge of across-species generalization that needs to be tackled in future research.</p><p>Since the number of nodes in ogbn-proteins is fairly small and easily fit onto common GPUs, we did not run the CLUSTERGCN and GRAPHSAINT experiments. Nonetheless, this dataset presents an interesting research question of how to utilize edge features in a more sophisticated way than just na?ve averaging, e.g., by the usage of attention or by treating the graph as a multi-relational graph (as there are 8 different association types between proteins). The challenge is to scalably handle the huge number of edge features efficiently on GPU, which might require clever graph partitioning based on the edge weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ogbn-arxiv: Paper Citation Network</head><p>The ogbn-arxiv dataset is a directed graph, representing the citation network between all Computer Science (CS) ARXIV papers indexed by MAG <ref type="bibr" target="#b84">(Wang et al., 2020)</ref>. Each node is an ARXIV paper and each directed edge indicates that one paper cites another one. Each paper comes with a 128-dimensional feature vector obtained by averaging the embeddings of words in its title and abstract. The embeddings of individual words are computed by running the WORD2VEC model (Mikolov In addition, all papers are also associated with the year that the corresponding paper was published. Prediction task. The task is to predict the 40 subject areas of ARXIV CS papers, 11 e.g., cs.AI, cs.</p><p>LG, and cs.OS, which are manually determined (i.e., labeled) by the paper's authors and ARXIV moderators. With the volume of scientific publications doubling every 12 years over the past century <ref type="bibr" target="#b25">(Dong et al., 2017b)</ref>, it is practically important to automatically classify each publication's areas and topics. Formally, the task is to predict the primary categories of the ARXIV papers, which is formulated as a 40-class classification problem.</p><p>Dataset splitting. The previously-used Cora, CiteSeer, and PubMed citation networks are split randomly <ref type="bibr" target="#b95">(Yang et al., 2016)</ref>. In contrast, we consider a realistic data split based on the publication dates of the papers. The general setting is that the ML models are trained on existing papers and then used to predict the subject areas of newly-published papers, which supports the direct application of them into real-world scenarios, such as helping the ARXIV moderators. Specifically, we propose to train on papers published until 2017, validate on those published in 2018, and test on those published since 2019.</p><p>Discussion. Our initial benchmarking results are shown in <ref type="table" target="#tab_7">Table 6</ref>, where the directed graph is converted to an undirected one for simplicity. First, we observe that the na?ve MLP baseline that does not utilize any graph information is significantly outperformed by the other three models that utilize graph information. This suggests that graph information can dramatically improve the performance of predicting a paper's category. Comparing models that do utilize graph information, we find GNN models, i.e., GCN and GRAPHSAGE, slightly outperform the NODE2VEC model. We also conduct additional experiments on conventional random split with the same split ratio. On the random split, we find that GCN achieves 73.54?0.13% test accuracy, suggesting that the realistic time split is indeed more challenging than the random split, providing an opportunity to improve the out-of-distribution generalization performance. Furthermore, we think it will be fruitful to explore how the edge direction information as well as the node temporal information (e.g., year in which papers are published) can be taken into account to improve prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ogbn-papers100M: Paper Citation Network</head><p>The ogbn-papers100M dataset is a directed citation graph of 111 million papers indexed by MAG <ref type="bibr" target="#b84">(Wang et al., 2020)</ref>. Its graph structure and node features are constructed in the same way as ogbn-arxiv in Section 4.3. Among its node set, approximately 1.5 million of them are ARXIV papers, each of which is manually labeled with one of ARXIV's subject areas (cf.Section 4.3). Overall, this dataset is orders-of-magnitude larger than any existing node classification datasets.</p><p>Prediction task. Given the full ogbn-papers100M graph, the task is to predict the subject areas of the subset of papers that are published in ARXIV. The majority of nodes (corresponding to non-ARXIV papers) are not associated with label information, and only their node features and reference information are given. The task is to leverage the entire citation network to infer the labels of the ARXIV papers. <ref type="bibr">12</ref> In total, there are 172 ARXIV subject areas, making the prediction task a 172-class classification problem.</p><p>Dataset splitting. The splitting strategy is the same as that used in ogbn-arxiv, i.e., the timebased split. Specifically, the training nodes (with labels) are all ARXIV papers published until 2017, while the validation nodes are the ARXIV papers published in 2018, and the models are tested on ARXIV papers published since 2019. Discussion. Our initial benchmarking results are shown in <ref type="table" target="#tab_8">Table 7</ref>, where the directed graph is converted to an undirected one for simplicity. As most existing models have difficulty handling such a gigantic graph, we benchmark the two simplest models, 13 MLP and SGC , which is a simplified variant of textscGCN (Kipf &amp; Welling, 2017) that essentially pre-processes node features using graph adjacency information. We obtain SGC node embeddings on the CPU (requiring more than 100GB of memory), after which we train the final MLP with mini-batches on an ordinary GPU.</p><p>We see from <ref type="table" target="#tab_8">Table 7</ref> that the graph-based model, SGC, despite its simplicity, performs much better than the na?ve MLP baseline. Nevertheless, we observe severe underfitting of SGC, indicating that using more expressive GNNs is likely to improve both training and test accuracy. It is therefore fruitful to explore how to scale expressive and advanced GNNs to the gigantic Web-scale graph, going beyond the simple pre-processing of node features. Overall, ogbn-papers100M is by far the largest benchmark dataset for node classification over a homogeneous graph, and is meant to significantly push the scalability of graph models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">ogbn-mag: Heterogeneous Microsoft Academic Graph (MAG)</head><p>The ogbn-mag dataset is a heterogeneous network composed of a subset of the Microsoft Academic Graph (MAG) <ref type="bibr" target="#b84">(Wang et al., 2020)</ref>. It contains four types of entities-papers (736,389 nodes), authors (1,134,649 nodes), institutions (8,740 nodes), and fields of study (59,965 nodes)-as well as four types of directed relations connecting two types of entities-an author "is affiliated with" an institution, 14 an author "writes" a paper, a paper "cites" a paper, and a paper "has a topic of" a field of study. Similar to ogbn-arxiv described in Section 4.3, each paper is associated with a 128-dimensional WORD2VEC feature vector, and all the other types of entities are not associated with input node features.</p><p>Prediction task. Given the heterogeneous ogbn-mag data, the task is to predict the venue (conference or journal) of each paper, given its content, references, authors, and authors' affiliations. This is of practical interest as some manuscripts' venue information is unknown or missing in MAG, due to the noisy nature of Web data. In total, there are 349 different venues in ogbn-mag, making the task a 349-class classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset splitting.</head><p>We follow the same time-based strategy as ogbn-arxiv and ogbn-papers100M to split the paper nodes in the heterogeneous graph, i.e., training models to predict venue labels of all papers published before 2018, validating and testing the models on papers published in 2018 and since 2019, respectively.</p><p>Discussion. As ogbn-mag is a heterogeneous graph, we consider slightly different sets of GNN and node embedding baselines. Specifically, for GCN and GRAPHSAGE, as they are originally designed for homogeneous graphs, we apply the models over the homogeneous subgraph, retaining only paper nodes and their citation relations. We also consider the RELATIONAL-GCN (R-GCN) <ref type="bibr" target="#b69">(Schlichtkrull et al., 2018)</ref> that is specifically designed for heterogeneous graphs and uses specialized message passing parameters for different edge types. Since only "paper" nodes come with node features, we use trainable embeddings for the remaining nodes. For the node embedding model, instead of NODE2VEC, we adopt METAPATH2VEC <ref type="bibr" target="#b24">(Dong et al., 2017a)</ref>, as it is specifically designed for heterogeneous graphs. For each relation, e.g., an author "writes" a paper, the reverse relation, e.g., a paper "is written by" an author, is added to allow bidirectional message passing in GNNs. 13 NODE2VEC is omitted as it is computationally costly on such a gigantic graph. 14 For each author, we include all the institutions that the author has ever belonged to. Our benchmarking results are shown in <ref type="table" target="#tab_9">Table 8</ref>. First, we see that MLP, GCN, and GRAPHSAGE perform worse than the models that actually utilize heterogeneous graph information, i.e., METAP-ATH2VEC, R-GCN, and the mini-batch GNNs. <ref type="bibr">15</ref> This highlights that exploiting the heterogeneous nature of the graph is essential to achieving good performance on this dataset.</p><p>Second, we see that the mini-batch GNNs, especially NEIGHBORSAMPLING and GRAPHSAINT, give surprisingly promising results, outperforming the full-batch R-GCN by a large margin. This is likely due to the regularization effect of the noise induced by mini-batch sampling and edge dropout <ref type="bibr" target="#b68">(Rong et al., 2020b)</ref>. In contrast, CLUSTERGCN gives worse performance than its full-batch variant, indicating that the bias introduced by the pre-computed partitioning has a negative effect on the model's performance (as can be also seen by its highly overfitting training performance).</p><p>Nevertheless, heterogeneous graph models as well as their mini-batch training methods have been much less explored compared to their homogeneous counterparts, due to the smaller number of established benchmarks. Overall, ogbn-mag is meant to catalyze the development of scalable and accurate heterogeneous graph models, going beyond homogeneous graphs. A fruitful research direction is to adopt advanced techniques developed for homogeneous graphs to improve the performance on heterogeneous graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OGB Link Property Prediction</head><p>We currently provide 6 datasets, adopted from diverse application domains for predicting the properties of links (pairs of nodes). Specifically, ogbl-ppa is a protein-protein association network <ref type="bibr" target="#b77">(Szklarczyk et al., 2019</ref>) (cf. Section 5.1), ogbl-collab is an author collaboration network <ref type="bibr" target="#b84">(Wang et al., 2020</ref>) (cf. Section 5.2), ogbl-ddi is a drug-drug interaction network (Wishart et al., 2018) (cf. Section 5.3), ogbl-citation2 is a paper citation network <ref type="bibr" target="#b84">(Wang et al., 2020</ref>) (cf. Section 5.4), ogbl-biokg is a heterogeneous knowledge graph compiled from a large number of biomedical repositories (cf. Section 5.6), and ogbl-wikikg2 is a Wikidata knowledge graph <ref type="bibr" target="#b82">(Vrande?i? &amp; Kr?tzsch, 2014</ref>) (cf. Section 5.5).</p><p>The different datasets are highly diverse in their graph structure, as shown in <ref type="table" target="#tab_3">Table 3</ref>. For example, the biological networks (ogbl-ppa and ogbl-ddi) are much denser than the academic networks (ogbl-collab and ogbl-citation2) and the knowledge graphs (ogbl-wikikg2 and ogbl-biokg), as can be seen from the large average node degree, small number of nodes, and the small graph diameter. On the other hand, the collaboration network, ogbl-collab, has more clustered graph structure than the other datasets, as can be seen from its high average clustering coefficient. Comparing the two knowledge graph datasets, ogbl-wikikg2 and ogbl-biokg, we see that the former is much more sparse and less clustered than the latter.</p><p>Baselines. We implement different sets of baselines for link prediction datasets that only have a single edge type, and KG completion datasets that have multiple edge/relation types.</p><p>Baselines for link prediction datasets. We consider the following representative models as our baselines for the link prediction datasets unless otherwise specified. For all models, edge features are obtained by using the Hadamard operator between pair-wise node embeddings, and are then inputted to an MLP for the final prediction. During training, we randomly sample edges and use them as negative examples. We use the same number of negative edges as there are positive edges. Below, we describe how each model obtains node embeddings:</p><p>? MLP: Input node features are directly used as node embeddings.</p><p>? NODE2VEC: The node embeddings are obtained by concatenating input features and NODE2VEC embeddings <ref type="bibr" target="#b33">(Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b63">Perozzi et al., 2014</ref>).  <ref type="bibr" target="#b16">(Chiang et al., 2019)</ref> that partitions the graphs into a fixed number of subgraphs and draws mini-batches from them. ? GRAPHSAINT (optional): A mini-batch training technique of GNNs <ref type="bibr">(Zeng et al., 2020)</ref> that samples subgraphs via a random walk sampler.</p><p>Similar to the node property prediction baselines, the mini-batch GNN training, NEIGHBORSAMPLING, CLUSTERGCN, and GRAPHSAINT, are experimented only for graph datasets where full-batch GCN and GRAPHSAGE did not fit into the common GPU memory size of 11GB. To choose the GNN architecture for the mini-batch GNNs, we first run full-batch GCN and GRAPHSAGE on a NVIDIA Quadro RTX 8000 with 48GB of memory, and then adopt the best performing full-batch GNN architecture for the mini-batch GNNs. All models are trained with a fixed hidden dimensionality of 256, a fixed number of three layers, and a tuned dropout ratio ? {0.0, 0.5}.</p><p>Baselines for KG completion datasets. We consider the following representative KG embedding models as our baselines for the KG datasets unless otherwise specified.</p><p>? TRANSE: Translation-based KG embedding model by <ref type="bibr" target="#b13">Bordes et al. (2013)</ref>.</p><p>? DISTMULT: Multiplication-based KG embedding model by <ref type="bibr" target="#b93">Yang et al. (2015)</ref>.</p><p>? COMPLEX: Complex-valued multiplication-based KG embedding model by <ref type="bibr" target="#b78">Trouillon et al. (2016)</ref>. ? ROTATE: Rotation-based KG embedding model by <ref type="bibr" target="#b75">Sun et al. (2019)</ref>.</p><p>For KGs with many entities and relations, the embedding dimensionality can be limited by the available GPU memory, as the embeddings need to be loaded into GPU all at once. We therefore choose the dimensionality such that training can be performed on a fixed-budget of GPU memory. Our training procedure follows <ref type="bibr" target="#b75">Sun et al. (2019)</ref>, where we perform negative sampling and use margin-based logistic loss for the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">ogbl-ppa: Protein-Protein Association Network</head><p>The ogbl-ppa dataset is an undirected, unweighted graph. Nodes represent proteins from 58 different species, and edges indicate biologically meaningful associations between proteins, e.g., physical interactions, co-expression, homology or genomic neighborhood <ref type="bibr" target="#b77">(Szklarczyk et al., 2019)</ref>. We provide a graph object constructed from training edges (i.e., no validation and test edges are contained). Each node contains a 58-dimensional one-hot feature vector that indicates the species that the corresponding protein comes from. Prediction task. The task is to predict new association edges given the training edges. The evaluation is based on how well a model ranks positive test edges over negative test edges. Specifically, we rank each positive edge in the validation/test set against 3,000,000 randomly-sampled negative edges, and count the ratio of positive edges that are ranked at the K-th place or above (Hits@K). We found K = 100 to be a good threshold to rate a model's performance in our initial experiments. Overall, this metric is much more challenging than ROC-AUC because the model needs to consistently rank the positive edges higher than nearly all the negative edges.</p><p>Dataset splitting. We provide a biological throughput split of the edges into training/validation/test edges. Training edges are protein associations that are measured experimentally by a high-throughput technology (e.g., cost-effective, automated experiments that make large scale repetition feasible <ref type="bibr" target="#b57">(Macarron et al., 2011;</ref><ref type="bibr" target="#b7">Bajorath, 2002;</ref><ref type="bibr" target="#b99">Younger et al., 2017)</ref>) or are obtained computationally (e.g., via text-mining). In contrast, validation and test edges contain protein associations that can only be measured by low-throughput, resource-intensive experiments performed in laboratories. In particular, the goal is to predict a particular type of protein association, e.g., physical protein-protein interaction, from other types of protein associations (e.g., co-expression, homology, or genomic neighborhood) that can be more easily measured and are known to correlate with associations that we are interested in.</p><p>Discussion. Our initial benchmarking results are shown in <ref type="table" target="#tab_10">Table 9</ref>. First, the MLP baseline 16 performs extremely poorly, which is to be expected since the node features are not rich in this dataset. Surprisingly, both GNN baselines (GCN, GRAPHSAGE) and NODE2VEC fail to overfit on the training data and show similar performances across training/validation/test splits. The poor training performance of GNNs suggests that positional information, which cannot be captured by GNNs alone <ref type="bibr" target="#b98">(You et al., 2019)</ref>, might be crucial to fit training edges and obtain meaningful node embeddings. On the other hand, we see that MATRIXFACTORIZATION, which learns a distinct embedding for each node (thus, it can express any positional information of nodes), is indeed able to overfit on the training data, while also reaching better validation and test performance. However, the poor generalization performance still encourages the development of new research ideas to close this gap, e.g., by injecting positional information into GNNs or by developing more sophisticated negative sampling techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ogbl-collab: Author Collaboration Network</head><p>The ogbl-collab dataset is an undirected graph, representing a subset of the collaboration network between authors indexed by MAG <ref type="bibr" target="#b84">(Wang et al., 2020)</ref>. Each node represents an author and edges indicate the collaboration between authors. All nodes come with 128-dimensional features, obtained by averaging the word embeddings of papers that are published by the authors. All edges are associated with two types of meta-information: the year and the edge weight, representing the number of co-authored papers published in that year. The graph can be viewed as a dynamic multi-graph since there can be multiple edges between two nodes if they collaborate in more than one year. Prediction task. The task is to predict the author collaboration relationships in a particular year given the past collaborations. As the task is a time-series problem, it is natural for models to incorporate the most recent edge information to make prediction, e.g., use validation edges when predicting test edges. The evaluation metric is similar to ogbl-ppa in Appendix 5.1, where we would like the model to rank true collaborations higher than false collaborations. Specifically, we rank each true collaboration among a set of 100,000 randomly-sampled negative collaborations, and count the ratio of positive edges that are ranked at K-place or above (Hits@K). We found K = 50 to be a good threshold in our preliminary experiments.</p><p>Dataset splitting. We split the data according to time, in order to simulate a realistic application in collaboration recommendation. Specifically, we use the collaborations until 2017 as training edges, those in 2018 as validation edges, and those in 2019 as test edges. Discussion. Our initial benchmarking results are shown in <ref type="table" target="#tab_1">Table 10</ref>. First, we consider the conventional setting where validation edges are used only for model selection. From the upper half of <ref type="table" target="#tab_1">Table 10</ref>, we see that NODE2VEC achieves the best results, followed by the two GNN models and MATRIXFACTORIZATION. This can be explained by the fact that positional information, i.e., past collaborations, is a much more indicative feature for predicting future collaboration than solely relying on the average paper representations of authors, i.e., the same research interest. Notably, MATRIXFACTORIZATION achieves nearly perfect training results, but cannot transfer the good results to the validation and test splits, even when heavy regularization is applied. Overall, it is fruitful to explore injecting positional information into GNNs, and develop better regularization methods. This dataset further provides a unique research opportunity for dynamic multi-graphs. To demonstrate the potential benefit of time-series modeling, we use the same GCN and GRAPHSAGE models as before but at test time, we additionally incorporate the most recent edges (i.e., validation edges) as input to the models. From the lower half of <ref type="table" target="#tab_1">Table 10</ref>, we see that the test performances of both GNN models increase significantly by using validation edges at the inference time. One promising direction to further increase the performance is to treat edges at different timestamps differently, as recent collaborations may be more indicative about the future collaborations than the past ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ogbl-ddi: Drug-Drug Interaction Network</head><p>The ogbl-ddi dataset is a homogeneous, unweighted, undirected graph, representing the drug-drug interaction network <ref type="bibr" target="#b87">(Wishart et al., 2018)</ref>. Each node represents an FDA-approved or experimental drug. Edges represent interactions between drugs and can be interpreted as a phenomenon where the joint effect of taking the two drugs together is considerably different from the expected effect in which drugs act independently of each other. Prediction task. The task is to predict drug-drug interactions given information on already known drug-drug interactions. The evaluation metric is similar to ogbl-collab discussed in Section 5.2, where we would like the model to rank true drug interactions higher than non-interacting drug pairs. Specifically, we rank each true drug interaction among a set of approximately 100,000 randomlysampled negative drug interactions, and count the ratio of positive edges that are ranked at K-place or above (Hits@K). We found K = 20 to be a good threshold in our preliminary experiments.</p><p>Dataset splitting. We develop a protein-target split, meaning that we split drug edges according to what proteins those drugs target in the body. As a result, the test set consists of drugs that predominantly bind to different proteins from drugs in the train and validation sets. This means that drugs in the test set work differently in the body, and have a rather different biological mechanism of action than drugs in the train and validation sets. The protein-target split thus enables us to evaluate to what extent the models can generate practically useful predictions <ref type="bibr" target="#b34">(Guney, 2017)</ref>, i.e., non-trivial predictions that are not hindered by the assumption that there exist already known and very similar medications available for training.</p><p>Discussion. Our initial benchmarking results are shown in <ref type="table" target="#tab_1">Table 11</ref>. Since ogbl-ddi does not contain any node features, we omit the graph-agnostic MLP baseline for this experiment. Furthermore, for GCN and GRAPHSAGE, node features are also represented as distinct embeddings and learned in an end-to-end manner together with the GNN parameters. Interestingly, both the GNN models and the MATRIXFACTORIZATION approach achieve significantly higher training results than NODE2VEC. However, only the GNN models are able to transfer this performance to the test set to some extent, suggesting that relational information is crucial to allow the model to generalize to unseen interactions. Notably, most of the models show high performance variance, which can be partly attributed to the dense nature of the graph and the challenging data split. We further perform the conventional random split of edges, where we find GRAPHSAGE is able to achieve 80.88?2.42% test Hits@20. This indicates that the protein-target split is indeed more challenging than the conventional random split. Overall, ogbl-ddi presents a unique challenge of predicting out-of-distribution links in dense graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">ogbl-citation2: Paper Citation Network</head><p>The ogbl-citation2 dataset 17 is a directed graph, representing the citation network between a subset of papers extracted from MAG <ref type="bibr" target="#b84">(Wang et al., 2020)</ref>. Similar to ogbn-arxiv in Section 4.3, each node is a paper with 128-dimensional WORD2VEC features that summarizes its title and abstract, and each directed edge indicates that one paper cites another. All nodes also come with meta-information indicating the year the corresponding paper was published. Prediction task. The task is to predict missing citations given existing citations. Specifically, for each source paper, two of its references are randomly dropped, and we would like the model to rank the missing two references higher than 1,000 negative reference candidates. The negative references are randomly-sampled from all the previous papers that are not referenced by the source paper. The evaluation metric is Mean Reciprocal Rank (MRR), where the reciprocal rank of the true reference among the negative candidates is calculated for each source paper, and then the average is taken over all source papers.</p><p>Dataset splitting. We split the edges according to time, in order to simulate a realistic application in citation recommendation (e.g., a user is writing a new paper and has already cited several existing papers, but wants to be recommended additional references). To this end, we use the most recent papers (those published in 2019) as the source papers for which we want to recommend the references. For each source paper, we drop two papers from its references-the resulting two dropped edges (pointing from the source paper to the dropped papers) are used respectively for validation and testing. All the rest of the edges are used for training.</p><p>Discussion. Our initial benchmarking results are shown in <ref type="table" target="#tab_1">Table 12</ref>, where the directed graph is converted to an undirected one for simplicity. Here, the GNN models achieve the best results, followed by MATRIXFACTORIZATION and NODE2VEC. Among the GNNs, GCN performs better than GRAPHSAGE. However, these GNNs use full-batch training; thus, they are not scalable and require more than 40GB of GPU memory to train, which is intractable on most of the GPUs available today. Hence, we also experiment with the scalable mini-batch training techniques of GNNs, NEIGHBORSAMPLING, CLUSTERGCN, and GRAPHSAINT. Interestingly, we see from <ref type="table" target="#tab_1">Table 12</ref> that these techniques give worse performance than their full-batch counterpart, which is in contrast to the node classification datasets (e.g., ogbn-products and ogbn-mag), where the mini-batch-based models give stronger generalization performances. This limitation presents a unique challenge for applying the mini-batch techniques to link prediction, differently from those pertaining to node prediction. Overall, ogbl-citation2 provides a research opportunity to further improve GNN models and their scalable mini-batch training techniques in the context of link prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">ogbl-wikikg2: Wikidata Knowledge Graph</head><p>The ogbl-wikikg2 dataset 18 is a Knowledge Graph (KG) extracted from the Wikidata knowledge base <ref type="bibr" target="#b82">(Vrande?i? &amp; Kr?tzsch, 2014)</ref>. It contains a set of triplet edges (head, relation, tail), capturing the different types of relations between entities in the world, e.g., Canada citizen ????? Hinton. We retrieve all the relational statements in Wikidata and filter out rare entities. Our KG contains 2,500,604 entities and 535 relation types. Prediction task. The task is to predict new triplet edges given the training edges. The evaluation metric follows the standard filtered metric widely used in KGs <ref type="bibr" target="#b13">(Bordes et al., 2013;</ref><ref type="bibr" target="#b93">Yang et al., 2015;</ref><ref type="bibr" target="#b78">Trouillon et al., 2016;</ref><ref type="bibr" target="#b75">Sun et al., 2019)</ref>. Specifically, we corrupt each test triplet edges by replacing its head or tail with randomly-sampled 1,000 negative entities (500 for head and 500 for tail), while ensuring the resulting triplets do not appear in KG. The goal is to rank the true head (or tail) entities higher than the negative entities, which is measured by Mean Reciprocal Rank (MRR).</p><p>Dataset splitting. We split the triplets according to time, simulating a realistic KG completion scenario that aims to fill in missing triplets that are not present at a certain timestamp. Specifically, we downloaded Wikidata at three different time stamps 19 (May, August, and November of 2015), and constructed three KGs where we only retain entities and relation types that appear in the earliest May KG. We use the triplets in the May KG for training, and use the additional triplets in the August and November KGs for validation and test, respectively. Note that our dataset split is different from the existing Wikidata KG dataset that adopts a conventional random split <ref type="bibr" target="#b86">(Wang et al., 2019b)</ref>, which does not reflect the practical usage.</p><p>Discussion. Our benchmark results are provided in <ref type="table" target="#tab_1">Table 13</ref>, where the upper-half baselines are implemented on a single commodity GPU with 11GB memory, while the bottom-half baselines are implemented on a high-end GPU with 45GB memory. 20 Training MRR in <ref type="table" target="#tab_1">Table 13</ref> is an unfiltered metric, 21 as filtering is computationally expensive for the large number of training triplets.</p><p>First, we see from the upper-half of <ref type="table" target="#tab_1">Table 13</ref> that when the limited embedding dimensionality is used, COMPLEX performs the best among the four baselines. With the increased dimensionality, all four models are able to achieve higher MRR on training, validation and test sets, as seen from the bottom-half of <ref type="table" target="#tab_1">Table 13</ref>. This suggests the importance of using a sufficient large embedding <ref type="bibr">18</ref> The older version ogbl-wikikg has been deprecated due to a bug in negative samples of validation and test sets. <ref type="bibr">19</ref> Available at https://archive.org/search.php?query=creator%3A%22Wikidata+ editors%22 20 Given a fixed 11GB GPU memory budget, we adopt 100-dimension embeddings for DISTMULT and TRANSE. Since ROTATE and COMPLEX require the entity embeddings with the real and imaginary parts, we train these two models with the dimensionality of 50 for each part. On the other hand, on the high-end GPU with 45GB memory, we are able to train all the models with 5? larger embedding dimensionality. <ref type="bibr">21</ref> This means that the training MRR is computed by ranking against randomly-selected negative entities without filtering out triplets that appear in KG. The unfiltered metric has the systematic bias of being smaller than the filtered counterpart (computed by ranking against "true" negative entities, i.e., the resulting triplets do not appear in the KG) <ref type="bibr" target="#b13">(Bordes et al., 2013)</ref>. dimensionality to achieve good performance in this dataset. Interestingly, although TRANSE and ROTATE underperform with the limited dimensionality, they obtain the best performances with the increased dimensionality. Nevertheless, the extremely low test MRR 22 suggests that our realistic KG completion dataset is highly non-trivial. It presents a realistic generalization challenge of discovering new triplets based on existing ones, which necessitates the development of KG models with more robust and generalizable reasoning capability. Furthermore, this dataset presents an important challenge of effectively scaling embedding models to large KGs-na?vely training KG embedding models with reasonable dimensionality would require a high-end GPU, which is extremely costly and not scalable to even larger KGs. A promising approach to improve scalability is to distribute training across multiple commodity GPUs <ref type="bibr" target="#b102">(Zheng et al., 2020;</ref><ref type="bibr" target="#b103">Zhu et al., 2019;</ref>. A different approach is to share parameters across entities and relations, so that a smaller number of embedding parameters need to be put onto the GPU memory at once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">ogbl-biokg: Biomedical Knowledge Graph</head><p>The ogbl-biokg dataset is a Knowledge Graph (KG), which we created using data from a large number of biomedical data repositories. It contains 5 types of entities: diseases (10,687 nodes), proteins (17,499), drugs (10,533 nodes), side effects (9,969 nodes), and protein functions (45,085 nodes). There are 51 types of directed relations connecting two types of entities, including 39 kinds of drug-drug interactions, 8 kinds of protein-protein interaction, as well as drug-protein, drug-side effect, drug-protein, function-function relations. All relations are modeled as directed edges, among which the relations connecting the same entity types (e.g., protein-protein, drug-drug, function-function) are always symmetric, i.e., the edges are bi-directional.</p><p>This dataset is relevant to both biomedical and fundamental ML research. On the biomedical side, the dataset allows us to get better insights into human biology and generate predictions that can guide downstream biomedical research. On the fundamental ML side, the dataset presents challenges in handling a noisy, incomplete KG with possible contradictory observations. This is because the ogbl-biokg dataset involves heterogeneous interactions that span from the molecular scale (e.g., protein-protein interactions within a cell) to whole populations (e.g., reports of unwanted side effects experienced by patients in a particular country). Further, triplets in the KG come from sources with a variety of confidence levels, including experimental readouts, human-curated annotations, and automatically extracted metadata. Prediction task. The task is to predict new triplets given the training triplets. The evaluation protocol is exactly the same as ogbl-wikikg2 in Section 5.5, except that here we only consider ranking against entities of the same type. For instance, when corrupting head entities of the protein type, we only consider negative protein entities. Dataset splitting. For this dataset, we adopt a random split. While splitting the triplets according to time is an attractive alternative, we note that it is incredibly challenging to obtain accurate information as to when individual experiments and observations underlying the triplets were made. We strive to provide additional dataset splits in future versions of the OGB.</p><p>Discussion. Our benchmark results are provided in <ref type="table" target="#tab_1">Table 14</ref>, where we adopt 2000-dimensional embeddings for DISTMULT and TRANSE, and 1000-dimensional embeddings for the real and imaginary parts of ROTATE and COMPLEX. Negative sampling is performed only over entities of the same types. Similar to <ref type="table" target="#tab_1">Table 13</ref> in Section 5.5, training MRR in <ref type="table" target="#tab_1">Table 14</ref> is an unfiltered metric. 23</p><p>Among the four models, COMPLEX achieves the best test MRR, while TRANSE gives significantly worse performance compared to the other models. The worse performance of TRANSE can be explained by the fact that TRANSE cannot model symmetric relations <ref type="bibr" target="#b78">(Trouillon et al., 2016)</ref> that are prevalent in this dataset, e.g., protein-protein and drug-drug relations are all symmetric. Overall, it is of great practical interest to further improve the model performance. A promising direction is to develop a more specialized method for the heterogeneous knowledge graph, where multiple node types exist and the entire graph follows the pre-defined schema.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">OGB Graph Property Prediction</head><p>We currently provide 4 datasets, adopted from 3 distinct application domains, for predicting the properties of entire graphs or subgraphs. Specifically, ogbg-molhiv and ogbg-molpcba are molecular graphs originally curated by <ref type="bibr" target="#b89">Wu et al. (2018)</ref> (cf. Section 6.1), ogbg-ppa is a set of protein-protein association subgraphs <ref type="bibr" target="#b105">(Zitnik et al., 2019</ref>) (cf. Section 6.2), and ogbg-code2 is a collection of ASTs of source code <ref type="bibr" target="#b40">(Husain et al., 2019</ref>) (cf. Section 6.3).</p><p>The different datasets are highly diverse in their graph structure, as shown in <ref type="table" target="#tab_3">Table 3</ref>. For example, compared with the other graph datasets, the biological subgraphs, ogbg-ppa, have much larger number of nodes per graph, as well as much denser and clustered graph structure, as seen by the large average node degree, large average clustering coefficient, and large graph diameter.</p><p>This is contrast to the molecular graphs, ogbg-molhiv and ogbg-molpcba, as well as the ASTs, ogbg-code2, both of which are tree-like graphs-in fact, ASTs are exactly trees-with small average node degrees, small average clustering coefficient, and large average graph diameter. Despite the similarity, the molecular graphs and the ASTs are distinct in that the ASTs have much larger number of nodes with well-defined root nodes.</p><p>Baselines. We consider the following representative GNNs as our baselines unless otherwise specified. GNNs are used to obtain node embeddings, which are then pooled to give the embedding of the entire graph. Finally, a linear model is applied to the graph embedding to make predictions.</p><p>? GCN: Graph Convolutioanl Networks <ref type="bibr" target="#b45">(Kipf &amp; Welling, 2016)</ref>.</p><p>? GCN+VIRTUAL NODE: GCN that performs message passing over augmented graphs with virtual nodes, i.e., additional nodes that are connected to all nodes in the original graphs <ref type="bibr" target="#b32">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b52">Li et al., 2017;</ref><ref type="bibr" target="#b41">Ishiguro et al., 2019)</ref>. ? GIN: Graph Isomorphism Network . ? GIN+VIRTUAL NODE: GIN that performs message passing over augmented graphs with virtual nodes.</p><p>To include edge features, we follow <ref type="bibr" target="#b37">Hu et al. (2020a)</ref> and add transformed edge features into the incoming node features. For all the experiments, we use 5-layer GNNs, average graph pooling, a hidden dimensionality of 300, and a tuned dropout ratio ? {0.0, 0.5}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">ogbg-mol * : Molecular Graphs</head><p>The ogbg-molhiv and ogbg-molpcba datasets are two molecular property prediction datasets of different sizes: ogbg-molhiv (small) and ogbg-molpcba (medium). They are adopted from the MOLECULENET <ref type="bibr" target="#b89">(Wu et al., 2018)</ref>, and are among the largest of the MOLECULENET datasets. Besides the two main molecule datasets, we also provide the 10 other MOLECULENET datasets, which are summarized and benchmarked in Appendix A. These datasets can be used to stress-test molecule-specific methods  and transfer learning <ref type="bibr" target="#b37">(Hu et al., 2020a)</ref>. All the molecules are pre-processed using RDKIT <ref type="bibr" target="#b48">(Landrum et al., 2006)</ref>. Each graph represents a molecule, where nodes are atoms, and edges are chemical bonds. Input node features are 9-dimensional, containing atomic number and chirality, as well as other additional atom features such as formal charge and whether the atom is in the ring. Input edge features are 3-dimensional, containing bond type, bond stereochemistry as well as an additional bond feature indicating whether the bond is conjugated. Note that the above additional features are not needed to uniquely identify molecules, and are not adopted in the previous work <ref type="bibr" target="#b37">(Hu et al., 2020a;</ref><ref type="bibr" target="#b41">Ishiguro et al., 2019)</ref>. In the experiments, we perform an ablation study on the molecule features and find that including the additional features improves generalization performance. Prediction task. The task is to predict the target molecular properties as accurately as possible, where the molecular properties are cast as binary labels, e.g., whether a molecule inhibits HIV virus replication or not. For evaluation metric, we closely follow <ref type="bibr" target="#b89">Wu et al. (2018)</ref>. Specifically, for ogbg-molhiv, we use ROC-AUC for evaluation. For ogbg-molpcba, as the class balance is extremely skewed (only 1.4% of data is positive) and the dataset contains multiple classification tasks, we use the Average Precision (AP) averaged over the tasks as the evaluation metric. 24</p><p>Dataset splitting. We adopt the scaffold splitting procedure that splits the molecules based on their two-dimensional structural frameworks. The scaffold splitting attempts to separate structurally different molecules into different subsets, which provides a more realistic estimate of model performance in prospective experimental settings. The scaffold splitting was originally proposed by <ref type="bibr" target="#b89">Wu et al. (2018)</ref> and has been adopted by the follow-up works <ref type="bibr" target="#b37">Hu et al., 2020a;</ref><ref type="bibr" target="#b41">Ishiguro et al., 2019;</ref><ref type="bibr" target="#b67">Rong et al., 2020a)</ref>; however, the precise implementation differs significantly across works, making their results not directly comparable to each other. In OGB, we aim to standardize the scaffold split by adopting its most challenging version where test molecules are maximally diverse.</p><p>Discussion. Benchmarking results are given in <ref type="table" target="#tab_1">Tables 15 and 16</ref>. We see that GIN with the additional features and VIRTUAL NODES provides the best performance in the two datasets. In Appendix A, we show that even for the other MOLECULENET datasets, the additional features consistently improve generalization performance. In OGB, we therefore include the additional node/edge features in our molecular graphs.</p><p>We further report the performance on the random splitting, keeping the split ratio the same as the scaffold splitting. We find the random split to be much easier than scaffold split. On random splits of ogbg-molhiv and ogbg-molpcba, the best GIN achieves the ROC-AUC of 82.73?2.02% (5.66 percentage points higher than scaffold) and AP of 34.40?0.90% (7.37 percentage points higher than scaffold), respectively. The same trend holds true for the other MOLECULENET datasets, e.g., the best GIN performance on the random split of ogbg-moltox21 is 86.03?1.37%, which is 8.46 percentage points higher than that of the best GIN for the scaffold split (77.57?0.62% ROC-AUC).</p><p>These results highlight the challenges of the scaffold split compared to the random split, and opens up a fruitful research opportunity to increase the out-of-distribution generalization capability of GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">ogbg-ppa: Protein-Protein Association Network</head><p>The ogbg-ppa dataset is a set of undirected protein association neighborhoods extracted from the protein-protein association networks of 1,581 different species <ref type="bibr" target="#b77">(Szklarczyk et al., 2019</ref>) that cover 37 broad taxonomic groups (e.g., mammals, bacterial families, archaeans) and span the tree of life <ref type="bibr" target="#b39">(Hug et al., 2016)</ref>. To construct the neighborhoods, we randomly selected 100 proteins from each species and constructed 2-hop protein association neighborhoods centered on each of the selected proteins <ref type="bibr" target="#b105">(Zitnik et al., 2019)</ref>. We then removed the center node from each neighborhood and subsampled the neighborhood to ensure the final protein association graph is small enough (less than 300 nodes). Nodes in each protein association graph represent proteins, and edges indicate biologically meaningful associations between proteins. The edges are associated with 7-dimensional features, where each element takes a value between 0 and 1 and represents the approximate confidence of a particular type of protein protein association such as gene co-occurrence, gene fusion events, and co-expression.</p><p>Prediction task. Given a protein association neighborhood graph, the task is a 37-way multi-class classification to predict what taxonomic group the graph originates from. The ability to successfully tackle this problem has implications for understanding the evolution of protein complexes across species <ref type="bibr" target="#b21">(De Juan et al., 2013)</ref>, the rewiring of protein interactions over time <ref type="bibr" target="#b70">(Sharan et al., 2005;</ref><ref type="bibr" target="#b105">Zitnik et al., 2019)</ref>, the discovery of functional associations between genes even for otherwise rarely-studied organisms <ref type="bibr" target="#b18">(Cowen et al., 2017)</ref>, and would give us insights into key bioinformatics tasks, such as biological network alignment <ref type="bibr" target="#b58">(Malod-Dognin et al., 2017)</ref>.</p><p>Dataset splitting. Similar to ogbn-proteins in Section 4.2, we adopt the species split, where the neighborhood graphs in validation and test sets are extracted from protein association networks of species that are not seen during training but belong to one of the 37 taxonomic groups. This split stress-tests the model's capability to extract graph features that are essential to the prediction of the taxonomic groups, which is important for biological understanding of protein associations.</p><p>Discussion. Benchmarking results are given in <ref type="table" target="#tab_1">Table 17</ref>. Interestingly, similar to the ogbg-mol * datasets, GIN with VIRTUAL NODE provides the best performance. Nevertheless, the generalization gap is huge (almost 30 percentage points). For reference, we also experiment with the random splitting scenario, where we use the same model (GIN+VIRTUAL NODE) on the same split ratio. On the random split, the test accuracy is 92.91?0.27%, which is more than 20 percentage points  <ref type="figure">Figure 4</ref>: Example input graph in ogbg-code2, obtained by augmenting the original Python AST. In our AST, node "#1" is always the main function definition (FunctionDef or AsyncFunctionDef), and our goal is predict its tokenized attribute, e.g., {run, model} in the above example. To avoid data leakage, we replace the attribute of node "#1" with a special "_mask_" token. We also mask out attributes of recursive function definitions if there are any.</p><p>higher than the species split. This again encourages future research to improve the out-of-distribution generalization with more challenging and meaningful split procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">ogbg-code2: Abstract Syntax Tree of Source Code</head><p>The ogbg-code2 dataset 25 is a collection of Abstract Syntax Trees (ASTs) obtained from approximately 450 thousands Python method definitions. Methods are extracted from a total of 13,587 different repositories across the most popular projects on GITHUB (where "popularity" is defined as number of stars and forks). Our collection of Python methods originates from GITHUB CodeSearch-Net <ref type="bibr" target="#b40">(Husain et al., 2019)</ref> 26 , a collection of datasets and benchmarks for machine-learning-based code retrieval. The authors paid particular attention to avoid common shortcomings of previous source code datasets <ref type="bibr" target="#b1">(Allamanis, 2019)</ref>, such as duplication of code and labels, low number of projects, random splitting, etc. In ogbg-code2, we contribute an additional feature extraction step, which includes: AST edges, AST nodes (associated with features such as their types and attributes), tokenized method name (see <ref type="figure">Figure 4)</ref>. Altogether, ogbg-code2 allows us to capture source code with its underlying graph structure, beyond its token sequence representation. Prediction task. The task is to predict the sub-tokens forming the method name, given the Python method body represented by AST and its node features-i.e., node type (from a pool of 97 types), node attributes (such as variable names, with a vocabulary size of 10030, depth in the AST, pre-order traversal index (as illustrated in <ref type="figure">Figure 4</ref>). This task is often referred to in the literature as "code summarization" <ref type="bibr" target="#b2">(Allamanis et al., 2016;</ref><ref type="bibr" target="#b6">Alon et al., 2019</ref><ref type="bibr" target="#b5">Alon et al., , 2018</ref>, because the model is trained to find succinct and precise description (i.e., the method name chosen by the developer) for a complete logical unit (i.e., the method body). Code summarization is a representative task in the field of machine learning for code not only for its straightforward adoption in developer tools, but also because it is a proxy measure for assessing how well a model captures the code semantic <ref type="bibr" target="#b4">(Allamanis et al., 2018)</ref>. Following <ref type="bibr" target="#b6">Alon et al. (2019</ref><ref type="bibr" target="#b5">Alon et al. ( , 2018</ref>, we use an F1 score to evaluate predicted sub-tokens against ground-truth sub-tokens. <ref type="bibr">27</ref> The average length of a method name in the ground-truth is 2.6 sub-tokens, following a power-law distribution.</p><p>Dataset splitting. We adopt a project split <ref type="bibr" target="#b1">(Allamanis, 2019)</ref>, where the ASTs for the train set are obtained from GITHUB projects that do not appear in the validation and test sets. This split respects the practical scenario of training a model on a large collection of source code (obtained, for instance, from the popular GITHUB projects), and then using it to predict method names on a separate code base. The project split stress-tests the model's ability to capture code's semantics, and avoids a model that trivially memorizes the idiosyncrasies of training projects (such as the naming conventions and the coding style of a specific developer) to achieve a high test score.</p><p>Discussion. Benchmarking results are given in <ref type="table" target="#tab_1">Table 18</ref>, where we add "next-token edges" on top of the AST (as illustrated in <ref type="figure">Figure 4</ref>) to better capture the semantics of code graphs <ref type="bibr">(Dinella et al., 2020)</ref>. <ref type="bibr">28</ref> For the decoder, we use independent linear classifiers to predict sub-tokens at each position of the sub-token sequence. <ref type="bibr">29</ref> The evaluation is performed against the ground-truth sub-tokens. We see from <ref type="table" target="#tab_1">Table 18</ref> that GCN with VIRTUAL NODES provides the best performance. Nevertheless, we observe a huge generalization gap (around 15 percentage points). For reference, we also experiment with the random splitting scenario, where we apply the same model (GCN+VIRTUAL NODE) on the same split ratio. On the random split, the test F1 score is 21.64?0.26%, which is approximately 6 percentage points higher than that of the project split in <ref type="table" target="#tab_1">Table 18</ref>, indicating that the project split is indeed harder than the random split. Overall, this dataset presents an interesting research opportunity to improve out-of-distribution generalization under the meaningful project split, with a number of fruitful future directions: how to leverage the fact that the original graphs are actually trees with well-defined root nodes, how to pre-train GNNs to improve generalization <ref type="bibr" target="#b37">(Hu et al., 2020a)</ref>, and how to design a better encoder-decoder architecture with the graph data. To facilitate these directions, we provide enough meta-information, such as the original code snippet as well as an easy-to-use script to transform raw Python code snippets into the ASTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">OGB Package</head><p>The OGB package is designed to make the pipeline of <ref type="figure">Figure 2</ref> easily accessible to researchers, by automating the data loading and the evaluation parts. OGB is fully compatible with PYTORCH and its associated graph libraries: PYTORCH GEOMETRIC and DEEP GRAPH LIBRARY. OGB additionally provides library-agnostic dataset objects that can be used by any other Python deep learning frameworks such as TENSORFLOW <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and MXNET . Below, we explain the data loading (cf. Section 7.1) and evaluation (cf. Section 7.2). For simplicity, <ref type="bibr">27</ref> The previous works find that the F1 score over sub-tokens is suitable to assess the quality of a method name prediction, as the semantic of a method name depends solely on its sub-tokens. Note that the F1 score does not take the sub-token ordering into account; thus, "run_model" and "model_run" are considered as exact match. <ref type="bibr">28</ref> The inverse edges are also added to allow bidirectional message passing. The edge direction is recorded in the edge features. <ref type="bibr">29</ref> Although the F1 score is order-insensitive, in our preliminary experiments, we find that our order-sensitive decoder performs slightly better than order-insensitive decoder (predicting whether each vocabulary is included in the target sequence or not). During training, all the target sequences are truncated to the length of 5 (covering 99% of the target sequences), and vocabulary size of 5,000 is used for prediction (covering 90% of the sub-tokens in the target sequences). We additionally added one vocabulary "UNK" to handle any rare/unknown sub-tokens. Predicting "UNK" sub-token is counted as false positive when the F1 score is calculated. we focus on the task of the graph property prediction (cf. Section 6) using PYTORCH GEOMETRIC. For the other tasks, libraries, and more details, refer to https://ogb.stanford.edu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">OGB Data Loaders</head><p>The OGB package makes it easy to obtain a dataset object that is fully compatible with PYTORCH GEOMETRIC. As shown in Code Snippet 1, it can be done with only a single line of code, with the end-users only needing to specify the name of the dataset. The OGB package will then download, process, store, and return the requested dataset object. Furthermore, the standardized dataset splitting can be readily obtained from the dataset object. Code Snippet 1: OGB Data Loader</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">OGB Evaluators</head><p>OGB also enables standardized and reliable evaluation with the ogb. * .Evaluator class. As shown in Code Snippet 2, the end-users first specify the dataset they want to evaluate their models on, after which the users can learn the format of the input they need to pass to the Evaluator object. The input format is dataset-dependent. For example, for the ogbg-molpcba dataset, the Evaluator object requires as input a dictionary with y_true (a matrix storing the ground-truth binary labels 30 ), and y_pred (a matrix storing the scores output by the model). Once the end-users pass the specified dictionary as input, the Evaluator object returns the model performance that is appropriate for the dataset at hand, e.g., the Average Precision for ogbg-molpcba. &gt;&gt;&gt; from ogb.graphproppred import Evaluator # Get Evaluator for ogbg-molpcba &gt;&gt;&gt; evaluator = Evaluator(name = "ogbg-molpcba") # Learn about the specification of input to the Evaluator. &gt;&gt;&gt; print(evaluator.expected_input_format) # Prepare input that follows input spec. &gt;&gt;&gt; input_dict = {"y_true": y_true, "y_pred": y_pred} # Get the model performance. result_dict = evaluator.eval(input_dict) Code Snippet 2: OGB Evaluator</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>To enable scalable, robust, and reproducible graph ML research, we introduce the Open Graph Benchmark (OGB)-a diverse set of realistic graph datasets in terms of scales, domains, and task categories. We employ realistic data splits for the given datasets, driven by application-specific use cases. Through extensive benchmark experiments, we highlight that OGB datasets present significant challenges for ML models to handle large-scale graphs and make accurate prediction under the realistic data splitting scenarios. Altogether, OGB presents fruitful opportunities for future research to push the frontier of graph ML.</p><p>OGB is an open-source initiative that provides ready-to-use datasets as well as their data loaders, evaluation scripts, and public leaderboards. We hereby invite the community to develop and contribute state-of-the-art graph ML models at https://ogb.stanford.edu. Here we perform benchmark experiments on the other 10 datasets from MOLECULENET <ref type="bibr" target="#b89">(Wu et al., 2018)</ref>. The datasets are summarized in <ref type="table" target="#tab_1">Table 19</ref>. The detailed description of each dataset is provided in <ref type="bibr" target="#b89">Wu et al. (2018)</ref>. We use the same experimental protocol and hyper-parameters as in Section 6.1. The dropout rate is fixed to 0.5. As evaluation metrics, we adopt ROC-AUC for all the binary classification datasets except for ogbg-molmuv that exhibits significant class imbalance (only 0.2% of labels are positive). For the ogbg-molmuv dataset, we use Average Precision (AP), which is a more appropriate metric for heavily-imbalanced data <ref type="bibr" target="#b89">(Wu et al., 2018;</ref><ref type="bibr" target="#b20">Davis &amp; Goadrich, 2006)</ref>. For the regression datasets, we adopt Root Mean Squared Error (RMSE); the lower, the better.</p><p>The benchmark results for each dataset are provided in Tables 20-29. We observe the followings.</p><p>? The additional features almost always help improve generalization performance. In fact, on top of GIN+VIRTUAL NODE, including the additional features gives either comparable or improved performance on 9 out of the 10 datasets (except for ogbg-molbace in <ref type="table" target="#tab_3">Table  23</ref>). This motivates us to include these additional features in our OGB molecular graphs. ? Adding VIRTUAL NODES often improves generalization performance; for example, on top of GIN, adding VIRTUAL NODES gives either comparable or improved performance on 9 out of the 10 datasets (except for ogbg-clintox in <ref type="table" target="#tab_6">Table 25</ref>). ? The optimal GNN architectures (GCN or GIN) vary across the datasets. This raises a natural question: can we design a GNN architecture that performs well across the molecule datasets?</p><p>Altogether, we hope our extensive benchmark results on a variety of molecule datasets provide useful baselines for further research on molecule-specific graph ML models.          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>T-SNE visualization of training/validation/test nodes in ogbn-products.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>? GCN: The node embeddings are obtained by full-batch Graph Convolutional Networks (GCN) (Kipf &amp; Welling, 2017). ? GRAPHSAGE: The node embeddings are obtained by full-batch GraphSAGE (Hamilton et al., 2017a), where we adopt its mean pooling variant and a simple skip connection to preserve central node features. ? MATRIXFACTORIZATION: The distinct embeddings are assigned to different nodes and are learned in an end-to-end manner together with the MLP predictor. ? NEIGHBORSAMPLING (optional): A mini-batch training technique of GNNs (Hamilton et al., 2017a) that samples neighborhood nodes when performing aggregation. ? CLUSTERGCN (optional): A mini-batch training technique of GNNs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>&gt;&gt;&gt; from ogb.graphproppred import PygGraphPropPredDataset &gt;&gt;&gt; dataset = PygGraphPropPredDataset(name="ogbg-molpcba") # Pytorch Geometric dataset object &gt;&gt;&gt; split_idx = dataset.get_idx_split() # Dictionary containing train/valid/test indices. &gt;&gt;&gt; train_idx = split_idx["train"] # torch.tensor storing a list of training indices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Overview of currently-available OGB datasets (denoted in green). Nature domain includes biological networks and molecular graphs, Society domain includes academic graphs and e-commerce networks, and Information domain includes knowledge graphs. More datasets will be added in the future to increase the coverage.</figDesc><table><row><cell>Task</cell><cell></cell><cell>Node property prediction ogbn-</cell><cell></cell></row><row><cell>Domain</cell><cell>Nature</cell><cell>Society</cell><cell>Information</cell></row><row><cell>Small</cell><cell></cell><cell>arxiv</cell><cell></cell></row><row><cell>Medium</cell><cell>proteins</cell><cell>products</cell><cell>mag</cell></row><row><cell>Large</cell><cell></cell><cell>papers100M</cell><cell></cell></row><row><cell>Task</cell><cell></cell><cell>Link property prediction ogbl-</cell><cell></cell></row><row><cell>Domain</cell><cell>Nature</cell><cell>Society</cell><cell>Information</cell></row><row><cell>Small</cell><cell>ddi</cell><cell>collab</cell><cell>biokg</cell></row><row><cell>Medium</cell><cell>ppa</cell><cell>citation2</cell><cell>wikikg2</cell></row><row><cell>Large</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Task</cell><cell></cell><cell>Graph property prediction ogbg-</cell><cell></cell></row><row><cell>Domain</cell><cell>Nature</cell><cell>Society</cell><cell>Information</cell></row><row><cell>Small</cell><cell>molhiv</cell><cell></cell><cell></cell></row><row><cell>Medium</cell><cell>molpcba / ppa</cell><cell></cell><cell>code2</cell></row><row><cell>Large</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Statistics of currently-available OGB datasets.</figDesc><table><row><cell>The first 3 statistics are calculated over</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Full-batch GraphSAGE (Hamilton et al., 2017a), where we adopt the mean pooling variant and a simple skip connection to preserve central node features.</figDesc><table><row><cell>).</cell></row><row><cell>? GCN: Full-batch Graph Convolutional Network (Kipf &amp; Welling, 2017).</cell></row><row><cell>? GRAPHSAGE:</cell></row></table><note>? NEIGHBORSAMPLING (optional): A mini-batch training technique of GNNs (Hamilton et al., 2017a) that samples neighborhood nodes when performing aggregation. ? CLUSTERGCN (optional): A mini-batch training technique of GNNs</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results for ogbn-products. 03?0.93 75.54?0.14 61.06?0.08 NODE2VEC 93.39?0.10 90.32?0.06 72.49?0.10 GCN ? 93.56?0.09 92.00?0.03 75.64?0.21 GRAPHSAGE ? 94.09?0.05 92.24?0.07 78.50?0.14 NEIGHBORSAMPLING 92.96?0.07 91.70?0.09 78.70?0.36 CLUSTERGCN 93.75?0.13 92.12?0.09 78.97?0.33 GRAPHSAINT 92.71?0.14 91.62?0.08 79.08?0.24</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell cols="2">Accuracy (%) Validation</cell><cell>Test</cell></row><row><cell>MLP</cell><cell cols="2">84.Train Validation</cell><cell>Test</cell></row></table><note>? Requires a GPU with 33GB of memory.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results for ogbn-proteins. 78?0.48 77.06?0.14 72.04?0.48 NODE2VEC 79.76?1.88 70.07?0.53 68.81?0.65 GCN 82.77?0.16 79.21?0.18 72.51?0.35 GRAPHSAGE 87.86?0.13 83.34?0.13 77.68?0.20</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell>ROC-AUC (%) Validation</cell><cell>Test</cell></row><row><cell>MLP</cell><cell>81.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Results for ogbn-arxiv. 58?0.81 57.65?0.12 55.50?0.23 NODE2VEC 76.43?0.81 71.29?0.13 70.07?0.13 GCN 78.87?0.66 73.00?0.17 71.74?0.29 GRAPHSAGE 82.35?1.51 72.77?0.16 71.49?0.27 et al., 2013) over the MAG corpus.</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell>Accuracy (%) Validation</cell><cell>Test</cell></row><row><cell>MLP</cell><cell>63.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Results for ogbn-papers100M. 84?0.43 49.60?0.29 47.24?0.31 SGC 67.54?0.43 66.48?0.20 63.29?0.19</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell>Accuracy (%) Validation</cell><cell>Test</cell></row><row><cell>MLP</cell><cell>54.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Results for ogbn-mag. Requires a GPU with 14GB of memory.</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell>Accuracy (%) Validation</cell><cell>Test</cell></row><row><cell>MLP</cell><cell cols="3">28.33?0.20 26.26?0.16 26.92?0.26</cell></row><row><cell>GCN</cell><cell cols="3">29.71?0.19 29.53?0.22 30.43?0.25</cell></row><row><cell>GRAPHSAGE</cell><cell cols="3">30.79?0.19 30.70?0.19 31.53?0.15</cell></row><row><cell>METAPATH2VEC</cell><cell cols="3">38.35?1.39 35.06?0.17 35.44?0.36</cell></row><row><cell>R-GCN  ?</cell><cell cols="3">75.87?4.19 40.84?0.41 39.77?0.46</cell></row><row><cell cols="4">NEIGHBORSAMPLING 68.53?7.27 47.61?0.68 46.78?0.67</cell></row><row><cell>CLUSTERGCN</cell><cell cols="3">79.65?4.12 38.40?0.31 37.32?0.37</cell></row><row><cell>GRAPHSAINT</cell><cell cols="3">79.64?1.70 48.37?0.26 47.51?0.22</cell></row></table><note>?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Results for ogbl-ppa.</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell>Hits@100 (%) Validation</cell><cell>Test</cell></row><row><cell>MLP</cell><cell>0.46?0.00</cell><cell>0.46?0.00</cell><cell>0.46?0.00</cell></row><row><cell>NODE2VEC</cell><cell cols="3">24.43?0.92 22.53?0.88 22.26?0.83</cell></row><row><cell>GCN</cell><cell cols="3">19.89?1.51 18.45?1.40 18.67?1.32</cell></row><row><cell>GRAPHSAGE</cell><cell cols="3">18.53?2.85 17.24?2.64 16.55?2.40</cell></row><row><cell cols="4">MATRIXFACTORIZATION 81.65?9.15 32.28?4.28 32.29?0.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Results for ogbl-collab.</figDesc><table><row><cell>Method</cell><cell>Use most recent edges</cell><cell>Training</cell><cell>Hits@50 (%) Validation</cell><cell>Test</cell></row><row><cell>MLP</cell><cell></cell><cell cols="3">45.70?1.66 24.02?1.45 19.27?1.29</cell></row><row><cell>NODE2VEC</cell><cell></cell><cell cols="3">99.73?0.36 57.03?0.52 48.88?0.54</cell></row><row><cell>GCN</cell><cell></cell><cell cols="3">84.28?1.78 52.63?1.15 44.75?1.07</cell></row><row><cell>GRAPHSAGE</cell><cell></cell><cell cols="3">93.58?0.59 56.88?0.77 48.10?0.81</cell></row><row><cell>MATRIXFACTORIZATION</cell><cell></cell><cell cols="3">100.00?0.00 48.96?0.29 38.86?0.29</cell></row><row><cell>GCN</cell><cell></cell><cell cols="3">84.28?1.78 52.63?1.15 47.14?1.45</cell></row><row><cell>GRAPHSAGE</cell><cell></cell><cell cols="3">93.58?0.59 56.88?0.77 54.63?1.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Results for ogbl-ddi. 82?1.35 32.92?1.21 23.26?2.09 GCN 63.95?2.17 55.50?2.08 37.07?5.07 GRAPHSAGE 72.24?0.45 62.62?0.37 53.90?4.74 MATRIXFACTORIZATION 56.56?13.88 33.70?2.64 13.68?4.75</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell>Hits@20 (%) Validation</cell><cell>Test</cell></row><row><cell>NODE2VEC</cell><cell>37.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Results for ogbl-citation2. Requires a GPU with 40GB of memory</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell>MRR Validation</cell><cell>Test</cell></row><row><cell>MLP</cell><cell cols="3">0.2884?0.0014 0.2891?0.0012 0.2895?0.0014</cell></row><row><cell>NODE2VEC</cell><cell cols="3">0.7004?0.0012 0.6124?0.0011 0.6141?0.0011</cell></row><row><cell>GCN  ?</cell><cell cols="3">0.9092?0.0019 0.8479?0.0023 0.8474?0.0021</cell></row><row><cell>GRAPHSAGE  ?</cell><cell cols="3">0.8970?0.0056 0.8263?0.0033 0.8260?0.0036</cell></row><row><cell cols="4">MATRIXFACTORIZATION 0.9185?0.0170 0.5181?0.0436 0.5186?0.0443</cell></row><row><cell>NEIGHBORSAMPLING</cell><cell cols="3">0.8645?0.0015 0.8054?0.0009 0.8044?0.0010</cell></row><row><cell>CLUSTERGCN</cell><cell cols="3">0.8749?0.0035 0.7994?0.0025 0.8004?0.0025</cell></row><row><cell>GRAPHSAINT</cell><cell cols="3">0.8682?0.0026 0.7975?0.0039 0.7985?0.0040</cell></row></table><note>?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Results for ogbl-wikikg2. Requires a GPU with 45GB of memory.</figDesc><table><row><cell>Method</cell><cell cols="3">MRR Training (Unfiltered) Validation (Filtered) Test (Filtered)</cell></row><row><cell>TRANSE</cell><cell>0.3408?0.0044</cell><cell>0.2465?0.0020</cell><cell>0.2622?0.0045</cell></row><row><cell>DISTMULT</cell><cell>0.4115?0.0077</cell><cell>0.3150?0.0088</cell><cell>0.3447?0.0082</cell></row><row><cell>COMPLEX</cell><cell>0.4573?0.0035</cell><cell>0.3534?0.0052</cell><cell>0.3804?0.0022</cell></row><row><cell>ROTATE</cell><cell>0.3464?0.0015</cell><cell>0.2250?0.0035</cell><cell>0.2530?0.0034</cell></row><row><cell>TRANSE (5?dim)  ?</cell><cell>0.6174?0.0026</cell><cell>0.4272?0.0030</cell><cell>0.4256?0.0030</cell></row><row><cell>DISTMULT (5?dim)  ?</cell><cell>0.4350?0.0038</cell><cell>0.3506?0.0042</cell><cell>0.3729?0.0045</cell></row><row><cell>COMPLEX (5?dim)  ?</cell><cell>0.4760?0.0030</cell><cell>0.3759?0.0016</cell><cell>0.4027?0.0027</cell></row><row><cell>ROTATE (5?dim)  ?</cell><cell>0.6111?0.0032</cell><cell>0.4353?0.0028</cell><cell>0.4332?0.0025</cell></row></table><note>?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 :</head><label>14</label><figDesc>Results for ogbl-biokg.</figDesc><table><row><cell>Method</cell><cell cols="3">MRR Training (Unfiltered) Validation (Filtered) Test (Filtered)</cell></row><row><cell>TRANSE</cell><cell>0.5145?0.0005</cell><cell>0.7456?0.0003</cell><cell>0.7452?0.0004</cell></row><row><cell>DISTMULT</cell><cell>0.5250?0.0006</cell><cell>0.8055?0.0003</cell><cell>0.8043?0.0003</cell></row><row><cell>COMPLEX</cell><cell>0.5315?0.0006</cell><cell>0.8105?0.0001</cell><cell>0.8095?0.0007</cell></row><row><cell>ROTATE</cell><cell>0.5363?0.0007</cell><cell>0.7997?0.0002</cell><cell>0.7989?0.0004</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 15 :</head><label>15</label><figDesc>Results for ogbg-molhiv.</figDesc><table><row><cell>Method</cell><cell>Additional Virtual Features Node</cell><cell>Training</cell><cell>ROC-AUC (%) Validation</cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell cols="3">88.65?1.01 83.73?0.78 74.18?1.22</cell></row><row><cell>GCN</cell><cell></cell><cell cols="3">88.65?2.19 82.04?1.41 76.06?0.97</cell></row><row><cell></cell><cell></cell><cell cols="3">90.07?4.69 83.84?0.91 75.99?1.19</cell></row><row><cell></cell><cell></cell><cell cols="2">93.89?2.96 84.1?1.05</cell><cell>75.2?1.30</cell></row><row><cell>GIN</cell><cell></cell><cell cols="3">88.64?2.54 82.32?0.90 75.58?1.40</cell></row><row><cell></cell><cell></cell><cell cols="3">92.73?3.80 84.79?0.68 77.07?1.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 16 :</head><label>16</label><figDesc>Results for ogbg-molpcba.</figDesc><table><row><cell>Method</cell><cell>Additional Virtual Features Node</cell><cell>Training</cell><cell>AP (%) Validation</cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell cols="3">36.25?0.71 23.88?0.22 22.91?0.37</cell></row><row><cell>GCN</cell><cell></cell><cell cols="3">28.04?0.58 20.59?0.33 20.20?0.24</cell></row><row><cell></cell><cell></cell><cell cols="3">38.25?0.50 24.95?0.42 24.24?0.34</cell></row><row><cell></cell><cell></cell><cell cols="3">45.70?0.61 27.54?0.25 26.61?0.17</cell></row><row><cell>GIN</cell><cell></cell><cell cols="3">37.05?0.31 23.05?0.27 22.66?0.28</cell></row><row><cell></cell><cell></cell><cell cols="3">46.96?0.57 27.98?0.25 27.03?0.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 17 :</head><label>17</label><figDesc>Results for ogbg-ppa. 68?0.22 64.97?0.34 68.39?0.84 97.00?1.00 65.11?0.48 68.57?0.61 GIN 97.55?0.52 65.62?1.07 68.92?1.00 98.28?0.46 66.78?1.05 70.37?1.07</figDesc><table><row><cell>Method</cell><cell>Virtual Node</cell><cell>Training</cell><cell>Accuracy (%) Validation</cell><cell>Test</cell></row><row><cell>GCN</cell><cell cols="2">97._mask_</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 18 :</head><label>18</label><figDesc>Results for ogbg-code2. 06?2.95 13.99?0.17 15.07?0.18 30.94?2.34 14.61?0.13 15.95?0.18 GIN 26.49?1.60 13.76?0.16 14.95?0.23 30.40?1.98 14.39?0.20 15.81?0.26</figDesc><table><row><cell>Method</cell><cell>Virtual Node</cell><cell>Training</cell><cell>F1 score (%) Validation</cell><cell>Test</cell></row><row><cell>GCN</cell><cell></cell><cell>30.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 19 :</head><label>19</label><figDesc>Summary of ogbg-mol * datasets. For all the datasets, we use the scaffold split with the split ratio of 80/10/10.</figDesc><table><row><cell>Category</cell><cell>Name</cell><cell>#Graphs</cell><cell cols="3">Average Average #Tasks #Nodes #Edges</cell><cell>Task Type</cell><cell>Metric</cell></row><row><cell></cell><cell>tox21</cell><cell>7,831</cell><cell>18.6</cell><cell>19.3</cell><cell cols="3">12 Binary class. ROC-AUC</cell></row><row><cell></cell><cell>toxcast</cell><cell>8,576</cell><cell>18.8</cell><cell>19.3</cell><cell cols="3">617 Binary class. ROC-AUC</cell></row><row><cell></cell><cell>muv</cell><cell>93,087</cell><cell>24.2</cell><cell>26.3</cell><cell cols="2">17 Binary class.</cell><cell>AP</cell></row><row><cell>Molecular Graph ogbg-mol</cell><cell>bace bbbp clintox sider</cell><cell>1,513 2,039 1,477 1,427</cell><cell>34.1 24.1 26.2 33.6</cell><cell>36.9 26.0 27.9 35.4</cell><cell cols="3">1 Binary class. ROC-AUC 1 Binary class. ROC-AUC 2 Binary class. ROC-AUC 27 Binary class. ROC-AUC</cell></row><row><cell></cell><cell>esol</cell><cell>1,128</cell><cell>13.3</cell><cell>13.7</cell><cell>1</cell><cell>Regression</cell><cell>RMSE</cell></row><row><cell></cell><cell>freesolv</cell><cell>642</cell><cell>8.7</cell><cell>8.4</cell><cell>1</cell><cell>Regression</cell><cell>RMSE</cell></row><row><cell></cell><cell>lipo</cell><cell>4,200</cell><cell>27.0</cell><cell>29.5</cell><cell>1</cell><cell>Regression</cell><cell>RMSE</cell></row><row><cell cols="6">A More Benchmark Results on ogbg-mol * Datasets</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 20 :</head><label>20</label><figDesc>Results for ogbg-moltox21. 01?1.81 81.12?0.37 75.51?1.00 92.06?0.93 79.04?0.19 75.29?0.69 93.28?2.18 82.05?0.43 77.46?0.86 GIN 93.13?0.94 81.47?0.3 76.21?0.82 93.06?0.88 78.32?0.48 74.91?0.51 93.67?1.03 82.17?0.35 77.57?0.62</figDesc><table><row><cell>Method</cell><cell>Add. Virt. Feat. Node Training Validation ROC-AUC (%)</cell><cell>Test</cell></row><row><cell></cell><cell>90.</cell><cell></cell></row><row><cell>GCN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 21 :</head><label>21</label><figDesc>Results for ogbg-moltoxcast. 89?0.88 70.52?0.34 66.33?0.35 85.21?1.69 67.48?0.33 63.54?0.42 89.89?0.8 71.65?0.38 66.71?0.45 GIN 85.51?0.59 69.62?0.66 66.18?0.68 84.65?1.56 68.62?0.63 63.41?0.74 86.42?0.49 72.32?0.35 66.13?0.50</figDesc><table><row><cell>Method</cell><cell>Add. Virt. Feat. Node Training Validation ROC-AUC (%)</cell><cell>Test</cell></row><row><cell></cell><cell>88.</cell><cell></cell></row><row><cell>GCN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 22 :</head><label>22</label><figDesc>Results for ogbg-molmuv. 67?3.87 8.48?1.58 2.48?2.83 22.72?6.9 21.4?1.46 11.39?2.87 23.64?7.12 22.1?1.98 10.98?2.91 GIN 26.49?7.24 15.74?2.19 7.91?2.13 17.94?4.06 19.00?2.15 8.78?2.07 25.95?7.85 17.42?1.32 9.84?2.71</figDesc><table><row><cell>Method</cell><cell>Add. Virt. Feat. Node Training Validation AP (%)</cell><cell>Test</cell></row><row><cell></cell><cell>6.</cell><cell></cell></row><row><cell>GCN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 23 :</head><label>23</label><figDesc>Results for ogbg-molbace. 85?5.08 78.99?2.03 71.44?4.01 91.74?1.90 73.74?1.49 79.15?1.44 91.16?2.86 80.25?1.43 68.93?6.95 GIN 87.84?1.75 77.21?1.01 76.41?2.68 92.07?2.62 73.30?1.95 72.97?4.00 92.04?5.77 80.81?1.71 73.46?5.24</figDesc><table><row><cell>Method</cell><cell>Add. Virt. Feat. Node Training Validation ROC-AUC (%)</cell><cell>Test</cell></row><row><cell></cell><cell>87.</cell><cell></cell></row><row><cell>GCN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 24 :</head><label>24</label><figDesc>Results for ogbg-molbbbp. 42?3.82 93.46?0.27 68.62?2.19 96.97?1.31 94.74?0.31 68.87?1.51 98.29?1.79 95.95?0.40 67.80?2.35 GIN 94.06?1.85 94.66?0.35 69.88?1.70 95.99?2.44 94.83?0.52 68.17?1.48 97.70?1.71 95.68?0.40 69.71?1.92</figDesc><table><row><cell>Method</cell><cell>Add. Virt. Feat. Node Training Validation ROC-AUC (%)</cell><cell>Test</cell></row><row><cell></cell><cell>90.</cell><cell></cell></row><row><cell>GCN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 25 :</head><label>25</label><figDesc>Results for ogbg-molclintox. 11?5.72 88.78?1.48 68.66?4.95 98.14?0.91 99.24?0.47 91.30?1.73 97.35?1.30 99.57?0.15 88.55?2.09 GIN 86.12?5.50 90.79?1.10 61.79?4.77 96.31?1.77 98.54?0.48 88.14?2.51 93.51?1.78 99.18?0.53 84.06?3.84</figDesc><table><row><cell>Method</cell><cell>Add. Virt. Feat. Node Training Validation ROC-AUC (%)</cell><cell>Test</cell></row><row><cell></cell><cell>83.</cell><cell></cell></row><row><cell>GCN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 26 :</head><label>26</label><figDesc>Results for ogbg-molsider. 82?1.02 59.86?0.81 61.65?1.06 82.74?2.99 64.64?0.82 59.60?1.77 77.50?2.58 61.88?0.89 59.84?1.54 GIN 72.37?0.78 59.84?0.86 57.75?1.14 80.13?2.91 64.14?1.24 57.60?1.40 76.60?1.38 62.41?0.99 57.57?1.56</figDesc><table><row><cell>Method</cell><cell>Add. Virt. Feat. Node Training Validation ROC-AUC (%)</cell><cell>Test</cell></row><row><cell></cell><cell>73.</cell><cell></cell></row><row><cell>GCN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 27 :</head><label>27</label><figDesc>Results for ogbg-molesol. 883?0.096 1.128?0.032 1.143?0.075 0.629?0.041 1.022?0.034 1.114?0.036 0.758?0.147 0.991?0.04 1.015?0.096 GIN 0.746?0.158 0.921?0.045 1.026?0.063 0.628?0.041 1.007?0.028 1.173?0.057 0.675?0.131 0.878?0.036 0.998?0.066</figDesc><table><row><cell>Method</cell><cell>Add. Virt. Feat. Node Training Validation RMSE</cell><cell>Test</cell></row><row><cell></cell><cell>0.</cell><cell></cell></row><row><cell>GCN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 28 :</head><label>28</label><figDesc>Results for ogbg-molfreesolv. 163?0.157 2.744?0.201 2.413?0.195 0.982?0.109 2.582?0.297 2.640?0.239 1.219?0.153 2.922?0.185 2.186?0.120 GIN 1.006?0.225 2.567?0.19 2.307?0.340 1.205?0.360 2.342?0.378 2.755?0.349 0.934?0.138 2.181?0.205 2.151?0.295</figDesc><table><row><cell>Method</cell><cell>Add. Virt. Feat. Node Training Validation RMSE</cell><cell>Test</cell></row><row><cell></cell><cell>1.</cell><cell></cell></row><row><cell>GCN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 29 :</head><label>29</label><figDesc>Results for ogbg-mollipo. 669?0.058 0.855?0.032 0.823?0.029 0.662?0.046 0.816?0.024 0.797?0.023 0.545?0.041 0.766?0.011 0.771?0.016 GIN 0.488?0.029 0.749?0.018 0.741?0.024 0.479?0.027 0.742?0.011 0.757?0.018 0.399?0.023 0.679?0.014 0.704?0.015</figDesc><table><row><cell>Method</cell><cell>Add. Virt. Feat. Node Training Validation RMSE</cell><cell>Test</cell></row><row><cell></cell><cell>0.</cell><cell></cell></row><row><cell>GCN</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://pytorch.org 2 https://pytorch-geometric.readthedocs.io 3 https://www.dgl.ai</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/snap-stanford/ogb 5 https://ogb.stanford.edu/docs/leader_overview</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Recently, some progress has been made to increase the dataset sizes: http://graphlearning.io. Nevertheless, most of them are still small compared to the OGB datasets, and evaluation protocols are not standardized.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Defined by the difference between training and test accuracy.8  The GRAPHSAGE architecture is used for neighbor aggregation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">In our preliminary experiments, we used one-hot encodings of species ID as node features, but that did not work well empirically, which can be explained by the fact that the species ID is used for dataset splitting. 10 Note that the input features here are graph-aware in some sense, because they are obtained by averaging the incoming edge features.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">https://arxiv.org/corr/subjectclasses 12 In practice, the trained models can also be used to predict labels of even non-ARXIV papers.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">The R-GCN architecture is used for neighbor aggregation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">Here we obtain node embeddings by applying a linear layer to the raw one-hot node features.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">The older version ogbl-citation has been deprecated due to a bug in negative samples of validation and test sets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22">Note that our test MRR on ogbl-wikikg2 is computed using only 500 negative entities per triplet, which is much less than the number of negative entities used to compute MRR in the existing KG datasets, such as FB15K and FB15K-237 (around 15,000 negative entitiesf). Nevertheless, ROTATE gives either lower or comparable test MRR on ogbl-wikikg2 compared to FB15K and FB15K-237<ref type="bibr" target="#b75">(Sun et al., 2019)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23">InTable 14, training MRR is lower than validation and test MRR because it is an unfiltered metric (computed by ranking against randomly-selected negative entities), and is expected to give systematically lower MRR than the filtered metric (computed by ranking against "true" negative entities, i.e., the resulting triplets do not appear in the KG).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24"><ref type="bibr" target="#b89">Wu et al. (2018)</ref> originally used a closely-related metric, PRC (Precision Recall Curve)-AUC, but<ref type="bibr" target="#b20">Davis &amp; Goadrich (2006)</ref> showed that AP is more appropriate to summarize the non-convex nature of PRC.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25">The older version ogbg-code has been deprecated due to the prediction target leakage in input AST. 26 https://github.com/github/CodeSearchNet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="30">The shape of the matrix is the number of data points times the number of tasks. The matrix can be either a PYTORCH tensor or NUMPY array.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Adrijan Bradaschia and Rok Sosic for their help in setting up the server and website. We also thank Emma Pierson and Shigeru Maya for their suggestions on the paper writing, and Charles Sutton for pointing out the data leakage in one of our datasets. Finally, we thank the entire community of graph ML for providing valuable feedback to improve OGB. Weihua Hu is supported by Funai Overseas Scholarship and Masason Foundation Fellowship. Matthias Fey is supported by the German Research Association (DFG) within the Collaborative Research Center SFB 876 "Providing Information by Resource-Constrained Analysis", project A6. Marinka Zitnik is in part supported by NSF IIS-2030459. We gratefully acknowledge the support of DARPA under Nos. FA865018C7880 (ASED), N660011924033 (MCS); ARO under Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP); NSF under Nos. OAC-1835598 (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), IIS-2030477 (RAPID); Stanford Data Science Initiative, Wu Tsai Neurosciences Institute, Chan Zuckerberg Biohub, Amazon, Boeing, JPMoran Chase, Docomo, Hitachi, JD.com, KDDI, NVIDIA, Dell. Jure Leskovec is a Chan Zuckerberg Biohub investigator.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Operating Systems Design and Implementation OSDI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The adverse effects of code duplication in machine learning models of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software</title>
		<meeting>the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="143" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A convolutional attention network for extreme summarization of source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning to represent programs with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Khademi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00740</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of machine learning for big code and naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaked</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01400</idno>
		<title level="m">Generating sequences from structured representations of code</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning distributed representations of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meital</forename><surname>Zilberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Integration of virtual and high-throughput screening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Bajorath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Drug Discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="882" to="894" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Network biology: understanding the cell&apos;s functional organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert-Laszlo</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zoltan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oltvai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews genetics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="113" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The third &apos;chime&apos;speech separation and recognition challenge: Dataset, task and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricard</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="504" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<title level="m">Graph convolutional matrix completion</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The extreme classification repository: Multi-label datasets and code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<ptr target="http://manikvarma.org/downloads/XC/XMLRepository.html" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special Interest Group on Management of Data (SIGMOD)</title>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS workshop on Machine Learning Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The gene ontology resource: 20 years and still going strong</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene</forename><surname>Ontology Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="330" to="338" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Network propagation: a universal amplifier of genetic associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenore</forename><surname>Cowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trey</forename><surname>Ideker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roded</forename><surname>Raphael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">551</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The comparative toxicogenomics database: update 2019</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Peter Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><forename type="middle">J</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Mcmorran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jolene</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mattingly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="948" to="954" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The relationship between precision-recall and roc curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Goadrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Emerging methods in protein co-evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florencio</forename><surname>David De Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfonso</forename><surname>Pazos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valencia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="249" to="261" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">cvpr</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hoppity: Learning graph transformations to detect and fix bugs in programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Dinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayur</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A century of science: Globalization of scientific collaborations, citations, and innovations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1437" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Easley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge university press Cambridge</publisher>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Openml-python: an extensible python api for openml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arlind</forename><surname>Kadra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Gijsbers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeratyoy</forename><surname>Mallik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahithya</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02490</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1273" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reproducible drug repurposing: When similarity does not suffice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Guney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Symposium on Biocomputing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="132" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International World Wide Web Conference</title>
		<meeting>the International World Wide Web Conference</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A new view of the tree of life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anantharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cindy</forename><forename type="middle">J</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><forename type="middle">N</forename><surname>Castelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">W</forename><surname>Butterfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Hernsdorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotaro</forename><surname>Amano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Microbiology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">16048</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Codesearchnet challenge: Evaluating the state of semantic code search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamel</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho-Hsiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiferet</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09436</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Graph warp module: An auxiliary module for boosting the power of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01020</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Understanding isomorphism bias in graph data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sviridov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12091</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Hierarchical generation of molecular graphs using structural motifs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03230</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://www.graphlearning.io/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Variational graph auto-encoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Landrum</surname></persName>
		</author>
		<title level="m">Open-source cheminformatics</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Pytorch-biggraph: A large-scale graph embedding system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothee</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Wehrstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Peysakhovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12287</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Snap: A general-purpose network analysis and graph-mining library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rok</forename><surname>Sosi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning graph-level representation for drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03741</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">eccv</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Sampling: design and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lohr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Nelson Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Impact of high-throughput screening in biomedical research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Macarron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejan</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bojanic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Cirovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garyantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Darren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hertzberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">W</forename><surname>Janzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paslay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Drug discovery</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="188" to="195" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unified alignment of protein-protein interaction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">No?l</forename><surname>Malod-Dognin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nata?a</forename><surname>Pr?ulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The DisGeNET knowledge platform for disease genomics: 2019 update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><surname>Pi?ero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Manuel Ram?rez-Anguita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Sa?ch-Pitarch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Ronzano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Centeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Sanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">I</forename><surname>Furlong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="845" to="855" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Netsmf: Large-scale network embedding as sparse matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International World Wide Web Conference (WWW)</title>
		<meeting>the International World Wide Web Conference (WWW)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1509" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grover</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02835</idno>
		<title level="m">Self-supervised message passing transformer on large-scale molecular data</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Conserved patterns of protein interaction in multiple species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roded</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silpa</forename><surname>Suthram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Mccuine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Uetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trey</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ideker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1974" to="1979" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Evaluating logical generalization in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koustuv</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shagun</forename><surname>Sodhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William L</forename><surname>Hamilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06560</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A deep learning approach to antibiotic discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><forename type="middle">M</forename><surname>Cubillos-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donghia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Craig R Macnair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zohar</forename><surname>Carfrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bloom-Ackerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="688" to="702" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">STITCH 5: augmenting protein-chemical interaction networks with tissue and affinity data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Szklarczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">Juhl</forename><surname>Christian Von Mering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peer</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="380" to="384" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">STRING v11: protein-protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Szklarczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annika</forename><forename type="middle">L</forename><surname>Gable</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Junge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Huerta-Cepas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Simonovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nadezhda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doncheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peer</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="607" to="613" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Openml: Networked science in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Torgo</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2641190.2641198</idno>
		<ptr target="http://doi.acm.org/10.1145/2641190.2641198" />
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: When experts are not enough</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.01315" />
	</analytic>
	<monogr>
		<title level="m">Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Kepler: A unified model for knowledge embedding and pre-trained language representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06136</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">DrugBank 5.0: a major update to the DrugBank database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David S Wishart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yannick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><forename type="middle">C</forename><surname>Feunang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elvis</forename><forename type="middle">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanvir</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sajed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinat</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sayeeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="1074" to="1082" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Analyzing learned molecular representations for property prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Eiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Guzman-Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hopper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Mathea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3370" to="3388" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Position-aware graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">High-throughput characterization of protein-protein interactions by reprogramming yeast mating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Klavins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">46</biblScope>
			<biblScope unit="page" from="12166" to="12171" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Graph-Saint: Graph sampling based inductive learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08532</idno>
		<title level="m">Dgl-ke: Training knowledge graph embeddings at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Graphvite: A high-performance cpugpu hybrid system for node embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International World Wide Web Conference (WWW)</title>
		<meeting>the International World Wide Web Conference (WWW)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2494" to="2504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Evolution of resilience in protein interactomes across the tree of life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="4426" to="4433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Dimensional reweighting graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJeLO34KwS" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
