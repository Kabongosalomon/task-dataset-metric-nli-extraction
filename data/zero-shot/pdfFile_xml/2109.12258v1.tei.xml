<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
							<email>brucelws@seas.upenn.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoo</forename><forename type="middle">Sung</forename><surname>Jang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason Hyung-Jong</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lxper</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">South</forename><surname>Seoul</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korea</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Univ. of Pennsylvania</orgName>
								<address>
									<addrLine>1 PA</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Univ</orgName>
								<address>
									<addrLine>of Wisconsin-Madison 2 WI</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report two essential improvements in readability assessment: 1. three novel features in advanced semantics and 2. the timely evidence that traditional ML models (e.g. Random Forest, using handcrafted features) can combine with transformers (e.g. RoBERTa) to augment model performance. First, we explore suitable transformers and traditional ML models. Then, we extract 255 handcrafted linguistic features using self-developed extraction software. Finally, we assemble those to create several hybrid models, achieving state-of-the-art (SOTA) accuracy on popular datasets in readability assessment. The use of handcrafted features help model performance on smaller datasets. Notably, our RoBERTA-RF-T1 hybrid achieves the near-perfect classification accuracy of 99%, a 20.3% increase from the previous SOTA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The long quest for advancing readability assessment (RA) mostly centered on handcrafting the linguistic features that affect readability <ref type="bibr">(Pitler and Nenkova, 2008)</ref>. RA is a time-honored branch of natural language processing (NLP) that quantifies the difficulty with which a reader understands a text <ref type="bibr">(Feng et al., 2010)</ref>. Being one of the oldest systematic approaches to linguistics (Collins- <ref type="bibr">Thompson, 2014)</ref>, RA developed various linguistic features. These range from simple measures like the average count of syllables to those as sophisticated as semantic complexity <ref type="bibr">(Buchanan et al., 2001)</ref>.</p><p>Perhaps due to the abundance of dependable linguistic features, an overwhelming majority of RA systems are Support Vector Machines (SVM) with handcrafted features <ref type="bibr">(Hansen et al., 2021)</ref>. Such traditional machine learning (ML) methods were linguistically explainable, expandable, and most importantly, competent against the modern neural models. As a fragmentary example, <ref type="bibr">Filighera et al. (2019)</ref> reports that a large ensemble of 6 BiLSTMs with <ref type="bibr">BERT (Devlin et al., 2019)</ref>, <ref type="bibr">ELMo (Peters et al., 2018)</ref>, <ref type="bibr">Word2Vec (Mikolov et al., 2013)</ref>, and GloVe <ref type="bibr">(Pennington et al., 2014)</ref> embeddings showed only ?1% accuracy improvement from a single SVM model developed by <ref type="bibr" target="#b13">Xia et al. (2016)</ref>.</p><p>Even though deep neural networks have achieved state-of-the-art (SOTA) performance in almost all semantic tasks where sufficient data were available <ref type="bibr">(Collobert et al., 2011;</ref><ref type="bibr" target="#b18">Zhang et al., 2015)</ref>, neural models started showing promising results in RA only quite recently <ref type="bibr">(Martinc et al., 2021)</ref>. A known challenge for the researchers in RA is the lack of large public datasets -with the unique exception of WeeBit . Technically speaking, even WeeBit is not entirely public since it has to be directly obtained from the authors. <ref type="bibr">Martinc et al. (2021)</ref> raised the SOTA classification accuracy on the popular WeeBit dataset  by about 4% using BERT. This was the first solid proof that neural models with auto-generated features can show significant improvement compared to traditional ML with handcrafted features. However, neural models, or transformers (which is the interest of this paper), still show not much better performance than traditional ML on smaller datasets like OneStopEnglish <ref type="bibr" target="#b8">(Vajjala and Lu?i?, 2018)</ref>, despite the complexity.</p><p>From our observations, the reported low performances of transformers on small RA datasets can be accounted for two reasons. 1. Only BERT was applied to RA, and there could be other transformers that perform better, even on small datasets. 2. If a transformer shows weak performance on small datasets, there must be some additional measures done to supply the final model (e.g. ensemble) with more linguistic information, but such a study is rare in RA. Hence, we tackle the abovementioned issues in this paper. In particular, we 1. perform a wide search on transformers, traditional ML models, and handcrafted features &amp; 2. develop a hybrid architecture for SOTA and robustness on small datasets.</p><p>However, before we move on to hybrid models, we begin by supplementing an underexplored linguistic branch of handcrafted features. According to survey research on RA (Collins- <ref type="bibr">Thompson, 2014)</ref>, the study on advanced semantics is scarce. We lack a model to capture how deeper semantic structures affect readability. We attempt to solve this issue by viewing a text as a collection of latent topics and calculating the probability distribution.</p><p>Then, we move on to combine traditional ML (w handcrafted features 1 ) and transformers. Such a hybrid system is only reported by <ref type="bibr">Deutsch et al. (2020)</ref>, concluding, "(hybrid models) did not achieve SOTA performance." But we obtain contrary results. Through a large study on the optimal combination, we obtain SOTA results on WeeBit and OneStopEnglish. Also, our BERT-GB-T1 hybrid beats the (previous) SOTA accuracy with only 30% of the full dataset, in section 4.7.</p><p>Our main objectives are creating advanced semantic features and hybrid models. But our contributions to academia are not limited to the abovementioned two. We make the following additions: 1. We numerically represent certain linguistic properties pertaining to advanced semantics. 2. We develop a large-scale, openly available 255 features extraction Python toolkit (which is highly scarce 2 in RA). We name the software LingFeat 3 . 3. We conduct wide searches and parametrizations on transformers 4 and traditional ML 5 for RA use. 4. We develop hybrid models for SOTA and robustness on small datasets. Notably, RoBERTa-RF-T1 achieves 99% accuracy on OneStopEnglish, 20.3% higher than the previous SOTA (table <ref type="bibr" target="#b34">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Advanced Semantics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definition, Background, and Overview</head><p>A text is a communication between author and reader, and its readability is affected by the reader having shared world/domain knowledge. According to <ref type="bibr">Collins-Thompson (2014)</ref>, the features resulting from topic modeling may characterize the deeper semantic structures of a text. These deeper representations accumulate and appear to us in the form of perceivable properties like sentiment and genre. But advanced semantics aims to capture the deeper representation itself.</p><p>Among the four branches of linguistic properties (in RA) identified by <ref type="bibr">Collins-Thompson (2014)</ref>, advanced semantics remain unexplored. <ref type="bibr">Lexicosemantic (Lu, 2011;</ref><ref type="bibr">Malvern and</ref><ref type="bibr">Richards, 2012), syntactic (Heilman et al., 2007;</ref><ref type="bibr">Petersen and Ostendorf, 2009)</ref>, and discourse-based <ref type="bibr">(McNamara et al., 2010)</ref> properties had several notable works but little dealt with advanced semantics as the given definition. The existing examples in higher-level semantics focus on word-level complexity (Collins- <ref type="bibr">Thompson and Callan, 2004;</ref><ref type="bibr">Crossley et al., 2008;</ref><ref type="bibr">Landauer et al., 2011;</ref><ref type="bibr">Nam et al., 2017)</ref>. Such a phenomenon is complex. The lack of investigation on advanced semantics could be due to its low correlation with readability. This is plausible because RA studies often test their features on a human-labeled dataset, potentially biased towards easily recognizable surface-level features <ref type="bibr">(Evans, 2006)</ref>. Such biases could cause low performance.</p><p>Further, it must be noted that: 1. world knowledge might not always directly indicate difficulty, and 2. there can be other existing substitute features that capture similar properties on a word level. S1) Kindness is good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2) Christmas is good.</head><p>S3) I return with the stipulation to dismiss Smith's case; the same being duly executed by me.</p><p>S2 above seems to require more world knowledge than S1. However, "Christmas", as a familiar entity, seems to have no apparent contribution to increased difficulty. If any, similar properties can be captured by word frequency/familiarity measures (lexico-semantics) in a large representative corpus <ref type="bibr">(Leroy and Kauchak, 2013)</ref>. Also, it seems that S3 is the most difficult, and this can be easily deduced using entity counts (discourse). Entities mostly introduce conceptual information <ref type="bibr">(Feng et al., 2010)</ref>.</p><p>Our key objective in studying advanced semantics is to identify features that add orthogonal information. In other words, we hope to see a performance increase in our overall RA model rather than specific features' high correlations with readability.</p><p>Given the considerations, we draw two guidelines: 1. develop passage-level features since most word-level attributes are captured by existing features, and 2. value the orthogonal addition of information, not individual feature's high correlation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Approach</head><p>Topics convey text meaning on a global level <ref type="bibr">(Holtgraves, 1999)</ref>. In order to capture the deeper structure of meaning (advanced semantics), we hypothesized that calculating the distribution of documenttopic probabilities from latent dirichlet allocation (LDA) <ref type="bibr">(Blei et al., 2003)</ref> could be helpful.</p><p>Moreover, domain/world knowledge can be accounted for in LDA-resulting measures since LDA can be trained on various data. As explored in <ref type="bibr">Qumsiyeh and Ng (2011)</ref>, it may seem sensible to use the count of discovered topics as the measure of required knowledge. However, such measures can be extremely sensitive to passage length. Along with the count of discovered topics, we develop three others that consider how these topics are distributed: semantic richness, clarity, and noise. <ref type="figure" target="#fig_0">Fig. 1</ref> depicts the steps: 1. obtain output from a trained LDA model, 2. ignore topic ID and create a sorted probabilities list, and 3. calculate semantic richness, clarity, and noise. We model "how" the topics are distributed, not "what" the topics are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semantic Richness</head><p>Traditionally, semantic richness is quantified according to word usage <ref type="bibr">(Pexman et al., 2008)</ref>. In a high-dimensional model of semantic space <ref type="bibr">(Li et al., 2000)</ref>, co-occurring words clustered as semantic neighbors, quantifying semantic richness. As such, the previous models of semantic richness were often studied for word-level complexity and made no explicit connection with readability on a global level. Also, they were often subjectdependent <ref type="bibr">(Buchanan et al., 2001)</ref>. As concluded by <ref type="bibr">Pexman et al. (2008)</ref>, semantic richness is defined in several ways. We propose a novel variation.</p><p>We apply the similar co-occurrence concept but on the passage level using LDA. Here, semantic richness is the measure of how "largely" populated the topics are. In <ref type="figure" target="#fig_0">fig. 1</ref>, we approximately define richness as the product total of SPL, which measures the count of discovered topics (n) and topic probability (p). Additionally, we multiply index (i) to reward longer n so that the overall richness increases faster with more topics. See eqn. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Richness</head><formula xml:id="formula_0">= n i=1 p i ? i<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Semantic Clarity</head><p>Semantic clarity is critical in understanding text <ref type="bibr">(Peabody and Schaefer, 2016)</ref>. Likewise, complex meaning structures lead to comprehension difficulty <ref type="bibr">(Pires et al., 2017)</ref>. Some existing studies quantify semantic complexity (or clarity) through various measures, but most on the fine line between lexical and semantic properties <ref type="bibr">(Collins-Thompson, 2014)</ref>. They rarely deal with the latent meaning representations or the clarity of the main topic. For semantic clarity, we quantify how the probability distribution ( <ref type="figure" target="#fig_0">fig. 1</ref>) is focused (skewed) towards the largest discovered topic. In other words, we hope to see how easily identifiable the main topic is. We wanted to adopt the standard skewness equation from statistics, but we developed an alternative (eqn. 2) because the standard equation failed to capture the anticipated trends in appendix A.</p><formula xml:id="formula_1">Semantic Clarity = 1 n ? n i=1 (max(p) ? p i ) (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Semantic Noise</head><p>Semantic noise is the measure of the less-important topics (those with low probability), also the "tailedness" of sorted probability lists ( <ref type="figure" target="#fig_0">fig. 1</ref> </p><formula xml:id="formula_2">Semantic Noise = n ? n i=1 (p i ?p) 4 ( n i=1 (p i ?p) 2 ) 2 (3)</formula><p>We study 255 linguistic features. For the already existing features, we add variations to widen coverage. The full list of features, feature codes, and definition are provided in appendix B. Also, we classify features into 14 subgroups. External dependencies (e.g. parser) are reported in appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Advanced Semantic Features (AdSem)</head><p>Here, we follow the methods provided in section 2. 1?3) Wikipedia (WoKF), WeeBit (WBKF), &amp; OneStop Knowledge Features (OSKF). Each subgroup name represents the respective training data. We train Online LDA <ref type="bibr">(Hoffman et al., 2010)</ref> with the 20210301 dump 6 from English Wikipedia for WoKF. The others are trained on two popular corpora in RA: WeeBit and OneStopEnglish.</p><p>For each training set, four variations of 50, 100, 150, 200 topics models are trained. Four featuressemantic richness, clarity, noise, and the total count of discovered topics -are extracted per model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discourse-Based Features (Disco)</head><p>A text is more than a series of random sentences. It indicates a higher-level structure of dependencies. 4) Entity Density Features (EnDF). Conceptual information is often introduced by entities. Hence, the count of entities affects the working memory burden <ref type="bibr">(Feng et al., 2009</ref>). We bring entity-related features from <ref type="bibr">Feng et al. (2010)</ref>. 5) Entity Grid Features (EnGF) Coherent texts are easy to comprehend. Thus, we measure coherence through entity grid, using the 16 transition pattern ratios approach by Pitler and Nenkova (2008) as features. Also, we adopt local coherence scores <ref type="bibr">(Guinaudeau and Strube, 2013)</ref>, using the code implemented by Palma and Atkinson (2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Syntactic Features (Synta)</head><p>Syntactic complexity is associated with longer processing times <ref type="bibr">(Gibson, 1998)</ref>. Such syntactic properties also affect the overall complexity of a text <ref type="bibr">(Hale, 2016)</ref>, an important indicator of readability. 6) Phrasal Features (PhrF). Ratios involving clauses correlate with learners' abilities to read <ref type="bibr">(Lu, 2010)</ref>. We implement several variations, including the counts of noun, verb, and adverb phrases. 7) Tree Structure Features (TrSF). We deal with the structural shape of parsed trees, inspired by the work on average parse tree height by Schwarm 6 dumps.wikimedia.org/enwiki and Ostendorf <ref type="bibr">(2005)</ref>. On a constituency parser (appendix D) output, <ref type="bibr">NLTK (Loper and Bird, 2002)</ref> is used for the final calculation of features.</p><p>8) Part-of-Speech Features (POSF). Several studies report the effectiveness of using POS counts as features <ref type="bibr" target="#b6">(Tonelli et al., 2012;</ref><ref type="bibr">Lee and Lee, 2020a)</ref>. We count based on Universal POS tags 7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Lexico-Semantic Features (LxSem)</head><p>Perhaps the most explored, lexico-semantics capture the attributes associated with the difficulty or unfamiliarity of words <ref type="bibr">(Collins-Thompson, 2014)</ref>. 9) Variation Ratio Features (VarF) Lu (2011) reports noun, verb, adjective, and adverb variations, which represent the proportion of the respective category's words to total. We implement the features with variants from <ref type="bibr" target="#b36">Vajjala and Meurers (2012)</ref>.</p><p>10) Type Token Ratio Features (TTRF). TTR has been widely used as a measure of lexical richness in language acquisition studies (Malvern and Richards, 2012). We bring five variations of TTR from <ref type="bibr" target="#b36">Vajjala and Meurers (2012)</ref>. For MTLD (Mc-Carthy and Jarvis, 2010), we default TTR to 0.72. 11) Psycholinguistic Features (PsyF) As explored in <ref type="bibr" target="#b11">Vajjala and Meurers (2016)</ref>, we implement various Age-of-Acquisition features from Kuperman study database <ref type="bibr">Kuperman et al. (2012)</ref>.</p><p>12) Word Familiarity Features (WorF) Word frequency in a large representative corpus often represents lexical difficulty <ref type="bibr">(Collins-Thompson, 2014)</ref> due to unfamiliarity. We use SubtlexUS database <ref type="bibr">(Brysbaert and New, 2009</ref>) to measure familiarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Shallow Traditional Features (ShaTr)</head><p>Classic readability formulas (e.g. Flesch-Kincaid Grade) <ref type="bibr">(Kincaid et al., 1975)</ref> or shallow measures often do not represent a specific linguistic branch.</p><p>13) Shallow Features (ShaF) These features capture surface-level difficulty. Our measures include the average count of tokens and syllables.</p><p>14) Traditional Formulas (TraF). For Flesh-Kincaid Grade Level, Automated Readability, and Gunning Fog, we follow the "new" formulas in <ref type="bibr">Kincaid et al. (1975)</ref>. We follow <ref type="bibr" target="#b3">Si and</ref><ref type="bibr">Callan (2001) for Smog Index (Mc Laughlin, 1969)</ref>. And we follow <ref type="bibr">Eltorai et al. (2015)</ref> for Linsear Write.  In our hybrid model, we take a simple approach of joining the soft label predictions of a neural model (e.g. BERT) with handcrafted features and wrapping it with a non-neural model (e.g. SVM).</p><p>In <ref type="figure" target="#fig_1">fig. 2</ref>, the non-neural model (i.e. secondary predictor) learns 1. predictions/outputs of the neural model and 2. handcrafted features. The addition of handcrafted features supplements what neural models (i.e. initial predictor) might miss, reinforcing performance on the secondary prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">In Pursuit of the Best Combination</head><p>Our hybrid architecture ( <ref type="figure" target="#fig_1">fig. 2</ref>) is simple; Deutsch et al. (2020) explored a similar concept but did not achieve SOTA. But the benefits (section 4.1) from its simplicity are critical for RA, which has a lacking number/size of public datasets, wide educational use, and diverse handcrafted features. We obtain SOTA with a wider search on combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Datasets and Evaluation Setups</head><p>WeeBit. Perhaps the most widely-used, WeeBit is often considered the gold standard in RA. It was first created as an expansion of the famous Weekly Reader corpus <ref type="bibr">(Feng et al., 2009)</ref>. To avoid classification bias, we downsample classes to equalize the number of items (passages) in each class to 625. It is common practice to downsample WeeBit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Properties</head><p>WeeBit OneStopEng Cambridge  OneStopEnglish. OneStopEnglish is an aligned passage corpus developed for RA and simplification studies. A passage is paraphrased into three readability classes. OneStopEnglish is designed to be a balanced dataset. No downsampling is needed.</p><p>Cambridge. Cambridge <ref type="bibr" target="#b13">(Xia et al., 2016)</ref> categorizes articles based on Cambridge English Exam levels (KET, PET, FCE, CAE, CPE). These five exams are targeted at learners at A2-C2 levels of the Common European Framework of Reference <ref type="bibr" target="#b13">(Xia et al., 2016)</ref>. We downsample to 60 items/class.</p><p>For evaluation, we calculate accuracy, weighted F1 score, precision, recall, and quadratic weighted kappa (QWK). The use of QWK is inspired by Chen et al. <ref type="formula" target="#formula_0">(2016)</ref>; Palma et al. <ref type="bibr">(2019)</ref>. We use stratified k-fold (k=5, train=0.8, val=0.1, test=0.1) and average the results for reliability. We use SciKitlearn (Pedregosa et al., 2011) for metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Search on Neural Model</head><p>Extending from the existing use of BERT on RA <ref type="bibr">(Deutsch et al., 2020;</ref><ref type="bibr">Martinc et al., 2021)</ref>, we explore RoBERTa, <ref type="bibr">(Liu et al., 2019)</ref>, <ref type="bibr">BART (Lewis et al., 2020)</ref>, and XLNet <ref type="bibr" target="#b16">(Yang et al., 2019)</ref>. We use base models for all (details in appendix D). For each of the four models (   <ref type="formula" target="#formula_0">(2021)</ref> reports that transformers are weak on parallel datasets (On-eStopEnglish) due to their reliance on semantic information. However, RoBERTa &amp; BART show great performances on OneStopEnglish as well. Such a phenomenon likely derives from numerous aspects of the architecture. We carefully posit that the varying pretraining steps could be a reason.</p><p>BERT uses two objectives, masked language model (MLM) and next sentence prediction (NSP). The latter was included to capture the relation between sentences for natural language inference. Thus, sentence/segment-level input is used. Likewise, XLNet adopts a similar idea, limiting input to sentence/segment-level. But RoBERTa disproved the efficiency of NSP, adopting document-level inputs. Similarly, BART, via random shuffling of sentences and in-filling scheme, does not limit itself to a sentence/segment size input. As in section 3, "readability" is possibly a global-level representation (accumulated across the whole document). Thus, the performance differences could stem from the pretraining input size; sentence/segment-level input likely loses the global-level information.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Search on Non-Neural Model</head><p>We explored SVM, Random Forest (RandomF), Gradient Boosting (XGBoost) (Chen and Guestrin, 2016), and Logistic Regression (LogR). With the exception of XGBoost, the chosen models are frequently used in RA but rarely go through adequate hyperparameter optimization steps <ref type="bibr">(Ma et al., 2012;</ref><ref type="bibr" target="#b15">Yaneva et al., 2017;</ref><ref type="bibr">Mohammadi and Khasteh, 2020)</ref>. We perform a randomized search to first identify the sensible range of hyperparameters to search. Then, we apply grid search to specify the optimal values. The parameters are in appendix F. In table 3, we report the performances of the parameter-optimized models trained with all 255 handcrafted features. Compared to transformers, these non-neural models show lower accuracy in general. Even on the smallest Cambridge dataset, non-neural models do not necessarily show higher performances than transformers. But it is important to note that they managed to show fairly good, "expectable" performances on a much smaller dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Search on Handcrafted Features</head><p>We start by ranking performances of the feature subgroups. In   after training the respective model using the specified feature subgroup. Importantly, the advanced semantic features show good performance in all measures. WorF and PsyF, features calculated from external databases, rank in the top 7 for all corpora, hinting they are strong measures of readability. Moving on, we constructed several types of feature combinations with varying aims. These include: 1. T-type to thoroughly capture linguistic properties and 2. P-type to collect features by performance. We tested the variations on LogR and SVM to determine the optimal. Two sets <ref type="table" target="#tab_10">(table 6)</ref> performed well. Appendix G reports all tested variations. We highlight that both advanced semantics and discourse added distinct (orthogonal) information, which was evidenced by performance change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Assembling Hybrid Model</head><p>Based on the exploration so far, we assemble our hybrid model. We perform a brute-force grid search on four neural models (table 2), four non-neural models <ref type="table" target="#tab_6">(table 3)</ref>, and 14 feature sets <ref type="table" target="#tab_3">(table 24)</ref>.</p><p>To interweave the model, we followed the steps of 1: obtain soft labels (probabilities that a text belongs to the respective readability class) from a neural model by softmax layer, 2: append the soft labels to handcrafted features (create a dataframe), 3. train non-neural model on the dataframe. As in <ref type="figure" target="#fig_1">fig 2,</ref> the neural models performed a sort of reprediction to the data used for training to match the dataframe dimensions in training and test stages. <ref type="table" target="#tab_9">Table 5</ref> reports the best performing combination per respective neural model. Under "hybrid" column are code names (e.g. GB-T1 under BERT = XGBoost trained with handcrafted feature set T1 and BERT outputs). Under "?" column, we report differences with the respective single model performance. We also include SOTA baseline results <ref type="bibr">Xia-16 ? Xia et al. (2016)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hybrid Model Results and Limitations</head><p>In table 5, our hybrid models achieve SOTA performances on WeeBit (BART-RF-T1) and OneSt-opEnglish (RoBERTa-RF-T1). With the exception of <ref type="bibr" target="#b13">Xia et al. (2016)</ref> which uses extra data to increase accuracy, we also achieve SOTA on Cambridge: 76.3% accuracy on a small dataset of only 60 items/class. Among the hybrids, RoBERTa-RF-T1 showed consistently high performance on all metrics. But all hybrid models beat previous SOTA results by a large margin. Notably, we achieve the near-perfect accuracy of 99% on OneStopEnglish, a massive 20.3% increase from the previous SOTA <ref type="bibr">(Martinc et al., 2021)</ref> by <ref type="bibr">HAN (Meng et al., 2020)</ref>.</p><p>Both neural and non-neural models benefit from the hybrid architecture. This is explicitly shown in BERT-GB-T1 performance on OneStopEnglish, achieving 98.2% accuracy. This is an 18.1% increase from BERT and a 26.3% increase from XGBoost. However, BART did not benefit much from the hybrid architecture on WeeBit and On-eStopEnglish, meaning that hybrid architectures do not augment model performance at all conditions. Along similar lines, the hybrid architecture performance on the larger WeeBit dataset showed only a small improvement from the transformer-only result. On the other hand, the hybrid architecture performance on the smaller Cambridge dataset was consistently better than the transformer-only performance. The hybrid shows ?10% improvement in accuracy on average for Cambridge. On the smallest dataset <ref type="bibr">(Cambridge)</ref>, the hybrid architecture benefited more from a non-neural, handcrafted features-based model like RF (Random Forest) and GB (XGBoost). On the largest dataset (WeeBit), the hybrid benefited more from a transformer.</p><p>Our explanation is that the handcrafted features do not add much, at the data size of WeeBit. But the handcrafted features could be a great help where data is insufficient like they did for the Cambridge dataset. OneStopEnglish, being the medium-sized parallel dataset, could have hit the sweet spot for the hybrid architecture. But it must be noted that the data size is not the only determining factor as to which model (neural or non-neural) the hybrid architecture benefits more from. It must also be questioned if the max performance (? label noise induced by subjectivity) (Fr?nay et al., 2014) is already achieved on WeeBit <ref type="bibr">(Deutsch et al., 2020)</ref>.</p><p>Also, it seems that the hybrid architecture benefits when each model (neural and non-neural) already shows considerably good performance. This is plausible as the neural model outputs are considered features for the non-neural model. Including more "fairly" well-performing features only creates extra distractions. The hybrid architecture's limit is that it gets a model from "good" to "great," not "fair" to "good." But determining the definition of "fair" performance is a difficult feat as it likely depends on the dataset and a researcher's intuition from the empirical experience of the model. Hence, the hybrid architecture's limit is that one must test several combinations to pick the effective one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Why Not Directly Append Features?</head><p>Regarding the model architecture, we examined appending the handcrafted features to transformer embeddings without the use of a secondary predic-  <ref type="bibr">Read-Net (Meng et al., 2020)</ref> hints that such a model is not robust to small datasets. ReadNet reports 52.8% accuracy on Cambridge, worse than any of our tested models (table <ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b34">5)</ref>. Besides, Read-Net claims to have achieved 91.7% accuracy on WeeBit, without reports on downsampling. Many studies, like <ref type="bibr">Deutsch et al. (2020)</ref>, report that the model accuracy can increase ?4% on the full, classimbalanced WeeBit. Hence, ReadNet is not directly comparable. We omitted ReadNet from table 5. <ref type="table" target="#tab_3">Noticeable in table 2 and table 5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">BERT vs BERT, Ours Was Better</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Data Size Effect</head><p>In table 5, our hybrid architecture generally did not contribute much to the classification on WeeBit. But we argue that it has much to do with data size.</p><p>To model how data size affects the accuracies of 1. hybrid model, 2. transformer, and 3. traditional ML, we conducted an additional experiment using the same test data (10% of WeeBit) explained in section 4.2.1. However, we random sampled the train data (80% of WeeBit) into the smaller sizes of from 50 to 750, with 50 passages increase each set. We sampled with equal class weights, meaning that a 250 passages train set has 50 from each readability class. We trained BERT-GB-T1 (table 5) on the sampled data and evaluated on the same test data throughout. We also recorded BERT and XGBoost (with T1 features) performances in <ref type="figure" target="#fig_4">fig. 3</ref>.</p><p>In <ref type="figure" target="#fig_4">fig. 3</ref>, the hybrid model performs consistently better than transformer (+0.01 ? 0.05) at all sizes. But the difference gap gets smaller as the train data size increases. The hybrid model does help the efficiency of learning RA linguistic properties.</p><p>Contrary to the conventional beliefs, the transformer (BERT) performed better than our expectations, even on smaller data sizes. BERT always outperformed XGBoost. The traditional ML performance was arguably more consistent but never better than a transfomer's.</p><p>BERT-GB-T1's trend line seemed like the mixture of GB-T1's and BERT's. Notably, BERT-GB-T1 achieves 85.8% accuracy on WeeBit using only 750 passages, 30% of the original train data. For comparison, 85.7% was the past SOTA (table 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Domain Overfitting and Cross Domain Evaluation</head><p>99% accuracy on OneStopEnglish <ref type="table" target="#tab_9">(table 5)</ref> shows that our model is capable of almost perfectly capturing the linguistic properties relating to readability on certain datasets. This is a positive and abnormally quick improvement, considering that the reported RA accuracies have never exceeded 90% on popular datasets <ref type="bibr" target="#b14">Xu et al., 2015;</ref><ref type="bibr" target="#b13">Xia et al., 2016;</ref><ref type="bibr" target="#b8">Vajjala and Lu?i?, 2018)</ref> until 2021. Since the reported in-domain accuracies in RA had much room for improvement, we were not at the stage to be seriously concerned about cross-domain evaluation <ref type="bibr" target="#b4">(?tajner and Nisioi, 2018)</ref> in this paper. It would be very favorable to run an extra crossdomain evaluation (which we believe to be a nextlevel topic). But realistically, performing a crossdomain evaluation requires a thorough study on at least two datasets, which is potentially out of scope in this research. The readability classes/levels are labeled by a few human experts, making the standards vary among datasets. To make two datasets suitable for cross-domain evaluation, much effort is needed to connect the two, such as the class mapping used in <ref type="bibr" target="#b13">Xia et al. (2016)</ref>. However, it should be noted for future researchers that the notion of domain overfitting is indeed a common problem faced in RA, which often uses one dataset for train/test/validation. Without a new methodology to connect several datasets or a new large public dataset for RA, it will forever be challenging to develop a RA model for general use <ref type="bibr">(Vajjala, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have reported the four contributions mentioned in section 1. We checked that the new advanced semantic features add orthogonal information to the model. Further, we created hybrid models (table 5) that achieved SOTA results. RoBERTA-RF-T1 achieved 99% accuracy on OneStopEnglish, and BERT-GB-T1 beat the previous SOTA on WeeBit using only 30% of the original train data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">As a Gentle Reminder</head><p>To the general NLP community, the most prominent characteristic of our proposed method might be that we utilize handcrafted features and traditional ML models, which are often considered "outdated." Interestingly, these outdated methods maintained SOTA in <ref type="bibr">RA until Martinc et al. (2021)</ref> utilized BERT (as already discussed).</p><p>The findings we report are not limited to the technical innovations that achieved the new SOTA. Rather, we want to stress that: 1. there are still many areas in NLP that insist on traditional methodologies, which potentially hinders the improvement in model accuracy, 2. but we must also take time to look back on these outdated methods and their linguistic values. If we achieved anything meaningful through this research, it was possible because we realized the abovementioned two situations.  <ref type="bibr">2,</ref><ref type="bibr">1,</ref><ref type="bibr">0.5,</ref><ref type="bibr">0.3,</ref><ref type="bibr">4,</ref><ref type="bibr">1,</ref><ref type="bibr">2,</ref><ref type="bibr">1,</ref><ref type="bibr">1,</ref><ref type="bibr">0.6,</ref><ref type="bibr">1.5,</ref><ref type="bibr">1.5,</ref><ref type="bibr">1.5,</ref><ref type="bibr">1.5,</ref><ref type="bibr">1.5 High 325 Low 8.34 Low 13.3</ref>   <ref type="figure" target="#fig_0">fig. 1</ref>.</p><p>Semantic Richness. List 4 and list 5 have the same lengths. However, list 5 contains more meaningful topics (? p) throughout the list, resulting in higher overall semantic richness. As such, semantic richness rewards long probability lists (? n) with more meaningful (? p) topics. Similarly, list 3 (? n,? p) has higher richness than list 2 (? n,? p).</p><p>Semantic Clarity. List 3 and list 4 have the same max(p) and two other same elements <ref type="bibr">(1)</ref>. However, the second element in list 3 is the same as the first element, resulting in increased difficulty in identifying the main topic (max(p)). Likewise, semantic clarity rewards the deviation between the max(p) and the other elements &amp; short probability lists (? n). Hence, list 1 has the highest clarity.</p><p>Semantic Noise. List 2 and list 4 have the same lengths of 6 elements. However, list 2 contains more extraneous topics (? p), resulting in higher semantic noise. As such, semantic noise rewards longer lists (? n) with more extraneous elements (? p). As a result, list 5 has the least semantic noise.         </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Features, Codes, and Definitions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Rules Behind Feature Codes</head><p>In table 8?22, "Code" columns show feature codes. The related linguistic features appear with quite a number of variations across academia, without a naming convention <ref type="bibr" target="#b21">(Zhu et al., 2009;</ref><ref type="bibr">Fitzsimmons et al., 2010;</ref><ref type="bibr" target="#b5">Tanaka-Ishii et al., 2010;</ref><ref type="bibr">Daowadung and Chen, 2011;</ref><ref type="bibr" target="#b10">Vajjala and Meurers, 2013;</ref><ref type="bibr">Ciobanu et al., 2015;</ref><ref type="bibr" target="#b17">Zhang et al., 2019;</ref><ref type="bibr">Blinova et al., 2020;</ref><ref type="bibr">Lee and Lee, 2020b)</ref>. For consistency, we set ourselves a few naming rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details, External Models</head><p>We use Online LDA implemented by Gensim v4.0 <ref type="bibr">(?eh??ek and Sojka, 2010)</ref>. For most general tasks, including sentence/entity recognition, POS tagging, and dependency parsing, we use spaCy v3.0 8 <ref type="bibr">(Honnibal et al., 2020)</ref> with en_core_web_sm pretrained model. For constituency parsing, we use CRF parser <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref> in SuPar v1.0 9 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Transformers</head><p>For transformers, we use the following models from HuggingFace transformers v4.5.0 <ref type="bibr" target="#b12">(Wolf et al., 2020</ref>). 1. bert-base-uncased 2. roberta-base 3. bart-base 4. xlnet-base-cased</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Non-Neural Models</head><p>For non-neural models, we use the following models from from SciKit-Learn v0.24.1. 1. support vector classifiers (svm.SVC) <ref type="bibr">(Hearst, 1998;</ref><ref type="bibr">Platt, 1999;</ref><ref type="bibr">Chang and Lin, 2011)</ref> 2. random forest classifiers (ensemble.RandomF orestClassifier) (Breiman, 2001) 3. logistic regression (linear_model.LogisticRegr ession)</p><p>For gradient boosting, we use the following from XGBoost v1.4.0 (Chen and Guestrin, 2016). 4. gradient boosting (XGBclassifier)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Preprocessing</head><p>Our preprocessing steps are inspired by <ref type="bibr">Martinc et al. (2021)</ref> and several other existing RA research. These steps are used only during the extraction of handcrafted features for increased accuracy. 1. remove all special characters 2. remove words less than 3 characters 3. lowercase all 4. tokenize 5. remove NLTK default stopwords F Full Hyperparameters F.1 Non-Neural, Traditional ML We perform grid search on the hyperparameters (table 3) after performing a large randomized search to identify the sensible range of hyperparameters to tune. In particular, logistic regression solver hyperparameter search include libfgs <ref type="bibr" target="#b20">(Zhu et al., 2011</ref><ref type="bibr">), liblinear (Fan et al., 2008</ref>, SAG <ref type="bibr" target="#b1">(Schmidt et al., 2017)</ref>, and SAGA <ref type="bibr">(Defazio et al., 2014)</ref>.</p><p>In table 3(a), C is the regularization parameter, G is the kernel coefficient (gamma), and K is the 8 github.com/explosion/spaCy 9 github.com/yzhangcs/parser  kernel. In table <ref type="bibr">3(b)</ref>, nEst is the number of trees, MDep is the max depth of a tree, and Mfea is the number of features considered. In table 3(c), eta is the learning rate, G is the minimum loss reduction need to make a further partition on the leaf node (gamma), and MDep is the max depth of a tree. In table 4(d), C is the inverse of the regularization strength, Pen is the norm used in penalization, and Solver is the algorithm used in optimization. The other parameters best performed at default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Neural, Transformers</head><p>We use AdamW (optimizer) (Kingma and Ba, 2014), linear (scheduler), 10% (warmup steps), 8 (batch size), 3 (epoch) for all tested transformers. We use the learning rate of 2e-5 for BERT and 3e-5 for the other three transformers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Full Explored Feature Combinations</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Graphical representation. Semantic Richness, Clarity, and Noise. abbrev: abbreviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Hybrid model. AdSem, Disco, LxSem, Synta, and ShaTr show handcrafted features' linguistic branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>As shown in section 3, myriad linguistic properties affect readability. Despite the continual effort at handcrafting features, they lack coverage. Deutsch et al. (2020) hint neural models can better model the linguistic properties for RA task. But the performance/flexibility of neural models could improve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, Fili-19 ? Filighera et al. (2019), Mar-21 ? Martinc et al. (2021).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Performance Change, WeeBit Data Size tor like SVM. But an existing example of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>is that our BERT implementation performed much better on WeeBit than what was reported. The dataset preparation methods and overall evaluation settings are the same or very similar across ours (accuracy: 89.3%), Deutsch et al. (2020)'s (accuracy: 83.9%), and Martinc et al. (2021)'s (accuracy: 85.7%). We believe that the differences stem from the hyperparameters. Notably, Deutsch et al. (2020) uses 128 input sequence length. This is ineffective as the downsampled WeeBit has 2374 articles of over 128 tokens but only 275 articles of over 512 tokens (which was our input sequence length). Hence, we can reasonably think that much semantic information was lost in Deutsch et al. (2020)'s implementation. Martinc et al. (2021) uses 512 input sequence length but lacks a report on other possibly critical hyperparameters, and we cannot compare in detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Statistics for datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>table 2 )</head><label>2</label><figDesc>, we perform grid searches on WeeBit validation sets to identify</figDesc><table><row><cell cols="2">Corpus</cell><cell cols="4">BERT RoBERTa BART XLNet</cell></row><row><cell></cell><cell cols="2">Accuracy 0.893</cell><cell>0.900</cell><cell>0.889</cell><cell>0.881</cell></row><row><cell></cell><cell>F1</cell><cell>0.893</cell><cell>0.900</cell><cell>0.889</cell><cell>0.880</cell></row><row><cell>WeeBit</cell><cell cols="2">Precision 0.896</cell><cell>0.902</cell><cell>0.892</cell><cell>0.881</cell></row><row><cell></cell><cell>Recall</cell><cell>0.896</cell><cell>0.902</cell><cell>0.892</cell><cell>0.881</cell></row><row><cell></cell><cell>QWK</cell><cell>0.966</cell><cell>0.970</cell><cell>0.963</cell><cell>0.966</cell></row><row><cell></cell><cell cols="2">Accuracy 0.801</cell><cell>0.965</cell><cell>0.968</cell><cell>0.804</cell></row><row><cell></cell><cell>F1</cell><cell>0.793</cell><cell>0.965</cell><cell>0.968</cell><cell>0.794</cell></row><row><cell>OneStopE</cell><cell cols="2">Precision 0.815</cell><cell>0.968</cell><cell>0.970</cell><cell>0.810</cell></row><row><cell></cell><cell>Recall</cell><cell>0.814</cell><cell>0.968</cell><cell>0.970</cell><cell>0.810</cell></row><row><cell></cell><cell>QWK</cell><cell>0.840</cell><cell>0.942</cell><cell>0.952</cell><cell>0.845</cell></row><row><cell></cell><cell cols="2">Accuracy 0.573</cell><cell>0.680</cell><cell>0.620</cell><cell>0.573</cell></row><row><cell></cell><cell>F1</cell><cell>0.517</cell><cell>0.658</cell><cell>0.598</cell><cell>0.554</cell></row><row><cell>Cambridge</cell><cell cols="2">Precision 0.528</cell><cell>0.693</cell><cell>0.643</cell><cell>0.591</cell></row><row><cell></cell><cell>Recall</cell><cell>0.525</cell><cell>0.693</cell><cell>0.643</cell><cell>0.591</cell></row><row><cell></cell><cell>QWK</cell><cell>0.809</cell><cell>0.881</cell><cell>0.835</cell><cell>0.832</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Best performances, neural models.</figDesc><table /><note>the well-performing hyperparameters based on 5- fold mean accuracy. Once identified, we used the same configuration for all the other corpora and performed no corpus-specific tweaking. We search the learning rates of [1e-5, 2e-5, 4e-5, 1e-4] and the batch sizes of [8, 16, 32]. The input sequence lengths are all set at 512, and we used Adam op- timizer. Last, we fine-tuned the model for three epochs. Full hyperparameters are in appendix F. In table 2, RoBERTa &amp; BART beat BERT &amp; XL- Net on most metrics. Martinc et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Best performances, non-neural models.</figDesc><table><row><cell>Subgr</cell><cell>Model LogR SVM</cell><cell>Subgr</cell><cell>Model LogR SVM</cell><cell>Subgr</cell><cell>Model LogR SVM</cell></row><row><cell cols="2">EnDF 0.442 0.374</cell><cell cols="2">TraF 0.513 0.620</cell><cell cols="2">TraF 0.640 0.593</cell></row><row><cell cols="2">ShaF 0.404 0.409</cell><cell cols="2">PsyF 0.437 0.696</cell><cell cols="2">WorF 0.613 0.593</cell></row><row><cell cols="2">TrSF 0.396 0.360</cell><cell cols="2">PhrF 0.429 0.608</cell><cell cols="2">ShaF 0.600 0.587</cell></row><row><cell cols="2">POSF 0.394 0.513</cell><cell cols="2">VarF 0.409 0.626</cell><cell cols="2">VarF 0.600 0.533</cell></row><row><cell cols="2">WorF 0.391 0.387</cell><cell cols="2">TrSF 0.391 0.614</cell><cell cols="2">PsyF 0.593 0.620</cell></row><row><cell cols="2">PsyF 0.378 0.437</cell><cell cols="2">WorF 0.387 0.637</cell><cell cols="2">POSF 0.553 0.407</cell></row><row><cell cols="2">WoKF 0.367 0.369</cell><cell cols="2">OSKF 0.359 0.605</cell><cell cols="2">WoKF 0.540 0.433</cell></row><row><cell cols="2">(a) WeeBit</cell><cell cols="2">(b) OneStopEnglish</cell><cell cols="2">(c) Cambridge</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Top 7 Feature Subgroups.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>table 4 ,</head><label>4</label><figDesc>we report the top 7 (upper half) by accuracy on WeeBit. The result is obtained SVM LSTM BERT HAN GB-T1 BERT GB RF-T1 RBRT RF RF-T1 BART RF RF-P3 XLNet RF WeeBit Accuracy 0.803 0.813 0.857 0.752 0.895 0.002 0.257 0.902 0.002 0.264 0.905 0.016 0.267 0.892 0.011 0.254 F1 --0.866 0.753 0.895 0.002 0.268 0.902 0.002 0.276 0.905 0.016 0.279 0.892 0.012 0.266 Precision --0.857 0.752 0.897 0.001 0.241 0.903 0.001 0.258 0.905 0.013 0.260 0.893 0.012 0.248 Xia-16 (Cambridge) uses semi-supervised learning (self-training) on a larger corpus to increase performance.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Model</cell></row><row><cell cols="2">Corpus</cell><cell cols="4">Baselines, Previous Studies Xia-16 Fili-19 Mar-21</cell><cell>BERT hybrid ?</cell><cell>RoBERTa ? hybrid ?</cell><cell>BART ? hybrid ?</cell><cell>XLNet ? hybrid ?</cell><cell>?</cell></row><row><cell></cell><cell>Recall</cell><cell>-</cell><cell>-</cell><cell cols="4">0.858 0.752 0.897 0.001 0.259 0.903 0.001 0.265 0.904 0.012 0.266 0.892 0.011 0.254</cell></row><row><cell></cell><cell>QWK</cell><cell>-</cell><cell>-</cell><cell cols="4">0.953 0.886 0.969 0.001 0.277 0.971 0.001 0.268 0.968 0.005 0.265 0.966 0.000 0.263</cell></row><row><cell></cell><cell>Accuracy</cell><cell>-</cell><cell>-</cell><cell cols="4">0.674 0.787 0.982 0.181 0.263 0.990 0.025 0.281 0.971 0.003 0.262 0.848 0.044 0.139</cell></row><row><cell></cell><cell>F1</cell><cell>-</cell><cell>-</cell><cell cols="4">0.740 0.798 0.982 0.189 0.281 0.995 0.030 0.289 0.971 0.003 0.265 0.848 0.050 0.142</cell></row><row><cell>OneStopE</cell><cell>Precision</cell><cell>-</cell><cell>-</cell><cell cols="4">0.674 0.787 0.983 0.168 0.249 0.995 0.027 0.269 0.972 0.002 0.246 0.852 0.042 0.126</cell></row><row><cell></cell><cell>Recall</cell><cell>-</cell><cell>-</cell><cell cols="4">0.677 0.789 0.982 0.168 0.263 0.996 0.028 0.287 0.971 0.001 0.262 0.848 0.038 0.139</cell></row><row><cell></cell><cell>QWK</cell><cell>-</cell><cell>-</cell><cell cols="4">0.708 0.825 0.973 0.133 0.610 0.996 0.054 0.562 0.952 0.000 0.518 0.855 0.010 0.369</cell></row><row><cell></cell><cell cols="2">Accuracy 0.786  *  *</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.687 0.114 0.002 0.763 0.083 0.090 0.727 0.107 0.054 0.687 0.114 0.014</cell></row><row><cell></cell><cell>F1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.682 0.165 0.001 0.752 0.094 0.089 0.727 0.129 0.064 0.676 0.122 0.013</cell></row><row><cell>Cambridge</cell><cell>Precision</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.732 0.204 0.031 0.792 0.099 0.096 0.760 0.117 0.064 0.710 0.119 0.014</cell></row><row><cell></cell><cell>Recall</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.687 0.162 0.013 0.753 0.060 0.080 0.727 0.084 0.054 0.687 0.096 0.014</cell></row><row><cell></cell><cell>QWK</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.873 0.064 0.021 0.919 0.038 0.039 0.889 0.054 0.009 0.888 0.056 0.008</cell></row></table><note>**</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Best performances, hybrid models.</figDesc><table><row><cell>Set Features</cell><cell>LogR</cell></row><row><cell>T1 AdSem+Disco+Synta+LxSem+ShaTr</cell><cell>0.622</cell></row><row><cell cols="2">P3 ShaTr+EnDF+TrSF+POSF+WorF+PsyF+TraF+VarF 0.647</cell></row></table><note>* Note: 5 letters (e.g. AdSem) mean linguistic branch. 4 letters (e.g. PhrF) mean subgroup. We report accuracy on WeeBit.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Best feature sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993-1022. Tim Steuer, and Christoph Rensing. 2019. Automatic text difficulty estimation using embeddings and neural networks. In European Conference on Technology Enhanced Learning, pages 335-348. Springer. Kincaid, R. P. Fishburne, R. L. Rogers, and B. S. Chissom. 1975. Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. Xiaofei Lu. 2010. Automatic analysis of syntactic complexity in second language writing. International journal of corpus linguistics, 15(4):474-496. Christian Soto, M?nica Veliz, Bernardo Riffo, and Antonio Guti?rrez. 2019. A data-driven methodology to assess text complexity based on syntactic and semantic measurements. In International Conference on Human Interaction and Emerging Technologies, pages 509-515. Springer.</figDesc><table><row><cell>OV Blinova, Tarasov NA, Modina VV, and IS Blekanov. 2020. Modeling lemma frequency bands for lexical complexity assessment of russian texts1. In Computational Linguistics and Intellec-tual Technologies: Proceedings of the International Conference "Dialogue 2020", Moscow, June 17-20, 2020, pages 76-92. Leo Breiman. 2001. Random forests. Machine learn-ing, 45(1):5-32. Marc Brysbaert and Boris New. 2009. Moving beyond ku?era and francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for ameri-can english. Behavior research methods, 41(4):977-990. Lori Buchanan, Chris Westbury, and Curt Burgess. 2001. Characterizing semantic space: Neighbor-hood effects in word recognition. Psychonomic Bul-letin &amp; Review, 8(3):531-544. Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: a library for support vector machines. ACM transac-tions on intelligent systems and technology (TIST), 2(3):1-27. Jing Chen, James H Fife, Isaac I Bejar, and Andr? A Rupp. 2016. Building e-rater? scoring models us-ing machine learning methods. ETS Research Re-port Series, 2016(1):1-12. Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowl-Alina Maria Ciobanu, Liviu P Dinu, and Flaviu Pepelea. 2015. Readability assessment of translated texts. In Proceedings of the International Conference Recent Advances in Natural Language Processing, pages 97-103. Kevyn Collins-Thompson. 2014. Computational as-and future research. ITL-International Journal of Applied Linguistics, 165(2):97-135. Kevyn Collins-Thompson and James P Callan. 2004. A language modeling approach to predicting reading technology conference of the North American chap-ter of the association for computational linguistics: HLT-NAACL 2004, pages 193-200. Ronan Collobert, Jason Weston, L?on Bottou, Michael Thomas Holtgraves. 1999. Comprehending indirect replies: When and how are their conveyed mean-ings activated? Journal of Memory and Language, Mining Society. 41(4):519-540. word knowledge. International Educational Data vocabulary learning via semantic features of partial Thompson. 2017. Predicting short-and long-term SungJin Nam, Gwen Frishkoff, and Kevyn Collins-difficulty. In Proceedings of the human language York. Association for Computational Linguistics. Matthew Hoffman, Francis R Bach, and David M Blei. 2010. Online learning for latent dirichlet allocation. In advances in neural information processing sys-tems, pages 856-864. Citeseer. neering (ICEE), pages 1-7. IEEE. 2020 28th Iranian Conference on Electrical Engi-ability assessment using a crowdsourced dataset. In A machine learning approach to persian text read-Hamid Mohammadi and Seyed Hossein Khasteh. 2020. sessment of text readability: A survey of current Michael Heilman, Kevyn Collins-Thompson, Jamie Callan, and Maxine Eskenazi. 2007. Combining lex-ical and grammatical features to improve readability measures for first and second language texts. In Hu-man Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 460-467, Rochester, New arXiv:1301.3781. representations in vector space. arXiv preprint frey Dean. 2013. Efficient estimation of word Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-33-49, Cham. Springer International Publishing. ysis. In Advances in Information Retrieval, pages former framework for web article readability anal-edge discovery and data mining, pages 785-794. Camille Guinaudeau and Michael Strube. 2013. Graph-of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 93-103. John Hale. 2016. Information-theoretical complex-ity metrics. Language and Linguistics Compass, 10(9):397-412. Hieronymus Hansen, Adam Widera, Johannes Ponge, readability assessment and text simplification in cri-sis communication: A systematic review. In Pro-ceedings of the 54th Hawaii International Confer-ence on System Sciences, page 2265. Marti A. Hearst. 1998. Support vector machines. IEEE Intelligent Systems, 13(4):18-28. nifer Neville. 2020. Readnet: A hierarchical trans-Changping Meng, Muhao Chen, Jie Mao, and Jen-Danielle S McNamara, Max M Louwerse, Philip M Discourse Processes, 47(4):292-330. metrix: Capturing linguistic features of cohesion. McCarthy, and Arthur C Graesser. 2010. Coh-and Bernd Hellingrath. 2021. Machine learning for research methods, 42(2):381-392. proaches to lexical diversity assessment. Behavior d, and hd-d: A validation study of sophisticated ap-Philip M McCarthy and Scott Jarvis. 2010. Mtld, vocd-ability formula. Journal of reading, 12(8):639-646. G Harry Mc Laughlin. 1969. Smog grading-a new read-Linguistics, pages 1-39. based local coherence modeling. In Proceedings ral Approaches to Text Readability. Computational ?ikonja. 2021. Supervised and Unsupervised Neu-Beno?t Fr?nay, Ata Kab?n, et al. 2014. A comprehen-of syntactic dependencies. Cognition, 68(1):1-76. Matej Martinc, Senja Pollak, and Marko Robnik-Edward Gibson. 1998. Linguistic complexity: Locality guistics. sive introduction to label noise. In ESANN. Citeseer. of lexical richness. The encyclopedia of applied lin-David Malvern and Brian Richards. 2012. Measures nal of the Royal College of Physicians of Edinburgh, 40(4):292-296. Thomas Fran?ois. 2014. An analysis of a french as a foreign language corpus for readability assessment. In Proceedings of the third workshop on NLP for computer-assisted language learning, pages 13-32. Ranking-based readability assessment for early pri-man Language Technologies, pages 548-552. the Association for Computational Linguistics: Hu-2012 Conference of the North American Chapter of mary children's literature. In Proceedings of the Yi Ma, Eric Fosler-Lussier, and Robert Lofthus. 2012. Anna Filighera, Paul R Fitzsimmons, BD Michael, Joane L Hulley, and G Orville Scott. 2010. A readability assessment of online parkinson's disease information. The jour-Xiaofei Lu. 2011. A corpus-based evaluation of syntac-tic complexity measures as indices of college-level esl writers' language development. TESOL quar-terly, 45(1):36-62. A Trend, Advanced Semantic Features Sorted Probability List R. out C. out N. out 9, 0.5, 0.5 Low 115 High 56.7 H-M 30.0 6,</cell><cell>Roberta: A robustly optimized bert pretraining ap-Rani Qumsiyeh and Yiu-Kai Ng. 2011. Readaid: a Citeseer. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. GIN CLASSIFIERS, pages 61-74. MIT Press. lihood methods. In ADVANCES IN LARGE MAR-vector machines and comparisons to regularized like-John C. Platt. 1999. Probabilistic outputs for support for Computational Linguistics. Ping Li, Curt Burgess, and Kevin Lund. 2000. The acquisition of word meaning through global lexical co-occurrences. In Proceedings of the thirtieth an-nual child language research forum, pages 166-178. Computational Linguistics. pages 186-195, Honolulu, Hawaii. Association for pirical Methods in Natural Language Processing, ity. In Proceedings of the 2008 Conference on Em-ability: A unified framework for predicting text qual-Linguistics, pages 7871-7880, Online. Association Emily Pitler and Ani Nenkova. 2008. Revisiting read-nual Meeting of the Association for Computational guistics, 24(4):319-349. and comprehension. In Proceedings of the 58th An-uating text readability. Journal of Quantitative Lin-training for natural language generation, translation, Towards the definition of linguistic metrics for eval-2020. BART: Denoising sequence-to-sequence pre-Carla Pires, Afonso Cavaco, and Marina Vig?rio. 2017. Levy, Veselin Stoyanov, and Luke Zettlemoyer. jan Ghazvininejad, Abdelrahman Mohamed, Omer chonomic Bulletin &amp; Review, 15(1):161-167. Mike Lewis, Yinhan Liu, Naman Goyal, Mar-semantic richness on visual word recognition. Psy-Scott A Crossley, Jerry Greenfield, and Danielle S Mc-Namara. 2008. Assessing text readability using cog-nitively based indices. Tesol Quarterly, 42(3):475-493. Patcharanut Daowadung and Yaw-Huei Chen. 2011. Using word segmentation and svm to assess readabil-ity of thai text for primary school students. In 2011 Science and Software Engineering (JCSSE), pages 170-174. IEEE. Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. 2014. Saga: A fast incremental gradient method with support for non-strongly convex com-Tovly Deutsch, Masoud Jasbi, and Stuart Shieber. 2020. Linguistic features for readability assessment. In Proceedings of the Fifteenth Workshop on Innovative pages 1-17, Seattle, WA, USA Online. Associa-tion for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ-ation for Computational Linguistics. preprint arXiv:2008.01564. Gondy Leroy and David Kauchak. 2013. The effect of many ways to be rich: Effects of three measures of Association, 21(e1):e169-e172. Glen E Bodner, and Jamie Pope. 2008. There are culty. Journal of the American Medical Informatics Penny M Pexman, Ian S Hargreaves, Paul D Siakaluk, word familiarity on actual and perceived text diffi-put. Speech Lang., 23:89-106. learning approach to reading level assessment. Com-S. E. Petersen and Mari Ostendorf. 2009. A machine sessment model for efl students in korea. arXiv Bruce W. Lee and Jason Lee. 2020a. LXPER index for L2 English students in Korea. In Proceedings of the 6th Workshop on Natural Language Processing 24, Suzhou, China. Association for Computational Linguistics. Bruce W Lee and Jason Hyung-Jong Lee. 2020b. Lx-per index: a curriculum-specific text readability as-for Computational Linguistics. 2227-2237, New Orleans, Louisiana. Association guage Technologies, Volume 1 (Long Papers), pages ation for Computational Linguistics: Human Lan-ence of the North American Chapter of the Associ-Techniques for Educational Applications, pages 20-resentations. In Proceedings of the 2018 Confer-Zettlemoyer. 2018. Deep contextualized word rep-Gardner, Christopher Clark, Kenton Lee, and Luke 2.0: Improving text readability assessment model Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Qatar. Association for Computational Linguistics. 108. Processing (EMNLP), pages 1532-1543, Doha, Use of NLP for Building Educational Applications, knowledge. Scientific Studies of Reading, 15(1):92-ence on Empirical Methods in Natural Language cione. 2011. Word maturity: A new metric for word representation. In Proceedings of the 2014 Confer-Thomas K Landauer, Kirill Kireyev, and Charles Panac-Manning. 2014. GloVe: Global vectors for word for 30,000 english words. Behavior research meth-Jeffrey Pennington, Richard Socher, and Christopher ods, 44(4):978-990. Learning research, 12:2825-2830. posite objectives. arXiv preprint arXiv:1407.0202. standard probability and statistics tables and formu-lae. Crc Press. Victor Kuperman, Hans Stadthagen-Gonzalez, and Marc Brysbaert. 2012. Age-of-acquisition ratings Machine learning in python. the Journal of machine Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Grisel, Mathieu Blondel, Peter Prettenhofer, Ron fort, Vincent Michel, Bertrand Thirion, Olivier Fabian Pedregosa, Ga?l Varoquaux, Alexandre Gram-Stephen Kokoska and Daniel Zwillinger. 2000. CRC Journal of Play Therapy, 25(4):197. Eighth International Joint Conference on Computer J. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. wards semantic clarity in play therapy. International Diego Palma, Mary Anne Peabody and Charles E Schaefer. 2016. To-</cell></row><row><cell>Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Matthew Honnibal, Ines Montani, Sofie Van Lan-</cell><cell>proach. arXiv preprint arXiv:1907.11692. robust and fully-automated readability assessment</cell></row><row><cell>2011. Natural language processing (almost) from deghem, and Adriane Boyd. 2020. spaCy: Diego Palma and John Atkinson. 2018. Coherence-</cell><cell>tool. In 2011 IEEE 23rd International Conference</cell></row><row><cell>scratch. Journal of Machine Learning Research, Industrial-strength Natural Language Processing in based automatic essay assessment. IEEE Intelligent</cell><cell>on Tools with Artificial Intelligence, pages 539-546.</cell></row><row><cell>12(76):2493-2537. Python. Systems, 33(5):26-36.</cell><cell>IEEE.</cell></row></table><note>Edward Loper and Steven Bird. 2002. Nltk: The natu- ral language toolkit. arXiv preprint cs/0205028.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Trends. Richness, Clarity, Noise. All numbers</figDesc><table><row><cell>?10 for conciseness. L-M: Low-Mid. H-M: High-Mid.</cell></row><row><cell>In table 7, we name each list as 1 ? 5 from top to</cell></row><row><cell>bottom. "out" refers to raw output from equations.</cell></row><row><cell>See what the sorted probabilities list is in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Wikipedia Knowledge Features (WoKF). Code Definition 17 BRich05_S Richness, 50 topics extracted from WeeBit Corpus 18 BClar05_S Clarity, 50 topics extracted from WeeBit Corpus 19 BNois05_S Noise, 50 topics extracted from WeeBit Corpus 20 BTopc05_S # of topics, 50 topics extracted from WeeBit Corpus 21 BRich10_S Richness, 100 topics extracted from WeeBit Corpus 22 BClar10_S Clarity, 100 topics extracted from WeeBit Corpus 23 BNois10_S Noise, 100 topics extracted from WeeBit Corpus 24 BTopc10_S # of topics, 100 topics extracted from WeeBit Corpus 25 BRich15_S Richness, 150 topics extracted from WeeBit Corpus 26 BClar15_S Clarity, 150 topics extracted from WeeBit Corpus 27 BNois15_S Noise, 150 topics extracted from WeeBit Corpus 28 BTopc15_S # of topics, 150 topics extracted from WeeBit Corpus 29 BRich20_S Richness, 200 topics extracted from WeeBit Corpus 30 BClar20_S Clarity, 200 topics extracted from WeeBit Corpus 31 BNois20_S Noise, 200 topics extracted from WeeBit Corpus 32 BTopc20_S # of topics, 200 topics extracted from WeeBit Corpus</figDesc><table><row><cell>idx</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>WeeBit Knowledge Features (WBKF). of topics, 100 topics extracted from OneStop Corpus 41 ORich15_S Richness, 150 topics extracted from OneStop Corpus 42 OClar15_S Clarity, 150 topics extracted from OneStop Corpus 43 ONois15_S Noise, 150 topics extracted from OneStop Corpus 44 OTopc15_S # of topics, 150 topics extracted from OneStop Corpus 45 ORich20_S Richness, 200 topics extracted from OneStop Corpus 46 OClar20_S Clarity, 200 topics extracted from OneStop Corpus 47 ONois20_S Noise, 200 topics extracted from OneStop Corpus 48 OTopc20_S # of topics, 200 topics extracted from OneStop Corpus</figDesc><table><row><cell>idx Code</cell><cell>Definition</cell></row><row><cell cols="2">33 ORich05_S Richness, 50 topics extracted from OneStop Corpus</cell></row><row><cell cols="2">34 OClar05_S Clarity, 50 topics extracted from OneStop Corpus</cell></row><row><cell cols="2">35 ONois05_S Noise, 50 topics extracted from OneStop Corpus</cell></row><row><cell cols="2">36 OTopc05_S # of topics, 50 topics extracted from OneStop Corpus</cell></row><row><cell cols="2">37 ORich10_S Richness, 100 topics extracted from OneStop Corpus</cell></row><row><cell cols="2">38 OClar10_S Clarity, 100 topics extracted from OneStop Corpus</cell></row><row><cell cols="2">39 ONois10_S Noise, 100 topics extracted from OneStop Corpus</cell></row><row><cell cols="2">40 OTopc10_S #</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>OneStop Knowledge Features (OSKF).</figDesc><table><row><cell>idx Code</cell><cell>Definition</cell></row><row><cell cols="2">49 to_EntiM_C total number of Entities Mentions</cell></row><row><cell cols="2">50 as_EntiM_C average number of Entities Mentions per sentence</cell></row><row><cell cols="2">51 at_EntiM_C average number of Entities Mentions per token (word)</cell></row><row><cell cols="2">52 to_UEnti_C total number of unique Entities</cell></row><row><cell cols="2">53 as_UEnti_C average number of unique Entities per sentence</cell></row><row><cell cols="2">54 at_UEnti_C average number of unique Entities per token (word)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 18 :</head><label>18</label><figDesc>Type Token Ratio Features (TTRF)</figDesc><table><row><cell>idx Code</cell><cell>Definition</cell></row><row><cell cols="2">203 to_AAKuW_C total AoA (Age of Acquisition) of words, Kuperman</cell></row><row><cell cols="2">204 as_AAKuW_C average AoA of words per sentence, Kuperman</cell></row><row><cell cols="2">205 at_AAKuW_C average AoA of words per token, Kuperman</cell></row><row><cell cols="2">206 to_AAKuL_C total AoA of lemmas, Kuperman</cell></row><row><cell cols="2">207 as_AAKuL_C average AoA of lemmas per sentence, Kuperman</cell></row><row><cell cols="2">208 at_AAKuL_C average AoA of lemmas per token, Kuperman</cell></row><row><cell cols="2">209 to_AABiL_C total AoA of lemmas, Bird norm</cell></row><row><cell cols="2">210 as_AABiL_C average AoA of lemmas, Bird norm per sent</cell></row><row><cell>211 at_AABiL_C</cell><cell>average AoA of lemmas, Bird norm per token</cell></row><row><cell cols="2">212 to_AABrL_C total AoA of lemmas, Bristol norm</cell></row><row><cell cols="2">213 as_AABrL_C average AoA of lemmas, Bristol norm per sent</cell></row><row><cell cols="2">214 at_AABrL_C average AoA of lemmas, Bristol norm per token</cell></row><row><cell cols="2">215 to_AACoL_C total AoA of lemmas, Cortese and Khanna norm</cell></row><row><cell cols="2">216 as_AACoL_C average AoA of lem, Cortese and K norm per sent</cell></row><row><cell cols="2">217 at_AACoL_C average AoA of lem, Cortese and K norm per token</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 19 :</head><label>19</label><figDesc>Psychollinguistic Features (PsyF)</figDesc><table><row><cell>idx Code</cell><cell>Definition</cell></row><row><cell cols="2">218 to_SbFrQ_C total SubtlexUS FREQcount value</cell></row><row><cell cols="2">219 as_SbFrQ_C average SubtlexUS FREQcount value per sentence</cell></row><row><cell cols="2">220 at_SbFrQ_C average SubtlexUS FREQcount value per token</cell></row><row><cell cols="2">221 to_SbCDC_C total SubtlexUS CDcount value</cell></row><row><cell cols="2">222 as_SbCDC_C average SubtlexUS CDcount value per sent</cell></row><row><cell cols="2">223 at_SbCDC_C average SubtlexUS CDcount value per token</cell></row><row><cell>224 to_SbFrL_C</cell><cell>total SubtlexUS FREQlow value</cell></row><row><cell cols="2">225 as_SbFrL_C average SubtlexUS FREQlow value per sent</cell></row><row><cell>226 at_SbFrL_C</cell><cell>average SubtlexUS FREQlow value per token</cell></row><row><cell cols="2">227 to_SbCDL_C total SubtlexUS CDlow value</cell></row><row><cell cols="2">228 as_SbCDL_C average SubtlexUS CDlow value per sent</cell></row><row><cell cols="2">229 at_SbCDL_C average SubtlexUS CDlow value per token</cell></row><row><cell cols="2">230 to_SbSBW_C total SubtlexUS SUBTLWF value</cell></row><row><cell cols="2">231 as_SbSBW_C average SubtlexUS SUBTLWF value per sent</cell></row><row><cell cols="2">232 at_SbSBW_C average SubtlexUS SUBTLWF value per token</cell></row><row><cell cols="2">233 to_SbL1W_C total SubtlexUS Lg10WF value</cell></row><row><cell cols="2">234 as_SbL1W_C average SubtlexUS Lg10WF value per sent</cell></row><row><cell cols="2">235 at_SbL1W_C average SubtlexUS Lg10WF value per token</cell></row><row><cell cols="2">236 to_SbSBC_C total SubtlexUS SUBTLCD value</cell></row><row><cell cols="2">237 as_SbSBC_C average SubtlexUS SUBTLCD value per sent</cell></row><row><cell cols="2">238 at_SbSBC_C average SubtlexUS SUBTLCD value per token</cell></row><row><cell cols="2">239 to_SbL1C_C total SubtlexUS Lg10CD value</cell></row><row><cell cols="2">240 as_SbL1C_C average SubtlexUS Lg10CD value per sent</cell></row><row><cell cols="2">241 at_SbL1C_C average SubtlexUS Lg10CD value per token</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 20 :</head><label>20</label><figDesc>Word Familiarity Features (WorF)idx Code Definition 242 TokSenM_S total count of tokens x total count of sentence 243 TokSenS_S sqrt(total count of tokens x total count of sentence) 244 TokSenL_S log(total count of tokens)/log(total count of sent) 245 as_Token_C average count of tokens per sentence 246 as_Sylla_C average count of syllables per sentence 247 at_Sylla_C average count of syllables per token 248 as_Chara_C average count of characters per sentence 249 at_Chara_C average count of characters per token</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 21 :</head><label>21</label><figDesc>Shallow Features (ShaF) idx Code Definition 250 SmogInd_S Smog Index 251 ColeLia_S Coleman Liau Readability Score 252 Gunning_S Gunning Fog Count Score (New, US Navy Report) 253 AutoRea_S Automated Readability Idx (New, US Navy Report) 254 FleschG_S Flesch Kincaid Grade Level (New, US Navy Report) 255 LinseaW_S Linsear Write Formula Score</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 22 :</head><label>22</label><figDesc>Shallow Features (ShaF)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 23 :</head><label>23</label><figDesc>Hyperparameters, non-neural models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 24 :</head><label>24</label><figDesc>Defining feature sets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For simplicity, we use "handcrafted features" and "linguistic features" interchangeably throughout this paper.2  A known exception is Dr. Vajjala's Java toolkit, available at bitbucket.org/nishkalavallabhi/complexity-features.3  github.com/brucewlee/lingfeat 4 github.com/yjang43/pushingonreadability_transformers 5 github.com/brucewlee/pushingonreadability_traditional_ML</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">universaldependencies.org/u/pos</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I More on LingFeat</head><p>Throughout our paper, we mention LingFeat as one of our contributions to academia. This is because a large-scale handcrafted features extraction toolkit is scarce in RA, despite its reliance on the features.</p><p>LingFeat is a Python research package for various handcrafted linguistic features. More specifically, LingFeat is an NLP feature extraction software, which currently extracts 255 linguistic features from English string input. The package is available on both PyPI and GitHub.</p><p>Due to the wide number of supported features, we had to define subgroups (section 3) for features. Hence, features are not accessible individually. Instead, one has to call the subgroups to obtain the dictionary of the corresponding features. The corresponding code is applicable to LingFeat v.1.0. 1 """ 2 Import 3 4 this is the only import you need 5 """ 6 from lingfeat import extractor 7 8 9 """ 10 Pass text 11</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Radim?eh??ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks<address><addrLine>Valletta, Malta. ELRA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Minimizing finite sums with the stochastic average gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="112" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reading level assessment using support vector machines and statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">E</forename><surname>Schwarm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<idno type="DOI">10.3115/1219840.1219905</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A statistical model for scientific readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth international conference on Information and knowledge management</title>
		<meeting>the tenth international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="574" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A detailed evaluation of neural sequence-to-sequence models for in-domain and cross-domain text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>?tajner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiu</forename><surname>Nisioi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh international conference on language resources and evaluation</title>
		<meeting>the eleventh international conference on language resources and evaluation</meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sorting texts by readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumiko</forename><surname>Tanaka-Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Tezuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Terada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="227" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Making readability indices readable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Tonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pianta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Predicting and Improving Text Readability for target reader populations</title>
		<meeting>the First Workshop on Predicting and Improving Text Readability for target reader populations</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<idno type="arXiv">arXiv:2105.00973</idno>
		<title level="m">Sowmya Vajjala. 2021. Trends, limitations and open challenges in automatic readability assessment research</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Onestopenglish corpus: A new corpus for automatic readability assessment and text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sowmya</forename><surname>Vajjala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Lu?i?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth workshop on innovative use of NLP for building educational applications</title>
		<meeting>the thirteenth workshop on innovative use of NLP for building educational applications</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On improving the accuracy of readability classification using insights from second language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sowmya</forename><surname>Vajjala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detmar</forename><surname>Meurers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Building Educational Applications Using NLP</title>
		<meeting>the Seventh Workshop on Building Educational Applications Using NLP<address><addrLine>Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="163" to="173" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the applicability of readability models to web texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sowmya</forename><surname>Vajjala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detmar</forename><surname>Meurers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Predicting and Improving Text Readability for Target Reader Populations</title>
		<meeting>the Second Workshop on Predicting and Improving Text Readability for Target Reader Populations</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Readability-based sentence ranking for evaluating text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sowmya</forename><surname>Vajjala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detmar</forename><surname>Meurers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06009</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Text readability assessment for second language learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Kochmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-0502</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 11th Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Problems in current text simplification research: New data can help</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="283" to="297" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Combining multiple corpora for readability assessment for people with cognitive disabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Yaneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantin</forename><surname>Or?san</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Rohanian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cryptocurrency, confirmatory bias and news readability-evidence from the largest chinese cryptocurrency exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accounting &amp; Finance</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1445" to="1468" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast and accurate neural CRF constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/560</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4046" to="4053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciyou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">Luis</forename><surname>Morales</surname></persName>
		</author>
		<title level="m">L-bfgs-b. Retrieved Feb</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A multi-dimensional model for assessing the quality of answers in social Q&amp;A sites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhemin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis. Table 11: Entity Density Features (EnDF</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grid 58 ra_SNToT_C ratio of SN transitions : total, count from Entity Grid 59 ra_OSToT_C ratio of OS transitions : total, count from Entity Grid 60 ra_OOToT_C ratio of OO transitions : total, count from Entity Grid 61 ra_OXToT_C ratio of OX transitions : total, count from Entity Grid 62 ra_ONToT_C ratio of ON transitions : total, count from Entity Grid 63 ra_XSToT_C ratio of XS transitions : total, count from Entity Grid 64 ra_XOToT_C ratio of XO transitions : total, count from Entity Grid 65 ra_XXToT_C ratio of XX transitions : total, count from Entity Grid 66 ra_XNToT_C ratio of XN transitions : total, count from Entity Grid 67 ra_NSToT_C ratio of NS transitions : total, count from Entity Grid 68 ra_NOToT_C ratio of NO transitions : total, count from Entity Grid 69 ra_NXToT_C ratio of NX transitions : total</title>
	</analytic>
	<monogr>
		<title level="m">idx Code Definition 55 ra_SSToT_C ratio of SS transitions : total, count from Entity Grid 56 ra_SOToT_C ratio of SO transitions : total, count from Entity Grid 57 ra_SXToT_C ratio of SX transitions : total, count from Entity</title>
		<imprint/>
	</monogr>
	<note>count from Entity Grid 70 ra_NNToT_C ratio of NN transitions : total, count from Entity Grid Table 16: Part-of-Speech Features (POSF</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">SquaNoV_S (unique Nouns**2)/total Nouns #Squared Noun Variation 188 CorrNoV_S unique Nouns/sqrt(2*total Nouns) #Corrected Noun Var 189 SimpVeV_S unique Verbs/total Verbs #Verb Variation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">SquaVeV_S (unique Verbs**2)/total Verbs #Squared Verb Variation 191 CorrVeV_S unique Verbs/sqrt(2*total Verbs) #Corrected Verb Var 192 SimpAjV_S unique Adjectives/total Adjectives #Adjective Var</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">SquaAjV_S (unique Adj**2)/total Adj #Squared Adj Variation 194 CorrAjV_S unique Adj/sqrt(2*total Adj) #Corrected Adj Var 195 SimpAvV_S unique Adverbs/total Adverbs #Adverb Variation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">SquaAvV_S (unique Adv**2)/total Adv #Squared Adv Variation 197 CorrAvV_S unique Adv/sqrt(2*total Adv) #Corrected Adv Var Table 17: Variation Ratio Features (VarF) /sqrt(2*total) #Corrected TTR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">BiLoTTR_S log(unique)/log(total) #Bi-Logarithmic TTR 201 UberTTR_S (log(unique)) 2 /log(total/unique) #Uber</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">MTLDTTR_S #Measure of Textual Lexical Diversity</title>
		<imprint/>
	</monogr>
	<note>TTR, 0.72) 1. Feature codes consist of 8 letters/numerals, with 1 or 2 underscores depending on feature types</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">All features classify into either count-based or score-based, following popular convention. ? Count-based -define: final calculation uses simple counts (i.e. total, avg per sent, avg per token, ratio) -format: xx_xxxxx_C</title>
		<imprint/>
	</monogr>
	<note>First two letters are &quot;to&quot; (total), &quot;as&quot; (avg per sent), &quot;at&quot; (avg per token), &quot;ra&quot; (ratio). Five letters in the middle explain what the feature is. Last letter always &quot;C.&quot; Two underscores in between</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Flesch-Kincaid, TTR). -format: xxxxxxx_S. Seven letters are all dedicated to explaining what the feature is</title>
	</analytic>
	<monogr>
		<title level="m">? Score-based -define: require additional calculation (e.g. log, square), or famous features with predefined names</title>
		<imprint/>
	</monogr>
	<note>Last letter always &quot;S.&quot; One underscore</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">For the &quot;explanation&quot; part of each feature code, capital letters denote new words. The small letters that follow are from the same word</title>
	</analytic>
	<monogr>
		<title level="j">Age of Acquisition) Kuperman of words ? AAKuW</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<publisher>Coleman Liau ? ColeLia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The five types of feature sets have varying aims</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">H-type captures the high-level properties, 3. Ltype captures the low, surface-level properties, 4. E-type uses features calculated from external data (out-of-model info</title>
	</analytic>
	<monogr>
		<title level="m">T-type thoroughly captures linguistic properties</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">This can be evidenced by the performance decreases (T1 ? T2 and T1 ? T3). We checked that all measures of F1, precision, recall, and QWK followed the same trend. Similar method was used in</title>
		<editor>Feng et al.</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Both advanced semantic and discourse features add distinctive information</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aluisio</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meurers</forename><surname>Vajjala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Falkenjack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">More linguistic branches generally indicated better performance. We use SciKit-learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Fran?ois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pedregosa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>to check if a feature added orthogonal information. for metrics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">These are the average training times for each fold, with 80% of the full dataset used to train. We used an NVIDIA Tesla V100 GPU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberta</forename><surname>Bert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><forename type="middle">)</forename><surname>Xlnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">H Transformers Training Time All numbers are in seconds. We report in the order of</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weebit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Onestopenglish</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="page">396</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">239) 12 here, text must be in string type 13</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cambridge</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">122</biblScope>
		</imprint>
	</monogr>
	<note>14 text = &quot;..</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">False): include short words 23 -see_token (def. False): return token list 24 -see_sent_token (def. False): return sent 25 26 output: 27 -n_token 28 -n_sent 29 -token_list (optional) 30 -sent_token_list (optional) 31</title>
		<idno>LingFeat = extractor.pass_text(text) 16 17 18</idno>
		<imprint>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note>19 Preprocess text 20 21 options (all boolean. def. 32 LingFeat.preprocess() 33 # or 34 # print(LingFeat.preprocess()) corresponding features 42 &quot;&quot;&quot; 43 # Advanced Semantic (AdSem) Features</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wokf=lingfeat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wokf_</surname></persName>
		</author>
		<title level="m">#Wiki Know. Features 45 WBKF=LingFeat.WBKF_() #WB Knowledge Features 46 OSKF=LingFeat.OSKF_() #OSE Knowledge Features 47 48 # Discourse (Disco) Features</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Endf=lingfeat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Endf_</surname></persName>
		</author>
		<title level="m">#Entity Dens. Features 50 EnGF=LingFeat.EnGF_() #Entity Grid Features 51 52 # Syntactic (Synta) Features</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phrf=lingfeat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phrf_</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">#Phrasal Features 54 TrSF=LingFeat.TrSF_() #(Parse) Tree Features 55 POSF=LingFeat.POSF_(</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname># Lexico Semantic</surname></persName>
		</author>
		<imprint>
			<pubPlace>LxSem) Features</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ttrf=lingfeat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ttrf_(</surname></persName>
		</author>
		<title level="m">#TTR Features 59 VarF=LingFeat.VarF_() #Variational Features 60 PsyF=LingFeat.PsyF_() #Psycholing Difficulty 61 WoLF=LingFeat.WorF_() #Word Familiarity 62 63 # Shallow Traditional (ShTra) Features</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaf=lingfeat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaf_</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">#Shallow Features 65 TraF=LingFeat.TraF_(</note>
	<note>#Traditional Formulas</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
