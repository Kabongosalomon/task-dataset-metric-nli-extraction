<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UniNet: Unified Architecture Search with Convolution, Transformer, and MLP</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CUHK</orgName>
								<address>
									<country>MMLab</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CUHK</orgName>
								<address>
									<country>MMLab</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CUHK</orgName>
								<address>
									<country>MMLab</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UniNet: Unified Architecture Search with Convolution, Transformer, and MLP</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Deep learning architectures, neural architecture search</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, transformer and multi-layer perceptron (MLP) architectures have achieved impressive results on various vision tasks. However, how to effectively combine those operators to form high-performance hybrid visual architectures still remains a challenge. In this work, we study the learnable combination of convolution, transformer, and MLP by proposing a novel unified architecture search approach. Our approach contains two key designs to achieve the search for high-performance networks. First, we model the very different searchable operators in a unified form, and thus enable the operators to be characterized with the same set of configuration parameters. In this way, the overall search space size is significantly reduced, and the total search cost becomes affordable. Second, we propose context-aware downsampling modules (DSMs) to mitigate the gap between the different types of operators. Our proposed DSMs are able to better adapt features from different types of operators, which is important for identifying high-performance hybrid architectures. Finally, we integrate configurable operators and DSMs into a unified search space and search with a Reinforcement Learning-based search algorithm to fully explore the optimal combination of the operators. To this end, we search a baseline network and scale it up to obtain a family of models, named UniNets, which achieve much better accuracy and efficiency than previous ConvNets and Transformers. In particular, our UniNet-B5 achieves 84.9% top-1 accuracy on ImageNet, outperforming EfficientNet-B7 and BoTNet-T7 with 44% and 55% fewer FLOPs respectively. By pretraining on the ImageNet-21K, our UniNet-B6 achieves 87.4%, outperforming Swin-L with 51% fewer FLOPs and 41% fewer parameters. Code is available at https://github.com/Sense-X/UniNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional Neural Networks (CNNs) dominate the learning of visual representations and show effectiveness on various visual tasks, including image classification, object detection, semantic segmentation, etc. Recently, convolutionfree backbones show impressive performances on image classification <ref type="bibr" target="#b9">[10]</ref>. Vision Transformer (ViT) <ref type="bibr" target="#b10">[11]</ref> demonstrates that pure transformer architecture that is mainly built on multi-head self-attentions (MSAs) can attain state-ofthe-art performance when trained on large-scale datasets (e.g., ImageNet-21K, JFT-300M). MLP-Mixer <ref type="bibr" target="#b40">[41]</ref> introduced a pure multi-layer perceptron (MLP) architecture that can almost match ViT's performance without using the timeconsuming attention mechanism. The main operators in those networks perform differently in terms of efficiency and data utilization. On the one hand, convolutions in CNNs are locally connected and their weights are input-independent, which makes it effective at extracting low-level representations and efficient under the low-data regime. On the other hand, MSAs in the transformer capture long-range dependency, and the attention weights are dynamically dependent on the input representations. Hence, it is more data and computation demanding. The token-mixing in MLP-Mixer performs like a depthwise convolution of a full receptive field with parameter sharing, which is also data demanding. It is an important topic to study how to combine them effectively to form high-performance hybrid visual architectures, which, however, remains a challenge.</p><p>There were recent papers on attempting to manually combine the different types of operators to form hybrid visual networks. In ViT <ref type="bibr" target="#b10">[11]</ref>, a hybrid architecture using ResNet and transformer is also studied and improves upon pure transformers for smaller model sizes. Besides, many other works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12]</ref> also explored the combination of convolution and transformer to form hybrid architectures to improve data or computation efficiency. Furthermore, the combination of convolution and MLP is studied in <ref type="bibr" target="#b24">[25]</ref>, and the combination of gated MLP and MSA is studied in <ref type="bibr" target="#b25">[26]</ref>. Those previous approaches focus on combining two distinct operators and can achieve satisfactory performances to some extent. However, a unified view and a systematical study are missed in prior arts.</p><p>We identify two key challenges when building high-performance hybrid architectures: <ref type="bibr" target="#b0">(1)</ref> The operators can be implemented with various styles, and it is infeasible to manually explore all possible implementations and combinations. Although we can automate the exploration with Neural Architecture Search (NAS) techniques, the search space should be properly designed so that the search cost is affordable. (2) Each operator has its own characteristics, and simply combining them together does not lead to optimal results. We conduct a simple pilot study on directly stacking different operators to form hybrid networks. As shown in <ref type="table" target="#tab_1">Table 1</ref>, however, the straightforward stacking of different operators achieves even worse performance than the vanilla ViT.</p><p>In this paper, we study the learnable combination of convolution, transformer, and MLP by proposing a novel unified architecture search approach. Our approach has two key designs to address the challenges mentioned above. First, we model distinct operators in a unified form, and use the same set of searchable configuration parameters (i.e., OP type, expansion, channels, etc) to characterize each of the different operators. The unified design enables us to greatly reduce the overall search space, and as a result, the total search cost becomes affordable. Besides, we propose context-aware downsampling modules (DSMs) to harmonize the combination of different operators. The proposed DSMs can be instantiated into three types, i.e., Local-DSM (L-DSM), Local-Global-DSM (LG-DSM), and    Global-DSM (G-DSM), aiming to better adapt the representations from one operator to another. Based on these designs, we build a unified search space consisting of a large family of different general operators (GOPs), DSMs, and network size, and jointly optimize model accuracy and FLOPs for identifying high-performance hybrid networks. We illustrate the search space and the backbone in <ref type="figure" target="#fig_6">Figure 6</ref>.</p><p>The discovered network, named UniNet, exhibits strong performance and efficiency improvements over common ConvNets, Transformers, or hybrid architectures on various visual benchmarks. Our experiments show that UniNet has the following characteristics: (1) placing convolutions in the shallow layers and transformers in the deep layers, (2) allocating a similar amount of FLOPs for both convolutions and transformers, and (3) inserting L-DSM to downsample for convolutions and LG-DSM for transformers. Our analysis shows that the conclusion is consistent among the top-5 models.</p><p>To go even further, we build a family of high-performance UniNet models by scaling up the searched baseline network, which achieves better accuracy and efficiency in both small and large model sizes. In particular, our UniNet-B5 achieves comparable accuracy (+0.1%) to EfficientNet-B7 while requires much less computation cost (-44%) <ref type="figure">(Figure 1 (a)</ref>). By pretraining on large-scale ImageNet-21K, our UniNet-B6 achieves 87.4% accuracy, outperforming Swin-L with fewer FLOPs (-51%) and parameters (-41%) <ref type="figure">(Figure 1 (c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Convolution, Transformer, and MLP. A host of ConvNets have been proposed to push forward the state-of-the-art computer vision approaches such as <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. Despite the numerous CNN models, their basic operators, convolution, are the same. Recently, <ref type="bibr" target="#b10">[11]</ref> proposed a pure transformer-based image classification model ViT, which achieves impressive performance on the Ima-geNet benchmark. DeiT <ref type="bibr" target="#b42">[43]</ref> shows that well-trained ViT can obtain a better performance-speed trade-off than ConvNets. PVT <ref type="bibr" target="#b49">[50]</ref> and Swin <ref type="bibr" target="#b28">[29]</ref> propose multi-stage vision transformers, which can be easily transferred to other downstream tasks. On the other hand, recent papers are attempting to use only MLP as the building block. MLP-Mixer <ref type="bibr" target="#b40">[41]</ref>, ResMLP <ref type="bibr" target="#b41">[42]</ref>, and ViP <ref type="bibr" target="#b19">[20]</ref> show that pure MLP architectures can also achieve near state-of-the-art performance. Combination of different operators. Another line of work tries to combine different operators to form new networks. CvT <ref type="bibr" target="#b50">[51]</ref> propose to incorporate selfattention and convolution by generating Q, K, and V in self-attention with convolution. CeiT <ref type="bibr" target="#b52">[53]</ref> replace the original patchy stem with a convolutional stem and add depthwise convolution to the FFN layer, which obtains fast convergence and better performance. ConViT <ref type="bibr" target="#b8">[9]</ref> tries to unify convolution and self-attention with gated positional self-attention and is more sample-efficient than self-attention. Many other works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12]</ref> also explored the combination of convolution and transformer to form hybrid architectures to improve the data or computation efficiency. Besides, ConvMLP <ref type="bibr" target="#b24">[25]</ref> studied the combination of convolution and MLP, and gMLP <ref type="bibr" target="#b25">[26]</ref> studied the combination of gated MLP and multi-head self-attentions (MSA). Instead of requiring manual exploration of the hybrid architectures, we propose a unified architecture search approach to automatically search for high-performance hybrid architecture. Downsampling module. In ConvNets, the downsampling module (DSM) is implemented with strided-Conv or pooling. As DSM breaks the shift-invariant of convolution, <ref type="bibr" target="#b55">[56]</ref> propose anti-aliased DSM to keep it. Besides, a line of works tries to preserve more information when downsampling with a learnable or dynamic kernel <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b48">49]</ref>. Most of their approaches are downsampling based on local context, which we show is not suitable for our unified network. In our work, we propose context-aware DSM and jointly search with operator combinations, which guarantees better performance.</p><formula xml:id="formula_0">? ! ? " Input image Output GOP DSM GOP GOP DSM ? GOPs: ? Convolution ? Transformer ? MLP DSMs: ? L-DSM ? G-DSM ? LG-DSM ? # Size: ? Repeats ? Channels ? Expansion</formula><p>Unified Search Space: <ref type="figure">Fig. 2</ref>: Unified Architecture Search. We jointly search different types of operators as well as downsampling modules (DSM) and network size in a unified search space. We construct UniNet architecture in a multi-stage fashion. Between two successive stages, one of the DSMs is inserted to change the spatial dimension or channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unified Architecture Search</head><p>As discussed in previous works <ref type="bibr" target="#b8">[9]</ref>, an appropriate combination of convolution and transformer operators can lead to performance improvements. However, the previous approaches <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53]</ref> only adopt convolution in self-attention or feedforward network (FFN) sub-layers and stack them repeatedly. Their approaches did not fully explore the combinations to take advantage of their different characteristics.</p><p>Prior arts <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b55">56]</ref> show that the downsampling module plays an important role in visual tasks. Most previous approaches adopt hand-crafted downsampling operations, i.e., strided convolution, max-pooling, or avg-pooling, to downsample the feature map based on only the local context. However, these operations are specifically designed for ConvNets, and might not be suitable to the transformer or MLP based architectures, which capture representation globally.</p><p>In this paper, we investigate the learnable combination of convolution, transformer, and MLP, trying to assemble them to create high-performance hybrid visual network architectures. For better transmitting features across different operator blocks, we proposed context-aware downsampling modules. We jointly search the operators, downsampling modules, and network size in a unified search space. In contrast, previous Neural Architecture Search (NAS) works achieved state-of-the-art performances mainly via searching the network sizes. We show that the searched hybrid architecture by our unified architecture search approach can achieve very promising performance.</p><p>In the remaining parts of the section, we firstly present how to properly define different operators into a unified search space and search them jointly. We then present the challenge of incorporating downsampling modules with differ-Here, MLP refers to a MLP-style sub-layer that captures spatial representations <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b19">20]</ref>, instead of pure 1 ? 1 convolution. ent operators and present our proposed context-aware downsampling module. Finally, we will introduce our UniNet architectures and NAS pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling Convolution, Transformer, MLP with a Unified Searchable Form</head><p>Recently, transformer and MLP based architectures are able to achieve comparable performance to convolution networks on different visual tasks. To achieve better performance, it is intuitive to assemble all the types of operators to build high-performance hybrid networks. Actually, a few works <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b8">9]</ref> have been studied to empirically combine convolution and self-attention. However, manually searching network architectures is quite time-consuming and cannot ensure optimal performances with different computational budgets. We introduce a unified search space that contains General Operators (GOPs, including convolution, transformer, and MLP), and then search for the optimal combination of those operators jointly. Compared with the prior art, we propose a unified form to characterize different operators. Specifically, we use the inverted residual <ref type="bibr" target="#b32">[33]</ref> to model a general operator block, which first expands the input channel c to a larger size ec, and then projects the ec channels back to c for residual connection. The e is defined as the expansion ratio, which is usually a small integer number, e.g., 4. The general operation block is therefore modeled as</p><formula xml:id="formula_1">y = x + Operation(x),<label>(1)</label></formula><p>where Operation can be convolution, MLP, or transformer, and x, y represent input and output features, respectively. For convolution, we place the convolution operation inside the bottleneck <ref type="bibr" target="#b32">[33]</ref>, which can be expressed as</p><formula xml:id="formula_2">Operation(x) = Proj ec?c (Conv(Proj c?ec (x))).<label>(2)</label></formula><p>The Conv operation can be either regular convolution or depth-wise convolution (DWConv) <ref type="bibr" target="#b3">[4]</ref>, and the Proj represents a linear projection. For self-attention in transformer and token-mixing in MLP, the computation cost on the large bottleneck feature map is quite huge. Following previous works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b40">41]</ref>, we separate them from the bottleneck for computation efficiency, and the Proj is implemented inside the FFN <ref type="bibr" target="#b45">[46]</ref> sub-layer. Each transformer block has a query-keyvalue self-attention sub-layer and an FFN sub-layer, and the token-mixing in the MLP block is implemented by transpose-FFN-transpose as that in <ref type="bibr" target="#b40">[41]</ref>,</p><formula xml:id="formula_3">y = y ? + FFN(y ? ),<label>(3)</label></formula><formula xml:id="formula_4">y ? = x + SA(x) or x + MLP(x),<label>(4)</label></formula><formula xml:id="formula_5">FFN(y ? ) = Proj ec?c (Proj c?ec (y ? )),<label>(5)</label></formula><p>where SA can be either vanilla self-attention or local self-attention LSA, and MLP refers to the token-mixing operation. There are two main advantages of representing the different types of operators in a unified format and search space: (1) We can characterize each operator with the same set of configuration parameters (i.e., OP type, expansion, channels, etc). As a result, the overall search space is greatly reduced, and the total search cost becomes affordable. <ref type="formula" target="#formula_2">(2)</ref> With the unified form, the comparison between different operators is fairer, which is important for NAS <ref type="bibr" target="#b37">[38]</ref> to identify the optimal hybrid architecture.</p><formula xml:id="formula_6">Input Output Conv2d, s2 (a) L-DSM Input Multi-Head Attention Conv2d s2 Q K V Output (b) LG-DSM Input Multi-Head Attention Conv1d s2 Q K V Output (c) G-DSM</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Context-Aware Downsampling Modules</head><p>As discussed in Section 3.1, the downsampling module (DSM) plays an important role in visual tasks. In addition to hand-crafted DSM (i.e., max-pooling or avg-pooling), a few works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b48">49]</ref> tried to preserve more information via downsampling with the learnable or dynamic kernel. Most of the approaches utilized downsampling based on local context, which suits conventional ConvNets well. However, in our unified search space, operators with different receptive fields can be assembled unrestrictedly to form a hybrid architecture, where the local context might be destroyed and therefore the previous downsampling operations might not be suitable.</p><p>In this paper, we propose context-aware DSM, which is instanced with Local-DSM (L-DSM), Local-Global-DSM (LG-DSM), and Global-DSM (G-DSM). The main difference between those DSMs is the considered context when performing downsampling. For L-DSM, only local context is involved, which fits ConvNets well as shown in previous works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b28">29]</ref>. For G-DSM, only global context is used for downsampling, which may fit other operators, e.g., transformers. The LG-DSM combines the characteristics of L-DSM and G-DSM. It uses both local and global context for downsampling. Our intuition is that one of the largest dissimilarities of different operators is the receptive field. Transformer and MLP naturally have global receptive filed, while convolution has local receptive field, e.g., 3 ? 3. When combining those operators, there is no single optimal DSM that satisfies all scenarios.</p><p>The proposed DSMs are visualized in <ref type="figure" target="#fig_3">Figure 3</ref>. To downsample based on global cues, we utilize the self-attention mechanism to capture global context, which is missed by the prior art. For G-DSM, we use Conv1D with stride 2 to downsample the query and use the downsampled query features to aggregate key features with downsampled output resolution. Note that, there is no local context preserved after downsampling of G-DSM. For LG-DSM, we first reshape the flattened token sequences back to the spatial grid and apply Conv2D with stride 2 to downsample the query, and then flatten the query back to calculate the attention weights.</p><p>Compared with previous works, which mainly try to improve ConvNets, our proposed DSMs are not designed for a specific architecture. Our motivation is that different DSMs might be suitable for different operators. For example, the optimal DSM might be L-DSM for ConvNets, but G-DSM for transformers. As thousands of operator combinations would be trained in our NAS process, it is unfeasible to decide which DSM to use by hand. To obtain the optimal architecture, we jointly search DSMs with other operators. In our searched optimal architecture, L-DSM is indeed used between operators with the local receptive field while LG-DSM is favored by operators with a global receptive field. The results validate the effectiveness of our proposed context-aware downsampling modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">UniNet Architecture</head><p>As shown in recent studies, combining different operators <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53]</ref> can bring performance improvements. Most previous approaches only repeatedly stack the same operator in the whole architecture and search only different channels in different stages. These approaches do not allow large architecture diversity in each block, which we show is crucial for achieving high accuracy for hybrid architectures.</p><p>On the contrary, in our UniNet, the operators are not fixed but searched from the unified search space. We construct our UniNet architecture in a multistage fashion, which can be easily transferred to downstream tasks. Between two successive stages, one of our proposed DSMs is inserted to reduce the spatial dimension. We jointly search the GOP and DSM for all stages. The GOP could be different for different stages but repeated multiple times in one stage, which can greatly reduce the search space size as pointed out before <ref type="bibr" target="#b37">[38]</ref>. The overall architecture and unified search space are illustrated in <ref type="figure" target="#fig_6">Figure 6</ref>.</p><p>Thanks to our unified form of GOPs, the network size of each stage can be configured with the repeat number r, channel size c, and expansion ratio e. To obtain better computation-accuracy trade-off, we jointly search the network size with the GOP and DSM. For GOP, we search for convolution, transformer, MLP, and their promising variants, i.e., {SA, LSA, Conv, DWConv, MLP}, as defined in Section 3.2; for e, we search from {2, 3, 4, 5, 6}. The LSA refers to the window self-attention with window size of 7 ? 7. Please note that we do not use shifted window like Swin Transformer <ref type="bibr" target="#b28">[29]</ref>. The kernel size for convolution operation is fixed to 3?3. The head dimension in self-attention is fixed to 32. We start the architecture search with an initial architecture, whose network size is determined based on a reference architecture, e.g., EfficientNetV2 <ref type="bibr" target="#b39">[40]</ref>. The initial channels and repeats are set according to the reference architecture. For c and r, we search from the sets {0.5, 0.75, 1.0, 1.25, 1.5} and {-2, -1, 0, 1, 2}, respectively. Channels are set to be divisible by 32 for self-attention. Suppose we partition the network into K stages, and each stage has a sub-search space of size S. Then the total search space is S K . In our implementation, K is set to 5 and S equals 1,875. As a result, our search space size is about 2?10 <ref type="bibr" target="#b15">16</ref> and covers a large set of operators with quite different characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Search Algorithm</head><p>We use Reinforcement Learning (RL)-based search algorithm to search for highperformance hybrid architecture in our unified search space by jointly optimizing the model accuracy and FLOPs. Concretely, we follow previous work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b37">38]</ref> and map an architecture in the unified search space to a list of tokens, which are determined by a sequence of actions generated by a Recurrent Neural Network (RNN). The RNN is optimized with the PPO algorithm <ref type="bibr" target="#b33">[34]</ref> by maximizing the expected reward. In our implementation, we simultaneously optimize accuracy and the theoretical computation cost (FLOPs). To handle the multi-objective optimization problem, we use a weighted product customized as <ref type="bibr" target="#b37">[38]</ref> to approximate Pareto optimal. For a sampled architecture m, the reward is formulated as</p><formula xml:id="formula_7">r(m) = a(m) ? ( t f (m) ) ? ,</formula><p>where function a(m) and f (m) return the accuracy and the FLOPs of m, t is the target FLOPs, and ? is a weight factor that balances the accuracy and computation cost. We include more details of the RL algorithm in the supplementary materials.</p><p>During the search process, thousands of combinations of GOPs and DSMs are trained on a proxy task with the same setting, which gives us a fair comparison between those combinations. When the search is over, the top-5 architectures with the highest reward are trained with full epochs, and the top-performing one is kept for model scaling and transferring to other downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup and Implementation</head><p>To find the optimal architecture in our search space, we directly search on the large-scale dataset, ImageNet-1K. We reserve 50k images from the training set as a validation set. We employ a proxy task setting in the search phase. For each sampled architecture, we train it for 5 epochs and calculate the reward of the architecture with its FLOPs and the accuracy on the validation set. We set the target FLOPs t and weight factor ? in the reward function to 550M and 0.07 respectively <ref type="bibr" target="#b38">[39]</ref>. During the search process, totally 2K models are trained on the proxy task. After that, we fully train the top-5 architectures on ImageNet-1K and preserve the top-performing one for model scaling and transferring to other downstream tasks.</p><p>For full training on the ImageNet-1K dataset, we follow the popular training recipe in DeiT <ref type="bibr" target="#b42">[43]</ref>. We employ AdamW optimizer <ref type="bibr" target="#b23">[24]</ref> with an initial learning rate of 0.001 and weight decay of 0.05 to train UniNet. The total batch size is set to 1024. We totally train for 300 epochs with a cosine learning rate decay  and 5 epochs of linear warm-up. We follow the augmentation strategy in DeiT <ref type="bibr" target="#b42">[43]</ref> and apply small augmentation for small models and heavy augmentation for large models as introduced in <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b35">36]</ref>. For training efficiency, UniNet-B5 and UniNet-B6 are trained with 224 ? 224 input size and then finetuned on the large resolution. We also pre-train UniNet on a larger ImageNet-21K dataset, which contains 14.2 million images and 21K classes, to further test UniNet. We pretrain for 90 epochs with AdamW optimizer. We then finetune on ImageNet-1K for 30 epochs and compare the top-1 accuracy on ImageNet-1K with other approaches. We list the details of training and finetuning hyper-parameters in the supplementary materials. Besides, we also transfer UniNet to downstream tasks, e.g., object detection and instance segmentation on COCO and semantic segmentation on ADE20K. For COCO training, we use the various detection frameworks and train UniNet with the widely-used 1x (12 epochs) and 3x (36 epochs) schedules. For ADE20K training, we use the UperNet framework and train with the same setting as <ref type="bibr" target="#b28">[29]</ref>. The training details are listed in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Main Results</head><p>In this section, we firstly present our searched UniNet architecture. We then show the performance of the scaled UniNets on classification, object detection, and semantic segmentation. <ref type="table" target="#tab_2">Table 2</ref> shows our searched UniNet-B0 architecture. Our searched architecture has the following characteristics: (1) Placing convolution in the shallow layers and transformers with SA in the deep layers. While the previous work <ref type="bibr" target="#b10">[11]</ref> shows that the early-stage transformer blocks learn to gather local representations, our searched architecture directly applies convolution at early stages, which is more efficient. We further compare the top-5 searched models in <ref type="table" target="#tab_3">Table 3</ref>, and find the conclusion is close to consistent. The exception is the 3rd model, which   uses DWConv at the last stage, but with inferior performance. (2) Allocating a similar amount of computations for both convolutions and transformers. Shown in <ref type="table" target="#tab_2">Table 2</ref>, the DWConv stages consume 245M FLOPs, and SA stages consume 250M FLOPs. While the operator combination has been studied in prior arts, the computation allocating for different operators is neglected. Our work shed some light on this question by jointly searching the network size in our unified search space. (3) Inserting L-DSM to downsample for convolutions and LG-DSM for transformers. Our search results show that the widely-used downsampling module is sub-optimal for hybrid architectures. We also notice that the MLP operator has not been chosen in the searched UniNet. We empirically find that the MLP-style operation breaks the spatial structure which is important for visual tasks <ref type="bibr" target="#b22">[23]</ref>, leading to inferior performance when combined with other operators. We add the visualization in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">UniNet Model Family</head><p>To go even further, we build a family of high-performance UniNet models by scaling up the searched UniNet-B0. We utilize the compound scaling <ref type="bibr" target="#b38">[39]</ref> to scale depth, width, and resolution simultaneously. Note that the resolution is scaled with a smaller coefficient compared to EfficientNet <ref type="bibr" target="#b38">[39]</ref> for training and memory efficiency. We list the details of UniNet-B1 to UniNet-B6 in the supplementary materials. While most previous transformer-based architectures outperform convolution-based architectures in large model sizes but underperform in small model sizes, UniNet achieves consistently better accuracy and efficiency across B0 to B6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ImageNet Classification Performance</head><p>ImageNet-1K. <ref type="table" target="#tab_4">Table 4</ref> presents the performance comparison of our searched UniNet with previous proposed architectures. Our searched UniNet has better accuracy and computation efficiency than previous ConvNets, Transformers, or hybrid architectures.</p><p>As shown in <ref type="table" target="#tab_4">Table 4</ref>, under mobile setting, our UniNet-B0 achieves 79.1% top-1 accuracy with 555M FLOPs, outperforming EfficientNetV2-B0 <ref type="bibr" target="#b39">[40]</ref> with less FLOPs. In the middle FLOPs setting, our UniNet-B3 achieves 83.5% top-1 accuracy with 4.3G FLOPs, which outperforms the pure convolution-based EfficientNet-B4, pure transformer-based Swin-B, and hybrid architecture CvT-21. For larger models, our UniNet-B5 achieves 84.9% with 20G FLOPs, outperforming EfficientNet-B7 and BoTNet-T7 with 44% and 55% fewer FLOPs, respectively. <ref type="figure">Figure 1 (a, b)</ref> further visualizes the comparison of UniNet with other architectures in terms of accuracy and FLOPs.</p><p>We further compare UniNet-B0 to previous searched efficient architectures in <ref type="table" target="#tab_6">Table 6</ref>. Note that for a more fair comparison, we train UniNet-B0 with knowledge distillation. The details of distillation are listed in the supplementary materials. Shown in <ref type="table" target="#tab_6">Table 6</ref>, UniNet-B0 achieves 80.8% accuracy with 555M FLOPs, outperforming other efficient convolution-based or hybrid architectures. ImageNet-21K. <ref type="table" target="#tab_5">Table 5</ref> presents the performance comparison of UniNet and other architectures with ImageNet-21K pretrain. Notably, UniNet-B5 obtains 87% top-1 accuracy, which outperforms Swin-L with 4? less computation. UniNet-B6 achieves 87.4% top-1 accuracy, which outperforms CoAtNet-2 <ref type="bibr" target="#b7">[8]</ref> with 47% less computation. We further visualize the comparison in <ref type="figure">Figure 1</ref> (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Object Detection and Semantic Segmentation Performance</head><p>For object detection and semantic segmentation, we pick UniNet-B1 and UniNet-B3 and use them as the backbone networks for detection and segmentation frameworks. We compare our UniNet with other convolution or transformer-based architectures. For COCO object detection, we use various detection frameworks and compare the performance under 1? and 3? schedules. For ADE20K semantic segmentation we use the UperNet framework and report mIoU (%) for different architectures under the same training setting.</p><p>As shown in <ref type="table" target="#tab_7">Table 7</ref>, our searched UniNet consistently outperforms convolutionbased ResNet <ref type="bibr" target="#b16">[17]</ref> and transformer-based PVT <ref type="bibr" target="#b49">[50]</ref> or Swin-Transformer <ref type="bibr" target="#b28">[29]</ref>. UniNet-B1 achieves 40.5 AP@box, which is 3.8% better than PVT-Tiny but  with 15% fewer parameters. UniNet-B3 achieves 45.2 AP@box with 1? schedule and 47.9 AP@box with 3? schedule, which is 1.5% and 1.9% better than Swin-T, respectively. We further test various detection framework and show the results in <ref type="table" target="#tab_8">Table 8</ref>, and find that UniNet achieves consistently better performance among others. For ADE20K semantic segmentation, we achieve 48.5% mIoU with 51M parameters. Compared with transformer-based Swin-T, our UniNet outperforms 4.0% mIoU with a similar parameter size. Besides, compared with convolutionbased ResNet101, we achieve 3.6% higher mIoU with 41% fewer parameters. All the results show the effectiveness of our searched UniNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Ablative Studies and Analysis</head><p>In this section, we study the impact of joint search of General Operators and discuss the importance of context-aware downsampling modules (DSMs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Single Operator vs. General Operators</head><p>Previous works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> mostly focus on the network size search, which uses a single operator, convolution, as the main feature extractor. In comparison, we jointly search the combination of different General Operators (GOPs), i.e., convolution, transformer, MLP, and their promising variants. To verify the importance of GOPs, we keep only one type of operator in the search space and re-run the search experiments under the same settings. After the search, we fully train the top-5 architectures with the highest reward on ImageNet-1K and report the best performance. As shown in <ref type="table" target="#tab_9">Table 9</ref>, our searched hybrid architecture consistently achieves better accuracy compared to single-operator-based architectures. The result verifies the effectiveness of our unified architecture search of GOPs, which can take advantage of the characteristics of different operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Fixed vs. Context-Aware downsampling</head><p>When combining different operators into a unified network, the traditional downsampling module, such as strided-conv or pooling, could be sub-optimal. To verify the effectiveness of our proposed context-aware DSMs, we replace the DSMs of our search UniNet with one fixed DSM and compare their performance under the same training setting.</p><p>As shown in <ref type="table" target="#tab_1">Table 12</ref>, our searched UniNet consistently outperforms its variants that use a single-fixed DSM in all stages. Although we see that using G-DSM or LG-DSM in all stages brings more computation and parameters, the performance does not become better. The result emphasizes the importance of our joint search of GOPs and DSMs.</p><p>Besides, we transfer our proposed DSMs to other popular transformer-based architectures, Swin-Transformer <ref type="bibr" target="#b28">[29]</ref> and PVT <ref type="bibr" target="#b49">[50]</ref>. Both Swin and PVT have 4 stages. We compare 2 settings: 1) using LG-DSM for 4 stages, as both PVT and Swin are pure transformer architectures 2) using L-DSM for the first two stages while LG-DSM for the latter two stages, which requires less computation. As shown in <ref type="table" target="#tab_1">Table 11</ref>, our proposed LG-DSM improves PVT-Tiny and Swin-T for 3.5% and 0.7%, respectively. Using L-DSM in the first two stages has a similar computation compared with the baseline, which improves PVT-Tiny and Swin-T for 2.4% and 0.4%, respectively. To note that, PVT uses a strided-conv for downsampling. As discussed in Section 3.3, it is harmful to the main operator in PVT, which has a global receptive field. On the contrary, our proposed DSMs are able to downsample based on both local and global context, and can greatly improve the performance.</p><p>We integrate the proposed DSM into other architectures. We adopt the setting in rows 3/6 of <ref type="table" target="#tab_1">Table 11</ref> (i.e., L?LG-DSM) to avoid introducing much computation cost. As shown in <ref type="table" target="#tab_1">Table 12</ref>, we achieve consistent performance gains.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a novel unified architecture search approach to jointly search the combination of convolution, transformer, and MLP. We empirically identify that the widely-used downsampling modules become the performance bottlenecks when the operators are combined. To further improve the performance, we propose context-aware downsampling modules and jointly search them with all operators. We scale the search baseline network up and obtain a family of models, named UniNet, which achieve much better accuracy and efficiency than previous ConvNets and Transformers. Acknowledgement Hongsheng Li is also a Principal Investigator of Centre for Perceptual and Interactive Intelligence Limited (CPII). This work is supported in part by CPII, in part by the General Research Fund through the Research Grants Council of Hong Kong under Grants (Nos. 14204021, 14207319), in part by CUHK Strategic Fund.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">UniNet Implementations</head><p>To reduce the search space size, we do not search for the kernel size of convolution operators. The kernel size is fixed to 3?3. The head dimension in self-attention (i.e., in transformer block or LG-DSM) is fixed to 32. Following prior arts <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11]</ref>, we use BN <ref type="bibr" target="#b21">[22]</ref> for convolution blocks and LN <ref type="bibr" target="#b0">[1]</ref> for transformer or MLP blocks by default. GELU <ref type="bibr" target="#b17">[18]</ref> is utilized as the activation function. We use CPE <ref type="bibr" target="#b4">[5]</ref> as the default positional embedding for transformer blocks, which is easy to transfer to larger input resolution.</p><p>We utilize the compound scaling <ref type="bibr" target="#b38">[39]</ref> to scale depth, width, and resolution simultaneously. The details of UniNet-B1 to B6 are listed in <ref type="table" target="#tab_1">Table 13</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Inference Throughput</head><p>We show the results in <ref type="figure" target="#fig_4">Figure 4</ref>. The search of UniNet is based on optimizing FLOPs, and we believe directly optimizing latency could bring a better trade-off (like EffNetV2). Besides, we note that hardware speed heavily depends on software optimization. We believe that further optimization could bring the speed close to the theoretical optimum (FLOPs).    <ref type="bibr" target="#b36">[37]</ref> 0.1 Mixup <ref type="bibr" target="#b54">[55]</ref> 0.8 CutMix <ref type="bibr" target="#b53">[54]</ref> 1.0 drop path <ref type="bibr" target="#b20">[21]</ref> Table <ref type="formula" target="#formula_1">14</ref> 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.3 Training Details</head><p>ImageNet-1K To train UniNet on ImageNet-1K, we follow the training receipt in DeiT <ref type="bibr" target="#b42">[43]</ref>. We do not use repeated augmentation <ref type="bibr" target="#b18">[19]</ref> as it slows down the convergence process <ref type="bibr" target="#b51">[52]</ref>. Following <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b35">36]</ref>, we apply higher drop path rate d r on large models, as shown in <ref type="table" target="#tab_1">Table 14</ref>. UniNet-B5 and UniNet-B6 are trained with 224 ? 224 input size and finetuned on larger resolutions for training efficiency. The default training setting is in <ref type="table" target="#tab_1">Table 15</ref>. For finetuning, the weight decay rate is set to 10 ?8 <ref type="bibr" target="#b10">[11]</ref>, the ? in Mixup <ref type="bibr" target="#b54">[55]</ref> or CutMix <ref type="bibr" target="#b53">[54]</ref> is set to 0.1. We list the detailed finetune setting in <ref type="table" target="#tab_1">Table 16</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet-21K</head><p>To train UniNet on large-scale ImageNet-21K, we turn off the CutMix and Mixup augmentation. We utilize AdamW optimizer and train UniNet for 90 epochs. The detailed hyperparameters are in <ref type="table" target="#tab_1">Table 17</ref>.</p><p>ADE20K For transferring to ADE20K, we directly follow the task layer and most of the hyperparameters described in SETR-PUP <ref type="bibr" target="#b30">[31]</ref>. The detailed hyperparameters are described in <ref type="table" target="#tab_1">Table 18</ref>.</p><p>Knowledge Distillation of UniNet-B0 We utilize the vanilla Knowledge Distillation (KD) algorithm to train UniNet-B0 for comparison with other approaches. Formally, we consider a student model S with parameters ? s for distilling the teacher. We denote the output logits from the student and the teacher  <ref type="bibr" target="#b36">[37]</ref> 0.1 Mixup <ref type="bibr" target="#b54">[55]</ref> 0.1 CutMix <ref type="bibr" target="#b53">[54]</ref> 0.1 drop path <ref type="bibr" target="#b20">[21]</ref>    <ref type="bibr" target="#b36">[37]</ref> 0.1 Mixup <ref type="bibr" target="#b54">[55]</ref> 0.0 CutMix <ref type="bibr" target="#b53">[54]</ref> 0.0 drop path <ref type="bibr" target="#b20">[21]</ref> 0.1 (B5), 0.4 (B6)</p><p>as z s and z t , respectively. The student is trained to minimize the cross-entropy (CE) loss between its predicted probability p s and the teacher's output probability p t , where p s and p t are softened by a temperature ? . We ignore the one-hot labels from ImageNet-1K for simplicity. The ? is set to 1.0 in our experiments. min ?s ? 2 CE(p s , p t ),</p><p>where p s = softmax(z s /? ), p t = softmax(z t /? ).</p><p>For a more fair comparison with previous approaches <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15]</ref>, we utilize the RegNetY-16G (83.6%) <ref type="bibr" target="#b29">[30]</ref> as the teacher network. We use the PPO <ref type="bibr" target="#b33">[34]</ref> algorithm to update the Reinforcement Learning (RL) agent. The RL agent is implemented with a two-layer LSTM with 100 hidden units at each layer. We use Adam <ref type="bibr" target="#b23">[24]</ref> optimizer to train the RL agent with learning rate of 0.0005 and weight decay of 0. The weights of the controller are initialized uniformly between -0.1 and 0.1. We parallelly use about 80 GPUs to train the sampled architectures. We visualize the NAS pipeline in <ref type="figure" target="#fig_5">Figure 5</ref>. Note the other NAS algorithms such as one-shot-based <ref type="bibr" target="#b2">[3]</ref> or differentiable-based <ref type="bibr" target="#b26">[27]</ref> may also be applicable but are beyond the paper's scope. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Visualization of MLP-Mixer</head><p>In our searched UniNet, there is no MLP operator chosen. We find that the MLPstyle operation breaks the spatial structure which is important for visual tasks <ref type="bibr" target="#b22">[23]</ref>, leading to sub-optimal performance when combined with other operators. We visualize the hidden features in <ref type="figure" target="#fig_6">Figure 6</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Corresponding author. arXiv:2207.05420v2 [cs.CV] 12 Sep 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig</head><label></label><figDesc>Fig. 1: ImageNet top-1 accuracy vs. FLOPs. Our UniNet-B5 achieve 84.9% with ImageNet-1K dataset, outperforming EfficientNet-B7 and BoTNet-T7 with 44% and 55% fewer FLOPs, respectively. Our UniNet-B6 achieve 87.4% on ImageNet-1K with ImageNet-21K pre-training, outperforming EfficientNetV2-XL with 46% fewer FLOPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Structures of the context-aware downsampling modules. The three DSMs are described in Section 3.3. Shortcuts are omitted for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Performance vs. inference throughput. The throughput is measured on 80GB A100 GPU FP32 with batch size 128 using timm library.https://github.com/rwightman/pytorch-image-models/blob/master/benchmark.py</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>An overview of the NAS pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Hidden features of MLP-Mixer<ref type="bibr" target="#b40">[41]</ref>. Hidden features are reshaped back to 2D dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>ViT</cell><cell>12 T</cell><cell>22</cell><cell>4.6</cell><cell>78.0</cell></row><row><cell>MLP-Mixer</cell><cell>18 M</cell><cell>23</cell><cell>4.7</cell><cell>76.8</cell></row><row><cell>DWConv</cell><cell>18 C</cell><cell>22</cell><cell>4.3</cell><cell>78.1</cell></row><row><cell>ViT-MLP</cell><cell>7 T + 7 M</cell><cell>22</cell><cell>4.5</cell><cell>76.5</cell></row><row><cell>MLP-ViT</cell><cell>7 M + 7 T</cell><cell>22</cell><cell>4.5</cell><cell>77.8</cell></row><row><cell cols="2">DWConv-ViT 7 C + 7 T</cell><cell>22</cell><cell>4.3</cell><cell>79.5</cell></row></table><note>ImageNet top-1 accuracy of different operator combinations. T, M, and C refer to transformer block, MLP-Mixer block, and Depthwise Convolution block respectively. Different block numbers are chosen so that their computations are comparable.Model Configuration #Params (M) #FLOPs (G) Top-1 Acc.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>UniNet-B0 architecture. GOP and DSM represent General Operators and downsampling module respectively. DWConv and SA are described in Section 3.2.</figDesc><table><row><cell>Stage</cell><cell cols="3">Operator GOP DSM e c Network Size FLOPs(M) r</cell></row><row><cell cols="2">0 DWConv L-DSM 4 48</cell><cell>2</cell><cell>68</cell></row><row><cell cols="2">1 DWConv L-DSM 6 80</cell><cell>4</cell><cell>135</cell></row><row><cell cols="2">2 DWConv L-DSM 3 128</cell><cell>4</cell><cell>42</cell></row><row><cell>3</cell><cell>SA LG-DSM 2 128</cell><cell>4</cell><cell>63</cell></row><row><cell>4</cell><cell>SA LG-DSM 5 256</cell><cell>8</cell><cell>187</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of Top-5 models after fully training. D and A are short for DWConv and SA respectively.</figDesc><table><row><cell cols="2">Rank Configuration</cell><cell>Top-1 Acc.</cell></row><row><cell>0</cell><cell cols="2">DDDAA 79.1</cell></row><row><cell>1</cell><cell>DDDAA</cell><cell>78.7</cell></row><row><cell>2</cell><cell>DDDAD</cell><cell>77.9</cell></row><row><cell>3</cell><cell>DDDAA</cell><cell>78.6</cell></row><row><cell>4</cell><cell>DDDAA</cell><cell>78.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>UniNet performance on Ima-geNet. All UniNet models are trained on the ImageNet-1K dataset with 1.28M images. C, T, and H denote convolution, transformer, and hybrid architecture respectively.</figDesc><table><row><cell>Model</cell><cell>Family</cell><cell>Input Size</cell><cell>#FLOPs (G)</cell><cell>#Params (M)</cell><cell>Top-1 Acc.</cell></row><row><cell>EffNet-B0 [39]</cell><cell>C</cell><cell>224</cell><cell>0.39</cell><cell>5.3</cell><cell>77.1</cell></row><row><cell cols="2">EffNetV2-B0 [40] C</cell><cell>240</cell><cell>0.7</cell><cell>7.4</cell><cell>78.7</cell></row><row><cell>DeiT-Tiny [43]</cell><cell>T</cell><cell>224</cell><cell>1.3</cell><cell>5.7</cell><cell>72.2</cell></row><row><cell>PVT-Tiny [50]</cell><cell>T</cell><cell>224</cell><cell>1.9</cell><cell>13.2</cell><cell>75.1</cell></row><row><cell>ConViT-Ti+ [9]</cell><cell>H</cell><cell>224</cell><cell>2</cell><cell>10</cell><cell>76.7</cell></row><row><cell>UniNet-B0</cell><cell>H</cell><cell>160</cell><cell>0.56</cell><cell>11.5</cell><cell>79.1</cell></row><row><cell>EffNet-B2 [39]</cell><cell>C</cell><cell>260</cell><cell>1</cell><cell>9.2</cell><cell>80.1</cell></row><row><cell cols="2">EffNetV2-B1 [40] C</cell><cell>260</cell><cell>1.2</cell><cell>8.1</cell><cell>79.8</cell></row><row><cell cols="2">RegNetY-4G [30] C</cell><cell>224</cell><cell>4</cell><cell>20.6</cell><cell>81.9</cell></row><row><cell>DeiT-Small [43]</cell><cell>T</cell><cell>224</cell><cell>4.3</cell><cell>22</cell><cell>79.8</cell></row><row><cell>PVT-Small [50]</cell><cell>T</cell><cell>224</cell><cell>3.8</cell><cell>24.5</cell><cell>79.8</cell></row><row><cell>UniNet-B1</cell><cell>H</cell><cell>224</cell><cell>1.1</cell><cell>11.5</cell><cell>80.8</cell></row><row><cell>EffNet-B3 [39]</cell><cell>C</cell><cell>300</cell><cell>1.8</cell><cell>12</cell><cell>81.6</cell></row><row><cell cols="2">EffNetV2-B3 [40] C</cell><cell>300</cell><cell>3</cell><cell>14</cell><cell>82.1</cell></row><row><cell>Swin-T [29]</cell><cell>T</cell><cell>224</cell><cell>4.5</cell><cell>29</cell><cell>81.3</cell></row><row><cell>CoAtNet-0 [8]</cell><cell>H</cell><cell>224</cell><cell>4.2</cell><cell>25</cell><cell>81.6</cell></row><row><cell>UniNet-B2</cell><cell>H</cell><cell>256</cell><cell>2.2</cell><cell>16.2</cell><cell>82.5</cell></row><row><cell>EffNet-B4 [39]</cell><cell>C</cell><cell>380</cell><cell>4.2</cell><cell>19</cell><cell>82.9</cell></row><row><cell>NFNet-F0 [2]</cell><cell>C</cell><cell>256</cell><cell>12.4</cell><cell>71.5</cell><cell>83.6</cell></row><row><cell>Swin-B [29]</cell><cell>T</cell><cell>224</cell><cell>15.4</cell><cell>88</cell><cell>83.5</cell></row><row><cell>ConViT-B+ [9]</cell><cell>H</cell><cell>224</cell><cell>30</cell><cell>152</cell><cell>82.5</cell></row><row><cell>CoAtNet-1 [8]</cell><cell>H</cell><cell>224</cell><cell>8.4</cell><cell>42</cell><cell>83.3</cell></row><row><cell>CvT-21 [51]</cell><cell>H</cell><cell>384</cell><cell>24.9</cell><cell>32</cell><cell>83.3</cell></row><row><cell>UniNet-B3</cell><cell>H</cell><cell>288</cell><cell>4.3</cell><cell>24</cell><cell>83.5</cell></row><row><cell>EffNet-B7 [39]</cell><cell>C</cell><cell>600</cell><cell>37</cell><cell>66</cell><cell>84.3</cell></row><row><cell>EffNetV2-M [40]</cell><cell>C</cell><cell>480</cell><cell>24</cell><cell>54</cell><cell>85.1</cell></row><row><cell>NFNet-F2 [2]</cell><cell>C</cell><cell>352</cell><cell>62.6</cell><cell>193.8</cell><cell>85.1</cell></row><row><cell>BoTNet-T7 [35]</cell><cell>T</cell><cell>384</cell><cell>45.8</cell><cell>75.1</cell><cell>84.7</cell></row><row><cell>CoAtNet-1 [8]</cell><cell>H</cell><cell>384</cell><cell>27.4</cell><cell>42</cell><cell>85.1</cell></row><row><cell>UniNet-B4</cell><cell>H</cell><cell>320</cell><cell>9.4</cell><cell>43.8</cell><cell>84.4</cell></row><row><cell>UniNet-B5</cell><cell>H</cell><cell>384</cell><cell>20.4</cell><cell>72.9</cell><cell>84.9</cell></row><row><cell>UniNet-B6</cell><cell>H</cell><cell>448</cell><cell>51</cell><cell>117</cell><cell>85.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance on ImageNet with ImageNet-21K pre-train. All models are pre-trained on ImageNet-21K and finetuned on ImageNet-1K.</figDesc><table><row><cell>Model</cell><cell>Family</cell><cell>Input Size</cell><cell>#FLOPs (G)</cell><cell>#Params (M)</cell><cell>Top-1 Acc.</cell></row><row><cell>EffNetV2-M [40]</cell><cell>C</cell><cell>480</cell><cell>24</cell><cell>55</cell><cell>86.1</cell></row><row><cell>ViT-L/16 [11]</cell><cell>T</cell><cell>384</cell><cell>190.7</cell><cell>304</cell><cell>85.3</cell></row><row><cell>HaloNet-H4 [45]</cell><cell>T</cell><cell>384</cell><cell>-</cell><cell>85</cell><cell>85.6</cell></row><row><cell>Swin-B [29]</cell><cell>T</cell><cell>384</cell><cell>47.1</cell><cell>88</cell><cell>86.4</cell></row><row><cell>CvT-21 [51]</cell><cell>H</cell><cell>384</cell><cell>25</cell><cell>32</cell><cell>84.9</cell></row><row><cell>UniNet-B5</cell><cell>H</cell><cell>384</cell><cell>20.4</cell><cell>72.9</cell><cell>87</cell></row><row><cell>EffNetV2-L [40]</cell><cell>C</cell><cell>480</cell><cell>53</cell><cell>121</cell><cell>86.8</cell></row><row><cell cols="2">EffNetV2-XL [40] C</cell><cell>512</cell><cell>94</cell><cell>208</cell><cell>87.3</cell></row><row><cell>Swin-L [29]</cell><cell>T</cell><cell>384</cell><cell>103.9</cell><cell>197</cell><cell>87.3</cell></row><row><cell>CoAtNet-2 [8]</cell><cell>H</cell><cell>384</cell><cell>49.8</cell><cell>75</cell><cell>87.1</cell></row><row><cell>CoAtNet-2 [8]</cell><cell>H</cell><cell>512</cell><cell>96.7</cell><cell>75</cell><cell>87.3</cell></row><row><cell>UniNet-B6</cell><cell>H</cell><cell>448</cell><cell>51</cell><cell>117</cell><cell>87.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell cols="4">Comparison with previous ef-</cell></row><row><cell cols="4">ficient architectures. UniNet is trained</cell></row><row><cell cols="4">with knowledge distillation for a more</cell></row><row><cell cols="2">fair comparison.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Family #FLOPs (M) Top-1 Acc.</cell></row><row><cell cols="2">AttentiveNAS [48] C</cell><cell>491</cell><cell>80.1</cell></row><row><cell>AlphaNet [47]</cell><cell>C</cell><cell>491</cell><cell>80.3</cell></row><row><cell>FBNetv3 [7]</cell><cell>C</cell><cell>557</cell><cell>80.5</cell></row><row><cell>OFA [3]</cell><cell>C</cell><cell>595</cell><cell>80.0</cell></row><row><cell>LeViT [15]</cell><cell>H</cell><cell>658</cell><cell>80.0</cell></row><row><cell>UniNet-B0</cell><cell>H</cell><cell>555</cell><cell>80.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Object detection, instance segmentation, and semantic segmentation performance on the COCO val2017 and ADE20K val set. All UniNet models are pre-trained on the ImageNet-1K dataset.</figDesc><table><row><cell>Backbone</cell><cell>#Params (M) Det/Seg</cell><cell>#FLOPs (G) Det/Seg</cell><cell cols="5">Mask R-CNN 1x AP@box AP@mask AP@box AP@mask Mask R-CNN 3x UperNet mIoU (%)</cell></row><row><cell>ResNet18 [17]</cell><cell>31/ -</cell><cell>207/885</cell><cell>34.0</cell><cell>31.2</cell><cell>36.9</cell><cell>33.6</cell><cell>-</cell></row><row><cell>ResNet50 [17]</cell><cell>44/ -</cell><cell>260/951</cell><cell>38.0</cell><cell>34.4</cell><cell>41.0</cell><cell>37.1</cell><cell>-</cell></row><row><cell>PVT-Tiny [50]</cell><cell>33/ -</cell><cell>208/945</cell><cell>36.7</cell><cell>35.1</cell><cell>39.8</cell><cell>37.4</cell><cell>-</cell></row><row><cell>UniNet-B1</cell><cell>28/38</cell><cell>211/877</cell><cell>40.5</cell><cell>37.5</cell><cell>44.4</cell><cell>40.1</cell><cell>42.7</cell></row><row><cell>ResNet101 [17]</cell><cell>63/86</cell><cell>336/1029</cell><cell>40.4</cell><cell>36.4</cell><cell>42.8</cell><cell>38.5</cell><cell>44.9</cell></row><row><cell>PVT-Small [50]</cell><cell>44/ -</cell><cell>245/1039</cell><cell>40.4</cell><cell>37.8</cell><cell>43.0</cell><cell>39.9</cell><cell>-</cell></row><row><cell>Swin-T [29]</cell><cell>48/60</cell><cell>267/945</cell><cell>43.7</cell><cell>39.8</cell><cell>46.0</cell><cell>41.6</cell><cell>44.5</cell></row><row><cell>UniNet-B3</cell><cell>42/51</cell><cell>270/940</cell><cell>45.2</cell><cell>41.1</cell><cell>47.9</cell><cell>42.9</cell><cell>48.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Performance on the COCO val2017 with various detection frameworks. The AP@box is reported.</figDesc><table><row><cell>Framework</cell><cell cols="4">Cascade-Mask-R-CNN ATSS Sparse-R-CNN Mask-R-CNN</cell></row><row><cell>ResNet50 [17]</cell><cell>46.3</cell><cell>43.5</cell><cell>44.5</cell><cell>41.0</cell></row><row><cell>Swin-T [29]</cell><cell>50.5</cell><cell>47.2</cell><cell>47.9</cell><cell>46.0</cell></row><row><cell>UniNet-B3</cell><cell>51.3</cell><cell>49.8</cell><cell>48.9</cell><cell>47.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Performance on ImageNet with different search settings. One type of operator is kept for comparison with the hybrid UniNet.</figDesc><table><row><cell>Model</cell><cell cols="3">#FLOPs (G) #Params (M) Top-1 Acc.</cell></row><row><cell>UniNet-B0</cell><cell>0.56</cell><cell>11.5</cell><cell>79.1</cell></row><row><cell>Convolution-Only</cell><cell>0.59</cell><cell>11.0</cell><cell>77.7</cell></row><row><cell>Transformer-Only</cell><cell>1.2</cell><cell>11.2</cell><cell>78.2</cell></row><row><cell>MLP-Only</cell><cell>0.95</cell><cell>11.4</cell><cell>76.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Performance on ImageNet of UniNet with different DSMs. Note that the traditional strided-conv downsampling module is shown in row 2.</figDesc><table><row><cell>Model</cell><cell cols="3">#FLOPs (G) #Params (M) Top-1 Acc.</cell></row><row><cell>UniNet</cell><cell>0.56</cell><cell>11.5</cell><cell>79.1</cell></row><row><cell>w/ L-DSM</cell><cell>0.54</cell><cell>11.3</cell><cell>78.5</cell></row><row><cell>w/ G-DSM</cell><cell>0.77</cell><cell>12.7</cell><cell>76.8</cell></row><row><cell>w/ LG-DSM</cell><cell>0.72</cell><cell>14.1</cell><cell>78.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Performance comparison on ImageNet of different backbones when equipped with our proposed DSMs.</figDesc><table><row><cell>Model</cell><cell cols="3">#FLOPs (G) #Params (M) Top-1 Acc.</cell></row><row><cell>PVT-Tiny [50]</cell><cell>1.9</cell><cell>13.2</cell><cell>75.1</cell></row><row><cell>w/ LG-DSM</cell><cell>3.1</cell><cell>17.3</cell><cell>78.6</cell></row><row><cell>w/ L?LG-DSM</cell><cell>2.0</cell><cell>14.3</cell><cell>77.5</cell></row><row><cell>Swin-T [29]</cell><cell>4.5</cell><cell>29.0</cell><cell>81.2</cell></row><row><cell>w/ LG-DSM</cell><cell>6.4</cell><cell>33.4</cell><cell>81.9</cell></row><row><cell>w/ L?LG-DSM</cell><cell>4.7</cell><cell>30.0</cell><cell>81.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Applying proposed DSM to various architectures.</figDesc><table><row><cell></cell><cell cols="5">EffNet-B1 Swin-S Swin-B PVT-S PVT-M</cell></row><row><cell>w/o our DSM</cell><cell>79.1</cell><cell>83.2</cell><cell>83.5</cell><cell>79.8</cell><cell>81.2</cell></row><row><cell>w/ our DSM</cell><cell>79.6</cell><cell cols="3">83.4 83.9 81.6</cell><cell>82.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>UniNet-B1 to B6 architectures. c and r are channels and repeats, respectively. S denotes stage. Input size is set to be divisible by 32 as the total down-sampling stride is 32.</figDesc><table><row><cell></cell><cell>B1</cell><cell>B2</cell><cell>B3</cell><cell>B4</cell><cell>B5</cell><cell>B6</cell></row><row><cell></cell><cell cols="6">c r c r c r c r c r c r</cell></row><row><cell cols="7">S0 48 2 48 3 56 3 64 4 64 5 96 6</cell></row><row><cell cols="7">S1 80 4 80 6 96 7 112 9 112 10 160 12</cell></row><row><cell cols="7">S2 128 4 128 6 160 7 192 9 224 10 256 12</cell></row><row><cell cols="7">S3 128 4 128 6 160 7 192 9 224 10 256 12</cell></row><row><cell cols="7">S4 256 8 256 12 288 14 352 18 448 20 512 24</cell></row><row><cell>Input size</cell><cell cols="2">224 256</cell><cell>288</cell><cell>320</cell><cell>384</cell><cell>448</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Drop path rate for training UniNet models.</figDesc><table><row><cell cols="2">Model B0 B1 B2 B3 B4 B5 B6</cell></row><row><cell>dr</cell><cell>0 0 0.1 0.1 0.3 0.5 0.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>Training settings on ImageNet-1K.</figDesc><table><row><cell>config</cell><cell>pretrain</cell></row><row><cell>optimizer</cell><cell>AdamW [24]</cell></row><row><cell>learning rate</cell><cell>0.001</cell></row><row><cell>weight decay</cell><cell>0.05</cell></row><row><cell>batch size</cell><cell>1024</cell></row><row><cell>learning rate schedule</cell><cell>cosine decay</cell></row><row><cell>warmup epochs</cell><cell>5</cell></row><row><cell>training epochs</cell><cell>300</cell></row><row><cell>augmentation</cell><cell>RandAug(9, 0.5) [6]</cell></row><row><cell>LabelSmooth</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 16 :</head><label>16</label><figDesc>Finetune settings on ImageNet-1K.</figDesc><table><row><cell>config</cell><cell>value</cell></row><row><cell>optimizer</cell><cell>AdamW [24]</cell></row><row><cell>learning rate</cell><cell>2.5e-6</cell></row><row><cell>weight decay</cell><cell>1e-8</cell></row><row><cell>batch size</cell><cell>512</cell></row><row><cell>learning rate schedule</cell><cell>cosine decay</cell></row><row><cell>warmup epochs</cell><cell>0</cell></row><row><cell>training epochs</cell><cell>30</cell></row><row><cell>augmentation</cell><cell>RandAug(9, 0.5) [6]</cell></row><row><cell>LabelSmooth</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14</head><label>14</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 17 :</head><label>17</label><figDesc>Training settings on ImageNet-21K.</figDesc><table><row><cell>config</cell><cell>value</cell></row><row><cell>optimizer</cell><cell>AdamW [24]</cell></row><row><cell>learning rate</cell><cell>0.0006</cell></row><row><cell>weight decay</cell><cell>0.005</cell></row><row><cell>batch size</cell><cell>2048</cell></row><row><cell>learning rate schedule</cell><cell>cosine decay</cell></row><row><cell>warmup epochs</cell><cell>5</cell></row><row><cell>training epochs</cell><cell>90</cell></row><row><cell>augmentation</cell><cell>RandAug(9, 0.5) [6]</cell></row><row><cell>LabelSmooth</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 18 :</head><label>18</label><figDesc>Training settings on ADE20K.</figDesc><table><row><cell>config</cell><cell>value</cell></row><row><cell>optimizer</cell><cell>Adam [24]</cell></row><row><cell>learning rate</cell><cell>0.001</cell></row><row><cell>weight decay</cell><cell>0.05</cell></row><row><cell>batch size</cell><cell>16</cell></row><row><cell cols="2">learning rate schedule linear</cell></row><row><cell>warmup steps</cell><cell>1500</cell></row><row><cell>training steps</cell><cell>160K</cell></row><row><cell>input resolution</cell><cell>512 ? 512</cell></row><row><cell cols="2">8.4 Neural Architecture Search (NAS) with Reinforcement</cell></row><row><cell>Learning</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>ArXiv abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<title level="m">High-performance large-scale image recognition without normalization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09791</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Conditional positional encodings for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">W</forename><surname>Bo Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno>ArXiv abs/2102.10882</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<title level="m">Fbnetv3: Joint architecture-recipe search using neural acquisition function</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04803</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697</idno>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01401</idno>
		<title level="m">Container: Context aggregation network</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lip: Local importance-based pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3355" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nasvit: Neural architecture search for efficient vision transformers with gradient conflict aware supernet training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12259" to="12269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06263</idno>
		<title level="m">Cmt: Convolutional neural networks meet vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<title level="m">Gaussian error linear units (gelus). arXiv: Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8126" to="8135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12368</idno>
		<title level="m">Vision permutator: A permutable mlp-like architecture for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08248</idno>
		<title level="m">How much position information do convolutional neural networks encode? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04454</idno>
		<title level="m">Convmlp: Hierarchical convolutional mlps for vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pay attention to mlps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11694</idno>
		<title level="m">Fnas: Uncertaintyaware fast neural architecture search</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition 2021</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition 2021</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detail-preserving pooling in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saeedan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9108" to="9116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16519" to="16529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">How to train your vit? data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10270</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00298</idno>
		<title level="m">Efficientnetv2: Smaller models and faster training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<title level="m">Mlp-mixer: An all-mlp architecture for vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Resmlp: Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12894" to="12904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Alphanet: Improved training of supernets with alpha-divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10760" to="10771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attentivenas: Improving neural architecture search via attentive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6418" to="6427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Carafe++: Unified contentaware reassembly of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14881</idno>
		<title level="m">Early convolutions help transformers see better</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11816</idno>
		<title level="m">Incorporating convolution designs into visual transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6022" to="6031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
