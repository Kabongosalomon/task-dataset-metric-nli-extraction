<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lujia</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection in 3D with stereo cameras is an important problem in computer vision, and is particularly crucial in low-cost autonomous mobile robots without LiDARs. Nowadays, most of the best-performing frameworks for stereo 3D object detection are based on dense depth reconstruction from disparity estimation, making them extremely computationally expensive. To enable real-world deployments of vision detection with binocular images, we take a step back to gain insights from 2D imagebased detection frameworks and enhance them with stereo features. We incorporate knowledge and the inference structure from real-time one-stage 2D/3D object detector and introduce a light-weight stereo matching module. Our proposed framework, YOLOStereo3D, is trained on one single GPU and runs at more than ten fps. It demonstrates performance comparable to state-of-the-art stereo 3D detection frameworks without usage of LiDAR data. The code will be published in https://github.com/Owen-Liuyuxuan/visualDet3D.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>3D object detection is a fundamental problem in computer vision, and a crucial engineering problem for autonomous vehicles and mobile robots <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref>. With two horizontally-aligned RGB cameras with known displacement, it is possible to estimate depth by triangulation according to the pin-hole camera model. Though this is a non-direct measurement for depth and is, in most cases, less robust than LiDAR-based approaches, the binocular setup is generally much cheaper and it is promising for low-cost applications such as mobile robots and autonomous logistic vehicles.</p><p>Many of the state-of-the-art frameworks in stereo 3D object detection stem from the idea of pseudo-LiDAR and are motivated by general stereo matching algorithms. However, in 3D object detection, the model should focus on foreground objects. It is expected to *This work was supported by the National Natural Science Foundation of China <ref type="bibr">(</ref> be as accurate as possible since a disparity error of one or two pixels would cause a large error in terms of real-world distance. Many researchers have delved deep into these problems to improve the performance of pseudo-LiDAR-based algorithms; some directly finetune the estimation of point clouds to improve performance <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, while others utilize instance segmentation to focus the stereo matching network on foreground pixels <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. However, a high-performance disparity estimation network, e.g., PSMNet <ref type="bibr" target="#b7">[8]</ref>, usually takes more than 300 ms per frame on modern hardware on the KITTI dataset <ref type="bibr" target="#b8">[9]</ref> and requires a huge GPU memory to train. These issues hinder the deployment of stereo systems on low-cost robotic applications.</p><p>Many of the works mentioned above have shown in practice that transforming images into 3D features is usually sub-optimal and computationally expensive. To improve the efficiency of stereo 3D detection algorithms while maintaining as much of their performance as possible, we propose selecting a different architecture. Instead of casting the problem as a 3D detection problem with less accurate point clouds, we take a step back and treat it as a monocular 3D detection task with enhanced stereo features, which is the fundamental motivation of this work.</p><p>The framework we propose, YOLOStereo3D, is a light-weight one-stage stereo 3D detection network (Section III-A). To efficiently produce powerful stereo features, we re-introduce the pixel-wise correlation module to construct the cost-volume, instead of the popular concatenation-based module (Section III-B.1). Such a module produces a thin 2D feature map where each channel corresponds to a disparity hypothesis in stereo matching. We then apply this module hierarchically to efficiently produce stereo features as 2D feature maps (Section III-B.2), and we densely fuse these features (Section III-B.3) to form the base-feature of detection heads. The network is trained end-to-end without the use of LiDAR data (Section III-C).</p><p>The main contributions of this paper are three-fold.</p><p>? For the inference architecture, we incorporate and optimize the inference pipeline from one-stage monocular 3D detection into stereo 3D detection. ? For the design of the network, we introduce a point-wise correlation module in stereo detection tasks and propose a hierarchical, denselyconnected structure to utilize stereo features from multiple scales. YOLOStereo3D extracts multi-scale features from binocular images with a backbone network (a). These features are passed through a multi-scale stereo matching and fusion module (b) as described in Section III-B. Finally, the fused features are concatenated with the last feature from the left image and sent to the classification/regression branch to densely predict the 3D bounding boxes (c/d). The network also produces a disparity estimation during training (e).</p><p>? For the experimental results, the proposed YOLOStereo3D produces competitive results on the KITTI 3D benchmark without using point clouds and with an inference time of less than 0.1 seconds per frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS A. Stereo Matching</head><p>Stereo matching algorithms focus on estimating the disparity between binocular images. The current state-of-the-art frameworks for stereo matching apply siamese networks for feature extraction from two images, and construct 3D cost volumes to search the disparity value on each pixel exhaustively. Early research applied the dot-product between binocular feature maps, with the resulting correlation directly forming an estimation of the disparity distribution [10] <ref type="bibr" target="#b10">[11]</ref>. PSMNet <ref type="bibr" target="#b7">[8]</ref> and GCNet <ref type="bibr" target="#b12">[12]</ref> constructed concatenationbased cost volumes and applied multiple 3D convolutions to produce disparity outputs. The recent FADNet managed to perform a fast stereo estimation with a point-wise correlation module <ref type="bibr" target="#b13">[13]</ref>. Zhang et al. proposed stereo focal loss to improve the loss function formulation in disparity estimation <ref type="bibr" target="#b14">[14]</ref>. Our work, similar to many other stereo 3D object detection algorithms, is developed upon these studies and utilizes the stereo matching features to boost detection performance.</p><p>B. Visual 3D Object Detection 1) Stereo 3D Object Detection: Stereo 3D object detection is usually considered as a tractable but computationally hard problem. Recent advances in stereo 3D object detection algorithms are based on the idea of pseudo-LiDAR <ref type="bibr" target="#b15">[15]</ref>. DispRCNN <ref type="bibr" target="#b4">[5]</ref>, ZoomNet <ref type="bibr" target="#b5">[6]</ref>, and OC Stereo <ref type="bibr" target="#b6">[7]</ref> applied instance segmentation on binocular images to construct a local point cloud for each detected instance to improve the accuracy of disparity estimation on foreground objects. Pseudo-LiDAR++ <ref type="bibr" target="#b2">[3]</ref> recognized that uniform 3D convolution might not be suitable to process the disparity cost volume, and transformation to the depth cost volume may be needed.</p><p>We point out that all the aforementioned algorithms require more than 0.3 seconds runtime per frame. Moreover, Pseudo-LiDAR++ <ref type="bibr" target="#b2">[3]</ref>, ZoomNet <ref type="bibr" target="#b5">[6]</ref>, OC Stereo <ref type="bibr" target="#b6">[7]</ref> and DSGN <ref type="bibr" target="#b16">[16]</ref> required point cloud data during training or need point cloud data to help the training process. DispRCNN <ref type="bibr" target="#b4">[5]</ref> and the baseline Pseudo-LiDAR <ref type="bibr" target="#b15">[15]</ref> required off-the-shelf disparity modules, which are usually trained with depth images or point cloud data.</p><p>YOLOStereo3D is a light-weight model that performs most of the convolution operation in the perspective view, and the training and inference are significantly lighter and faster than all methods mentioned above. Moreover, the training process of YOLOStereo3D does not depend on point-cloud data.</p><p>2) Monocular 3D Object Detection: Monocular 3D object detection is an ill-posed problem, but it provides many insights into how depth information can be estimated from a single image. Tom et al. <ref type="bibr" target="#b17">[17]</ref> demonstrated that a typical monocular depth estimation network mainly estimates depth from the vertical position of an object. The authors <ref type="bibr" target="#b17">[17]</ref> provided the theoretical background for pseudo-LiDAR in monocular detection <ref type="bibr" target="#b18">[18]</ref> <ref type="bibr" target="#b19">[19]</ref>. YOLOStereo3D is built upon the inference structure of M3D-RPN <ref type="bibr" target="#b20">[20]</ref> and GAC <ref type="bibr" target="#b21">[21]</ref> and further enhances the final features with stereo matching results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>In this section, we elaborate on the network structure and methods applied in this paper. First, we introduce the output definition and data-preprocessing tricks imported into and optimized for YOLOStereo3D <ref type="bibr" target="#b21">[21]</ref>. Second, we re-introduce the light-weight cost volume that speeds up stereo matching and present the hierarchical densely-connected structure that fully exploits such thin features. Finally, we deliver the loss function as well as the training and inference scheme of YOLOStereo3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Anchors Definition and Preprocessings</head><p>Since we adopt the inference structure of a monocular 3D object detection framework, we need to import the basic definition of anchors and we propose multiple optimized processing methods. In this subsection, we present some of the preprocessing on the input and output of the network.</p><p>1) 3D Anchors and Statistical Priors: Each anchor is described by 12 regressed parameters including [x 2d , y 2d , w 2d , h 2d ] for the 2D bounding boxes;</p><p>[c x , c y , z] for the 3D centers of objects on the left image; [w 3d , h 3d , l 3d ] corresponding to the width, height and length of the 3D bounding boxes respectively; and [sin(2?), cos(2?)] to estimate the observation angle/orientation of objects.</p><p>We observe that [sin(?), cos(?)] and [sin(? + ?), cos(? + ?)] correspond to the same rectangular bounding box results in 3D object detection. As a result, we instead predict [sin(2?), cos(2?)] in the regression branch. We also add a classification channel to predict if |?| &gt; ? 2 to eliminate ambiguity, which intuitively means whether or not the object is facing the camera.</p><p>We incorporate 3D statistic priors into 2D anchors to improve the regression results. To collect prior statistics of the anchors, we iterate through the training set, and for each anchor box of different shapes, collect all the objects assigned to this anchor based on the IoU metric. Then, we compute the mean and the variance of z, sin(2?), cos(2?) for each box.</p><p>Furthermore, we explicitly exploit scene-specific knowledge for autonomous vehicles and utilize the statistical information from the anchor boxes. During training, we project dense anchor boxes into 3D with the mean depth value? and filter out anchor boxes that are far away from the ground plane based on their, as displayed in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>For multi-class training, since the statistics for different types of obstacles, e.g., cars and pedestrians, are significantly different, we compute 3D priors for each category, separately. During training, we filter out anchor boxes dynamically based on the categories assigned. During inference, we also filter out anchor boxes dynamically based on the anchors' local categorical predictions.</p><p>2) Data Augmentation for Stereo 3D Detection: Data augmentation is useful to improve the generalization ability in deep learning applications. However, the nature of stereo 3D detection limits the number of possible augmentation choices. We follow <ref type="bibr" target="#b20">[20]</ref> to apply photometric distortion concurrently on binocular images. We also follow <ref type="bibr" target="#b22">[22]</ref> to apply random flipping online during training. Random flipping includes flipping both RGB images, flipping the position and orientation of objects, and then switching left/right images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Scale Stereo Feature Extraction</head><p>The extraction of stereo features is one of the most time-consuming parts for many pre-existing stereo 3D object detection algorithms. In this subsection, we reintroduce the cost volume formulation based on dotproduct/cosine-similarity and present the hierarchical structure to utilize these features effectively.</p><p>1) Light-weight Cost Volume: Current state-of-the-art stereo matching algorithms usually construct 3D cost volume with concatenation, where the module iteratively shifts the right feature map horizontally over the left feature map, and at each step, concatenate the two features at each overlapping pixel. For binocular feature maps with the shape [B, C, H, W ], the shape of the output tensor f i is [B, 2 ? C, max disp, H, W ]. In this paper, we follow <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b13">[13]</ref> to apply a normalized dot-product to construct a thin cost volume. Such a module compute correlation between two overlapping pixels of the feature maps instead. The shape of the output tensor f i becomes [B, max disp, H, W ].</p><p>The stereo matching process can be much faster. Consider two input feature maps of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">64,</ref><ref type="bibr">72,</ref><ref type="bibr">320]</ref>, which is a common shape of a KITTI image down scaled by 4. The forward pass of concatenation-based cost volume construction takes about 200 ms while the correlationbased cost volume takes about 7 ms on an Nvidia-1080Ti.</p><p>However, the number of output channels is smaller, which could cause the network to be numerically skewed towards monocular features during the fusion stage and downsampling the stereo matching results could induce further information loss. We ease these two problems with densely connected ghost modules <ref type="bibr" target="#b23">[23]</ref> and a hierarchical fusion structure.</p><p>2) Densely Connected Ghost Module: As mentioned in Section III-B.1, we need to expand the width of the features to guide the network to skewed towards features produced by stereo matching.</p><p>Han et al. propose the ghost module, which is an efficient module to produce redundant features <ref type="bibr" target="#b23">[23]</ref>. It applies depthwise convolution to produce extra features, which requires significantly fewer parameters and FLOPs. We go one step further and densely concatenate the original input features with the output of the original ghost module, thereby tripling the number of channels. As indicated in <ref type="figure" target="#fig_0">Figure 1</ref>, the mauve blocks in (b) are the results from ghost module and others denote densly connected residuals.</p><p>Such a module preserves more information before downsampling and also rebalances the number of channels between stereo features and monocular semantic features during the fusion phase.</p><p>3) Hierachical Multi-scale Fusion Structure: To minimize the information loss during the stereo matching phase while keeping the computational time tractable, we engineer a hierarchical fusion scheme. At the downsampling level of 1 4 and 1 8 , we construct a light-weight cost volume of a max-disparity of 96 and 192, respectively. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, they are fed into a densely connected ghost module, downsampled, and concatenated with features at a smaller scale. At a downsampling level of <ref type="bibr">1 16</ref> , we first downsample the number of channels with 1 ? 1 convolution. We then construct a small concatenation-based cost volume (also flattened to be a 2D feature map) to preserve more semantic information from the right images.</p><p>This arrangement can also be justified with highlevel reasoning. Features with higher resolution are usually local features with higher frequency portions, which are suitable for dense and accurate disparity estimation. In contrast, features with low resolution contain semantic information at a larger scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Scheme and Loss Function</head><p>The overall network structure is presented in <ref type="figure" target="#fig_0">Figure 1</ref>. Multi-scale features from binocular images are extracted and fused into stereo features to construct hierarchical cost volumes. The stereo feature map is concatenated with the last feature map of the left image and fed to the regression/classification branch. The stereo feature map is also fed into a decoder to predict a disparity map trained with an auxiliary loss. The auxiliary loss can regularize the training process.</p><p>1) Auxiliary Disparity Supervision in Training: As pointed out by Chen et al. <ref type="bibr" target="#b16">[16]</ref>, disparity supervision is important to improve detection performance. We also observe a similar phenomenon in our framework. Without disparity supervision, the network may not be guided to produce local features useful in stereo matching to fully utilize the geometric potential of binocular images, and the network could be trapped in a local minimum similar to that of a monocular detection network.</p><p>We upsample the output of the final stereo features to [W/4, H/4], and supervise the prediction with a sparse "ground truth" disparity derived from the traditional block matching algorithm in OpenCV <ref type="bibr" target="#b24">[24]</ref> during training. During evaluation and testing, this disparity estimation branch is disabled to improve efficiency.</p><p>Though the disparity from the block matching algorithm is coarse and sparse, we empirically show that it significantly improves the network's performance.</p><p>2) Loss Function: We apply focal loss <ref type="bibr" target="#b26">[26]</ref>[27] on classification, and smoothed-L1 loss <ref type="bibr" target="#b29">[28]</ref> on bounding box regression. We follow the scheme of <ref type="bibr" target="#b14">[14]</ref> to apply stereo focal loss on the auxiliary disparity estimation. First, we compute the expected distribution of disparity with a hard-coded variance ? = 0.5:  Where d represents the disparity and c d indicates the predicted confidence at disparity d. Then, following <ref type="bibr" target="#b14">[14]</ref>, stereo focal loss is defined as:</p><formula xml:id="formula_0">P (d) = softmax ? |d ? d gt | ? = exp ?c gt d D?1 d =0 exp ?c gt d .</formula><formula xml:id="formula_1">L SF = 1 |P| p?P D?1 d=0 (1 ? P p (d)) ?? ? ?P p (d) ? logP p (d)</formula><p>where D is the max-disparity, ? is the focus weight, P presents the set of pixels involved, and P p (d),P p (d) represents the expected and predicted distribution map of disaparity d.</p><p>The final loss function is simply the sum of the three losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluate our method on the KITTI Object Detection Benchmark <ref type="bibr" target="#b8">[9]</ref>. The dataset consists of 7,481 training frames and 7,518 test frames. Chen et al. <ref type="bibr" target="#b30">[29]</ref> further split the training set into 3,712 training frames and 3,769 validation frames. In this section, we provide further training details and show the performance of YOLOStereo3D on the test set to compare it with existing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation and Training Details</head><p>Modern deep learning frameworks are sensitive to hyperparameters choices, and critical design choices could profoundly influence the final performance. We introduce some crucial design choices before showing the performance, and the code will be made open source upon publication.</p><p>We first determine the structure and the hyperparameters of the network on Chen's split <ref type="bibr" target="#b30">[29]</ref>. Then, we retrain the final network on the entire training set with the same hyperparameters before uploading the results for testing onto the KITTI server. An ablation study is also conducted on the validation set of Chen's split. The backbone of the network is ResNet-34 <ref type="bibr" target="#b31">[30]</ref>. The top 100 pixels of each image are cropped to speed up inference and training. The cropped input images are scaled to 288?1280. The network is trained with a batch size of 4 on a single Nvidia 1080Ti GPU (it takes about 7 GB of GPU memory, significantly less than other SOTA stereo detection algorithms) for 50 epochs on the KITTI training dataset. During inference, the network is fed one image at a time, and the total average processing time, including file IO, is about 0.08 s per frame. In contrast, most other stereo-based networks in the KITTI benchmark are several times slower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on Test Set</head><p>The results are presented in <ref type="table" target="#tab_1">Table I</ref> alongside those of other state-of-the-art stereo 3D detection algorithms.</p><p>The proposed YOLOStereo3D is fast and outperforms many pseudo-LiDAR methods or local point cloud methods and is the best performing algorithm without LiDAR usage. It also outperforms DSGN <ref type="bibr" target="#b16">[16]</ref> on pedestrian detection without an additional training schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Test results for Monocular 3D Setting</head><p>To verify the effectiveness of the proposed anchor pre-processing techniques, we further test them in the task of monocular 3D object detection. Recall that we claim YOLOStereo3D being a monocular detector enhanced with stereo features. By taking away the image from the right camera, the multi-scale fusion module, and the disparity estimation branch, we obtain a standalone monocular detector. We enhance the backbone to be ResNet-101 <ref type="bibr" target="#b31">[30]</ref>. Following the proposed YOLOStereo3D, we compute the statistic for each anchor box and filter out deviated anchor boxes during training.</p><p>We also follow M3D-RPN <ref type="bibr" target="#b20">[20]</ref> to post-process the prediction results to maximize the 2D-3D coherence. Notice that in YOLOStereo3D, we empirically find this post-processing step deteriorate the final performance, but it is beneficial in the monocular setting.</p><p>The results are presented at <ref type="table" target="#tab_1">Table III</ref>. As shown in <ref type="table" target="#tab_1">Table III</ref>, the proposed framework achieves state-of-theart performance in KITTI Object Detection Benchmark under the monocular setting. The running time of the proposed monocular detector is about 50 ms per frame. In this section, we further analyze the performance of YOLOStereo3D and discuss the effectiveness of several important design choices. The baseline model here is only trained on the "Car" type. We first conduct an ablation study to validate the contribution of anchor preprocessing, hierarchical fusion, and the densely connected ghost module on the validation set. Then, we present and discuss some qualitative results.</p><p>A. Ablation Study 1) Anchor Preprocessing: We first test the effectiveness of including statistical information in each anchor. In the first experiment, instead of predicting a depth value normalized by the depth prior, the network outputs a transformed depth output?, where z = 1/?(?) ? 1, following <ref type="bibr" target="#b36">[34]</ref>. In the second experiment, we do not filter out anchors during training, and the training loss is evaluated with all anchors. We conduct these two experiments in both the stereo setting and the monocular setting. The results are presented in <ref type="table" target="#tab_1">Table IV</ref> respectively.</p><p>From the two table, we can observe that anchor priors significantly boost the performance of the network, and filtering out irrelevant anchors during training is also helpful. The performance gain can be observed in both monocular 3D detection and stereo 3D detection. We suggest that we can ease the difficulty of depth inferencing by properly defining and preprocessing anchors specifically for 3D scene understanding in autonomous driving.</p><p>The improvement we apply on anchors can also be applied and verified in monocular 3D detection. We further provide ablation experiments to validate the effectiveness of these processing methods under monocular 3D detection setting.</p><p>2) Densely Connected Ghost Module: Denselyconnected ghost modules are useful in expanding the number of channels in stereo processing. We conduct two experiments to verify its effectiveness. In the first experiment, we use a BasicBlock in resnet <ref type="bibr" target="#b31">[30]</ref> to replace the ghost module without expanding the number of channels, resulting in fewer channels during the fusion between RGB features and stereo features. In the second experiment, we directly upsample the number of channels with 1 ? 1 convolution before feeding the tensor into a BasicBlock.</p><p>We can observe from <ref type="table" target="#tab_1">Table IV</ref> that the denselyconnected ghost module is useful in improving the network's capability. From the first experiment, we demonstrate that expanding the number of channels is crucial for the network's performance. In the second experiment, we further show that the densely-connected ghost module is better at preserving information than the naive 1 ? 1 convolution.</p><p>3) Hierachical Fusion: We respectively disable the stereo matching output on scale 8/16 to produce two networks to justify the usage of multi-scale fusion. We can observe from <ref type="table" target="#tab_1">Table IV</ref> that the results of the two ablated models are inferior to that of the baseline model. We also point out that the forward pass of stereo matching modules on scale 8/16 is several times faster than that on scale 4.</p><p>As a result, we argue that hierarchically fusing stereo features from scale 8 and 16 is worth the effort. The baseline structure of hierarchical fusion in YOLOStereo3D achieves a fair balance between speed and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Disparity Supervision:</head><p>We also have an ablation study on the importance of disparity supervision. Similar to the conclusion in DSGN <ref type="bibr" target="#b16">[16]</ref>, disparity supervision significantly boosts the performance of the network.</p><p>In the experiment, we show that such supervision is essential, but the results are not sensitive to the accuracy of the "target" disparity map. The insight is that the network may only need slight regularizations in stereo matching submodules. The auxiliary loss is required to drive the network from falling back to a naive local optimal of monocular 3D object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Results</head><p>We show qualitative validation results in <ref type="figure" target="#fig_2">Figure 3</ref>. The model displayed is YOLOStereo3D sharing the same hyperparameters as the model submitted to the KITTI server, but it is only trained on the training subsplit.</p><p>From the RGB images, we can observe that most of the successful predictions of YOLOStereo3D are visually consistent with the context. As shown in the bird's-eye-view images, though the disparity estimation may not correctly align with the ground truth 3D bounding boxes, the bounding box predictions from YOLOStereo3D are still reasonably accurate.</p><p>The examples suggest that priors in anchor heads and the fusion between stereo matching features and RGB features could help the network to produce more visually consistent predictions and make the network more robust against potentially misleading disparity matching results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we presented YOLOStereo3D, an efficient stereo 3D object detection framework. This work's major contribution is to take a step back to consider stereo 3D object detection as an enhanced monocular detection problem, rather than as an inaccurate LiDARbased detection problem. To achieve this, we first incorporated knowledge from real-time monocular 3D object detection frameworks and used priors in anchors for depth inference. Then, we introduced the pointwise correlation module into the detection problems. Finally, we used a hierarchical fusion framework that balances information preservation and computational burden. We tested YOLOStereo3D on the KITTI Object Detection Benchmark. The model produces competitive results among stereo frameworks while running at more than ten frames per second without any usage of LiDAR data.</p><p>It should be noted that we are converting the features from the right image to the left image. In other words, the computational roles of the two images are not equal. Information loss in the right image is significant. As a result, when an object is occluded in the left image but is more visible in the right image, the model could be significantly sub-optimal.</p><p>Nevertheless, since the model can achieve a competitive result with only one GPU and a short training time, YOLOStereo3D lowers the bar of stereo detection research. With a significantly faster inference speed and competitive performance, YOLOStereo3D can also boost the deployment of stereo setups on self-driving cars and mobile robots in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Network inference structure of YOLOStereo3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>We project the center of each anchor box from the left image plane to 3D with its mean distance?. We visualize the projected 3D bounding boxes with the mean width/height/length of the cars. We filter out anchors that are far from the ground plane during training (transparentized in the figure). Point clouds are displayed to indicate 3D positions in the figure. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Qualitative examples from the validation set. The RGB images show the detection results and ground truth 3D bounding boxes on the left images. The bird's eye view images show the disparity prediction from the networks, along with detection results. The blue bounding boxes are 3D predictions from YOLOStereo3D, the pink bounding boxes are ground truth 3D bounding boxes, and point clouds are predictions from the disparity estimation branch of YOLOStereo3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Grant No. U1713211), the Research Grant Council of Hong Kong SAR Government, China, under Project No. 11210017, and No. 21202816, and Shenzhen Science, Technology and Innovation Comission (SZSTI) JCYJ20160428154842603, awarded to Prof. Ming Liu. And it was supported by the Guangdong Science and Technology Plan Guangdong-Hong Kong Cooperation Innovation Platform (Grant Number 2018B050502009) awarded to Lujia Wang. (Lujia Wang is the corresponding author). Yuxuan Liu and Ming Liu are with the Robotics and Multi-Perception Laborotary, Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology yliuhb@connect.ust.hk ,eelium@ust.hk 2 Lujia Wang is with Cloud Computing Lab of Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China. lj.wang1@siat.ac.cn</figDesc><table /><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>3D object detection results on the KITTI test set on Car. "*" indicates usage of point cloud data or pretrained disparity estimation module.</figDesc><table><row><cell>Methods</cell><cell>Easy/Moderate/Hard</cell><cell>Time</cell></row><row><cell>RT3DStereo[25]</cell><cell>29.90 %/23.28 %/ 18.96 %</cell><cell>0.08s</cell></row><row><cell>StereoRCNN[22]</cell><cell>47.58 %/30.23 %/ 23.72 %</cell><cell>0.30s</cell></row><row><cell>Pseudo-LiDAR*[19]</cell><cell>54.53 %/34.05 %/ 28.25 %</cell><cell>0.40s</cell></row><row><cell>OC Stereo*[7]</cell><cell>55.15 %/37.60 %/ 30.25 %</cell><cell>0.35s</cell></row><row><cell>ZoomNet*[6]</cell><cell>55.98 %/38.64 %/ 30.97 %</cell><cell>0.35s</cell></row><row><cell>Disp R-CNN(velo)*[5]</cell><cell>59.58 %/39.34 %/ 31.99 %</cell><cell>0.42s</cell></row><row><cell>Pseudo-LiDAR++*[3]</cell><cell>61.11 %/42.43 %/ 36.99 %</cell><cell>0.40s</cell></row><row><cell>DSGN*[16]</cell><cell>73.50 %/52.18 %/ 45.14 %</cell><cell>0.67s</cell></row><row><cell>Ours YOLOStereo3D</cell><cell>65.68 %/41.25 %/ 30.42 %</cell><cell>0.08s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>3D object detection results on the KITTI test set on Pedestrians.</figDesc><table><row><cell>Methods</cell><cell>Easy/Moderate/Hard</cell><cell>Time</cell></row><row><cell>RT3DStereo[25]</cell><cell>3.28 %/ 2.45 %/ 2.35 %</cell><cell>0.08s</cell></row><row><cell>OC Stereo*[7]</cell><cell>24.48 %/ 17.58 %/ 15.60 %</cell><cell>0.35s</cell></row><row><cell>DSGN*[16]</cell><cell>20.53 %/ 15.55 %/ 14.15 %</cell><cell>0.67s</cell></row><row><cell>Ours YOLOStereo3D</cell><cell>28.49 %/ 19.75 %/ 16.48 %</cell><cell>0.08s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Monocular 3D object detection results of Cars on the KITTI test set.</figDesc><table><row><cell>Methods</cell><cell>IoU ? 0.7 3D Easy/Moderate/Hard</cell><cell>Time</cell></row><row><cell>M3D-RPN[20]</cell><cell>14.76 % / 9.71 % / 7.42 %</cell><cell>0.16s</cell></row><row><cell>RTM3D[31]</cell><cell>14.41 % / 10.34 % / 8.77 %</cell><cell>0.05s</cell></row><row><cell>AM3D[32]</cell><cell>16.50 % / 10.74 % / 9.52 %</cell><cell>0.40s</cell></row><row><cell>D4LCN[33]</cell><cell>16.65 % / 11.72 % / 9.51 %</cell><cell>0.20s</cell></row><row><cell>Ours</cell><cell>19.24 %/ 12.37 %/ 8.67 %</cell><cell>0.05s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Ablation study results of cars on the KITTI validation set</figDesc><table><row><cell>Methods</cell><cell>IoU ? 0.7 3D Easy/Moderate/Hard</cell></row><row><cell>YOLOStereo3D</cell><cell>72.06 %/ 46.58 %/ 35.53 %</cell></row><row><cell>w/o Anchor Prior</cell><cell>65.09 %/ 41.38 %/ 30.90 %</cell></row><row><cell>w/o Anchor Filtering</cell><cell>71.37 %/ 45.03 %/ 35.83 %</cell></row><row><cell>w/o Channel Expand</cell><cell>64.16 %/ 39.96 %/ 30.02 %</cell></row><row><cell>w Naive Channel-expand</cell><cell>70.70 %/ 45.74 %/ 34.87 %</cell></row><row><cell>w/o Scale 8</cell><cell>70.80 %/ 45.71 %/ 35.86 %</cell></row><row><cell>w/o Scale 16</cell><cell>68.64 %/ 44.54 %/ 33.95 %</cell></row><row><cell>w/o Disparity supervision</cell><cell>62.58 %/ 39.09 %/ 30.34 %</cell></row><row><cell>w PC supervision</cell><cell>72.05 %/ 46.59 %/ 35.62 %</cell></row><row><cell>YOLOMono3D</cell><cell>21.66 %/ 14.20 %/ 11.07 %</cell></row><row><cell>Mono w/o Prior</cell><cell>19.90 %/ 13.36 %/ 9.68 %</cell></row><row><cell>Mono w/o Filtering</cell><cell>20.50 %/ 13.45 %/ 10.50 %</cell></row><row><cell cols="2">V. MODEL ANALYSIS AND DISCUSSION</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hdnet: Exploiting hd maps for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v87/yang18b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 2nd Conference on Robot Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, A. Billard, A. Dragan, J. Peters, and J. Morimoto</editor>
		<meeting>The 2nd Conference on Robot Learning, ser. Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-10" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end multiview fusion for 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Robot Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, L. P. Kaelbling, D. Kragic, and K. Sugiura</editor>
		<meeting>the Conference on Robot Learning, ser. Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end pseudo-lidar for image-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Zoomnet: Part-aware adaptive zooming neural network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00529</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Objectcentric stereo matching for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07566</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1803.08669</idno>
		<ptr target="http://arxiv.org/abs/1803.08669" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5695" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1510.05970</idno>
		<ptr target="http://arxiv.org/abs/1510.05970" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
		<idno>abs/1703.04309</idno>
		<ptr target="http://arxiv.org/abs/1703.04309" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fadnet: A fast and accurate network for disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10758</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adaptive unimodal cost volume filtering for deep stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03751</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07179</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dsgn: Deep stereo geometry network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How do neural networks see depth in single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C H E</forename><surname>De Croon</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.07005" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Refinedmpl: Refined monocular pseudolidar for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M U</forename><surname>Vianney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection with pseudo-lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1903.09847" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">M3D-RPN: monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.06038" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ground-aware monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yixuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="919" to="926" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stereo R-CNN based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1902.09738" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ghostnet: More features from cheap operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The OpenCV Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dr. Dobb&apos;s Journal of Software Tools</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Realtime 3d object detection for automated driving using stereo vision and semantic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hendrik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Christoph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1405" to="1410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Focal loss in 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1809.06065</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<ptr target="http://arxiv.org/abs/1809.06065" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick ; R-Cnn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>Corr</surname></persName>
		</author>
		<idno>abs/1504.08083</idno>
		<ptr target="http://arxiv.org/abs/1504.08083" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5644-3d-object-proposals-for-accurate-object-class-detection.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<ptr target="http://arxiv.org/abs/1512.03385" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
		<idno>abs/2001.03343</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1903.11444" />
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
