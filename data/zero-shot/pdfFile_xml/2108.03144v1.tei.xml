<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ELSED: Enhanced Line SEgment Drawing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iago</forename><surname>Su?rez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Graffter</orgName>
								<address>
									<addrLine>Campus Montegancedo s/n. Centro de Empresas</addrLine>
									<postCode>28223</postCode>
									<settlement>Pozuelo de Alarc?n</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Departamento de Inteligencia Artificial</orgName>
								<orgName type="institution">Universidad Polit?cnica de Madrid</orgName>
								<address>
									<addrLine>Campus Montegancedo s/n, Boadilla del Monte</addrLine>
									<postCode>28660</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">ETSII. Universidad Rey Juan Carlos</orgName>
								<orgName type="institution" key="instit2">C/ Tulip?n</orgName>
								<address>
									<postCode>28933</postCode>
									<settlement>s/n, M?stoles</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Baumela</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Departamento de Inteligencia Artificial</orgName>
								<orgName type="institution">Universidad Polit?cnica de Madrid</orgName>
								<address>
									<addrLine>Campus Montegancedo s/n, Boadilla del Monte</addrLine>
									<postCode>28660</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ELSED: Enhanced Line SEgment Drawing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image edge detection</term>
					<term>Efficient Line Segment Detection</term>
					<term>Line Segment Detection Evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting local features, such as corners, segments or blobs, is the first step in the pipeline of many Computer Vision applications. Its speed is crucial for real time applications. In this paper we present ELSED, the fastest line segment detector in the literature. The key for its efficiency is a local segment growing algorithm that connects gradient aligned pixels in presence of small discontinuities. The proposed algorithm not only runs in devices with very low end hardware, but may also be parametrized to foster the detection of short or longer segments, depending on the task at hand. We also introduce new metrics to evaluate the accuracy and repeatability of segment detectors. In our experiments with different public benchmarks we prove that our method is the most efficient in the literature and quantify the accuracy traded for such gain 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Detecting segments and full lines in digital images is a recurrent problem in Computer Vision (CV). Line segments play a important role towards understanding the geometric content of a scene as they are a compressed and meaningful representation of the image content. Moreover, segments are still present in low textured settings where the classical methods based on corners or blobs usually fail. Segment detection has been employed in a large number of CV tasks such as 3D reconstruction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, SLAM <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, Visual Odometry <ref type="bibr" target="#b4">[5]</ref>, 3D camera orientation via Vanishing Points Detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, cable detection in aircrafts <ref type="bibr" target="#b7">[8]</ref>, or road detection in Synthetic Aperture Radar images <ref type="bibr" target="#b8">[9]</ref>.</p><p>Nowadays CV algorithms are ubiquitous and they are expected to run on resource limited devices. To this end, low level algorithms such as the local feature detectors must be very efficient. Traditional global line detection approaches based on the Hough transform lack efficiency. Thus, various local methods emerged addressing the issue of efficiency. LSD <ref type="bibr" target="#b9">[10]</ref> was one of the first approaches to achieve excellent results with a local approach. Edge drawing methods further improve the efficiency <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. In a first step, they work by connecting edge pixels following the direction perpendicular to the gradient. In a second step, they fit the desired curve, a line in the simplest case, to these edges.</p><p>The method presented in this paper improves on the drawing methods by fitting a line segment to the connected edge pixels and using its direction to guide the drawing process. Fusing the drawing and line segment fitting in a single step saves time and improves the overall quality of the detected segments. In addition, our proposal allows to jump over gradient discontinuities and detect full lines or just detect the individual linear segments without jumping. This is important because line segments are features that, at the gradient level, can be easily broken by occlusions, shadows, glitches, etc. In this way, the user can define the type of segments that best suits the application. For example, we may choose to detect large segments if the goal is to do Vanishing Points estimation or short ones for reconstruction and matching.</p><p>In this paper we present an efficient method for line segment detection termed Enhanced Line SEgment Drawing (ELSED). In our experiments we compare the accuracy and efficiency of ELSED with that of the most relevant detectors in the literature. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, ELSED is not only the most efficient (note the logarithmic scale in the speed dimension) but also the most accurate in line segment detection and more repeatable among the fastest in the literature, as we show in the experimental section. It improves the efficiency of present methods in resource limited devices, opening the door to new CV applications running on any type of hardware. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous work</head><p>In this section we review the segment detection literature. To this end we organize it in three broad groups: full line detectors with global methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, those that use local properties to greedily detect line segments <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b12">13]</ref> and the deep line segment detectors <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Global information based approaches</head><p>Global methods are able to detect full lines in the image with enough edge pixels support in spite of discontinuities. These methods start with an edge detection, for example Canny <ref type="bibr" target="#b26">[27]</ref>, and then they apply a Hough Transform-like <ref type="bibr" target="#b27">[28]</ref> voting scheme. There are well known issues with these methods: the omission of some weak edges, the generation of false positives in regions with high edge density (e.g. tree leaves) or the large amount of memory required to store the accumulators.</p><p>Progressive Probabilistic Hough Transforms (PPHT) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14]</ref> solves the efficiency problem changing the entire image voting scheme by a random sampling scheme. More recent works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> address the quantization problem and also allows an efficient execution of the method. In <ref type="bibr" target="#b29">[30]</ref> the direction, length, and width of a line segment is extracted in a closed form that uses a fitted quadratic polynomial curve. To generate better segments, MCMLSD <ref type="bibr" target="#b16">[17]</ref> uses the Elder &amp; Zucker edge detector <ref type="bibr" target="#b30">[31]</ref> that propagates edge uncertainty to the Hough histogram accumulator. Last they split the detected full lines in line segments using a Markov Chain Model and a standard dynamic programming algorithm. The main drawback of this method is the efficiency, it takes 3.7 seconds to process a 640?480 image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Local methods</head><p>Local methods overcome some of the drawbacks of global approaches by departing from strong gradient pixels and greedily add neighboring pixels using the gradient information. LSD <ref type="bibr" target="#b9">[10]</ref> groups and validates image regions with a significant gradient magnitude and a similar gradient orientation in O(N ), being N the number of image pixels. Unlike the NMS used in the Canny edge detector, LSD uses a region growing process to select interesting pixels. Then, each region is validated based on the expected Number of False Alarms (NFA), computed with an A-Contrario statistical model <ref type="bibr" target="#b31">[32]</ref>. LSD is is efficient and it is able to deal with areas with high density of edge pixels (e.g. trees).</p><p>EDLines <ref type="bibr" target="#b11">[12]</ref> is also an efficient algorithm (O(N )) based on local gradient orientations. It performs segment detection in two steps: 1) edge detection and 2) line segment detection using a local approach. The edge detection step is performed with the Edge Drawing (ED) algorithm <ref type="bibr" target="#b10">[11]</ref>. ED applies the first three Canny steps: Gaussian filtering, gradient estimation, non-maximal suppression (NMS) and tries to connect the anchor pixels (local maxima in gradient magnitude) with a greedy procedure. In a second step EDLines performs line fitting. OTLines <ref type="bibr" target="#b17">[18]</ref> uses an orientation transformation to improve EDLines and avoid segment detections on circular structures. AG3line <ref type="bibr" target="#b12">[13]</ref> instead of drawing over all pixels only finds aligned anchors. They also implement a continuous validation strategy to decide whether the segment has reached its endpoint and a jumping scheme to overcome gradient discontinuities. A key difference between AG3line and ELSED is that we use every pixel in the image as part of the drawing process, which generates a chain of continuous pixels, while AG3line uses only the detected anchors. This makes the method fast but unstable, needing to validate each step and thus being slower than ELSED.</p><p>The approach of Cho et al. <ref type="bibr" target="#b18">[19]</ref> is based on linelets detection, i.e. chunks of horizontally or vertically connected pixels that result from line digitization. The linelet detection is O(N 2 ) time complexity and thus it takes takes 16.7 seconds per image. Adjacent linelets are further grouped into line segments using a probabilistic model with O(L 2 ) complexity, where L is the number of detected linelets. Last, they validate using a mixture of experts model learnt from the gradient magnitudes, orientation and from the line length of the segments in a labeled data set. They also propose a quantitative evaluation that we improve in our experiments (section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Deep line segment detectors</head><p>A closely related problem to line segments detection is wireframe parsing. It consists of predicting the scene salient straight lines and their junctions. The ShanghaiTech Wireframe data set <ref type="bibr" target="#b19">[20]</ref> contains over 5,000 hand-labelled images that allowed different methods to train obtaining competitive results. AFM <ref type="bibr" target="#b20">[21]</ref> uses an attraction field map that is next squeezed to obtain line segments. L-CNN <ref type="bibr" target="#b21">[22]</ref> proposes an End-to-end model that uses a stacked hourglass backbone to obtain a junction proposal heatmap that is extensively sampled to obtain the output segments. HAWP <ref type="bibr" target="#b22">[23]</ref> improves the L-CNN sampling step by reparametrizing the line segments in a holistic 4-D attraction field map from which segments can be obtained faster. HT-HAWP <ref type="bibr" target="#b22">[23]</ref> Adds some Deep Hough layers to the HAWP model improving its capabilities to capture lines and slightly improving the performance in some benchmarks. F-Clip <ref type="bibr" target="#b23">[24]</ref> proposes a simple yet effective approach to cast line segment detection as an object detection problem that can be solved with a fully convolutional one-stage method. LERT <ref type="bibr" target="#b24">[25]</ref> detects segments using transformers that replace the junction heatmap and segment proposals to directly predict the segment endpoints. SOLD 2 <ref type="bibr" target="#b25">[26]</ref> proposes a self-supervised way towards line detection and description that optimizes the repeatability of the detected segments.</p><p>Despite the good results of these deep methods, their computational requirements are still far away from the classical methods based on gradient. This fact makes them non-viable for limited devices where there is no GPU or rather its battery consumption is prohibitive. For this reason we introduce our drawing method that has been carefully designed to avoid floating point operation and minimize its complexity, being CPU friendly and achieving the fastest execution times on the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Enhanced Line SEgment Drawing (ELSED)</head><p>In this section we introduce our line segment detection method and explain the different steps in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Enhanced Edge Drawing algorithm (EED)</head><p>The EED entails the following high level steps: 1) Gaussian smoothing to suppress noise; 2) Gradient magnitude and orientation computation; 3) Extraction of anchor pixels, local maxima in the gradient magnitude; 4) Connect the anchors using the enhanced routing algorithm For the noise reduction step we use a convolution with a 5 ? 5 Gaussian kernel and ? = 1, for Gradient magnitude and orientation computation we first compute the horizontal, G x , and vertical, G y , gradients by applying the Sobel operator and then we use the L1 norm, G = |G x |+|G y |. We also define a gradient threshold and set G = 0 for those pixels below it and quantize the gradient orientation, O, into two possible values: vertical edge, |G x | ? |G y |, or horizontal edge, |G x | &lt; |G y |. The other two steps are explained in the next subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Extraction of anchor pixels</head><p>The anchors are pixels where the drawing process begins. We scan image pixels with G &gt; 0 and test if it is a local maxima in the gradient magnitude, G, along the quantized direction of the gradient, O. If the pixel orientation O(x, y) corresponds to a vertical edge, it is an anchor if G[x, y]?G[x?1, y] ? T anchor and G[x, y] ? G[x + 1, y] ? T anchor . The same applies for horizontal edge in vertical direction. To increase the processing speed, the number of anchors can be limited by increasing the value of T anchor and also by scanning pixels every SI = 2 columns and rows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Connecting the anchors by an enhanced routing algorithm</head><p>ED is faster than LSD's region growing because from an initial anchor, it only walks along a chain of edge pixels, evaluation 3 neighbours at each step (see <ref type="figure" target="#fig_2">Fig. 2a</ref>). These evaluations are critical for the speed of the algorithm since they are done for each reachable edge pixel walking from an anchor point.</p><p>In our EED procedure, we perform the edge drawing and line fitting at the same time. This will enable us to save computations by reducing the number of checked pixels. In each step of the walk process we consider the previous and the current pixel. This allows us to avoid situations where the original ED algorithm would draw a corner breaking the line assumption ( <ref type="figure" target="#fig_2">Fig. 2b</ref>). Thus, we explore the same pixels in the walking processes (see "Go right", "Go left", "Go up" and "Go down" in <ref type="figure" target="#fig_4">Fig. 3b</ref>) as long as the edge orientation does not change from the previous pixel to the current one.  In the case of (b) the walk is coming from the blue pixel and finds an horizontal edge pixel (pink one). Thus, it will start a walk to the left and another to the right than could give edge chains with the displayed configurations (blue-pink-yellow pixel sequence).</p><p>When the previous pixel is in a vertical edge, and the current pixel is in an horizontal one, ED has 6 candidate pixels to be added to the current line segment (see <ref type="figure" target="#fig_4">Fig. 3a</ref>). The same happens when previous pixel is in an horizontal edge and current is on a vertical one. However, if we now add the assumption that the edge chain should form a line, then the number of initial candidates changes from 6 to only 2 (see the four "diagonal" cases in <ref type="figure" target="#fig_4">Fig. 3b</ref>). This is a feature of EED algorithm. It has two advantages: 1) it is faster than the original ED routing algorithm as it explore less pixels and, 2) it avoids non meaningful cases for finding line segments. The second important idea is a also consequence of trying to find aligned edge pixels. Whereas ED changes the walking process direction when a change of edge orientation is detected (See <ref type="figure" target="#fig_5">Fig. 4a</ref>), EED tries to continue in the same direction following a line. However, any change of edge orientation is not forgotten and it is pushed into an stack for later processing (see <ref type="bibr">Algorithm 1)</ref>. EED tries to fit a line to the current chain of pixels, if more than T minLength pixels have been chained and the squared error of alignment of the pixels is lower than T LineF itErr . The last parameter for the segment search is the T P xT oSegDist that is the maximum distance in pixels from which we consider whether a pixel fits or not in the current segment. This is done internally in the function addPxToSegment in line 15 of Algorithm 1). The  process stops when no more pixels can be chained (i.e. at the limits of the image, with only already visited pixels or weak edge pixels as candidates) or a line edge discontinuity is detected (e.g. the gap between two aligned windows, a tree branch occluding part of a building, etc.). In case a discontinuity was detected we execute, in order, the following actions:</p><p>1. If we were walking along a line segment (i.e. we have fitted a line), try to extend it going on the line direction (the walking process stacked using canContinueForward and forwardPxAndDir functions in algorithm 1, lines 23 to 26) 2. If were walking along a line segment and we cannot continue forward, try to extend the line segment backwards (the walking process stacked using canContinueBackward and backwardPxAndDir functions in algorithm 1), lines 27 to 30) 3. Continue in the gradient direction, that is changing in the discontinuity (the walking process stacked in Alg. 1, line 22). This sequence of ordered actions guarantees that if a line segment is detected, all its pixels will be detected together.</p><p>We show an illustrative example in <ref type="figure" target="#fig_5">Fig. 4b</ref> where EED starts two walking processes, one up and another down. Differently to ED (see <ref type="figure" target="#fig_5">Fig. 4a</ref>), EED detects the discontinuities in the edge orientation in the chessboard corners and continues walking in the current line direction. After pixels in segments (1) to <ref type="bibr" target="#b4">(5)</ref> are chained together, the last edge orientation change detected is extracted from the stack and processed in the same way as the first long segment pixels (i.e. chaining the pixels of segments (6) to <ref type="bibr" target="#b8">(9)</ref>. The routing algorithm ends when there are no more edge orientation changes to process in the stack. Then, the next anchor point is processed by the routing algorithm. Be able to detect more segments from a single anchor is an important feature of our method that increases the detection recall with respect to EDLines as we will show in experiment 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The line segment discontinuities</head><p>A line segment can be interrupted by several edge discontinuities. These are regions of the image where the gradient orientation changes or the gradient magnitude goes to zero. Whether we aim to detect full lines or just Algorithm 1 Enhanced Edge Drawing Algorithm 1: procedure EED(a, d 0 ) 2: Input: Anchor pixel: a. Anchor gradient direction: d 0 3: Output: S: List of Segments, P: List of edge pixels 4:</p><formula xml:id="formula_0">P ? {a} ; S ? ? 5: stack ? ? ; stack.push([a, d 0 ])</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>while stack = ? do 7:</p><p>segmentFound ? false 8: </p><formula xml:id="formula_1">nOutliers ? 0 9: c, d ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>23:</head><p>if canContinueForward(s) then 24:</p><formula xml:id="formula_2">p f , d f ? s.forwardPxAndDir(c, d) 25: stack.push([p f , d f ])</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>26:</head><p>end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>27:</head><p>if canContinueBackward(s) then 28:</p><formula xml:id="formula_3">p b , d b ? s.backwardPxAndDir(c, d) 29: stack.push([p b , d b ])</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>30:</head><p>end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>31:</head><p>end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>32:</head><p>end while 33: end procedure line segments, the discontinuities of different lengths (i.e. the number of pixels where there is no edge or the edge direction is not aligned with the line segment) should be correctly skipped in a drawing process in order to detect the line segments correctly. Algorithm 1 naturally deals with this phenomena. Once a discontinuity is detected, our aim is to jump over it and continue drawing in the line segment direction if possible. A priori, the discontinuity length is unknown and our algorithm test different length candidates. As we are using a 5 ? 5 Gaussian smoothing kernel, any 1 pixel discontinuity will have effect in at least a neighbourhood of size 5 pixels, therefore we set the minimum discontinuity length to 5 pixels. In the functions canContinueFordward (Alg. 1, line 23) and can-ContinueBackward (Alg. 1, line 27), different jump lengths J are checked (in the default parameters of the algorithm we use J ? <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>). The drawing process will continue after the discontinuity if the ordered conjunction of the following conditions is true:</p><p>1. The segment is longer than the number of pixels, J, we want to jump. 2. The pixel, a, that is aligned with the segment and J pixels away from current pixel, c, is inside the image and has G[a] &gt; 0 (i.e. is not a weak edge pixel). 3. Starting from a, EED is able to draw at least J pixels following the edge direction. We call this set of J pixels the extension pixels. 4. The extension pixels are well aligned with the line segment. To check this we calculate the auto-correlation matrix of the image gradients, M, in a small neighbourhood (we take one pixel on each side of the extension pixels) and then, we assert that:</p><formula xml:id="formula_4">? 1 ? 2 ? T EigenExt<label>(1)</label></formula><p>where ? 1 and ? 2 , (? 1 &gt; ? 2 ), are the eigenvalues of the M, and</p><formula xml:id="formula_5">?(v 1 , n) ? T AngleExt<label>(2)</label></formula><p>?(?, ?) is the angular distance, v 1 is the first eigenvector of M and n is a vector normal to the segment.</p><p>In <ref type="figure" target="#fig_7">Fig.5</ref> we show different steps of the discontinuity management algorithm in a synthetic image. In <ref type="figure" target="#fig_7">Fig. 5d</ref> we show the detection process starting from the left side of the image, we fit the edge pixels (blue) to a horizontal line (green). When the edge orientation changes from horizontal to vertical, we detect the last edge pixels as outliers (orange) and thus our method detects that we are in a discontinuity. In this case, the red pixel is the last one detected when |E| &gt; T ol , where T ol the maximum number of outliers.</p><p>Next sub- <ref type="figure" target="#fig_7">figure 5e</ref> shows the check done to decide whether we should continue drawing straight or the segment has finished. Purple pixels are the pixels in the discontinuity and are skipped using the Bresenham's algorithm drawing in the current line segment direction. Red pixels are drawn following the edge direction (in green) starting from the first pixel after the discontinuity, a. We also mark in red the neighbor pixels used to validate the region using the eigenvalues of M if the extension pixels in function can-ContinueForward. Pixels in this synthetic example do not have a uniform gradient direction, thus, the process discard all extensions tested with lengths   <ref type="figure">Figure 6</ref>: Positive (green) and negative (red) segments detected by ELSED. We obtain these label comparing the ground truth segments of the YorkUrban-LineSegment data set (blue) with unvalidated ELSED detections (magenta).</p><p>J ? <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>. In the figure we show the last one, J = 9. Consequently, the algorithm closes the first segment and continues drawing in the dominant gradient direction upwards. When enough pixels are gathered, we fit another segment <ref type="figure" target="#fig_7">(Fig. 5f</ref>). When we reach the top of the image, the segment is extended in the backward direction downwards. When this happens, the mechanism to manage discontinuities is activated again as <ref type="figure" target="#fig_7">Fig. 5g</ref> shows, but this time the region meets all the criteria defined and thus the jump is executed. <ref type="figure" target="#fig_7">Fig. 5h and 5i</ref> show respectively the fitted edges and the segments. One of the limitations of local segment detection approaches is the generation of many small segments produced by gradient discontinuities. The process described above help us alleviate this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Validation of the generated segment candidates</head><p>After EED, we have several line segments detections, many of them potentially wrong (see red segments in <ref type="figure">Fig. 6</ref>). They occur mainly in regions with high density of edge pixels. To validate a line we use the segment pixels' gradient orientation, comparing its angular error with the direction normal to the segment. This validation can be performed efficiently, without damaging the overall performance.</p><p>For a good validation we discard the pixels lying in a discontinuity and those near the endpoints, because they usually have a different gradient orientation even in correct detections. Despite this, the gradient orientation error in a true segment is a noisy signal. In <ref type="figure" target="#fig_8">Fig. 7</ref> we can see the probability distribution function (PDF) of orientation errors for pixels on true positive segment detections (TP) (blue), false positive segments (FP) (orange) and false positives segments detected in a random noise intensity image (green). It is clear that the pixels on TP segments have less angular error than those on FP ones. However, there is a significant overlap between FP and TP distributions.</p><p>Therefore, we use a validation criteria robust to noise. We validate a segment if at least 50% of its pixels have an angular error lower than a threshold, T valid . To separate positive detections (i.e. valid ones) from negative ones, we learn a threshold T valid = 0.15 radians which keeps a high recall discarding few true detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Parameter selection</head><p>Most ELSED parameters have been set empirically and do not need to be changed by the user. We use a Gaussian smoothing filter (? = 1, kernel size = 5?5), a gradient threshold T grad = 30, anchor threshold T anchor = 8 and SI = 2, that defines the scan interval of anchor every SI row/column. For the line segment fitting: T ol = 3, T minLength = 15, T LineF itErr = 0.2, T P xT oSegDist = 1.5 and for validation T EigenExt = 10, T AngleExt = 10/, T valid = 0.15 radians.</p><p>However, other parameters may be tuned by the user to define the type of segments to be detected; this is the case for the list of jump lengths that will be tested in the discontinuity management. Since this is directly related with the size of the Gaussian smoothing kernel in the first step (5 ? 5 in our implementation) and the size of the gradient convolution kernel (3 ? 3 in our case), we define a set of default values, <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9)</ref>, that in the experiments of section 4.3 provide good results.  <ref type="figure">Figure 8</ref>: Comparison of (a) Structural distance used by ELSED to fill the affinity matrix A and (b) point-sample based distance used by MCMLSD <ref type="bibr" target="#b16">[17]</ref>. In (b) the detected segment s C is matched to the GT segment s GT while the best possible match is s C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we introduce a methodology to evaluate the accuracy and repeatability of segment detectors and follow it to compare our detector with the best in the literature. We also present an ablation study to analyze how each component of our algorithm contributes to the final result.</p><p>We perform our evaluation in two dimensions, accuracy and efficiency. To this end we have grouped the algorithms in two sets, following the two clusters in <ref type="figure" target="#fig_0">Fig. 1</ref>. In the first set we find algorithms that run efficiently in CPU (LSD, EDLine, AG3line and ELSED). In the second set those that require more than one second to process an image (MCMLSD, Linelet, HAWP, SOLD 2 , F-Clip). Although HAWP, SOLD 2 and F-Clip are DL methods and should be run in GPU, we run them also on CPU to show the different computational requirements of each approach. It is also important to note, that most low-power devices like smartphones, drones or IoT devices are usually not prepared to run the GPU for long periods of time. In our experiments we compare the accuracy and efficiency of ELSED with the approaches in each group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Segment detection evaluation</head><p>We evaluate the segment detection in the YUD data set <ref type="bibr" target="#b32">[33]</ref> that contains indoor and outdoor man-made scenes where some salient segments have been manually labeled. The data set was extensively re-labeled later in Linelet <ref type="bibr" target="#b18">[19]</ref>, which contains all segments in the scene, but also some inconsistencies such as the labels in tiles and cables, segments smaller than 2 pixels or segments on curves and objects of doubtful value.</p><p>Linelet <ref type="bibr" target="#b18">[19]</ref> and MCMLSD <ref type="bibr" target="#b16">[17]</ref> were the first two methods that proposed an empirical evaluation. Whereas Linelet evaluation allows 1-to-N detections between the ground truth labels and the detected ones, MCMLSD methodology obtains the 1-to-1 assignation based on a cost matrix A that is computed with points sampled along each segment, which is slow to compute and can lead to erroneous matches in segments really close (see <ref type="figure">Figure 8b</ref>). MCMLSD also evaluates the entire precision-recall curves obtained from the assignation, while Linelet uses only one operation point on it. On the other hand, the wireframe line segment detection metric <ref type="bibr" target="#b33">[34]</ref>, proposes the structural Average Precision (sAP) <ref type="bibr" target="#b21">[22]</ref>. This metric can generate non meaningful matches because there is no check in the overlap, angle nor perpendicular distance, necessary to match small segments correctly. Furthermore, the 1-to-1 assignation is solved non-optimally because nearest neighbor assignation is used instead of Hungarian algorithm. Good matches can be also omitted if they have a large structural distance, which happens in case of fragmentation.</p><p>We propose a new evaluation framework that joins the benefits of previous evaluation protocols, namely, it is fast to compute and is a fair and stable metric for line segment detection. We ensure a good 1-to-1 match by using the Hungarian algorithm to find the optimal bipartite match, the assignation problem is defined with a cost matrix A, that is filled using the structural distance (see <ref type="figure">Fig. 8a</ref>). This metric is a good trade off between perpendicular distance, misalignment and overlap. It is also faster to compute than the matched number of sampled points. To speed up the Hungarian algorithm and ensure a meaningful matching, we require matched segments to have an intersection over union bigger than ? overlap = 0.1, to have an angular distance smaller than ? ang = 15 ? and a perpendicular distance smaller than ? dist = 2 ? 2. The pairs of segments that do not meet this criteria, have infinite cost in the corresponding entry of A avoiding their matching. Let x be the set of detected segments and y the set of ground truth segments. With the 1-to-1 assignations A * between them we define:</p><p>? Precision: Length of the matched intersection measured over the detected segment x i , divided by the length of the detected segments,</p><formula xml:id="formula_6">P = i,j?A * x i ? x i y j i |x i | .<label>(3)</label></formula><p>? Recall: Length of the matched intersection measured over the ground truth segment y j , divided by the length of the ground truth segments, ? Intersection over Union: Length of the matched intersection measured over the ground truth segments, divided by Length of the matched union measured over the ground truth segments.</p><formula xml:id="formula_7">R = i,j?A * y j ? y j x i j |y j | .<label>(4)</label></formula><formula xml:id="formula_8">IoU = i,j?A * y j ? y j x i i,j?A * y j ? y j x i<label>(5)</label></formula><p>In <ref type="figure" target="#fig_10">Fig. 9</ref> we show the Precision-Recall of our algorithm and the state-ofthe-art detectors. This curve is computed w.r.t. the original YUD data set annotations and we sort the segments produced by each method using the score provided by their original code. For ELSED, the score is the percentage of pixels that have an angular error lower than T valid . The curve of ELSED is better than those of LSD, EDLines and AG3line, the methods able to run on CPU efficiently (see <ref type="table" target="#tab_7">Table 4</ref>). We obtain a better precision and reaching similar or higher recall. MCMLSD gets better recall but at the cost of very bad precision. This was expected since it is the only method based on the HT and detects all full lines with enough support, even the hallucinated ones over highly textured regions. ELSED is able to improve AG3line because it has a better drawing scheme tailored to the line segment detection problem. ELSED is on par with the most efficient DL approaches, F-Clip (HG1), in the range of recall where ELSED works. Deeper networks such as HAWP that needs a GPU and are far away from real time in CPU obtain, as expected, results with better recall and precision.</p><p>In <ref type="table">Table 1</ref>   <ref type="table">Table 1</ref>: Results with our evaluation protocol (top half: efficient methods, bottom half: slow medhods).We use bold for best results in each experiment. The columns show for each method the Precision (P), Recall (R), Intersection over Union (IoU), F-Score (F sc), Average Precision (AP) and bounded Average Precision (bAP).</p><p>detector to detect as many segments as possible. The last two columns show metrics over all the P-R points of the curve. If we look at the results with the original YUD labels, ELSED has the best precision (0.3198), F-score (0.4148) and IoU (0.7111) among the efficient detectors on CPU. It also obtains the best results for the overall metrics along all the Precision-Recall points (Average Precision, AP). AP is the classic classification metric that is biased towards the detectors with a wider recall range. Thus, we also show the AP bounded to the recall interval where the curve of each method is defined (bAP). ELSED has a bAP comparable with F-Clip (HR) and better than F-Clip (HG1). This means that, although ELSED detects less segments (lower recall range than F-Clip) it has a precision similar to the top DL-based methods in the detected segments.</p><p>We also present the results for the "YorkUrban-LineSegment annotation" <ref type="bibr" target="#b18">[19]</ref>. In the new annotation the definition of segments changes to short and broken lines from the global lines in the original YUD labels. In this case, we also show the results of ELSED without jumps (ELSED-NJ ). ELSED-NJ obtains competitive results, being the best in terms of precision and IoU in this data set. This shows the nice to have property of ELSED, allowing to adapt the definition of segment to the application by changing that jump length over discontinuities.</p><p>With these experiments we can conclude that, despite ELSED has being designed with the aim of reducing the execution time, it is also a competitive algorithm in terms of segment detection accuracy. The reason is that the EED process suits the segment detection problem and the jumps over dis-  continuities produces outputs that agree with the definition of the segments (i.e. long segments) in the annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Repeatability</head><p>Regardless the type of segments aimed for the detection, a desirable property is the robustness against changes in viewpoint, scale, rotation or lighting. In this subsection we evaluate the detectors repeatability. Given two images of the same scene under different conditions, the capacity to detect the same segments in both situations. Specifically, given two images, we define line segments repeatability as the ratio between length of 1-to-1 segment matches and the total length of segments detected in both images. We take into account only the segments located in the part of the scene present in both images, adjusting their endpoints accordingly.</p><p>We use the images of HPatches <ref type="bibr" target="#b34">[35]</ref> where the repeatability of segment detections in images A and B is computed as:</p><formula xml:id="formula_9">repeatability = i,j?A * x A i ? x A i x A|B j i |x A i | + j x A|B j + i,j?B * x B i ? x B i x B|A j i |x B i | + j x B|A j<label>(6)</label></formula><p>Where x I are the segments detected in image I and x I|J the segments detected in image J , projected to I using the homography between them. The matching matrix A * contains the 1-to-1 segment assignations between segments x A and x A|B obtained with the matching process described in subsection 4.1 and B * the one between x B and x B|A .</p><p>In <ref type="table" target="#tab_3">Table 2</ref> the second column also shows the repeatability in terms of the number of matched segments. We employ here ? overlap = 0.5 and ? dist = 5  according to <ref type="bibr" target="#b25">[26]</ref>. It both cases ELSED obtains the most repeatable results. This is because EED provides stability to the edge detection and also because the jump strategy is able to overcome small discontinuities that cause other local methods to fail. We have also observed that deep models like SOLD 2 or HAWP get highly repeatable results in some scenes and very bad results in others, this is possibly because they have been trained in a quite specific problem (the wireframe parsing for indoor scenes) and not for the diverse images that can be found in Hpatches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>We start with the simplest version of ELSED: no discontinuity jumps and no validation step (see <ref type="table" target="#tab_5">Table 3</ref>). This version of the algorithm obtains the worst results (IoU= 0.646 and F sc= 0.360 in <ref type="table" target="#tab_5">Table 3</ref>) for long segment detections. When the validation is activated, the precision increases from 0.271 to 0.307 whereas the recall remains high (0.59).</p><p>If we now add the discontinuity jump component with a fixed length of 5 pixels it removes some small detection errors. For example, in the second row and column of <ref type="figure" target="#fig_0">Fig. 10</ref> the broken segments of the wall in the left, with the added fixed length jump capability are detected as a unique segment. On the other hand, now the algorithm performs some incorrect jumps going beyond the endpoint of the segment. This effect can be observed in the results of <ref type="table" target="#tab_5">Table 3</ref> where the Recall take a big leap (from 0.609 to 0.702) and the Precision also increases moderately (from 0.271 to 0.278). To fix the problem with incorrect jumps, we add the validation of the jump destination region of section 3.2 (see the well-fitted endpoints of the third column in <ref type="figure" target="#fig_0">Fig. 10</ref>).</p><p>On the other hand, a jump of 5 pixels is not big enough if the discontinuity is large. In this case, the jump validation is performed over pixels on the discontinuity. Thus, since discontinuities contains gradients in different  <ref type="bibr">(5, 7, 9 px)</ref> and jump validation, Validation with multi-size jumps <ref type="bibr">(5, 7, 9 px)</ref>   directions to the normal of the segment, the jump validation fails. This is the reason why we add the multi-length jumps in the fourth column in <ref type="figure" target="#fig_0">Fig. 10</ref>.</p><p>With it, we can deal with longer segment discontinuities. The last step is the validation of the whole detected segment (see section 3.3) shown in the last column of <ref type="figure" target="#fig_0">Fig. 10</ref>. Segment validation increases the precision (from 0.285 to 0.320) with a small penalty in the recall (from 0.686 to 0.664).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Efficiency evaluation</head><p>Nowadays CV applications not only require good accuracy, but also fast execution times and low energy consumption. This experiments measures the average execution time in the images of YUD data set which contains 101 640 ? 480 images. The times are measured on four different platforms: a laptop with an Intel Core i7 8750H CPU, 12 cores and 16GB of RAM; a smartphone Samsung J5 2017 with an Exynox Octa S CPU, 8 cores and 2GB of RAM; a smartphone One Plus 7 Pro with Snapdragon 855 CPU, 8 cores and 6GB of RAM and a GPU GeForce GTX 1050 with 4GB of RAM.</p><p>We use the implementation provided by the authors of each method: LSD, EDLines AG3line and ELSED in C++, Linelet and MCMLSD in Matlab and HAWP, SOLD 2 and F-Clip in Python. ELSED is implemented in C++ with Python bindings. To keep fast execution times we compute only L1 gradient norm, which is faster than L2, and predominant gradient direction (vertical or horizontal). We fit the segments with a least squares approach oriented vertically or horizontally that we compute incrementally and we reuse the top element of the stack if possible in the EED Algorithm.</p><p>In all platforms, ELSED is around 2? faster than AG3line, and ED-Lines, 6? faster than LSD and much faster than MCMLSD and Linelet. DL methods are designed to run in the GPU, however GPU may not always be available in some platforms like drones, IoT or mobile phones and when it is, it usually involves an unaffordable energy consumption. Looking at the CPU times, the DL methods need between 2300? (HAWP) and 1200? (F-Clip HG1) more computation than ELSED. Moreover, even when we run the DL methods on a laptop GPU (Geforce GTX 1050) still ELSED is faster than any of the methods. Thus, for limited platforms ELSED represents a clear alternative because not only has it the faster execution times but also it detects better than the other efficient methods (see <ref type="table">Table 1</ref>) getting the most repeatable segments ( <ref type="table" target="#tab_3">Table 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In the experimental section we have shown that ELSED is a general purpose line segment detector able to detect accurate segments in a few milliseconds. The execution times achieved by ELSED arise as a result of joining the processes of edge drawing and segment detection in one single shot, with and Enhance Edge Drawing (EED) strategy conceived for the problem of line segment detection.</p><p>Another important problem that ELSED solves for the efficient methods, is the robustness against small image noise and occlusions that cause most of the detectors to break the segments. This is especially important in problems such as Vanishing Points estimation or segment based reconstruction where the duplicity of geometric information can make the next steps of the algorithm fail.</p><p>Overall, ELSED is the fastest segment detector in the literature, getting also quite competitive results in segment detection and repeatability benchmarks, reaching a state of art accuracy at the same level as other algorithms orders of magnitude slower. These properties make it ideal for real time applications like Visual Odometry, SLAM, or self-Localization in low-power devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Average Precision (AP) vs execution time (ms) curve in the line segment detection problem. Local features based methods are displayed with circular markers and global ones with square markers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ED next pixel selection. Previous edge pixel Current edge pixel Horizontal current edge pixel Selected next pixel (b) Examples of corner shaped edges that can arise from ED.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>ED greedy segment growing. In (a) it only takes into account the current edge pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>diagonal right Go diagonal left Vertical current edge pixel (b) Enhanced Edge Drawing (EED).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Original ED (6) vs EED (3 or 2) candidate pixels when the previous pixel was in a different orientation (vertical vs horizontal) than the current one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Results of the drawing process for ED and EED (purple pixels) from one single anchor (blue point). The green arrows and the numbers establish which segments are visited first.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Example of ELSED discontinuity management algorithm. Gradient orientations in (c) coded with red (right direction), purple (up), cyan (left) and light green (bottom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>PDF of the gradient orientation error in correct segments (blue), negative segments (orange) and segments detected in a random intensity image (green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>: 38.53) F-Clip (HG1) (AP: 31.40) F-Clip (HR) (AP: 34.74) MCMLSD (AP: 21.98) Linelet (AP: 21.37) ELSED (AP: 21.17) Precision-Recall of the detected segments evaluated using the proposed evaluation framework. Evaluation in YUD with original labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Form left to right columns: No Validation no jumps, No validation with fixed size jumps (5 px) and no jump validation, No validation with fixed size jumps (5 px) and jump validation, No validation with multi-size jumps</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>columns 2 to 5 show the highest recall values in the P-R curve for each method. They corresponds to the case where we require the</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Original YUD annotations</cell><cell></cell><cell></cell><cell cols="2">YorkUrban-LineSegment annotations</cell></row><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell cols="3">IoU F sc AP</cell><cell>bAP</cell><cell>P</cell><cell>R</cell><cell>IoU F sc AP</cell><cell>bAP</cell></row><row><cell>LSD</cell><cell cols="9">0.26 0.53 0.59 0.34 17.33 34.78 0.68 0.52 0.67 0.58 38.23 76.35</cell></row><row><cell>EDLines</cell><cell cols="9">0.27 0.60 0.64 0.36 18.48 43.42 0.67 0.56 0.68 0.60 36.61 76.88</cell></row><row><cell>AG3line</cell><cell cols="9">0.27 0.67 0.69 0.37 21.02 42.32 0.60 0.62 0.66 0.60 34.69 69.06</cell></row><row><cell>ELSED</cell><cell cols="9">0.32 0.66 0.71 0.41 21.17 44.94 0.68 0.53 0.68 0.58 31.01 71.69</cell></row><row><cell>ELSED-NJ</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.71 0.51 0.69 0.59 32.43 76.93</cell></row><row><cell>MCMLSD</cell><cell cols="9">0.26 0.77 0.74 0.37 21.98 37.50 0.55 0.62 0.62 0.57 30.24 57.71</cell></row><row><cell>Linelet</cell><cell cols="9">0.24 0.60 0.63 0.33 21.37 43.62 0.62 0.58 0.66 0.59 39.35 75.52</cell></row><row><cell>HAWP</cell><cell cols="9">0.49 0.60 0.83 0.51 38.53 51.41 0.65 0.30 0.70 0.40 33.48 56.84</cell></row><row><cell cols="10">F-Clip(HG1) 0.47 0.42 0.80 0.42 31.40 40.35 0.67 0.22 0.72 0.32 32.13 50.06</cell></row><row><cell cols="10">F-Clip(HR) 0.53 0.47 0.82 0.47 34.74 44.77 0.69 0.22 0.72 0.33 32.78 51.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Mean repeatability of each segment detector in the HPatches data set. The higher the repeatability, the more robust the detector.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results of the ablation study using our 1-to-1 evaluation protocol over the original Yourk Urban Data set annotations. Best results are marked in bold. Execution times are measured in (Intel Core i7).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>and jump validation</figDesc><table><row><cell>Method</cell><cell>Intel Core i7</cell><cell>Snapdragon</cell><cell>Exynox</cell></row><row><cell>LSD</cell><cell>36.51 (?1.60)</cell><cell>58.68 (?0.81)</cell><cell>390.91 (?0.92)</cell></row><row><cell>EDLines</cell><cell>7.64 (?0.33)</cell><cell>13.79 (?0.15)</cell><cell>65.79 (?0.16)</cell></row><row><cell>AG3line</cell><cell>13.04 (?0.76)</cell><cell>18.57 (?0.20)</cell><cell>100.54 (?0.19)</cell></row><row><cell>ELSED-NJ</cell><cell>4.18 (?0.23)</cell><cell cols="2">8.28 (?0.03) 45.84 (?0.02)</cell></row><row><cell>ELSED</cell><cell>5.38 (?0.30)</cell><cell>10.20 (?0.07)</cell><cell>59.99 (?0.16)</cell></row><row><cell>MCMLSD</cell><cell>4.68K (?1.78K)</cell><cell cols="2">GeForce GTX 1050</cell></row><row><cell>Linelet</cell><cell>20.9K (?10.1K)</cell><cell></cell><cell></cell></row><row><cell>HAWP</cell><cell>12.4K (?0.8K)</cell><cell cols="2">212.25 (?8.35)</cell></row><row><cell>SOLD 2</cell><cell>3.17K (?0.52K)</cell><cell cols="2">417.72 (?6.78)</cell></row><row><cell>F-Clip HR</cell><cell>7.94K (?0.15K)</cell><cell cols="2">47.39 (?1.46)</cell></row><row><cell>F-Clip HG1</cell><cell>6.78K (?0.22K)</cell><cell cols="2">11.00 (?0.47)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Executions times for different state of art line segment detectors. The times are the average processing times per image, in the YUD<ref type="bibr" target="#b32">[33]</ref> images of size 640 ? 480 for different devices.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The source code of the method and experiments will be made public upon acceptance. https://github.com/iago-suarez/ELSED</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research was funded by Doctorado Industrial grant DI-16-08966 and MINECO project TIN2016-75982-C2-2-R.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d manhattan wireframes from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Computer Vision</title>
		<meeting>of International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7698" to="7707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A minimal closed-form solution for multi-perspective pose estimation using points and lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Miraldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="490" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structure-SLAM: Low-drift monocular slam in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="6583" to="6590" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PL-SLAM: A stereo SLAM system through the combination of points and line segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gomez-Ojeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-A</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zuniga-No?l</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Jimenez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="734" to="746" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-direct monocular visual odometry by combining points and line segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gomez-Ojeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Briales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pl-</forename><surname>Svo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Conf. on Intell. Robots Systems</title>
		<meeting>of Int. Conf. on Intell. Robots Systems</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4211" to="4216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding vanishing points via point alignments in image primal and dual domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grompone Von Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition</title>
		<meeting>Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="509" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fsg: A statistical approach to line detection via fast segments grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enrique</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buenaposada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Conf. on Intell. Robots Systems</title>
		<meeting>of Int. Conf. on Intell. Robots Systems</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Noncontact cable force estimation with unmanned aerial vehicle and computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Civil and Infrastructure Engineering</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="88" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LSDSAR, a Markovian a contrario framework for line segment detection in SAR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abergel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tupin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">107034</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LSD: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="722" to="732" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Edge Drawing: A combined real-time edge and segment detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Topal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Akinlar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="862" to="872" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">EDLines: A real-time line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Akinlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1633" to="1642" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">AG3line: Active grouping and geometrygradient combined validation for fast line segment extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page">107834</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust detection of lines using the progressive probabilistic hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galambos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="137" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An accurate method for line detection and manhattan frame estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Asian Conference on Computer Vision</title>
		<meeting>of Asian Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="580" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time line detection through an improved hough transform voting scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="299" to="314" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MCSLD: A dynamic programming approach to line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition</title>
		<meeting>Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5854" to="5862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">OTLines: A novel line-detection algorithm without the interference of smooth curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="238" to="258" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A novel linelet-based representation for line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to parse wireframes in images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition</title>
		<meeting>Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="626" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning attraction field representation for robust line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition</title>
		<meeting>Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1595" to="1603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Computer Vision</title>
		<meeting>of International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="962" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Holistically-attracted wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition</title>
		<meeting>Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2788" to="2797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11207</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Fully convolutional line parsing</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Line segment detection using transformers without edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition</title>
		<meeting>Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4257" to="4266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pautrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03362</idno>
		<title level="m">SOLD2: Self-supervised occlusion-aware line description and detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in Computer Vision</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="184" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generalizing the hough transform to detect arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in computer vision</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="714" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A probabilistic hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kiryati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="316" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Closed form line-segment extraction using the hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4012" to="4023" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Local scale control for edge detection and blur estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="699" to="716" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desolneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Moisan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meaningful alignments</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="7" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient edge-based methods for estimating manhattan frames in urban imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="197" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to parse wireframes in images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition</title>
		<meeting>Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="626" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Hpatches: A benchmark and evaluation of handcrafted and learned local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5173" to="5182" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
