<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decoupling the Depth and Scope of Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-19">19 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
							<email>zengh@usc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
							<email>yxia@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
							<email>ajiteshs@usc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malevich</surname></persName>
							<email>amalevich@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
							<email>rajgopal.kannan.civ@mail.mil</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
							<email>prasanna@usc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Facebook</forename><surname>Long</surname></persName>
							<email>longjin@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Chen</surname></persName>
							<email>renchen@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">USC</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>BIGAI</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">USC</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<country>US ARL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">USC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Decoupling the Depth and Scope of Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-19">19 Jan 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art Graph Neural Networks (GNNs) have limited scalability with respect to the graph and model sizes. On large graphs, increasing the model depth often means exponential expansion of the scope (i.e., receptive field). Beyond just a few layers, two fundamental challenges emerge: 1. degraded expressivity due to oversmoothing, and 2. expensive computation due to neighborhood explosion. We propose a design principle to decouple the depth and scope of GNNs -to generate representation of a target entity (i.e., a node or an edge), we first extract a localized subgraph as the bounded-size scope, and then apply a GNN of arbitrary depth on top of the subgraph. A properly extracted subgraph consists of a small number of critical neighbors, while excluding irrelevant ones. The GNN, no matter how deep it is, smooths the local neighborhood into informative representation rather than oversmoothing the global graph into "white noise". Theoretically, decoupling improves the GNN expressive power from the perspectives of graph signal processing (GCN), function approximation (GraphSAGE) and topological learning (GIN). Empirically, on seven graphs (with up to 110M nodes) and six backbone GNN architectures, our design achieves significant accuracy improvement with orders of magnitude reduction in computation and hardware cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) have now become the state-of-the-art models for graph mining <ref type="bibr" target="#b27">[48,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">58]</ref>, facilitating applications such as social recommendation <ref type="bibr">[35,</ref><ref type="bibr" target="#b31">52,</ref><ref type="bibr" target="#b16">37]</ref>, knowledge understanding <ref type="bibr" target="#b19">[40,</ref><ref type="bibr" target="#b17">38,</ref><ref type="bibr" target="#b38">59]</ref> and drug discovery <ref type="bibr" target="#b22">[43,</ref><ref type="bibr">32]</ref>. With the numerous architectures proposed <ref type="bibr">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">44,</ref><ref type="bibr" target="#b28">49]</ref>, it still remains an open question how to effectively scale up GNNs with respect to both the model size and graph size. There are two fundamental obstacles when we increase the number of GNN layers:</p><p>? Expressivity challenge (i.e., oversmoothing <ref type="bibr">[30,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b18">39,</ref><ref type="bibr">17]</ref>): iterative mixing of neighbor features collapses embedding vectors of different nodes into a fixed, low-dimensional subspace.</p><p>? Scalability challenge (i.e., neighbor explosion <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">55]</ref>): recursive expansion of multi-hop neighborhood results in exponentially growing receptive field size (and thus computation cost).</p><p>To address the expressivity challenge, most remedies focus on neural architecture exploration: <ref type="bibr" target="#b23">[44,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">49,</ref><ref type="bibr">29]</ref> propose more expressive aggregation functions when propagating neighbor features. <ref type="bibr" target="#b29">[50,</ref><ref type="bibr">28,</ref><ref type="bibr">18,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr">33</ref>, 31] use residue-style design components to construct flexible and dynamic receptive fields. Among them, <ref type="bibr" target="#b29">[50,</ref><ref type="bibr">28,</ref><ref type="bibr">18</ref>] use skip-connection across multiple GNN layers, and [34, 1, 33, 31] encourage multi-hop message passing within each single layer. As for the scalability challenge, sampling methods have been explored to improve the training speed and efficiency. Importance based layer-wise sampling <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b40">61]</ref> and subgraph-based sampling <ref type="bibr" target="#b33">[54,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">55]</ref> alleviate neighbor explosion, while preserving training accuracy. Unfortunately, such sampling methods cannot be naturally generalized to inference without accuracy loss (see also <ref type="bibr">Section 4)</ref>.</p><p>The above lines of research have only guided us to partial solutions. Yet what is the root cause of both the expressivity and scalability challenges? Setting aside the design of GNN architectures or sampling schemes, we provide an alternative perspective by interpretting the data in a different way.</p><p>Two views on the graph. Given an input graph G with node set V, the most straightforward way to understand G is by viewing it as a single global graph. So any two nodes u and v belong to the same G, and if u and v lie in the same connected component, they will ultimately see each other in their own neighborhood no matter how far away u and v are. Alternative to the above global view, we can take a local view on G. For each node v, there is a latent G <ref type="bibr">[v]</ref> surrounding it which captures the characteristics of just v itself. The full G is observed (by the data collection process) as the union of all such G <ref type="bibr">[v]</ref> . Consequently, V <ref type="bibr">[v]</ref> rather than V defines v's neighborhood: if u ? V <ref type="bibr">[v]</ref> , v will never consider u as a neighbor. Our "decoupling" design is based on the local view.</p><p>Scope of GNNs. Both the expressivity and scalability challenges are closely related to the enlargement of the GNN's scope (i.e., receptive field). More importantly, how we define the scope is determined by how we view G. With the global view above, an L-layer GNN has the scope of the full L-hop neighborhood. With the local view, the GNN scope is simply V <ref type="bibr">[v]</ref> regardless of the GNN depth. The two existing lines of research, one on architectural exploration and the other on sampling, both take the global view. Consequently, the depth (i.e., number of layers) and scope of such GNNs are tightly coupled. Such coupling significantly limits the design space exploration of GNNs with various depths <ref type="bibr" target="#b32">[53]</ref>. Consider the example of ogbn-products, a medium-scale graph in Open Graph Benchmark <ref type="bibr" target="#b15">[16]</ref>. The average number of 4-hop neighbors is around 0.6M, corresponding to 25% of the full graph size. To generate representation of a single target node, a 4-layer coupled GNN needs to propagate features from the 0.6M neighbors. Such propagation can be inefficient or even harmful since most nodes in the huge neighborhood would be barely relevant to the target node.</p><p>Decoupling the GNN depth and scope. Taking the local view on G, we propose a general design principle to decouple the GNN depth and scope. To generate the representation of the target node v, we first extract from G a small subgraph G <ref type="bibr">[v]</ref> surrounding v. On top of G <ref type="bibr">[v]</ref> , we apply a GNN whose number of layers and message passing functions can be flexibly chosen. "Decoupling" means we treat the scope extraction function and GNN depth as two independently tuned parameterseffectively we introduce a new dimension in the GNN design space. We intuitively illustrate the benefits of decoupling by an example GNN construction, where the scope is the L-hop neighborhood and depth is L ? (L ? &gt; L). When we use more layers (L ? ) than hops (L), each pair of subgraph nodes may exchange messages multiple times. The extra message passing helps the GNN better absorb and embed the information within scope, and thus leads to higher expressivity. We further justify the above intuition with multifaceted theoretical analysis. From the graph signal processing perspective, we prove that decoupled-GCN performs local-smoothing rather than oversmoothing, as long as the scopes of different target nodes are different. From the function approximation perspective, we construct a linear target function on neighbor features and show that decoupling the GraphSAGE model reduces the function approximation error. From the topological learning perspective, we apply deep GIN-style message passing to differentiate non-regular subgraphs of a regular graph. As a result, our model is more powerful than the 1-dimensional Weisfeiler-Lehman test <ref type="bibr" target="#b20">[41]</ref>.</p><p>Practical implementation: SHADOW-GNN. The decoupling principle leads to a practical implementation, SHADOW-GNN: Decoupled GNN on a shallow subgraph. In SHADOW-GNN, the scope is a shallow yet informative subgraph, only containing a fraction of the 2-or 3-hop neighbors of G (see <ref type="bibr">Section 5)</ref>. On the other hand, the model of SHADOW-GNN is deeper (e.g., L ? = 5).</p><p>To efficiently construct the shallow scope on commodity hardware, we propose various subgraph extraction functions. To better utilize the subgraph node embeddings after deep message passing, we propose neural architecture extensions such as pooling and ensemble. Empirically, our "decoupling" design improves both the accuracy and scalability. On seven benchmarks (including the largest ogbn-papers100M graph with 111M nodes) and across two graph learning tasks, SHADOW-GNNs achieve significant accuracy gains compared to the original models. Meanwhile, the computation and hardware costs are reduced by orders of magnitude. Our code is available at https://github.com/facebookresearch/shaDow_GNN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Let G (V, E, X) be an undirected graph, with node set V, edge set E ? V ?V and node feature matrix X ? R |V|?d . Let as the adjacency matrix after symmetric normalization (" * " means augmented with self-edges), and A = D ?1 A (or D ?1 * A * ) as the one after random walk normalization. Let subscript "[u]" mark the quantities corresponding to a subgraph surrounding node u. For example, the subgraph itself is G <ref type="bibr">[u]</ref> . Subgraph matrices X <ref type="bibr">[v]</ref> and A <ref type="bibr">[v]</ref> have the same dimension as the original X and A. Yet, row vector</p><formula xml:id="formula_0">X [v] u = 0 for u ? V [v] . Element A [v] u,w = 0 if either u ? V [v] or w ? V [v] .</formula><p>For an L-layer GNN, let superscript "(?)" denote the layer-? quantities. Let d (?) be the number of channels for layer ?; H (??1) ? R |V|?d (??1) and H (?) ? R |V|?d (?) be the input and output features. So H (0) = X and d (0) = d. Further, let ? be the activation and W (?) be the learnable weight. For example, a GCN layer performs H (?) = ? AH (??1) W (?) . A GraphSAGE layer performs</p><formula xml:id="formula_1">H (?) = ? H (??1) W (?) 1 + AH (??1) W (?) 2 .</formula><p>Our analysis in Section 3 mostly focuses on the node classification task. Yet our design can be generalized to the link prediction task, as demonstrated by our experiments in Section 5.</p><formula xml:id="formula_2">Definition 2.1. (Depth of subgraph) Assume the subgraph G [v] is connected. The depth of G [v] is defined as max u?V [v] d (u, v), where d (u, v) denotes the shortest path distance from u to v.</formula><p>The above definition enables us to make comparison such as "the GNN is deeper than the subgraph". For "decoupling the depth and scope", we refer to the model depth rather than the subgraph depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Decoupling the Depth and Scope of GNNs</head><p>"Decoupling the depth and scope of GNNs" is a design principle to improve the expressivity and scalability of GNNs without modifying the layer architecture. We name a GNN after decoupling a SHADOW-GNN (see Section 3.6 for explanation of the name). Compared with a normal GNN, SHADOW-GNN contains an additional component: the subgraph extractor EXTRACT. To generate embedding of a target node v, SHADOW-GNN proceeds as follows:</p><formula xml:id="formula_3">1. We use EXTRACT (v, G) to return a connected G [v] , where G [v] is a subgraph containing v, and the depth of G [v] is L. 2. We build an L ? -layer GNN on G [v] by treating G [v]</formula><p>as the new full graph and by ignoring all nodes / edges not in G <ref type="bibr">[v]</ref> . So G <ref type="bibr">[v]</ref> is the scope of SHADOW-GNN. The key point reflecting "decoupling" is that L ? &gt; L.</p><p>A normal GNN is closely related to a SHADOW-GNN. Under the normal setup, an L-layer GNN operates on the full G and propagates the influence from all the neighbors up to L hops away from v. Such a GNN is equivalent to a model where EXTRACT returns the full L-hop subgraph and L ? = L.</p><p>We theoretical demonstrate how SHADOW-GNN improves expressivity from three different angles. On SHADOW-GCN (Section 3.1), we come from the graph signal processing perspective. The GCN propagation can be interpreted as applying filtering on the node signals <ref type="bibr" target="#b26">[47]</ref>. Deep models correspond to high-pass filters. Filtering the local graph G <ref type="bibr">[v]</ref> preserves richer information than the global G. On SHADOW-SAGE (Section 3.2), we view the GNN as a function approximator. We construct a target function and study how decoupling reduces the approximation error. On SHADOW-GIN (Section 3.3), we focus on learning topological information. We show that decoupling helps capture local graph structure which the 1D Weisfeiler-Lehman test fails to capture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Expressivity Analysis on SHADOW-GCN: Graph Signal Processing Perspective</head><p>GCNs [22] suffer from "oversmoothing" [30] -Each GCN layer smooths the features of the direct (i.e., 1-hop) neighbors, and many GCN layers smooths the features of the full graph. Eventually, such repeated smoothing process propagates to any target node just the averaged feature of all V. "Oversmoothing" thus incurs significant information loss by wiping out all local information.</p><p>Formally, suppose the original features X reside in a high-dimensional space R |V|?d . Oversmoothing pushes X towards a low-dimensional subspace R |V|?d ? , where d ? &lt; d. Corresponding analysis comes from two perspectives: oversmoothing by a deep GCN, and oversmoothing by repeated GCNstyle propagation. The former considers the full neural network with non-linear activation, weight and bias. The later characterizes the aggregation matrix M = lim L?? A L X. It is shown that even with the vanilla architecture, a deep GCN with bias parameters does not oversmooth <ref type="bibr">[17]</ref>. In addition, various tricks <ref type="bibr" target="#b39">[60,</ref><ref type="bibr" target="#b18">39,</ref><ref type="bibr">34]</ref> can prevent oversmoothing from the neural network perspective. However, a deep GCN still suffers from accuracy drop, indicating that the GCN-style propagation (rather than other GCN components like activation and bias) may be the fundamental reason causing difficulty in learning. Therefore, we study the asymptotic behavior of the aggregation matrix M under the normal and SHADOW design. In other words, here in Section 3.1, we ignore the nonlinear activation and bias parameters. Such setup is consistent with many existing literature such as <ref type="bibr">[30,</ref><ref type="bibr">33,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b39">60]</ref>. Proposition 3.1. ? number of feature propagation by SHADOW-GCN leads to </p><formula xml:id="formula_4">m [v] = e [v] v ? e T [v] X [v] (1) where e [v] is defined by e [v] u = ? [v] (u) w?V [v] ? [v] (w) ; ? [v] (u) returns the degree of u in G [v</formula><formula xml:id="formula_5">(?) v = ? W (?) 1 T h (??1) v + W (?) 2 T 1 |Nv | u?Nv h (??1) u .</formula><p>We can prove Point 1 by making an L ? -layer SHADOW-SAGE identical to an L-layer GraphSAGE with the following steps: 1. let EXTRACT return the full L-hop neighborhood, and 2. set W</p><formula xml:id="formula_6">(?) 1 = I, W (?) 2 = 0 for L + 1 ? ? ? L ? . For point 2, we consider a target function: ? X, G [v] = C ? u?V [v] ? [v] (u) ? x u for some neighborhood G [v]</formula><p>, scaling constant C and ? <ref type="bibr">[v]</ref> (u) as defined in Proposition 3.</p><p>1. An expressive model should be able to learn well this simple linear function ? .</p><p>GraphSAGE cannot learn ? accurately, while SHADOW-SAGE can. We first show the GraphSAGE case. Let the depth of G [v] be L. Firstly, we need GraphSAGE to perform message passing for exactly L times (where such a model can be implemented by, e.g., L layers or L ? layers with W 2 = 0 for L ? ? L layers). Otherwise, the extra L ? ? L message passings will propagate influence from nodes v ? ? V <ref type="bibr">[v]</ref> , violating the condition that ? is independent of v ? . Next, suppose GraphSAGE can learn a function ? such that on some G ?</p><p>[</p><formula xml:id="formula_7">v] , we have ? G ? [v] = ? G ? [v] . We construct another G ?? [v]</formula><p>by adding an extra edge e connecting two depth-L nodes in G ? <ref type="bibr">[v]</ref> . Edge e changes the degree distribution ? <ref type="bibr">[v]</ref> (?), and thus ? G ?</p><p>[</p><formula xml:id="formula_8">v] = ? G ?? [v]</formula><p>. On the other hand, there is no way for GraphSAGE to propagate the influence of edge e to the target v, unless the model performs at least L + 1 message passings. So</p><formula xml:id="formula_9">? G ? [v] = ? G ?? [v]</formula><p>regardless of the activation function and weight parameters. Therefore, ? = ? .</p><p>For SHADOW-SAGE, let EXTRACT return G <ref type="bibr">[v]</ref> . Then the model can output</p><formula xml:id="formula_10">? ? = A L ? [v] X v,:</formula><p>after we 1. set W = I for all layers, and 2. either remove the non-linear activation or bypass ReLU by shifting X with bias. With known results in Markov chain convergence theorem [27], we derive the following theorem by analyzing the convergence of A L ?</p><p>[v] when L ? ? ?. Theorem 3.3. SHADOW-SAGE can approximate ? with error decaying exponentially with depth.</p><p>We have the following conclusions from above: 1. SHADOW-SAGE is more expressive than Graph-SAGE. 2. appropriate EXTRACT function improves SHADOW-GNN expressivity, 3. There exists cases where it may be desirable to set the SHADOW-GNN depth much larger than the subgraph depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Expressivity Analysis on SHADOW-GIN: Topological Learning Perspective</head><p>While GCN and GraphSAGE are popular architectures in practice, they are not the theoretically most discriminative ones. The work in <ref type="bibr" target="#b28">[49]</ref> establishes the relation in discriminativeness between GNNs and 1-dimensional Weisfeiler-Lehman test (i.e., 1-WL). And GIN <ref type="bibr" target="#b28">[49]</ref> is an example architecture achieving the same discriminativeness as 1-WL. We show that applying the decoupling principle can further improve the discriminativeness of such GNNs, making them more powerful than 1-WL.   1-WL is a graph isomorphism test aiming at distinguishing graphs of different structures. A GNN as expressive as 1-WL thus well captures the topological property of the target node. While 1-WL is already very powerful, it may still fail in some cases. e.g., it cannot distinguish certain non-isomorphic, regular graphs. To understand why SHADOW-GNN works, we first need to understand why 1-WL fails. In a regular graph, all nodes have the same degree, and thus the "regular" property describes a global topological symmetry among nodes. Unfortunately, 1-WL (and the corresponding normal GNN) also operates globally on G. Intuitively, on two different regular graphs, there is no way for 1-WL (and the normal GNN) to assign different labels by breaking such symmetry.</p><p>On the other hand, SHADOW-GNN can break such symmetry by applying decoupling. In Section 1, we have discussed how SHADOW-GNN is built from the local perspective on the full graph. The key property benefiting SHADOW-GNN is that a subgraph of a regular graph may not be regular. Thus, SHADOW-GNN can distinguish nodes in a regular graph with the non-regular subgraphs as the scope. We illustrate the intuition with the example in <ref type="figure" target="#fig_4">Figure 1</ref>. The graph G is 3-regular and we assume all nodes have identical features. Our goal is to discriminate nodes u and v since their neighborhood structures are different. No matter how many iterations 1-WL runs, or how many layers the normal GNN has, they cannot distinguish u and v. On the other hand, a SHADOW-GNN with 1-hop EXTRACT and at least 2 layers can discriminate u and v. Theorem 3.4. Consider GNNs whose layer function is defined by</p><formula xml:id="formula_11">h (?) v = f (?) 1 h (??1) v , u?Nv f (?) 2 h (??1) v , h (??1) u ,<label>(2)</label></formula><p>where f The theorem also implies that SHADOW-GIN is more discriminative than a normal GIN due to the correspondence between GIN and 1-WL. See Appendix A for the proof of all theorems in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Subgraph Extraction Algorithms</head><p>Our decoupling principle does not rely on specific subgraph extraction algorithms. Appropriate EXTRACT can be customized given the characteristic of G, and different EXTRACT leads to different implementation of our decoupling principle. In general, we summarize three approaches to design EXTRACT: 1. heuristic based, where we pick graph metrics that reflect neighbor importance and then design EXTRACT by such metrics; 2. model based, where we assume a generation process on G and set EXTRACT as the reverse process, and 3. learning based, where we integrate the design of EXTRACT as part of the GNN training. In the following, we present several examples on heuristic based EXTRACT, which we also empirically evaluate in Section 5. We leave detailed evaluation on the model based and learning based EXTRACT as future work. See also Appendix C for details.</p><p>Example heuristic based EXTRACT. The algorithm is derived from the selected graph metrics. For example, with the metric being shortest path distance, we design a L-hop extractor. i.e., EXTRACT returns the full set or randomly selected subset of the target node's L-hop neighbors in G. Picking the random walk landing probability as the metric, we can design a PPR-based extractor. i.e., we first run the Personalized PageRank (PPR) algorithm on G to derive the PPR score of other nodes relative to the target node. Then EXTRACT define V <ref type="bibr">[v]</ref> by picking the top-K nodes with the highest PPR scores. The subgraph</p><formula xml:id="formula_12">G [v] is the node-induced subgraph 1 of G from V [v]</formula><p>. One can easily extend this approach by using other metrics such as Katz index [20], SimRank [19] and feature similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Architecture</head><p>Subgraph pooling. For a normal GNN performing node classification, the multi-layer message passing follows a "tree structure". The nodes at level L of the tree correspond to the L-hop neighborhood. And the tree root outputs the final embedding of the target node. Thus, there is no way to apply subgraph pooling or READOUT on the final layer output, since the "pool" only contains a single vector. For a SHADOW-GNN, since we decouple the L th layer from the L-hop neighborhood, it is natural to let each layer (including the final layer) output embeddings for all subgraph nodes. This leads to the design to READOUT all the subgraph node embeddings as the target node embedding.</p><p>We can understand the pooling for SHADOW-GNN from another perspective. In a normal GNN, the target node at the final layer receives messages from all neighbors, but two neighbor nodes may not have a chance to exchange any message to each other (e.g., two nodes L-hop away from the target may be 2L-hop away from each other). In our design, a SHADOW-GNN can pass messages between any pair of neighbors when the model depth is large enough. Therefore, all the subgraph node embeddings at the final layer capture meaningful information of the neighborhood.</p><p>In summary, the power of the decoupling principle lies in that it establishes the connection between the node-/ link-level task and the graph-level task. e.g., to classify a node is seen as to classify the subgraph surrounding the node. From the neural architecture perspective, we can apply any subgraph pooling / READOUT operation originally designed for graph classification (e.g., <ref type="bibr" target="#b36">[57,</ref><ref type="bibr">24,</ref><ref type="bibr" target="#b3">4]</ref>) to enhance the node classification / link prediction of SHADOW-GNN. In particular, in the vanilla SHADOW-GNN, we can implement a trivial READOUT as "discarding all neighbor embeddings", corresponding to performing center pooling. See Appendix D and F.3 for algorithm and experiments.</p><p>Subgraph ensemble. It may be challenging in practice to design a single EXTRACT capturing all meaningful characteristics of the neighborhood. We can use multiple EXTRACT to jointly define the receptive field, and then ensemble multiple SHADOW-GNN at the subgraph level.</p><formula xml:id="formula_13">Consider R candidates {EXTRACT i }, each returning G i [v]</formula><p>. To generate v's embedding, we first use R branches of L ? -layer GNN to obtain intermediate embeddings for each G i v , and then aggregate the R embeddings by some learnable function g. In practice, we design g as an attention based aggregation function (see Appendix D.2). Subgraph ensemble is useful both when {EXTRACT i } consists of different algorithms and when each EXTRACT i performs the same algorithm under different parameters.</p><p>CASE STUDY Consider PPR-based EXTRACT i with different threshold ? i on the neighbor PPR score. A SHADOW-GNN-ensemble can approximate PPRGo <ref type="bibr" target="#b5">[6]</ref>. PPRGo generates embedding as:</p><formula xml:id="formula_14">? v = u?V [v] ? u h v , where ? u is u's PPR score and h v = MLP (x v ). We can partition V [v] = R i=1 V i [v] s.t. nodes in V i [v] have similar PPR scores denoted by ? i , and ? i ? ? i+1 . So ? v = R i=1 ? i u?V ? i h u , where ? i = ? i ? j&lt;i ? j and V ? i = R k=i V k [v] . Now for each branch of SHADOW-GNN-ensemble, let parameter ? i = ? i so that EXTRACT i returns V ? i . The GNN on V ? i can then learn u?V ? i h u (e.g.</formula><p>, by a simple "mean" READOUT). Finally, set the ensemble weight as ? i . SHADOW-GNN-ensemble learns ? v . As EXTRACT also preserves graph topology, our model can be more expressive than PPRGo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Practical Design: SHADOW-GNN</head><p>We now discuss the practical implementation of decoupled GNN -SHADOW-GNN. As the name suggests, in SHADOW-GNN, the scope is a shallow subgraph (i.e., with depth often set to 2 or 3).</p><p>In many realistic scenarios (e.g., citation networks, social networks, product recommendation graphs), a shallow neighborhood is both necessary and sufficient for the GNN to learn well. On "sufficiency", we consider the social network example: the friend of a friend of a friend may share little commonality with you, and close friends may be at most 2 hops away. Formally, by the ?decaying theorem <ref type="bibr" target="#b35">[56]</ref>, a shallow neighborhood is sufficient to accurately estimate various graph metrics. On "necessity", since the neighborhood size may grow exponentially with hops, a deep neighborhood would be dominated by nodes irrelevant to the target. The corresponding GNN would first need to differentiate the many useless nodes from the very few useful ones, before it can extract meaningful features from the useful nodes. Finally, a shallow subgraph ensures scalability by avoiding "neighborhood explosion".</p><p>Remark on decoupling. So far we have defined a decoupled model as having the model depth L ? larger than the subgraph depth L. Strictly speaking, a decoupled model also admits L ? = L. For example, suppose in the full L-hop neighborhood, there are 70% nodes L hops away. Applying decoupling, the EXTRACT excludes most of the L-hop neighbors, and the resulting subgraph G <ref type="bibr">[v]</ref> contains only 20% nodes L hops away. Then it is reasonable to consider an L-layer model on such a depth-L subgraph as also a decouple model. Compared with an L-layer model on the full L-hop neighborhood, an L-layer model on such a depth-L G <ref type="bibr">[v]</ref> propagates much less information from nodes L hops away. So the L message passings are indeed decoupled from the full L-hop neighborhood.</p><p>Remark on neighborhood. The "sufficiency" and "necessity" in shallow neighborhood are not universal. In many other applications, long-range dependencies can be critical, as studied in <ref type="bibr" target="#b1">[2]</ref>. In such cases, our practical implementation of SHADOW-GNN would incur accuracy loss. However, our decoupling principle in general may still be beneficial -"shallow subgraph" is a practical guideline rather than a theoretical requirement. We leave the study on such applications as future work. all include some variants of residue connection, either across multiple layers or within a single layer. In principle, such architectures can also benefit the feature propagation of a deep SHADOW-GNN, since their design does not rely on a specific neighborhood (e.g., L-hop). In addition to architectures, DropEdge <ref type="bibr" target="#b18">[39]</ref> and Bayesian-GDC <ref type="bibr" target="#b13">[14]</ref> propose regularization techniques by adapting dropout <ref type="bibr" target="#b21">[42]</ref> to graphs. Such techniques are only applied during training, and inference may still suffer from issues such as oversmoothing.</p><p>Sampling based methods. Neighbor or subgraph sampling techniques have been proposed to improve training efficiency. FastGCN <ref type="bibr" target="#b7">[8]</ref>, VR-GCN <ref type="bibr" target="#b6">[7]</ref>, AS-GCN [18], LADIES <ref type="bibr" target="#b40">[61]</ref> and MVS-GNN <ref type="bibr" target="#b9">[10]</ref> sample neighbor nodes per GNN layer. Cluster-GCN <ref type="bibr" target="#b8">[9]</ref> and GraphSAINT <ref type="bibr" target="#b34">[55]</ref> sample a subgraph as the training minibatch. While sampling also changes the receptive field, all the above methods are fundamentally different from ours. The training samplers aim at estimating the quantities related to the full graph (e.g., the aggregation of the full L-hop neighborhood), and so the inference model still operates on the full neighborhood to avoid accuracy loss. For SHADOW-GNN, since the decoupling principle is derived from a local view on G, our EXTRACT does not estimate any full neighborhood quantities. Consequently, the sampling based methods only improve the training efficiency, while SHADOW-GNN addresses the computation challenge for both training and inference.</p><p>Re-defining the neighborhood. Various works reconstruct the original graph and apply the GNN on the re-defined neighborhood. GDC [23] views the reconstructed adjacency matrix as the diffusion matrix. SIGN <ref type="bibr" target="#b10">[11]</ref> applies reconstruction for customized graph operators. AM-GCN <ref type="bibr" target="#b25">[46]</ref> utilizes the reconstruction to separate feature and structure information. The above work re-define the neighborhood. However, they still have tightly coupled depth and scope. SHADOW-GNN can also work with the reconstructed graph G ? by simply applying EXTRACT on G ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Setup. We evaluate SHADOW-GNN on seven graphs. Six of them are for the node classification task: Flickr <ref type="bibr" target="#b34">[55]</ref>, Reddit <ref type="bibr" target="#b11">[12]</ref>, Yelp <ref type="bibr" target="#b34">[55]</ref>, ogbn-arxiv, ogbn-products and ogbn-papers100M <ref type="bibr" target="#b15">[16]</ref>. The remaining is for the link prediction task: ogbl-collab <ref type="bibr" target="#b15">[16]</ref>. The sizes of the seven graphs range from 9K nodes (Flickr) to 110M nodes (ogbn-papers100M). Flickr, Reddit and Yelp are under the inductive setting. ogbn-arxiv, ogbn-products and ogbn-papers100M are transductive. Consistent with the original papers, for the graphs on node classification, we measure "F1-micro" score for Yelp and "accuracy" for the remaining five graphs. For the link prediction task, we use "Hits@50" as the metric. See Appendix E.1 for details.</p><p>We construct SHADOW with six backbone models: GCN [22], GraphSAGE <ref type="bibr" target="#b11">[12]</ref>, GAT <ref type="bibr" target="#b23">[44]</ref>, JK-Net <ref type="bibr" target="#b29">[50]</ref>, GIN <ref type="bibr" target="#b28">[49]</ref>, SGC <ref type="bibr" target="#b26">[47]</ref>. The first five are representatives of the state-of-the-art GNN architectures, jointly covering various message aggregation functions as well as the skip connection design. SGC simplifies normal GCN by moving all the neighbor propagation to the pre-processing step. Therefore, SGC is suitable for evaluating oversmoothing. The non-SHADOW models are trained with both full-batch and GraphSAINT-minibatch <ref type="bibr" target="#b34">[55]</ref>. Due to the massive size of the full L-hop neighborhood, we need to perform sampling when training normal GNNs in order to make the computation time tractable. GraphSAINT is suitable for our purpose since 1. it is the state-of-the-art minibatch method which achieves high accuracy, and 2. it supports various GNN architectures and scales well to large graphs. On the other hand, for SHADOW-GNN, both training and inference are always executed  <ref type="figure">Figure 3</ref>: SGC oversmoothing in minibatches. One advantage of SHADOW-GNN is that the decoupling enables straightforward minibatch construction: each target just independently extracts the small subgraph on its own. We implement two EXTRACT described in Section 3.4: 1. "PPR", where we set the node budget K as {200, 400} for the largest ogbn-papers100M and K ? 200 for all other graphs; 2. "L-hop", where we set the depth as {1, 2}. We implement various subgraph pooling functions: "mean" and "max" evaluated in this section and others evaluated in Appendix F.3. For the model depth, since L ? = 3 is the standard setting in the literature (e.g., see the benchmarking in OGB <ref type="bibr" target="#b15">[16]</ref>), we start from L ? = 3 and further evaluate a deeper model of L ? = 5. All accuracy are measured by 5 runs without fixing random seeds. Hyperparameter tuning and architecture configurations are in Appendix E.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Power on</head><formula xml:id="formula_15">A or A [v] Test accuracy S-SGC, F S-SGC, R S-SGC, A SGC, F SGC, R SGC, A</formula><p>SHADOW-GNN neighborhood. For both normal and SHADOW GNNs, <ref type="figure">Figure 2</ref> shows on average how many neighbors are L hops away from the target. For a normal GNN, the size of the neighborhood grows rapidly with respect to L, and the nodes 4 hops away dominate the neighborhood. For SHADOW-GNN using the <ref type="table" target="#tab_2">Table 1</ref> EXTRACT, most neighbors concentrate within 2 hops. A small number of nodes are 3 hops away. Almost no nodes are 4 or more hops away. Importantly, not only does the composition of the two kinds of neighborhood differ significantly, but also the size of SHADOW-GNN scope is much smaller (see also <ref type="table" target="#tab_2">Table 1</ref>). Such small subgraphs are essential to high computation efficiency. Finally, we can ignore the very few distant neighbors (L ? 4), and regard the (effective) depth of SHADOW-GNN subgraph as L = 2 (or at most 3). Under such practical value of L, a model with L ? ? 3 is indeed a SHADOW-GNN (see "Remark on decoupling" in Section 3.6). Comparison with baselines. <ref type="table" target="#tab_2">Table 1</ref> shows the performance comparison of SHADOW-GNN with the normal GNNs. All models on all datasets have uniform hidden dimension of 256. We define the metric "inference cost" as the average amount of computation to generate prediction for one test node. Inference cost is a measure of computation complexity (see Appendix B for the calculation) and is independent of hardware / implementation factors such as parallelization strategy, batch processing, etc. For cost of SHADOW-GNN, we do not include the overhead on computing EXTRACT, since it is hard to calculate such cost analytically. Empirically, subgraph extraction is much cheaper than model computation (see <ref type="figure">Figure 8</ref>, 9 for time evaluation on CPU and GPU). During training, we apply DropEdge <ref type="bibr" target="#b18">[39]</ref> to both the baseline and SHADOW models. DropEdge helps improve the baseline accuracy by alleviating oversmoothing, and benefits SHADOW-GNN due to its regularization effects. See Appendix F.2 for results on other architectures including GIN and JK-Net.</p><p>ACCURACY We aim at answering the following questions: 1. Can we improve accuracy by decoupling a baseline model? 2. What architecture components can we tune to improve accuracy of a decoupled model? 3. What EXTRACT can we tune to improve the accuracy of a decoupled model? To answer Q1, we fix the backbone architecture and remove pooling. Then we inspect "3-layer normal GNN vs. 3-layer SHADOW-GNN" and "5-layer normal GNN vs. 5-layer SHADOW-GNN". Clearly, SHADOW-GNNs (with scope size no more than 200) in general achieve significantly higher accuracy than the normal GNNs. This indicates that a shallow neighborhood contains sufficient information, and customizing the scope can benefit accuracy even without architecture changes (from <ref type="figure">Figure 2</ref>, a depth-3 G [v] differs significantly from the 3-hop neighborhood). To answer Q1, we focus on the PPR EXTRACT and thus compare the rows in blue background. We use the 3-layer SHADOW-GNN without pooling as the baseline and analyze the effects of 1. increasing the GNN depth without expanding scope, and 2. adding subgraph pooling. Comparing among the rows in light blue background, we observe that in many cases, simply increasing the depth from 3 to 5 leads to significant accuracy gain. Comparing the ligh blue rows with the dark blue rows, we observe that sometimes pooling can further improve the accuracy of a SHADOW-GNN. In conclusion, both types of architecture tuning are effective ways of optimizing a SHADOW-GNN. Finally, to answer Q3, we compare the light blue rows with the light yellow rows. In general, PPR EXTRACT leads to higher accuracy than 2-hop EXTRACT, demonstrating the importance of designing a good EXTRACT.</p><p>INFERENCE COST Inference cost of SHADOW-GNN is orders of magnitude lower than the normal GNNs (a 5-layer SHADOW-GNN is still much cheaper than a 3-layer normal GNN). The high cost of the baselines is due to the "neighborhood explosion" with respect to more layers. SHADOW-GNN is efficient and scalable as the cost only grows linearly with the model depth. Note that GraphSAINT only improves efficiency during training since its inference operates on the full L-hop neighborhood. Scaling to 100 million nodes. We further scale SHADOW-GNN to ogbn-papers100M, one of the largest public dataset. Even through the full graph size is at least two orders of magnitude larger than the graphs in <ref type="table" target="#tab_2">Table 1</ref>, the localized scope of SHADOW-GNN barely needs to increase. Since SHADOW-GNN performs minibatch computation, a low-end GPU with limited memory capacity can compute SHADOW-GNN on ogbn-papers100M efficiently. We show in Appendix F.1 that we can train and inference our model with as little as 4GB GPU memory consumption. This is infeasible using normal GNNs. <ref type="table" target="#tab_3">Table 2</ref> summarizes our comparison with the top leaderboard methods <ref type="bibr" target="#b24">[45,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">47]</ref>. We only include those methods that do not use node labels as the model input (i.e., the most standard setup). We achieve at least 3 orders of magnitude reduction in neighborhood size without sacrificing accuracy. For SIGN-XL and SGC, their neighborhood is too large to count the exact size. Also, their preprocessing consumes 5? more CPU memory than SHADOW-GNN (Appendix F.1). Extending to link-level task. We further show that SHADOW-GNN is general and can be extended to the link prediction task. There are two settings of ogbl-collab. We follow the one where validation edges cannot be used in training updates. This is the setting which most leaderboard methods follow. <ref type="table" target="#tab_4">Table 3</ref> shows the comparison with the top GNN mod-els under the same setting. SHADOW-SAGE outperforms the rank-1 model with significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oversmoothing.</head><p>To validate Theorem 3.2, we pick SGC as the backbone architecture. SGC with power L is equivalent to L-layer GCN without activation. Performance comparison between SGC and SHADOW-SGC thus reveals the effect of oversmoothing without introducing other factors due to optimizing deep neural networks (e.g., vanishing gradients). In <ref type="figure">Figure 3</ref>, we vary the power of SGC and SHADOW-SGC from 1 to 40 (see Appendix E.5 for details). While SGC gradually collapses local information into global "white noise", accuracy of SHADOW-SGC does not degrade. This validates our theory that extracting local subgraphs prevents oversmoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a design principle to decouple the depth and scope of GNNs. Applying such a principle on various GNN architectures simultaneously improves expressivity and computation scalability of the corresponding models. We have presented thorough theoretical analysis on expressivity from three different perspectives, and also rich design components (e.g., subgraph extraction functions, architecture extensions) to implement such design principle. Experiments show significant performance improvement over a wide range of graphs, GNN architectures and learning tasks.</p><p>[17] Wenbing Huang, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Tackling oversmoothing for general Graph Convolutional Networks, 2020.</p><p>[18] Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph representation learning. In Advances in neural information processing systems, pages 4558-4567, 2018.</p><p>[19] Glen Jeh and Jennifer Widom. Simrank: a measure of structural-context similarity. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 538-543, 2002.</p><p>[20] Leo Katz. A new status index derived from sociometric analysis. Psychometrika, 18(1): <ref type="bibr" target="#b18">[39]</ref><ref type="bibr" target="#b19">[40]</ref><ref type="bibr" target="#b20">[41]</ref><ref type="bibr" target="#b21">[42]</ref><ref type="bibr" target="#b22">[43]</ref><ref type="bibr">1953</ref>.</p><p>[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p><p>[22] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.</p><p>[23] Johannes Klicpera, Stefan Wei?enberger, and Stephan G?nnemann. Diffusion improves graph learning, 2019.</p><p>[24] Junhyun Lee, Inyeop Lee, and Jaewoo Kang. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof on SHADOW-GCN Expressivity</head><p>Proof of Proposition 3.1. The GCN model performs symmetric normalization on the adjacency matrix. SHADOW-GCN follows the same way to normalize the subgraph adjacency matrix as:</p><formula xml:id="formula_16">A [v] = D [v] + I [v] ? 1 2 ? A [v] + I [v] ? D [v] + I [v] ? 1 2<label>(3)</label></formula><p>where A <ref type="bibr">[v]</ref> ? R N ?N is the binary adjacency matrix for G <ref type="bibr">[v]</ref> .</p><p>A <ref type="bibr">[v]</ref> is a real symmetric matrix and has the largest eigenvalue of 1. Since EXTRACT ensures the subgraph G <ref type="bibr">[v]</ref> is connected, so the multiplicity of the largest eigenvalue is 1. By Theorem 1 of [17], we can bound the eigenvalues ? i by 1 = ? 1 &gt; ? 2 ? . . . ? ? N &gt; ?1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performing eigen-decomposition on A [v]</head><p>, we have</p><formula xml:id="formula_17">A [v] = E [v] ?E ?1 [v] = E [v] ?E T [v]<label>(4)</label></formula><p>where ? is a diagonal matrix ? i,i = ? i and matrix E <ref type="bibr">[v]</ref> consists of all the normalized eigenvectors.</p><p>We have:</p><formula xml:id="formula_18">A L [v] = E [v] ? L E T [v]<label>(5)</label></formula><p>Since</p><formula xml:id="formula_19">|? i | &lt; 1 when i = 1, we have lim L?? A L [v] = e [v] e T [v]</formula><p>, where e v is the eigenvector corresponding to ? 1 . It is easy to see that</p><formula xml:id="formula_20">e [v] u ? ? [v] (u) [17]. After normalization, e [v] u = ? [v] (u) w?V [v] ? [v] (w) . It directly follows that m [v] = e [v] v ? e T [v] X [v] , with value of e [v] defined above.</formula><p>Proof of Theorem 3.2. We first prove the case of m <ref type="bibr">[v]</ref> = m <ref type="bibr">[v]</ref> . i.e., ? G (v) = 1.</p><p>According to Proposition 3.1, the aggregation for each target node equals m</p><formula xml:id="formula_21">[v] = e [v] v e T [v] X [v]</formula><p>. Let N = |V|. Let? <ref type="bibr">[v]</ref> ? R N ?1 be a "padded" vector from e <ref type="bibr">[v]</ref> , such that the u th element is e [v] u if u ? V <ref type="bibr">[v]</ref> , and 0, otherwise. Therefore,</p><formula xml:id="formula_22">m [v] = e [v] v? T</formula><p>[v] X. Then, the difference in aggregation of two nodes u and v is given by</p><formula xml:id="formula_23">m [v] ? m [u] = e [v] v? T [v] X ? e [u] u? T [u] X (6) = ? T X,<label>(7)</label></formula><p>where</p><formula xml:id="formula_24">? = e [v] v ?? T [v] ? e [u] u ?? T [u]</formula><p>. When two nodes u and v have identical neighborhood as V [u] = V <ref type="bibr">[v]</ref> , then the aggregation vectors are identical as ? = 0. However, when two nodes have different neighborhoods, we claim that they almost surely have different aggregations. Let us assume the contrary, i.e., for some v and u with</p><formula xml:id="formula_25">V [v] = V [u]</formula><p>, their aggregations are the same: m [v] = m <ref type="bibr">[u]</ref> . Then we must have ? T X = 0.</p><p>Note that, given G, EXTRACT and some continuous distribution to generate X, there are only finite values for ?. The reasons are that 1. G is finite due to which there are only finite possible subgraphs and, 2. even though X can take infinitely many values, ? does not depend on X. Each of such ? = 0 defines a hyperplane in R N by ? ? x = 0 (where x ? R N ). Let H be the finite set of all such hyperplanes.</p><p>In other words, ?i, X :,i must fall on one of the hyperplanes in H. However, since X is generated from a continuous distribution in R N ?f , X :,i almost surely does not fall on any of the hyperplanes in H. Therefore, for any v and u such that</p><formula xml:id="formula_26">V [v] = V [u] , we have m [v] = m [u] almost surely.</formula><p>For a more general ? G (v) applied on m <ref type="bibr">[v]</ref> , since ? G does not depend on X, the proof follows exactly the same steps as above.</p><p>v </p><formula xml:id="formula_27">v ? G ? [v] v e v ? G ?? [v]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof on SHADOW-SAGE Expressivity</head><p>In Section 3.2, we have already shown why the normal GraphSAGE model cannot learn the ? function. Here we illustrate the idea with an example in <ref type="figure">Figure 4</ref>.</p><formula xml:id="formula_28">The neighborhood G ? [v] (or, G ?? [v] ) is denoted by the solid lines as the edge set E ? [v] (or, E ?? [v] ) and solid nodes as the node set V ? [v] (or, V ?? [v]</formula><p>). The red dashed edge and the red node v ? is in the full graph G but outside G ?</p><p>[v] or G ?? <ref type="bibr">[v]</ref> . Due to such v ? , candidate GraphSAGE models approximating ? cannot have message passing for more than 2 times. Further, G ??</p><p>[v] differs from G ? [v] by the green edge e connecting two 2-hop neighbors of the target v. The influence of e cannot be propagated to target v by only two message passings. Thus, as discussed in Section 3.2, there is no way for GraphSAGE to learn the ? function.</p><p>Proof of Theorem 3.3. SHADOW-SAGE follows the GraphSAGE way of normalization on the adjacency matrix. Thus, each row of A <ref type="bibr">[v]</ref> sums up to 1. Such normalization enables us to view A <ref type="bibr">[v]</ref> as the transition matrix of a Markov chain. The coefficients of the linear function ? (i.e., ? <ref type="bibr">[v]</ref> (?)) equal the limiting distribution of such a Markov chain. Therefore, we can use the convergence theorem of Markov chain to characterize the convergence of A L ?</p><p>[v] towards ? 's coefficients. We can also use the mixing time of the Markov chain to derive the error bound of shaDow-SAGE. Our proof is built on the existing theoretical results in <ref type="bibr">[27]</ref>.</p><p>We rewrite ? as ? = C ? ?X, where ? is a length-|V| vector with V [v] non-zero elements -each non-zero corresponds to ? <ref type="bibr">[v]</ref> (u) of the neighborhood G <ref type="bibr">[v]</ref> . Further, denote ? as the normalized ? vector. i.e., each non-zero element of ? equals</p><formula xml:id="formula_29">? [v] (u) w?V [v]</formula><p>? <ref type="bibr">[v]</ref> (w) . So ? = C ? ? ?X with some scaling factor C ? . For ease of discussion, we ignore C ? , as any scaling factor can be easily expressed by the model weight parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By Section 3.2, SHADOW-SAGE can express A L ?</head><p>[v] X v,:</p><formula xml:id="formula_30">= A L ? [v] v,:</formula><p>X. So now we need to show</p><formula xml:id="formula_31">how A L ? [v] v,:</formula><p>converges to ? when L ? ? ?. We establish the following correspondence: we can prove it by the stationary distribution of A <ref type="bibr">[v]</ref> . As shown by the next paragraph, ? is the stationary distribution of A <ref type="bibr">[v]</ref> . So we need to show that</p><formula xml:id="formula_32">A [v</formula><formula xml:id="formula_33">? u A [v] u,w = ? w A [v] w,u<label>(8)</label></formula><p>Consider two cases. If (u, w) ? E <ref type="bibr">[v]</ref> , then (w, u) ? E <ref type="bibr">[v]</ref> and both sides of Equation 8 equal 0. If ? as the limiting distribution. By definition, the stationary distribution ? satisfies</p><formula xml:id="formula_34">(u, w) ? E [v] , then ? u = ? [v] (u) x?V [v] ? [v] (x) and A [v] u,w = 1 ? [v] (u) .</formula><formula xml:id="formula_35">? = ? A [v]<label>(9)</label></formula><p>It is easy to see that setting ? = ? T can satisfy Equation 9. So ? is the stationary distribution. For irreducible and aperiodic Markov chain, the stationary distribution is also the limiting distribution, and thus</p><formula xml:id="formula_36">lim L ? ?? A L ? [v] v,: = ?<label>(10)</label></formula><p>So far, we can already show that when the SHADOW-SAGE model is deep enough (i.e., with large L ? ), the model output approximates ? : In summary, SHADOW-SAGE can approximate ? with error decaying exponentially with depth L ? .</p><formula xml:id="formula_37">lim L ? ?? A L ? [v] X v,: = ?<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof on SHADOW-GIN Expressivity</head><p>Proof of Theorem 3.4. We consider GNNs whose layer function is defined by Equation 2. Define G L</p><p>[v] as the subgraph induced from all of v's ?-hop neighbors in G, where 1 ? ? ? L. First, we show that a SHADOW-GNN following Equation 2 is at least as discriminative as the 1dimensional Weisfeiler-Lehman test (1-WL). We first prove that given any G L</p><p>[v] , an L ? -layer GNN can have exactly the same output as an L-layer GNN, where L ? &gt; L. We note that for the target node v, the only difference between these two architectures is that an L-layer GNN exactly performs L message passing iterations to propagate node information from at most L hops away, while an L ?layer GNN has L ? ? L more message passing iterations before performing the rest of L message passings. Thanks to the universal approximation theorem <ref type="bibr" target="#b14">[15]</ref>, we can let the MLPs implementing f 1 and f 2 learn the following GNN layer function: </p><formula xml:id="formula_38">u v G u G 1 [u] G 1 [v] v</formula><formula xml:id="formula_39">h (?) v =f (?) 1 h (??1) v , u?Nv f (?) 2 h (??1) v , h (??1) u =h (??1) v , ?1 ? ? ? L ? ? L This means h (0) v = h (L ? ?L) v</formula><p>. Then the L ? -layer GNN will have the same output as the L-layer GNN. According to <ref type="bibr" target="#b28">[49]</ref>, the normal GNN (i.e., L-layer on G L</p><p>[v] ) following Equation 2 is as discriminative as 1-WL. Thus, SHADOW-GNN (i.e., L ? -layer on G L</p><p>[v] ) following Equation 2 is at least as discriminative as 1-WL.</p><p>Next, we show that there exists a graph where SHADOW-GNN can discriminate topologically different nodes, while 1-WL cannot. The example graph mentioned in Section 3.3 is one such graph. We duplicate the graph here in <ref type="figure">Figure 5</ref>. G is connected and is 3-regular. The nodes u and v marked in red and blue have different topological roles, and thus an ideal model / algorithm should assign different labels to them. Suppose all nodes in G have identical features. For 1-WL, it will assign the same label to all nodes (including u and v) no matter how many iterations it runs. For SHADOW-GNN, if we let EXTRACT return G 1</p><p>[v] and G 1 [u] (i.e., 1-hop based EXTRACT), and set L ? &gt; 1, then our model can assign different labels to u and v. To see why, note that G 1</p><p>[u] and G 1</p><p>[v] are non-isomorphic, and more importantly, non-regular. So if we run the "SHADOW" version of 1-WL (i.e., 1-WL on G 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[v]</head><p>and G 1</p><p>[u] rather than on G), the nodes u and v will be assigned to different labels after two iterations. Equivalently, SHADOW-GNN can discriminate u and v with at least 2 layers.</p><p>We can further generalize to construct more such example graphs (although such generalization is not required by the proof). The guideline we follow is that, the full graph G should be regular. Yet the subgraphs around topologically different nodes (e.g., G k</p><p>[v] ) should be non-isomorphic and non-regular.</p><p>The graph in <ref type="figure">Figure 6</ref> is another example 2-regular graph, where nodes u and v can only be differentiated by decoupling the GIN.</p><p>Finally, combining the above two parts, SHADOW-GNN following Equation 2 is more discriminative than the 1-dimensional Weisfeiler-Lehman test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Inference Complexity Calculation</head><p>Here we describe the equations to compute the "inference cost" of <ref type="table" target="#tab_2">Table 1</ref>. Recall that inference cost is a measure of computation complexity to generate node embeddings for a given GNN architecture.</p><p>The numbers in <ref type="table" target="#tab_2">Table 1</ref> shows on average, how many arithmetic operations is required to generate the embedding for each node. For a GNN layer ?, denote the number of input nodes as n (??1) and the number of output nodes as n (?) . Denote the number of edges connecting the input and output nodes as m <ref type="bibr">(?)</ref> . Recall that we use d (?) to denote the number of channels, or, the hidden dimension.</p><p>In the following, we ignore the computation cost of non-linear activation, batch-normalization and applying bias, since their complexity is negligible compared with the other operations.</p><p>For the GCN architecture, each layer mainly performs two operations: aggregation of the neighbor features and linear transformation by the layer weights. So the number of multiplication-addition (MAC) operations of a layer-? equals:</p><formula xml:id="formula_40">P (?) GCN = m (?) d (??1) + n (?) d (??1) d (?)<label>(12)</label></formula><p>Similarly, for the GraphSAGE architecture, the number of MAC operations equals:</p><formula xml:id="formula_41">P (?) SAGE = m (?) d (??1) + 2 ? n (?) d (??1) d (?)<label>(13)</label></formula><p>where the 2 factor is due to the weight transformation on the self-features.</p><p>For GAT, suppose the number of attention heads is t. Then the layer contains t weight matrices</p><formula xml:id="formula_42">W i , each of shape d (??1) ? d (?) t .</formula><p>We first transform each of the n (??1) nodes by W i . Then for each edge (u, v) connecting the layer input u to the layer output v, we obtain its edge weight (i.e., a scalar) by computing the dot product between u's, v's transformed feature vectors and the model attention weight vector. After obtaining the edge weight, the remaining computation is to aggregate the n (??1) features into the n (?) nodes. The final output is obtained by concatenating the features of different heads. The number of MAC operations equals:</p><formula xml:id="formula_43">P (?) GAT =t ? n (??1) d (??1) d (?) t + 2t ? m (?) d (?) t + m (?) d (?) =3m (?) d (?) + n (??1) d (??1) d (?)<label>(14)</label></formula><p>On the same graph, GCN is less expensive than GraphSAGE. GraphSAGE is in general less expensive than GAT, due to n (??1) &gt; n (?) (on the other hand, note that n (??1) = n (?) for any SHADOW-GNN). In addition, for all architectures, P (?) * grows proportionally with n (?) and m (?) . For the normal GNN architecture, since we are using the full ?-hop neighborhood for each node, the value of n (?) and m (?) may grow exponentially with ?. This is the "neighbor explosion" phenonemon and is the root cause of the high inference cost of the baseline in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>For SHADOW-GNN, suppose the subgraph contains n nodes and m edges. Then n (?) = n and m (?) = m. The inference cost of any SHADOW-GNN is ensured to only grow linearly with the depth of the GNN.</p><p>Remark The above calculation on normal GNNs is under the realistic setting of minibatch inference. By "realistic setting", we mean 1. the graph size is often gigantic (of similar scale as ogbn-papers100M); 2. the total number of target nodes can be much, much smaller than the full graph size (e.g., &lt; 1% of all nodes are target nodes, as in ogbn-papers100M), and 3. we may only want to generate embedding for a small subset of the target nodes at a time. As a result, the above calculated computation complexity reflects what we can achieve in practice, under minibatch computation.</p><p>On the other hand, most of the benchmark graphs evaluated in the paper may not reflect the realistic setting. For example, ogbn-arxiv is downscaled 657? from ogbn-papers100M. Consequently, the full graph size is small and the number of target nodes is comparable to the full graph size. Such a "benchmark setting" enables full-batch inference: e.g., for GCN, one multiplication on the full graph adjacency matrix (i.e., A ? H ? W ) generates the next layer hidden features for all nodes at once.</p><p>While full-batch computation leads to significantly lower computation complexity than Equations 12, 13 and 14, it has strict constraints on the GPU memory size and the graph size -the full adjacency matrix and the various feature data of all nodes need to completely fit into the GPU memory. For example, according to the official OGB guideline <ref type="bibr" target="#b15">[16]</ref>, full-batch training of ogbn-products on a 3-layer GNN requires 48GB of GPU memory. On one hand, only the most powerful NVIDIA GPUs (e.g., RTX A6000) have memory as large as 48GB. On the other hand, ogbn-products is still 45? smaller than ogbn-papers100M. Therefore, full batch computation is not feasible in practice.</p><p>In summary, the inference complexity shown in <ref type="table" target="#tab_2">Table 1</ref> based on Equations 12, 13 and 14 reflects the feasibility of deploying the GNN models in real-life applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Designing Subgraph Extraction Algorithms</head><p>We illustrate how we can follow the three main approaches (i.e., heuristic based, model based and learning based. See Section 3.4) to design various subgraph extraction functions. Note that the Lhop and PPR based EXTRACT are only example algorithms to implement the principle of "decoupling the depth and scope".</p><p>Heuristic based. As discussed in Section 3.4, we can design EXTRACT by selecting appropriate graph metrics. We have shown some examples in Section 3.4. Here we provide detailed description on the two EXTRACT used in our experiments.</p><p>L-HOP EXTRACT. Starting from the target node v, the algorithm traverses up to L hops. At a hop-? node u, the sampler will add to its hop-(? + 1) node set either all neighbors of u, or b randomly selected neighbors of u. The subgraph G <ref type="bibr">[v]</ref> is induced from all the nodes selected by the EXTRACT. Here depth L and budget b are the hyperparameters.</p><p>PPR EXTRACT. Given a target v, our PPR EXTRACT proceeds as follows: 1. Compute the approximate PPR vector ? v ? R |V| for v. 2. Select the neighborhood V <ref type="bibr">[v]</ref> such that for u ? V <ref type="bibr">[v]</ref> , the PPR score [? v ] u is large. 3. Construct the induced subgraph from V <ref type="bibr">[v]</ref> . For step 1, even though vector ? v is of length-|V|, most of the PPR values are close to zero. So we can use a fast algorithm <ref type="bibr" target="#b2">[3]</ref> to compute the approximate PPR score by traversing only the local region around v. Throughout the ? v computation, most of its entries remain as the initial value of 0. Therefore, the PPR EXTRACT is scalable and efficient w.r.t. both time and space complexity. For step 2, we can either select V <ref type="bibr">[v]</ref> based on top-p values in ? v , or based on some threshold [? v ] &gt; ?. Then p, ? are hyperparameters. For step 3, notice that our PPR sampler only uses PPR scores as a node filtering condition. The original graph structure is still preserved among V <ref type="bibr">[v]</ref> , due to the induced subgraph step. In comparison, other related works [23, 6] do not preserve the original graph structure.</p><p>We also empirically highlight that the approximate PPR is both scalable and efficient. The number of nodes we need to visit to obtain the approximate ? v is much smaller than the graph size |V|. In addition, each visit only involves a scalar update, which is orders of magnitude cheaper than the cost of neighbor propagation in a GNN. We profile the three datasets, Reddit, Yelp and ogbn-products. As shown in <ref type="table" target="#tab_10">Table 4</ref>, for each target node on average, the number of nodes touched by the PPR computation is comparable to the full 2-hop neighborhood size. This also indicates that faraway neighbors do not have much topological significance. We further show the empirical execution time measurement on multi-core CPU machines in <ref type="figure">Figure 8</ref>. . More specifically, to extract the subgraph around a node v, we can imagine a process of adding v into the partial graph G ? consisting of V \ {v}. Then the nodes selected by EXTRACT would correspond to the nodes touched by such an imaginary process of adding v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning based.</head><p>We may treat the design of EXTRACT as part of GNN training. However, due to the combinatorial nature of subgraph extraction, simultaneously learning EXTRACT and the SHADOW-GNN layer parameters is challenging. The learning of EXTRACT can be made possible with appropriate approximation and relaxations. e.g., we may use the techniques proposed in <ref type="bibr" target="#b30">[51]</ref> to design a two-phased learning process. In Phase 1, we train a normal L-layer GNN, and then use <ref type="bibr" target="#b30">[51]</ref> to identify the important neighbors among the full L-hop neighborhood. In Phase 2, we use the Algorithm 1 Inference algorithm for the general SHADOW-GNN model</p><formula xml:id="formula_44">Input: G (V, E, X); Target nodes V t ; GNN model; C number of samplers {EXTRACT i }; Output: Node embedding matrix Y t for V t ; for v ? V t do for i = 1 to C do Get G [v],i by EXTRACT i on G Propagate G [v],i in the i th branch of L-layer GNN H [v],i ? f L i X [v],i , A [v],i y v,i ? MLP READOUT H [v],i H [v],i v,: end for y v ? ENSEMBLE ({y v,i }) end for</formula><p>neighborhood identified in Phase 1 as the subgraph returned by EXTRACT. Then we train an L ? -layer SHADOW-GNN on top of such neighborhood.</p><p>Detailed design and evaluation on the model and learning based approaches are left as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D The Complete SHADOW-GNN Framework</head><p>Algorithm 1 shows the inference algorithm of SHADOW-GNN after integrating the various architecture extensions discussed in Section 3.5. The f i function specifies the layer propagation of a given GNN architecture (e.g., GCN, GraphSAGE, etc.), and f L i is a shorthand for L times iterative layer propagation of the L-layer model. The READOUT (?) function performs subgraph pooling and ENSEMBLE (?) performs subgraph ensemble. The implementation of such functions are described in the following subsections.</p><p>After the pooling by READOUT (?), we further feed into an MLP the vector summarized from the subgraph and the vector for the target. This way, even if two target nodes u and v have the same neighborhood, we can still differentiate their embeddings based on the vectors H [v] v,: and H [u] u,: .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Architecture for Subgraph Pooling</head><p>The READOUT (?) function in Algorithm 1 can perform subgraph-level operation such as sum pooling, max pooling, mean pooling, sort pooling <ref type="bibr" target="#b36">[57]</ref>, etc. <ref type="table" target="#tab_11">Table 5</ref> summarizes all the pooling operations that we have integrated into the SHADOW-GNN framework, where H <ref type="bibr">[v]</ref> is the subgraph embedding matrix as shown in Algorithm 1. In particular, sort pooling 1. sorts the last column of the feature matrix, 2. takes the indices of the top s values after sorting (s is a hyperparameter of the sort pooling function), and 3. slice a submatrix of H <ref type="bibr">[v]</ref> based on the top-s indices. The input to the MLP (?) (last row of <ref type="table" target="#tab_11">Table 5</ref>) is a submatrix of H <ref type="bibr">[v]</ref> consisting of s rows, and the output of the MLP (?) is a single vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Architecture for Subgraph Ensemble</head><p>For the ensemble function, we implement the following after applying attention on the outputs of the different model branches:</p><formula xml:id="formula_45">w i =MLP (y v,i ) ? q<label>(15)</label></formula><formula xml:id="formula_46">y v = C i=1 w i ? y v,i<label>(16)</label></formula><p>where q is a learnable vector; w is normalized from w by softmax; y v is the final embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Detailed Experimental Setup</head><p>We used the tool "Weight &amp; Biases" <ref type="bibr" target="#b4">[5]</ref> for experiment tracking and visualization to develop insights for this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Additional Dataset Details</head><p>The statistics for the seven benchmark graphs is listed in <ref type="table" target="#tab_12">Table 6</ref>. Note that for Yelp, each node may have multiple labels, and thus we follow the original paper <ref type="bibr" target="#b34">[55]</ref> to report its F1-micro score. For all the other node classification graphs, a node is only associated with a single label, and so we report accuracy. Note that for Reddit and Flickr, other papers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">55]</ref> also report F1-micro score. However, since each node only has one label, "F1-micro score" is exactly the same as "accuracy". For the link prediction graph ogbl-collab, we use "Hits@50" as the metric.</p><p>For ogbn-papers100M, only around 1% of the nodes are associated with ground truth labels. The training / validation / test sets are split only among those labelled nodes. Note that Reddit is a pre-exisiting dataset collected by Stanford, available at http://snap.stanford.edu/graphsage. Facebook did not directly collect any data from Reddit. None Reddit content is reproduced in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Hardware</head><p>We have tested SHADOW-GNN under various hardware configurations, ranging from low-end desktops to high-end servers. We observe that the training and inference of SHADOW-GNN can be easily adapted to the amount of available hardware resources by adjusting the batch size.</p><p>In summary, we have used the following three machines to compute SHADOW-GNN.</p><p>? MACHINE 1: This is a low-end desktop machine with 4-core Intel Core i7-6700 CPU @3.4GHz, 16GB RAM and one NVIDIA GeForce GTX 1650 GPU of 4GB memory. ? MACHINE 2: This is a low-end server with 28-core Intel Xeon Gold 5120 CPU @2.2GHz, 128GB RAM and one NVIDIA Titan Xp GPU of 12GB memory. ? MACHINE 3: This is another server with 64-core AMD Ryzen Threadripper 3990x CPU @2.9GHz, 256GB RAM and three NVIDIA GeForce RTX3090 GPUs of 24GB memory.</p><p>From the GPU perspective, the low-end GTX 1650 GPU can support the SHADOW-GNN computation on all the graphs (including ogbn-papers100M). However, the limited RAM size of MACHINE 1 limits its usage to only Flickr, Reddit, ogbn-arxiv and ogbl-collab. The other two servers, MACHINE 2 and MACHINE 3, are used to train all of the seven graphs. <ref type="table" target="#tab_13">Table 7</ref> summarizes our recommended minimum hardware resources to run SHADOW-GNN. Note that regardless of the model, larger graphs inevitibly requires larger RAM due to the growth of the raw features (the raw data files of ogbn-papers100M already takes 70GB). However, larger graphs do not correspond to higher GPU requirement for SHADOW-GNN, since the GPU memory consumption is controlled by the batch size parameter.</p><p>See Appendix F.1 for how to control the GPU memory-speed tradeoff by batch size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Software</head><p>The code is written in Python 3.8.5 (where the sampling part is written with C++ parallelized by OpenMP, and the interface between C++ and Python is via PyBind11). We use PyTorch 1.7.1 on CUDA 11.1 to train the model on GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Hyperparameter Tuning</head><p>For all models in <ref type="table" target="#tab_2">Table 1</ref>, we set the hidden dimension to be d (?) = 256. In addition, for GAT and SHADOW-GAT, we set the number of attention heads to be t = 4. For all the GIN and SHADOW-GIN experiments, we use a 2-layer MLP (with hidden dimension 256) to perform the injective mapping in each GIN layer. For all the JK-Net and SHADOW-JK experiments, we use the concatenation operation to aggregate the hidden features of each layer in the JK layer. Additional comparison on GIN, JK-Net and their SHADOW versions is shown in <ref type="table" target="#tab_2">Table 12</ref>.</p><p>All experiments are repeated five times without fixing random seeds.</p><p>For all the baseline and SHADOW-GNN experiments, we use Adam optimizer <ref type="bibr">[21]</ref>. We perform grid search on the hyperparameter space defined by:</p><p>? Activation: {ReLU, ELU, PRELU} The EXTRACT hyperparameters are tuned as follows.</p><p>For the PPR EXTRACT, we consider two versions: one based on fixed sampling budget p and the other based on PPR score thresholding ?.</p><p>? If with fixed budget, then we disable the ? thresholding. We tune the budget by p ? {150, 175, 200}.</p><p>? If with thresholding, we set ? ? {0.01, 0.05}. We still have an upper bound p on the subgraph size. So if there are q nodes in the neighborhood with PPR score larger than ?, the final subgraph size would be max{p, q}. Such an upper bound eliminates the corner cases which may cause hardware inefficiency due to very large q. In this case, we set the upper bound p to be either 200 or 500.    <ref type="figure">Figure 3</ref>, due to lack of space, we use S-SGC to denote SHADOW-SGC. We use "F", "R" and "A" to denote the datasets of Flickr, Reddit and ogbn-arxiv respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Understanding the Low Memory Overhead of SHADOW-GNN</head><p>SHADOW-GNN in general consumes much less memory than the other GNN-based methods. We can understand the low memory overhead of SHADOW-GNN from two perspectives.</p><p>Comparing with the normal GNN model, SHADOW-GNN requires much less GPU memory since both the training and inference of SHADOW-GNN proceeds in minibatches. While other minibatching training algorithms exist for the normal GNNs (e.g., layer sampling <ref type="bibr" target="#b6">[7]</ref> and graph sampling <ref type="bibr" target="#b34">[55]</ref>), such algorithms either result in accuracy degradation or limited scalability. Therefore, most of the OGB leaderboard methods are tuned with full-batch training.</p><p>Comparing with the simplified GNN models (e.g., SIGN <ref type="bibr" target="#b10">[11]</ref> and SGC <ref type="bibr" target="#b26">[47]</ref>), SHADOW-GNN requires much less RAM size. The reason is that both SIGN and SGC rely on preprocessing of node features over the full graph. Specifically, for SIGN, its preprocessing step concatenates the original node features with the smoothened values. For the (p, s, t) architecture of SIGN (see the SIGN paper for definition of p, s and t), the memory required to store the preprocessed features equals:</p><formula xml:id="formula_47">M = N ? f ? ((p + 1) + (s + 1) + (t + 1))<label>(17)</label></formula><p>where N is the number of nodes in the full graph and f is the original node feature size. For the (3, 3, 3) SIGN / SIGN-XL architecture on the ogbn-papers100M leaderboard (see <ref type="table" target="#tab_3">Table 2</ref>), the required RAM size equals M = 682GB. This number does not even consider the RAM consumptions due to temporary variables or other buffers for the trainig operations.</p><p>For SGC, even though it does not perform concatenation of the smoothened features, it still requires double the original feature size to store the temporary values of A K ? X. The original features of ogbn-papers100M takes around 56GB, and the full graph adjacency matrix consumes around 25GB. In sum, SGC requires at least 2 ? 56 + 25 = 137GB of RAM. <ref type="table" target="#tab_2">Table 11</ref> summarizes the RAM / GPU memory consumption for the leaderboard methods listed in <ref type="table" target="#tab_3">Table 2</ref>. Note that our machines do not support the training of SIGN (due to the RAM size constraint), and thus we only show the lower bound of SIGN's RAM consumption in <ref type="table" target="#tab_2">Table 11</ref>.  On the other hand, SHADOW-GNN can flexibly adjust its batch size based on the available memory. Even for ogbn-papers100M, a typical low-end server with 4GB of GPU memory and 128GB of RAM is sufficient to train the 5-layer SHADOW-GAT. Increasing the batch size of SHADOW-GNN may further lead to higher GPU utilization for more powerful machines. <ref type="figure" target="#fig_9">Figure 7</ref> shows the computation time speedup (compared with batch size 32) and GPU memory consumption for SHADOW-GAT under batch size of 32, 64 and 128. A 5-layer SHADOW-GAT only requires around 5GB of GPU memory to saturate the computation resources of the powerful GPU cards such as NVIDIA RTX 3090.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 SHADOW-GNN on Other Architectures</head><p>In addition to the GCN, GraphSAGE and GAT models in <ref type="table" target="#tab_2">Table 1</ref>, we further compare JK-Net and GIN with their SHADOW versions in <ref type="table" target="#tab_2">Table 12</ref>. For all the results in Tabel 1, we do not apply pooling or ensemble on SHADOW-GNN. Similar to DropEdge, the skip connection (or "jumping knowledge") of JK-Net helps accuracy improvement on deeper models. Compared with the normal JK-Net, increasing the depth benefits SHADOW-JK more. The GIN architecture theoretically does not oversmooth. However, we observe that the GIN training is very sensitive to hyperparameter settings. We hypothesize that such a challenge is due to the sensitivity of the sum aggregator on noisy neighbors (e.g., for GraphSAGE, a single noisy node can hardly cause a significant perturbation on the aggregation, due to the averaging over the entire neighborhood). The accuracy improvement of SHADOW-GIN compared with the normal GIN may thus be due to noise filtering by the EXTRACT. The impact of noises / irrelevant neighbors can be critical, as reflected by the 5-layer GIN accuracy on Reddit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Benefits of Pooling</head><p>In <ref type="table" target="#tab_2">Table 13</ref>, we summarize the average accuracy of 5-layer SHADOW-SAGE obtained from different pooling functions (see <ref type="table" target="#tab_11">Table 5</ref> for the equations for pooling). For both graphs, adding a pooling layer helps improve accuracy. On the other hand, the best pooling function may depend on the graph characteristics. We leave the in-depth analysis on the effect of pooling as future work.  Flickr 0.5351?0.0026 0.5361?0.0015 0.5354?0.0021 0.5367?0.0026 ogbn-arxiv 0.7302?0.0014 0.7304?0.0015 0.7342?0.0017 0.7295?0.0018</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Cost of Subgrpah Extraction</head><p>We evaluate the PPR EXTRACT in terms of its execution time overhead and accuracy-time tradeoff. In <ref type="figure">Figure 8</ref>, we parallelize EXTRACT using half of the available CPU cores of MACHINE 3 (i.e., 32 cores) and execute the GNN computation on the RTX 3090 GPU. Clearly, the PPR EXTRACT is lightweight: the sampling time is lower than the GNN computation time in most cases. In addition, the sampling time per node does not grow with the full graph size. This shows that SHADOW-GNN is scalable to massive scale graphs. By the discussion in Appendix C, the approximate PPR computation achieves efficiency and scalability by only traversing a local region around each target node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Tradeoff Between Inference Time and Accuracy</head><p>To evaluate the accuracy-time tradeoff, we take the 5-layer models of <ref type="table" target="#tab_2">Table 1</ref> as the pretrained models. Then for SHADOW-GNN, we vary the PPR budget p from 50 to 200 with stride 50. In <ref type="figure">Figure 9</ref>, the inference time of SHADOW-GNN has already included the PPR sampling time. Firstly, consistent with <ref type="table" target="#tab_2">Table 1</ref>, inference of SHADOW-GNN achieves higher accuracy than the normal GNNs, with orders of magnitude speedup as well. In addition, based on the application requirements (e.g., latency constraint), SHADOW-GNNs have the flexibility of adjusting the sampling size without the need of retraining. For example, on Reddit and ogbn-arxiv, directly reducing the subgraph  <ref type="figure">Figure 9</ref>: Inference performance tradeoff. We test pre-trained models by subgraphs of various sizes. size from 200 to 50 reduces the inference latency by 2? to 4? at the cost of less than 1% accuracy drop.  <ref type="table" target="#tab_2">Table 14</ref> shows additional results on subgraph ensemble and deeper SHADOW-GCN models. The PPR and 2-hop EXTRACT follow the same configuration as <ref type="table" target="#tab_14">Table 8 and 9</ref>. When varying the model depth, we keep all the other hyperparameters unchanged. From both the Flickr and ogbn-products results, we observe that ensemble of the PPR EXTRACT and the 2-hop EXTRACT helps improve the SHADOW-GCN accuracy. From the ogbn-products results, we additionally observe that increasing SHADOW-GCN to deeper than 5 layers may still be beneficial. As analyzed by <ref type="figure">Figure 2</ref>, the model depth of 5 is already much larger than the hops of the subgraph. The 7-layer results in <ref type="table" target="#tab_2">Table 14</ref> indicate future research opportunities to improve the SHADOW-GNN accuracy by going even deeper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>N v denote the set of v's direct neighbors in G. The u th row of X corresponds to the length-d feature of node u. Let A be the adjacency matrix of G where A u,v = 1 if edge (u, v) ? E and A u,v = 0 otherwise. Let D be the diagonal degree matrix of A. Denote A = D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>vFigure 1 :</head><label>1</label><figDesc>Example 3-regular graph and the 1-hop subgraphs of the target nodes u and v.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2</head><label></label><figDesc>are the update and message functions of layer-?, implemented as MLPs. Then, such SHADOW-GNN is more discriminative than the 1-dimensional Weisfeiler-Lehman test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4 Figure 2</head><label>42</label><figDesc>Figure 2: Neighborhood composition</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Example 3-regular graph and the 1-hop subgraph of the target nodes Example 2-regular graph with two connected components (CC)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>?</head><label></label><figDesc>Dropout: 0 to 0.5 with stride of 0.05 ? DropEdge: 0 to 0.5 with stride of 0.05 ? Learning rate: {1e?2, 2e?3, 1e?3, 2e?4, 1e?4, 2e?5} ? Batch size: {16, 32, 64, 128}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Controlling the GPU memory-speed tradeoff by SHADOW-GAT batch size(32, 64, 128)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>F l i c k r R e d d i t Y e l p a r x i v p r o d u c t s p a p e r s 1 Figure 8 :</head><label>18</label><figDesc>Measured execution time for PPR EXTRACT and the GNN model computation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Given G, EXTRACT and some continuous probability distribution in R |V|?d to generate X, thenm [v] = m [u] if V [u] = V [v] , almost surely. Consider EXTRACT 2 , where ? u, v ? V, V [v] = V[u]  . Then |M| = |V| a.s. Theorem 3.2 proves SHADOW-GCN does not oversmooth: 1. A normal GCN pushes the aggregation of same-degree nodes to the same point, while SHADOW-GCN with EXTRACT 2 ensures any two nodes (even with the same degree) have different aggregation. 2. A normal GCN wipes out all information in X after many times of aggregation, while SHADOW-GCN always preserves feature information. Particularly, with ? G (v) = ?[v]  (v) ?1/2 , a normal GCN generates only one unique value of m for all v. By contrast, SHADOW-GNN generates |V| different values for any ? G function.</figDesc><table><row><cell>Corollary 3.2.1. Consider EXTRACT 1 , where ?v ? V, V [v] ? n. Then |M| ? |V| n We compare the expressivity by showing 1. SHADOW-SAGE can express all functions GraphSAGE a.s. can, and 2. SHADOW-SAGE can express some function GraphSAGE cannot. Recall, a GraphSAGE Corollary 3.2.2. 3.2 Expressivity Analysis on SHADOW-SAGE: Function Approximation Perspective layer performs the following: h</cell></row></table><note>] plus 1. Oversmoothing by normal GCN propagation. With a large enough L, the full L-hop neighbor- hood becomes V (assuming connected G). So ? u, v, we have G [u] = G [v] = G, implying e [u] = e [v] and X [u] = X [v] = X. From Proposition 3.1, the aggregation converges to a point where no fea- ture and little structural information of the target is preserved. The only information in m [v] is v's degree. Local-smoothing by SHADOW-GCN propagation. With a fixed subgraph, no matter how many times we aggregate using A [v] , the layers will not include the faraway irrelevant nodes. From Propo- sition 3.1, m [v] is a linear combination of the neighbor features X [v] . Increasing the number of layers only pushes the coefficients of each neighbor features to the stationary values. The domain X [v] of such linear transformation is solely determined by EXTRACT and is independent of the model depth. Intuitively, if EXTRACT picks non-identical subgraphs for two nodes u and v, the aggregations should be different due to the different domains of the linear transformation. Therefore, SHADOW- GCN preserves local feature information whereas normal GCN preserves none. For structural in- formation in m [v] , note that e [v] is a normalized degree distribution of the subgraph around v, and e [v] v indicates the role of the target node in the subgraph. By simply letting EXTRACT return the 1- hop subgraph, e [v] v alone already contains all the information preserved by a normal GCN, which is v's degree in G. For the general EXTRACT, e [v] additionally reflects v's ego-net structure. Thus, a deep SHADOW-GCN preserves more structural information than a deep GCN. Theorem 3.2. Let m [v] = ? G (v) ? m [v] where ? G is any non-zero function only depending on the structural property of v. Let M = {m [v] | v ? V}.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>4</head><label></label><figDesc>Related Work Deep GNNs. To improve the GNN performance while increasing the model depth, various layer architectures have been proposed. AS-GCN [18], DeepGCN [28], JK-net [50], MixHop [1], Snowball [33], DAGNN [31] and GCNII [34]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison on test accuracy / F1-micro score and inference cost (tuned with DropEdge) 5286?0.0013 1E0 0.9624?0.0002 1E0 0.5393?0.0036 2E0 0.7223?0.0018 2E0 0.7914?0.0044 2E0 5395?0.0013 2E0 0.9703?0.0003 2E0 0.6564?0.0004 1E0 0.7258?0.0017 2E0 0.8067?0.0037 1E0</figDesc><table><row><cell>Method</cell><cell>Layers</cell><cell>Flickr</cell><cell></cell><cell>Reddit</cell><cell></cell><cell>Yelp</cell><cell></cell><cell cols="2">ogbn-arxiv</cell><cell cols="2">ogbn-products</cell></row><row><cell></cell><cell></cell><cell>Accuracy</cell><cell>Cost</cell><cell>Accuracy</cell><cell>Cost</cell><cell>F1-micro</cell><cell>Cost</cell><cell>Accuracy</cell><cell>Cost</cell><cell>Accuracy</cell><cell>Cost</cell></row><row><cell>GCN</cell><cell>3 5</cell><cell cols="10">0.5159?0.0017 2E0 0.9532?0.0003 6E1 0.4028?0.0019 2E1 0.7170?0.0026 1E1 0.7567?0.0018 5E0 0.5217?0.0016 2E2 0.9495?0.0012 1E3 OOM 1E3 0.7186?0.0017 1E3 OOM 9E2</cell></row><row><cell>GCN-SAINT</cell><cell>3 5</cell><cell cols="10">0.5155?0.0027 2E0 0.9523?0.0003 6E1 0.5110?0.0012 2E1 0.7093?0.0003 1E1 0.8003?0.0024 5E0 0.5165?0.0026 2E2 0.9523?0.0012 1E3 0.5012?0.0021 1E3 0.7039?0.0020 1E3 0.7992?0.0021 9E2</cell></row><row><cell>SHADOW-GCN</cell><cell>3</cell><cell>0.5234?0.0009</cell><cell>(1)</cell><cell>0.9576?0.0005</cell><cell>(1)</cell><cell>0.5291?0.0020</cell><cell>(1)</cell><cell>0.7180?0.0024</cell><cell>(1)</cell><cell>0.7742?0.0037</cell><cell>(1)</cell></row><row><cell>(PPR)</cell><cell>5</cell><cell cols="10">0.5268?0.0008 1E0 0.9564?0.0004 1E0 0.5323?0.0020 2E0 0.7206?0.0025 2E0 0.7821?0.0043 2E0</cell></row><row><cell cols="12">+Pooling 0.GraphSAGE 3/5 3 0.5140?0.0014 3E0 0.9653?0.0002 5E1 0.6178?0.0033 2E1 0.7192?0.0027 1E1 0.7858?0.0013 4E0 5 0.5154?0.0052 2E2 0.9626?0.0004 1E3 OOM 2E3 0.7193?0.0037 1E3 OOM 1E3</cell></row><row><cell>SAGE-SAINT</cell><cell>3 5</cell><cell cols="10">0.5176?0.0032 3E0 0.9671?0.0003 5E1 0.6453?0.0011 2E1 0.7107?0.0003 1E1 0.7923?0.0023 4E0 0.5201?0.0032 2E2 0.9670?0.0010 1E3 0.6394?0.0003 2E3 0.7013?0.0021 1E3 0.7964?0.0022 1E3</cell></row><row><cell>SHADOW-SAGE</cell><cell>3</cell><cell cols="10">0.5312?0.0019 1E0 0.9672?0.0003 1E0 0.6542?0.0002 1E0 0.7163?0.0028 1E0 0.7935?0.0031 1E0</cell></row><row><cell>(2-hop)</cell><cell>5</cell><cell cols="10">0.5335?0.0015 2E0 0.9675?0.0005 2E0 0.6525?0.0003 2E0 0.7180?0.0030 2E0 0.7995?0.0022 2E0</cell></row><row><cell>SHADOW-SAGE</cell><cell>3</cell><cell>0.5356?0.0013</cell><cell>(1)</cell><cell>0.9688?0.0002</cell><cell>(1)</cell><cell>0.6538?0.0003</cell><cell>(1)</cell><cell>0.7227?0.0012</cell><cell>(1)</cell><cell>0.7905?0.0026</cell><cell>(1)</cell></row><row><cell>(PPR)</cell><cell>5</cell><cell cols="10">0.5417?0.0006 2E0 0.9692?0.0007 2E0 0.6518?0.0002 2E0 0.7238?0.0007 2E0 0.8005?0.0040 2E0</cell></row><row><cell cols="4">+Pooling 0.GAT 3/5 3 0.5070?0.0032 2E1 5 0.5164?0.0033 2E2</cell><cell>OOM OOM</cell><cell>3E2 2E3</cell><cell>OOM OOM</cell><cell cols="3">2E2 0.7201?0.0011 1E2 2E3 OOM 3E3</cell><cell>OOM OOM</cell><cell>3E1 2E3</cell></row><row><cell>GAT-SAINT</cell><cell>3 5</cell><cell cols="10">0.5225?0.0053 2E1 0.9671?0.0003 3E2 0.6459?0.0002 2E2 0.6977?0.0003 1E2 0.8027?0.0028 3E1 0.5153?0.0034 2E2 0.9651?0.0024 2E3 0.6478?0.0012 2E3 0.6954?0.0013 3E3 0.7990?0.0072 2E3</cell></row><row><cell>SHADOW-GAT</cell><cell>3</cell><cell>0.5349?0.0023</cell><cell>(1)</cell><cell>0.9707?0.0004</cell><cell>(1)</cell><cell>0.6575?0.0004</cell><cell>(1)</cell><cell>0.7235?0.0020</cell><cell>(1)</cell><cell>0.8006?0.0014</cell><cell>(1)</cell></row><row><cell>(PPR)</cell><cell>5</cell><cell>0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>5352?0.0028 2E0 0.9713?0.0004 2E0 0.6559?0.0002 2E0 0.7274?0.0022 2E0 0.8071?0.0004 2E0 +Pooling 3/5 0.5364?0.0026 1E0 0.9710?0.0004 2E0 0.6566?0.0005 1E0 0.7265?0.0028 2E0 0.8142?0.0031 1E0</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Leaderboard comparison on papers100M</figDesc><table><row><cell>Method</cell><cell>Test accuracy</cell><cell>Val accuracy</cell><cell>Neigh size</cell></row><row><cell cols="3">GraphSAGE+incep 0.6706?0.0017 0.7032?0.0011</cell><cell>4E5</cell></row><row><cell>SIGN-XL</cell><cell cols="2">0.6606?0.0019 0.6984?0.0006</cell><cell>&gt; 4E5</cell></row><row><cell>SGC</cell><cell cols="2">0.6329?0.0019 0.6648?0.0020</cell><cell>&gt; 4E5</cell></row><row><cell>SHADOW-GAT 200</cell><cell cols="2">0.6681?0.0016 0.7019?0.0011</cell><cell>2E2</cell></row><row><cell>SHADOW-GAT 400</cell><cell cols="2">0.6708?0.0017 0.7073?0.0011</cell><cell>3E2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Leaderboard comparison on collab</figDesc><table><row><cell>Method</cell><cell>Test Hits@50</cell><cell>Val Hits@50</cell></row><row><cell>SEAL</cell><cell cols="2">0.5371?0.0047 0.6389?0.0049</cell></row><row><cell>DeeperGCN</cell><cell cols="2">0.5273?0.0047 0.6187?0.0045</cell></row><row><cell>LRGA+GCN</cell><cell cols="2">0.5221?0.0072 0.6088?0.0059</cell></row><row><cell>SHADOW-SAGE</cell><cell></cell><cell></cell></row></table><note>0.5492?0.0022 0.6524?0.0017</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Proof of Corollary 3.2.1. Note that any subgraph contains the target node itself. i.e., ?u ? V, u ? V[u]  . Therefore, for any node v, there are at most n? 1 other nodes in V with the same neighborhood as V[v]  . Such n ? 1 possible nodes are exactly those in V[v]  . By Theorem 3.2, ?v ? V, there are at most n ? 1 other nodes in V having the same aggregation as m[v]  . Equivalently, total number of possible aggregations is at least ?|V| /n?.</figDesc><table><row><cell>Figure 4: Example G ? [v] and G ?? [v] as described in Section 3.2</cell></row></table><note>Proof of Corollary 3.2.2. By definition of EXTRACT 2 , any pair of nodes has non-identical neighbor- hood. By Theorem 3.2, any pair of nodes have non-identical aggregation. Equivalently, all nodes have different aggregation and |M| = |V|.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>(x) . Thus, Equation 8 holds and A [v] is reversible.</figDesc><table><row><cell></cell><cell></cell><cell>So both sides of Equation 8</cell></row><row><cell>equal</cell><cell>1 x?V [v]</cell><cell>? [v]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Desired model depth as the mixing time. Next, we want to see if we want the SHADOW-SAGE model to reach a given error bound, how many layers L ? are required. Firstly, directly applying Theorem 4.9 of [27], we know that the error of SHADOW-SAGE approximating ? reduces exponentially with the model depth L ? . Then From Equation 4.36 of [27], we can directly relate the mixing time of Markov chain with the required shaDow-SAGE depth to reach any ? error. Finally, by Theorem 12.3 of [27], the mixing time can be bounded by the "absolute spectral gap" of the transition matrix A[v]  . Note that Theorem 12.3 applies when the Markov chain is reversible -a condition satisfied by A[v]  as we have already discussed. The absolute spectral gap is calculated from the eigenvalues of the transition matrix A[v]  .</figDesc><table><row><cell>11)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Average number of nodes touched by the approximate PPR computation Dataset Avg. 2-hop size Avg. # nodes touched by PPR From Section 1, treating G as the union of G [v] 's describes a generation process on G. EXTRACT then describes the reverse of such a process. This links to the graph generation literature [26]. e.g., forest-fire generation model would correspond to EXTRACT being forest-fire sampler [25]</figDesc><table><row><cell>Reddit</cell><cell>11093</cell><cell>27751</cell></row><row><cell>Yelp</cell><cell>2472</cell><cell>5575</cell></row><row><cell>ogbn-products</cell><cell>3961</cell><cell>5405</cell></row><row><cell>Model based.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>READOUT (?) function for different pooling operations.</figDesc><table><row><cell>Name</cell><cell>READOUT (?)</cell></row><row><cell>Center</cell><cell>H [v] v,:</cell></row><row><cell>Sum Mean</cell><cell>u?V [v] H [v] u,: |V[v]| u?V [v] H [v] u,: 1</cell></row><row><cell>Max</cell><cell>max u?V [v] H [v] u,:</cell></row><row><cell>Sort</cell><cell>MLP H [v] arg sort[H [v] ] :,?1 :s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Dataset statistics</figDesc><table><row><cell>Dataset</cell><cell>Setting</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="4">Degree Feature Classes Train / Val / Test</cell></row><row><cell>Flickr</cell><cell>Inductive</cell><cell>89,250</cell><cell>899,756</cell><cell>10</cell><cell>500</cell><cell>7</cell><cell>0.50 / 0.25 / 0.25</cell></row><row><cell>Reddit</cell><cell>Inductive</cell><cell>232,965</cell><cell>11,606,919</cell><cell>50</cell><cell>602</cell><cell>41</cell><cell>0.66 / 0.10 / 0.24</cell></row><row><cell>Yelp</cell><cell>Inductive</cell><cell>716,847</cell><cell>6,977,410</cell><cell>10</cell><cell>300</cell><cell>100</cell><cell>0.75 / 0.10 / 0.15</cell></row><row><cell>ogbn-arxiv</cell><cell>Transductive</cell><cell>169,343</cell><cell>1,166,243</cell><cell>7</cell><cell>128</cell><cell>40</cell><cell>0.54 / 0.18 / 0.29</cell></row><row><cell>ogbn-products</cell><cell>Transductive</cell><cell>2,449,029</cell><cell>61,859,140</cell><cell>25</cell><cell>100</cell><cell>47</cell><cell>0.10 / 0.02 / 0.88</cell></row><row><cell cols="4">ogbn-papers100M Transductive 111,059,956 1,615,685,872</cell><cell>29</cell><cell>128</cell><cell>172</cell><cell>0.78 / 0.08 / 0.14</cell></row><row><cell>ogbl-collab</cell><cell>-</cell><cell>235,868</cell><cell>1,285,465</cell><cell>8</cell><cell>128</cell><cell>-</cell><cell>0.92/0.04/0.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell cols="5">Recommended minimum hardware resources for SHADOW-GNN</cell></row><row><cell>Dataset</cell><cell cols="2">Num. nodes CPU cores</cell><cell>RAM</cell><cell>GPU memory</cell></row><row><cell>ogbn-arxiv</cell><cell>0.2M</cell><cell>4</cell><cell>8GB</cell><cell>4GB</cell></row><row><cell>ogbn-products</cell><cell>2.4M</cell><cell>4</cell><cell>32GB</cell><cell>4GB</cell></row><row><cell>ogbn-papers100M</cell><cell>111.1M</cell><cell>4</cell><cell>128GB</cell><cell>4GB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Training configuration of SHADOW-GNN for Table 1 (PPR EXTRACT)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Training configuration of SHADOW-GNN for Table 1 (2-hop EXTRACT) Arch. Dataset Layers Dim. Pooling Learning Rate Batch Size Dropout DropEdge Budget b</figDesc><table><row><cell></cell><cell>Flickr</cell><cell>3 5</cell><cell>256 256</cell><cell>--</cell><cell>0.0005 0.0005</cell><cell>64 64</cell><cell>0.45 0.45</cell><cell>0.05 0.05</cell><cell>20 20</cell></row><row><cell></cell><cell>Reddit</cell><cell>3 5</cell><cell>256 256</cell><cell>--</cell><cell>0.0001 0.0001</cell><cell>128 128</cell><cell>0.20 0.20</cell><cell>0.15 0.15</cell><cell>20 20</cell></row><row><cell>SHADOW-SAGE</cell><cell>Yelp</cell><cell>3 5</cell><cell>256 256</cell><cell>--</cell><cell>0.0005 0.0005</cell><cell>16 16</cell><cell>0.10 0.10</cell><cell>0.00 0.00</cell><cell>20 20</cell></row><row><cell></cell><cell>ogbn-arxiv</cell><cell>3 5</cell><cell>256 256</cell><cell>--</cell><cell>0.00005 0.00005</cell><cell>16 16</cell><cell>0.25 0.25</cell><cell>0.15 0.15</cell><cell>20 20</cell></row><row><cell></cell><cell>ogbn-products</cell><cell>3 5</cell><cell>256 256</cell><cell>--</cell><cell>0.002 0.002</cell><cell>128 128</cell><cell>0.40 0.40</cell><cell>0.05 0.05</cell><cell>20 20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Training configuration of SHADOW-GNN forTable 2and 3 (PPR EXTRACT) SGC are trained with the same hyperparameters (i.e., learning rate equals 0.001 and dropout equals 0.1, across all datasets). SHADOW-SGC uses the same EXTRACT as the SHADOW-GCN model inTable 1. In the legend of</figDesc><table><row><cell>Dataset</cell><cell>Arch.</cell><cell cols="8">Layers Dim. Pooling Learning Rate Dropout DropEdge Budget p Threshold ?</cell></row><row><cell>ogbn-papers100M</cell><cell>SHADOW-GAT 200 SHADOW-GAT 400</cell><cell>5 3</cell><cell>800 800</cell><cell>max max</cell><cell>0.0002 0.0002</cell><cell>0.30 0.35</cell><cell>0.10 0.10</cell><cell>200 400</cell><cell>0.002 0.002</cell></row><row><cell>ogbl-collab</cell><cell>SHADOW-SAGE</cell><cell>5</cell><cell>256</cell><cell>sort</cell><cell>0.00002</cell><cell>0.25</cell><cell>0.10</cell><cell>200</cell><cell>0.002</cell></row><row><cell cols="2">SGC and SHADOW-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>Memory consumption of the ogbn-papers100M leaderboard methods Method CPU RAM GPU memory</figDesc><table><row><cell></cell><cell cols="3">GraphSAGE+incep</cell><cell>80GB</cell><cell></cell><cell>16GB</cell></row><row><cell></cell><cell>SIGN-XL</cell><cell></cell><cell></cell><cell>&gt;682GB</cell><cell></cell><cell>4GB</cell></row><row><cell></cell><cell>SGC</cell><cell></cell><cell></cell><cell>&gt;137GB</cell><cell></cell><cell>4GB</cell></row><row><cell></cell><cell cols="2">SHADOW-GAT</cell><cell></cell><cell>80GB</cell><cell></cell><cell>4GB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ogbn-products</cell><cell></cell><cell cols="2">ogbn-papers100M</cell></row><row><cell>Execution speedup (?)</cell><cell>1 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell></row><row><cell></cell><cell cols="6">GPU (RTX 3090) memory consumption (GB)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>Test accuracy on other architectures (PPR EXTRACT) 4945?0.0070 0.5317?0.0027 0.9649?0.0010 0.9682?0.0003 0.7130?0.0026 0.7201?0.0017 JK<ref type="bibr" target="#b4">(5)</ref> 0.4940?0.0083 0.5328?0.0026 0.9640?0.0013 0.9685?0.0006 0.7166?0.0053 0.7226?0.0024 GIN (3) 0.5132?0.0031 0.5228?0.0028 0.9345?0.0034 0.9578?0.0006 0.7087?0.0016 0.7173?0.0029 GIN (5) 0.5004?0.0067 0.5255?0.0023 0.7550?0.0039 0.9552?0.0007 0.6937?0.0062 0.7140?0.0027</figDesc><table><row><cell></cell><cell>Flickr</cell><cell></cell><cell>Reddit</cell><cell></cell><cell cols="2">ogbn-arxiv</cell></row><row><cell></cell><cell>Normal</cell><cell>SHADOW</cell><cell>Normal</cell><cell>SHADOW</cell><cell>Normal</cell><cell>SHADOW</cell></row><row><cell>JK (3)</cell><cell>0.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>Effect of subgraph pooling on 5-layer SHADOW-SAGE</figDesc><table><row><cell>Dataset</cell><cell>None</cell><cell>Mean</cell><cell>Max</cell><cell>Sort</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc>SHADOW-GCN test accuracy L</figDesc><table><row><cell cols="2">? EXTRACT</cell><cell>Flickr</cell><cell>ogbn-products</cell></row><row><cell cols="2">3 PPR</cell><cell>0.5257?0.0021</cell><cell>0.7773?0.0032</cell></row><row><cell></cell><cell>2-hop</cell><cell>0.5210?0.0023</cell><cell>0.7794?0.0039</cell></row><row><cell>5</cell><cell>PPR</cell><cell>0.5273?0.0020</cell><cell>0.7836?0.0034</cell></row><row><cell></cell><cell cols="2">Ensemble 0.5304?0.0017</cell><cell>0.7858?0.0021</cell></row><row><cell cols="2">7 PPR</cell><cell>0.5225?0.0023</cell><cell>0.7850?0.0044</cell></row><row><cell cols="2">F.6 Ensemble and Deeper Models</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Unlike other PPR-based models[23,<ref type="bibr" target="#b5">6]</ref> which rewire the graph by treating top PPR nodes as direct neighbors, our PPR neighborhood preserves the original multi-hop topology by returning node-induced subgraph.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For L-hop EXTRACT, we define the hyperparameter space as:</p><p>The hyperparameters to reproduce the <ref type="table">Table 1 SHADOW-GNN results are listed in Tables 8 and 9</ref>. The hyperparameters to reproduce the <ref type="table">Table 2</ref> and 3 SHADOW-GNN results are listed in <ref type="table">Table 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Setup of the SGC Experiments</head><p>Following <ref type="bibr" target="#b26">[47]</ref>, we compute the SGC model as Y = softmax A K XW where A = D ? 1 2 A D ? 1 2 and A = I + A. Matrix W is the only learnable parameter. K is the power on the adjacency matrix and we vary it as K ? {1, 3, 5, 10, 20, 30, 40} in the <ref type="figure">Figure 3</ref> experiments. For SHADOW-SGC, we compute the embedding for target v as y v = softmax A K</p><p>[v] X [v] W v,:</p><p>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00067</idno>
		<title level="m">Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Local graph partitioning using pagerank vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">47th Annual IEEE Symposium on Foundations of Computer Science (FOCS&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="475" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral clustering with graph neural networks for graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="874" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Experiment tracking with weights and biases, 2020. Software available from wandb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Biewald</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedek</forename><surname>R?zemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2464" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FastGCN: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clustergcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;19</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Minimal variance sampling with provable guarantees for fast training of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rana</forename><surname>Forsati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmut</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1393" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<title level="m">SIGN: Scalable inception graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Representation learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
	</analytic>
	<monogr>
		<title level="m">Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bayesian graph neural networks with adaptive connection sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahin</forename><surname>Boluki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PinnerSage: Multi-modal user embedding framework for recommendations at Pinterest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chantat</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimating node importance in knowledge graphs using graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namyong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><forename type="middle">Luna</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="596" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DropEdge: Towards deep Graph Convolutional Networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A deep learning approach to antibiotic discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><forename type="middle">M</forename><surname>Cubillos-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donghia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Craig R Macnair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zohar</forename><surname>Carfrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bloom-Ackerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="688" to="702" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep graph library: A graph-centric, highly-performant package for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AM-GCN: Adaptive multi-channel graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1243" to="1253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<title level="m">Simplifying graph convolutional networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Ichi Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">GNNExplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Design space for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17009" to="17021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rajgopal Kannan, and Viktor Prasanna. Accurate, efficient and scalable graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph sampling based inductive learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning based recommender system: A survey and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Efficient probabilistic logic reasoning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PairNorm: Tackling oversmoothing in gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Layerdependent importance sampling for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
