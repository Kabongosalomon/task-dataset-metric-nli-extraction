<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Chitta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Prakash</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Jaeger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Renz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
						</author>
						<title level="a" type="main">TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Autonomous Driving</term>
					<term>Imitation Learning</term>
					<term>Sensor Fusion</term>
					<term>Transformers</term>
					<term>Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How should we integrate representations from complementary sensors for autonomous driving? Geometry-based fusion has shown promise for perception (e.g. object detection, motion forecasting). However, in the context of end-to-end driving, we find that imitation learning based on existing sensor fusion methods underperforms in complex driving scenarios with a high density of dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate image and LiDAR representations using self-attention. Our approach uses transformer modules at multiple resolutions to fuse perspective view and bird's eye view feature maps. We experimentally validate its efficacy on a challenging new benchmark with long routes and dense traffic, as well as the official leaderboard of the CARLA urban driving simulator. At the time of submission, TransFuser outperforms all prior work on the CARLA leaderboard in terms of driving score by a large margin. Compared to geometry-based fusion, TransFuser reduces the average collisions per kilometer by 48%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>L IDAR sensors provide accurate 3D information for autonomous vehicles. While LiDAR-based methods have recently shown impressive results for end-to-end driving <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>, they are evaluated in settings that assume access to privileged information not available through the LiDAR. This includes test-time access to HD maps and ground truth traffic light states. In practice, the information missing in the LiDAR must be recovered from other sensors on the vehicle, such as RGB cameras <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b12">[13]</ref>.</p><p>This raises important questions: Can we integrate representations from these two modalities to exploit their complementary advantages for autonomous driving? To what extent should we process the different modalities independently, and what kind of fusion mechanism should we employ for maximum performance gain? Prior works in the field of sensor fusion have mostly focused on the perception aspect of driving, e.g. 2D and 3D object detection <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b22">[23]</ref>, motion forecasting <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b31">[31]</ref>, and depth estimation <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref>. These methods focus on learning a state representation that captures the geometric and semantic information of the 3D scene. They operate primarily based on geometric feature projections between the image space and different LiDAR projection spaces, e.g. Bird's Eye View (BEV) <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b21">[22]</ref> and Range View (RV) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref>. Information is typically aggregated from a local neighborhood around each feature in the projected 2D or 3D space.</p><p>We observe that the locality assumption in these architecture designs hampers performance in complex urban scenarios ( <ref type="table" target="#tab_4">Table 1)</ref>. For example, when handling traffic at ? K. Chitta, B. Jaeger, Z. <ref type="bibr">Yu</ref>  Consider an intersection with oncoming traffic from the left. To safely navigate the intersection, the agent (green) must capture the global context of the scene involving the interaction between the traffic light (yellow) and the crossing traffic (red). Our TransFuser model integrates geometric and semantic information across multiple modalities via attention mechanisms to capture global context, leading to safe driving behavior in CARLA.</p><p>intersections with multiple lanes, the ego-vehicle needs to account for interactions between nearby dynamic agents and traffic lights that are farther away. While deep convolutional networks can be used to capture global context within a single modality, it is non-trivial to extend them to multiple modalities or model interactions between pairs of features. To overcome these limitations, we use the attention mechanism of transformers <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b37">[37]</ref> to tightly integrate global contextual reasoning about the 3D scene directly into the feature extraction layers of different modalities. We con-sider image and LiDAR inputs since they are complementary to each other, and focus on integrating representations between these modalities <ref type="figure" target="#fig_0">(Fig. 1</ref>). The inputs are processed by two independent convolutional encoder branches, which are interconnected using transformers <ref type="figure">(Fig. 2)</ref>. We call the resulting model TransFuser and integrate it into an autoregressive waypoint prediction framework designed for end-to-end driving.</p><p>To show the advantages of our approach, we conduct a comprehensive study using the CARLA driving simulator <ref type="bibr" target="#b4">[5]</ref>. We consider a more challenging evaluation setting than existing closed-loop driving benchmarks (e.g. NoCrash benchmark <ref type="bibr" target="#b38">[38]</ref>, NEAT routes <ref type="bibr" target="#b39">[39]</ref>) based on the new CARLA version 0.9.10 leaderboard <ref type="bibr" target="#b40">[40]</ref>. Our proposed Longest6 benchmark involves ?1.5km long routes, increased traffic density, and challenging pre-crash traffic scenarios. To tackle these challenges, we incorporate auxiliary supervision signals in a multi-task learning setup to train Trans-Fuser and several strong baselines. On both the proposed benchmark and the secret routes of the official CARLA leaderboard, TransFuser achieves a significantly higher driving score than prior work.</p><p>Our contributions can be summarized as follows:</p><p>? We design a new evaluation setting in CARLA which demonstrates that imitation learning policies based on existing sensor fusion approaches are unable to handle challenging scenarios with dense traffic. <ref type="bibr">?</ref> We propose a novel multi-modal fusion transformer (TransFuser) to incorporate global context and pairwise interactions into the feature extraction layers of different input modalities. <ref type="bibr">?</ref> We conduct a detailed empirical analysis demonstrating state-of-the-art driving performance with TransFuser on both the proposed evaluation setting and the official CARLA leaderboard. Our analysis provides insights and explores the current limitations of end-to-end driving models.</p><p>This journal paper is an extension of a conference paper published at CVPR 2021 <ref type="bibr" target="#b41">[41]</ref>: we enhance the TransFuser model from <ref type="bibr" target="#b41">[41]</ref> to obtain state-of-the-art performance by incorporating (1) an improved expert demonstrator for data collection, (2) a new sensor configuration with an increased field of view through multiple cameras, (3) an improved vision backbone architecture, and (4) an updated training procedure involving multi-task learning. We also provide a new image-only baseline, Latent TransFuser, which significantly outperforms prevalent baselines used for CARLA. Our updated code, dataset, and trained models are available at https://github.com/autonomousvision/transfuser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Multi-Modal Autonomous Driving: Recent multi-modal methods for end-to-end driving <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b42">[42]</ref>- <ref type="bibr" target="#b45">[45]</ref> have shown that complementing RGB images with depth and semantics has the potential to improve driving performance. Xiao et al. <ref type="bibr" target="#b42">[42]</ref> explore RGBD input from the perspective of early, mid and late fusion of camera and depth modalities and observe significant gains. Behl et al. <ref type="bibr" target="#b43">[43]</ref> and Zhou et al. <ref type="bibr" target="#b44">[44]</ref> demonstrate the effectiveness of semantics and depth as explicit intermediate representations for driving. Natan et al. <ref type="bibr" target="#b45">[45]</ref> combine 2D semantics and depth into a semantic point cloud for end-to-end driving. In this work, we focus on image and LiDAR inputs since they are complementary to each other in terms of representing the scene and are readily available in autonomous driving systems. In this respect, Sobh et al. <ref type="bibr" target="#b35">[35]</ref> exploit a late fusion architecture for LiDAR and image modalities where each input is encoded in a separate stream from which the final feature vectors are concatenated together. The concurrent work of Chen et al. <ref type="bibr" target="#b46">[46]</ref> performs sensor fusion via PointPainting, which concatenates semantic class information extracted from the RGB image to the LiDAR point cloud <ref type="bibr" target="#b47">[47]</ref>. We observe that the late fusion mechanism of Sobh et al. <ref type="bibr" target="#b35">[35]</ref> suffers from high infraction rates and the PointPainting-based system has a reduced route completion percentage in comparison to several baselines <ref type="table" target="#tab_4">(Table 1)</ref>. To mitigate these limitations, we propose a multi-modal fusion transformer (TransFuser) that is effective in integrating information from different modalities at multiple stages during feature encoding using attention. TransFuser helps to capture the global context of the 3D scene which requires fusing information from distinct spatial locations across sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensor Fusion Methods for Object Detection and Motion</head><p>Forecasting: Most sensor fusion works consider perception tasks, e.g. object detection <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b47">[47]</ref>- <ref type="bibr" target="#b60">[60]</ref> and motion forecasting <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b61">[61]</ref>, <ref type="bibr" target="#b62">[62]</ref>. They operate on multi-view LiDAR, e.g. Bird's Eye View (BEV) and Range View (RV), or complement the camera input with depth information from LiDAR. This is typically achieved by projecting LiDAR features into the image space or projecting image features into the BEV or RV space. The closest approach to ours is ContFuse <ref type="bibr" target="#b19">[20]</ref> which performs multi-scale dense feature fusion between image and LiDAR BEV features. For each pixel in the LiDAR BEV representation, it computes the nearest neighbors in a local neighborhood in 3D space, projects these neighboring points into the image space to obtain the corresponding image features, aggregates these features using continuous convolutions, and combines them with the LiDAR BEV features. Concurrent to our work, EPNET++ <ref type="bibr" target="#b63">[63]</ref> and CAT-Det <ref type="bibr" target="#b64">[64]</ref> also employ multi-scale bidirectional fusion between image and LiDAR point clouds using attention to learn enhanced feature representations for 3D object detection. Other projection-based fusion methods follow a similar trend and aggregate information from a local neighborhood in 2D or 3D space. However, the state representation learned by these methods is insufficient since they do not capture the global context of the 3D scene, which is important for safe maneuvers in dense traffic. To demonstrate this, we implement a multi-scale geometrybased fusion mechanism, inspired by <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, involving both image-to-LiDAR and LiDAR-to-image feature fusion for end-to-end driving and observe high infraction rates in the dense urban setting ( <ref type="table" target="#tab_4">Table 1)</ref>.</p><p>for predicting vehicle controls. Li et al. <ref type="bibr" target="#b31">[31]</ref> utilize attention to capture temporal and spatial dependencies between actors by incorporating a transformer module into a recurrent neural network. SA-NMP <ref type="bibr" target="#b75">[75]</ref> learns an attention mask over features extracted from a 2D CNN, operating on LiDAR BEV projections and HD maps, to focus on dynamic agents for safe motion planning. Chen et al. <ref type="bibr" target="#b65">[65]</ref> utilize spatial and temporal attention in a hierarchical deep reinforcement learning framework to focus on the surrounding vehicles for lane changing in the TORCS simulator. PYVA <ref type="bibr" target="#b80">[80]</ref> uses attention to perform image translation from the perspective view to the BEV. This idea has also been adopted by several concurrent papers on BEV semantic segmentation from image inputs <ref type="bibr" target="#b81">[81]</ref>- <ref type="bibr" target="#b85">[85]</ref>. NEAT <ref type="bibr" target="#b39">[39]</ref> uses intermediate attention maps to iteratively compress high dimensional 2D image features into a compact BEV representation for driving. Compared to NEAT, our attention mechanism is simpler since it does not require iterative refinement of the attention at test-time. Unlike NEAT, we also apply our attention mechanism at multiple feature resolutions, enabling sensor fusion for both shallow and deep features in the network. Furthermore, none of the existing attention-based driving approaches consider multiple sensor modalities. Our work uses the self-attention mechanism of transformers for dense fusion of image and LiDAR features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TRANSFUSER</head><p>In this work, we propose a novel architecture for end-toend driving <ref type="figure">(Fig. 2</ref>). It has two main components: (1) a multi-modal fusion transformer for integrating information from multiple sensor modalities (image and LiDAR), and (2) an auto-regressive waypoint prediction network. The following sections detail our problem setting, input and output parameterization, and each component of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setting</head><p>We consider the task of point-to-point navigation in an urban setting <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b86">[86]</ref>, <ref type="bibr" target="#b87">[87]</ref> where the goal is to complete a given route while safely reacting to other dynamic agents and following traffic rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Imitation Learning (IL):</head><p>The goal of IL is to learn a policy ? that imitates the behavior of an expert ? * . In our setup, a policy is a mapping from inputs to waypoints that are provided to a separate low-level controller to output actions. We consider the Behavior Cloning (BC) approach of IL which is a supervised learning method. An expert policy is first rolled out in the environment to collect a dataset, D = {(X i , W i )} Z i=1 of size Z, which consists of high-dimensional observations of the environment, X , and the corresponding expert trajectory, defined by a set of 2D waypoints in BEV space, i.e., W = {w t = (x t , y t )} T t=1 . This BEV space uses the coordinate frame of the ego-vehicle. The policy, ?, is trained in a supervised manner using the collected data, D, with the loss function, L.</p><formula xml:id="formula_0">argmin ? E (X ,W)?D [L(W, ?(X ))]<label>(1)</label></formula><p>The high-dimensional observation, X , includes a front camera image input and a LiDAR point cloud from a single timestep. We use a single time-step input since prior works on IL for autonomous driving have shown that using observation histories may not lead to performance gain <ref type="bibr" target="#b88">[88]</ref>- <ref type="bibr" target="#b92">[92]</ref>. For L, we use the L 1 distance between the predicted trajectory, ?(X ), and the expert trajectory, W, as the primary loss function. Furthermore, we use several auxiliary losses used to boost performance, which are detailed in Section 3.6. We assume access to an inverse dynamics model <ref type="bibr" target="#b93">[93]</ref>, implemented as a PID Controller I, which performs the low-level control, i.e., steer, throttle, and brake, provided the future trajectory W. The actions are determined as a = I(W).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Planner:</head><p>We follow the standard protocol of CARLA 0.9.10 and assume that high-level goal locations G are provided as GPS coordinates. Note that these goal locations are sparse and can be hundreds of meters apart, as opposed to the local waypoints predicted by the policy ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Input and Output Parameterization</head><p>Input Representation: Following previous LiDAR-based driving approaches <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b86">[86]</ref>, we convert the LiDAR point cloud into a 2-bin histogram over a 2D BEV grid with a fixed resolution. We consider the points within 32m in front of the ego-vehicle and 16m to each of the sides, thereby encompassing a BEV grid of 32m ? 32m. We divide the grid into blocks of 0.125m ? 0.125m which results in a resolution of 256 ? 256 pixels. For the histogram, we discretize the height dimension into 2 bins representing the points on/below and above the ground plane. We also rasterize the 2D goal location in the same 256 ? 256 BEV space as the LiDAR point cloud and concatenate this channel to the 2 histogram bins. This results in a three-channel pseudo-image of size 256 ? 256 pixels. We represent the goal location in the BEV as this correlates better with the waypoint predictions compared to the perspective image domain <ref type="bibr" target="#b87">[87]</ref>. For the RGB input, we use three cameras (facing forward, 60 ? left and 60 ? right). Each camera has a horizontal FOV of 120 ? . We extract the images at a resolution of 960 ? 480 pixels, which we crop to 320 ? 160 to remove radial distortion at the edges. These three undistorted images are composed into a single image input to the encoder, which has a resolution of 704 ? 160 pixels and 132 ? FOV. We find that this FOV is sufficient to observe both near and far traffic lights in all public towns of CARLA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Representation:</head><p>We predict the future trajectory W of the ego-vehicle in BEV space, centered at the current coordinate frame of the ego-vehicle. The trajectory is represented by a sequence of 2D waypoints, {w t = (x t , y t )} T t=1 . We use T = 4, which is the default number of waypoints required by our inverse dynamics model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Modal Fusion Transformer</head><p>Our key idea is to exploit the self-attention mechanism of transformers <ref type="bibr" target="#b36">[36]</ref> to incorporate the global context for image and LiDAR modalities, given their complementary nature. The transformer architecture takes as input a sequence consisting of discrete tokens, each represented by a feature vector. The feature vector is supplemented by a positional encoding to incorporate spatial inductive biases.</p><p>Formally, we denote the input sequence as F in ? R N ?D f , where N is the number of tokens in the sequence,  <ref type="figure">Fig. 2</ref>: Architecture. We consider RGB image and LiDAR BEV representations (Section 3.2) as inputs to our multi-modal fusion transformer (TransFuser) which uses several transformer modules for the fusion of intermediate feature maps between both modalities. This fusion is applied at multiple resolutions throughout the feature extractor, resulting in a 512-dimensional feature vector output from both the image and LiDAR BEV stream, which are combined via element-wise summation. This 512-dimensional feature vector constitutes a compact representation of the environment that encodes the global context of the 3D scene. It is then processed with an MLP before passing it to an auto-regressive waypoint prediction network. We use a single layer GRU followed by a linear layer that takes in the hidden state and predicts the differential ego-vehicle waypoints {?w t } T t=1 , represented in the ego-vehicle's current coordinate frame.</p><p>and each token is represented by a feature vector of dimensionality D f . The transformer uses linear projections for computing a set of queries, keys, and values (Q, K, and V),</p><formula xml:id="formula_1">Q = F in M q , K = F in M k , V = F in M v (2) where M q ? R D f ?Dq , M k ? R D f ?D k and M v ? R D f ?Dv are weight matrices.</formula><p>It uses the scaled dot products between Q and K to compute the attention weights and then aggregates the values for each query,</p><formula xml:id="formula_2">A = softmax QK T ? D k V<label>(3)</label></formula><p>Finally, the transformer uses a non-linear transformation to calculate the output features, F out which are of the same shape as the input features, F in .</p><formula xml:id="formula_3">F out = MLP(A) + F in<label>(4)</label></formula><p>The transformer applies the attention mechanism multiple times throughout the architecture, resulting in L attention layers. Each layer in a standard transformer has multiple parallel attention 'heads', which involve generating several Q, K and V values per F in for Eq. (2) and concatenating the resulting values of A from Eq. (3). Unlike the token input structures in NLP, we operate on grid structured feature maps. Similar to prior works on the application of transformers to images <ref type="bibr" target="#b94">[94]</ref>- <ref type="bibr" target="#b97">[97]</ref>, we consider the intermediate feature maps of each modality to be a set rather than a spatial grid and treat each element of the set as a token. The convolutional feature extractors for the image and LiDAR BEV inputs encode different aspects of the scene at different layers. Therefore, we fuse these features at multiple scales ( <ref type="figure">Fig. 2</ref>) throughout the encoder.</p><p>Let the intermediate grid structured feature map of a single modality indexed by s be a 3D tensor of dimension H s ? W s ? C. For S different modalities, these fea-tures are stacked together to form a sequence of dimension S s=1 (H s * W s ) ? C. We add a learnable positional embedding, which is a trainable parameter of the same dimension as the stacked sequence, so that the network can infer spatial dependencies between different tokens at train time. The input sequence and positional embedding are combined using element-wise summation to form a tensor of dimension S s=1 (H s * W s ) ? C. As shown in <ref type="figure">Fig. 2</ref>, this tensor is fed as input to the transformer, which produces an output of the same dimension. We have omitted the positional embedding and velocity embedding inputs in <ref type="figure">Fig. 2</ref> for clarity. The output is then reshaped into S feature maps of dimension H s ?W s ?C each and fed back into each of the individual modality branches using an element-wise summation with the existing feature maps. The mechanism described above constitutes feature fusion at a single scale. This fusion is applied multiple times throughout the feature extractors of the image and BEV branches at different resolutions ( <ref type="figure">Fig. 2</ref>). However, processing feature maps at high spatial resolutions is computationally expensive. Therefore, we downsample higher resolution feature maps from the early encoder blocks using average pooling to the same resolution as the final feature map before passing them as inputs to the transformer. We upsample the output to the original resolution using bilinear interpolation before element-wise summation with the existing feature maps.</p><p>After carrying out dense feature fusion at multiple resolutions ( <ref type="figure">Fig. 2)</ref>, we obtain a feature map of dimensions 22 ? 5 ? C from the feature extractors of the image branch, and 8 ? 8 ? C from the BEV branch. Where C is the number of channels at the current resolution in the feature extractor and lies in {72, 216, 576, 1512}. These feature maps are reduced to a dimension of 512 by average pooling, followed by a fully-connected layer of 512 units. The feature vector of dimension 512 from both the image and the LiDAR BEV streams are then combined via element-wise summation.</p><p>This 512-dimensional feature vector constitutes a compact representation of the environment that encodes the global context of the 3D scene. This is then fed to the waypoint prediction network, which we describe next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Waypoint Prediction Network</head><p>As shown in <ref type="figure">Fig. 2</ref>, we pass the 512-dimensional feature vector through an MLP (comprising 2 hidden layers with 256 and 128 units) to reduce its dimensionality to 64 for computational efficiency before passing it to the auto-regressive waypoint network implemented using GRUs <ref type="bibr" target="#b98">[98]</ref>. We initialize the hidden state of the GRU with the 64-dimensional feature vector. The update gate of the GRU controls the flow of information encoded in the hidden state to the output and the next time-step. It also takes in the current position and the goal location (Section 3.1) as input, which allows the network to focus on the relevant context in the hidden state for predicting the next waypoint. We provide the GPS coordinates of the goal location (registered to the ego-vehicle coordinate frame) as input to the GRU in addition to the encoder since it can more directly influence the waypoint predictions. Following <ref type="bibr" target="#b1">[2]</ref>, we use a single layer GRU followed by a linear layer which takes in the hidden state and predicts the differential ego-vehicle waypoints {?w t } T t=1 for T = 4 future time-steps in the ego-vehicle current coordinate frame. Therefore, the predicted future waypoints are given by {w t = w t?1 + ?w t } T t=1 . The input to the first GRU unit is given as (0,0) since the BEV space is centered at the ego-vehicle's position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Controller</head><p>We use two PID controllers for lateral and longitudinal control to obtain steer, throttle, and brake values from the predicted waypoints, {w t } T t=1 . The longitudinal controller takes in the magnitude of a weighted average of the vectors between waypoints of consecutive time steps, whereas the lateral controller takes in their orientation. For the PID controllers, we use the same configuration as in the authorprovided codebase of <ref type="bibr" target="#b87">[87]</ref>. Additional details regarding the controllers can be found in the supplementary material.</p><p>Creeping: If the car has not moved for a long duration (55 seconds, chosen to be higher than the expected wait time at a red light), we make it creep forward by setting the target speed of the PID controller to 4 m/s for a short time (1.5 seconds). This creeping behavior is used to recover from the inertia problem observed in IL for autonomous driving <ref type="bibr" target="#b38">[38]</ref>. When a vehicle is still, the probability that it continues to stay in place (e.g. in dense traffic) is very high in the training data. This can lead to the trained agent never starting to drive again after having stopped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Safety Heuristic:</head><p>The creeping behavior alone would be unsafe, e.g. in situations where the agent is stuck in traffic where creeping forward could lead to a collision. To prevent this, we implement a safety check that overwrites the creeping behavior if there are any LiDAR hits in a small rectangular area in front of the car. While this heuristic is essential during creeping, it can also be applied during regular driving to enhance the safety <ref type="bibr" target="#b99">[99]</ref>. We study the impact of applying a global safety heuristic during both creeping and regular driving in Section 4.11.   <ref type="formula" target="#formula_4">5)</ref>), we incorporate four auxiliary tasks: depth prediction and semantic segmentation from the image branch; HD map prediction and vehicle detection from the BEV branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Loss Functions</head><p>Following <ref type="bibr" target="#b87">[87]</ref>, we train the network using an L 1 loss between the predicted waypoints and the ground truth waypoints (from the expert), registered to the current coordinate frame. Let w gt t represent the ground truth waypoint for time-step t, then the loss function is given by:</p><formula xml:id="formula_4">L = T t=1 ||w t ? w gt t || 1<label>(5)</label></formula><p>Note that the ground truth waypoints {w gt t } which are available only at training time are different from the sparse goal locations G provided at both training and test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auxiliary Tasks:</head><p>To account for the complex spatial and temporal scene structure encountered in autonomous driving, the training objectives used in IL-based driving agents have evolved by incorporating auxiliary tasks. Training signals aiming to reconstruct the scene have become common, such as image auto-encoding <ref type="bibr" target="#b100">[100]</ref>, 2D semantic segmentation <ref type="bibr" target="#b101">[101]</ref>, Bird's Eye View (BEV) semantic segmentation <ref type="bibr" target="#b102">[102]</ref>, 2D semantic prediction <ref type="bibr" target="#b103">[103]</ref>, and BEV semantic prediction <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b39">[39]</ref>. Performing auxiliary tasks has been shown to lead to more interpretable and robust models <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b39">[39]</ref>. In this work, we consider four auxiliary tasks: depth prediction, semantic segmentation, HD map prediction, and vehicle detection ( <ref type="figure" target="#fig_1">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Depth and Semantics:</head><p>Combining 2D depth estimation and 2D semantic segmentation as auxiliary tasks has been an effective approach for image-based end-to-end driving <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b104">[104]</ref>. We use the same decoder architecture as the AIM-MT baseline of <ref type="bibr" target="#b39">[39]</ref> to decode depth and semantics from the image branch features. The depth output is supervised with an L 1 loss, and the semantics with a cross-entropy loss. Following <ref type="bibr" target="#b39">[39]</ref>, we consider 7 semantic classes: (1) unlabeled, (2) vehicle, (3) road, (4) red light, <ref type="bibr" target="#b4">(5)</ref> pedestrian, (6) lane marking, and (7) sidewalk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HD Map:</head><p>We predict a three-channel BEV segmentation mask containing the classes road, lane marking and other. This encourages the intermediate features to encode information regarding drivable and non-drivable areas. The map uses the same coordinate frame as the LiDAR input, and is therefore obtained from the feature map of the LiDAR branch with a convolutional decoder. However, we predict a downsampled version of the HD map (64 ? 64 instead of 256 ? 256) for computational efficiency. The HD map prediction task uses a cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bounding Boxes:</head><p>We locate other vehicles in the scene via keypoint estimation with a CenterNet decoder <ref type="bibr" target="#b105">[105]</ref>. Specifically, we predict a position mapP ? [0, 1] 64?64 from the BEV features using a convolutional decoder. Similar to the HD map prediction task, the 256 ? 256 input is mapped to smaller 64 ? 64 predictions for vehicle detection. The 2D target label for this task is rendered with a Gaussian kernel at each object center of our training dataset. While the orientation is a single scalar value, directly regressing this value is challenging, as observed in existing 3D detectors <ref type="bibr" target="#b105">[105]</ref>- <ref type="bibr" target="#b108">[107]</ref>. Therefore, our CenterNet implementation takes a two-stage approach of predicting an initial coarse orientation followed by a fine offset angle. To predict the coarse orientation, we discretize the relative yaw of each ground truth vehicle into 12 bins of size 30 ? , and predict this class via a 12-channel classification label at each pixel, O ? [0, 1] 64?64?12 , as in <ref type="bibr" target="#b108">[107]</ref>. Finally, we predict a regression mapR ? R 64?64?5 . This regression map holds three regression targets: vehicle size (? R 2 ), position offset (? R 2 ) and orientation offset (? R). The position offset is used to make up for quantization errors introduced by predicting position maps at a lower resolution than the inputs. The orientation offset corrects the orientation discretization error <ref type="bibr" target="#b108">[107]</ref>. Note that only locations with vehicle centers are supervised for predicting? andR. The position map, orientation map and regression map use a focal loss <ref type="bibr" target="#b109">[108]</ref>, cross-entropy loss, and L 1 loss respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Latent TransFuser</head><p>CILRS <ref type="bibr" target="#b38">[38]</ref> is a widely used image-only baseline for the old CARLA version 0.8.4. It learns to directly predict vehicle controls (as opposed to waypoints) from visual features while being conditioned on a discrete navigational command (follow lane, change lane left/right, turn left/right). However, as shown in <ref type="bibr" target="#b39">[39]</ref> this approach obtains poor results on the challenging CARLA leaderboard. Despite this, recent studies involving image-based driving on CARLA have shifted from IL towards more complex Reinforcement Learning (RL) based training, while using CILRS as the primary IL baseline <ref type="bibr" target="#b110">[109]</ref>, <ref type="bibr" target="#b111">[110]</ref>. To provide a more meaningful IL baseline for future studies, we now introduce an imageonly version of our approach, called Latent TransFuser.</p><p>Latent TransFuser replaces the 2-channel LiDAR BEV histogram input to our architecture with a 2-channel positional encoding of identical dimensions (256 ? 256 ? 2). The 2D positional encoding is a grid with equally-spaced values from -1 to 1, with one channel for the left-right axis, and one for the top-down axis. Other than this change, the architecture, training procedure, and auxiliary losses remain identical to the LiDAR-based TransFuser. Additionally, our controller uses the LiDAR input for its safety heuristic while creeping (Section 3.5). For Latent TransFuser, we check for an intersection between the small rectangular safety area in front of the car and any detected object from the auxiliary CenterNet detection head instead. The creeping is disabled whenever such an intersection occurs.</p><p>The positional encoding input of Latent TransFuser acts as a proxy for the BEV LiDAR. Since the LiDAR branch is supervised to predict the HD map and bounding boxes, which are in the BEV coordinate frame, fusing image features with the latent features in this branch acts as an attentionbased projection from the perspective view to the BEV. The architecture shares similarities with existing attention-based camera to BEV projection techniques such as NEAT <ref type="bibr" target="#b39">[39]</ref> and PYVA <ref type="bibr" target="#b80">[80]</ref>. However, unlike these methods, Latent TransFuser projects features at multiple feature resolutions. We show in our experiments that Latent TransFuser is a powerful baseline, outperforming far more complex RLbased methods on the CARLA leaderboard in the imageonly input setting <ref type="table" target="#tab_7">(Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we describe our experimental setup, compare the driving performance of TransFuser against several baselines, visualize the attention maps of TransFuser and present ablation studies to highlight the importance of different components of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task</head><p>We consider the task of navigation along a set of predefined routes in a variety of areas, e.g. freeways, urban areas, and residential districts. The routes are defined by a sequence of sparse goal locations in GPS coordinates provided by a global planner. Each route consists of several scenarios, initialized at predefined positions, which test the ability of the agent to handle different kinds of adversarial situations, e.g. obstacle avoidance, unprotected turns at intersections, vehicles running red lights, and pedestrians emerging from occluded regions to cross the road at random locations. The agent needs to complete the route within a specified time limit while following traffic regulations and coping with high densities of dynamic agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Dataset</head><p>We use the CARLA <ref type="bibr" target="#b4">[5]</ref> simulator for training and testing, specifically CARLA 0.9.10 which consists of 8 publicly available towns. We use all 8 towns for training. Our dataset is collected along a set of training routes: around 2500 routes through junctions with an average length of 100m, and around 1000 routes along curved highways with an average length of 400m. For generating training data, we roll out an expert policy designed to drive using privileged information from the simulation and store data at 2FPS. The expert is a rule-based algorithm similar to the CARLA traffic manager autopilot 1 . Our training dataset contains 228k frames in total. In the following, we provide more details regarding the expert algorithm.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Expert</head><p>For generating training data, we roll out an expert policy designed to drive using privileged information from the simulator. The waypoints of the expert are the ground truth labels for the imitation loss, so it can be viewed as an automatic labeling algorithm. The ground truth labels for the auxiliary tasks are provided by the simulator. We build upon the code provided by the authors of <ref type="bibr" target="#b87">[87]</ref>. This approach is based on simple handcrafted rules. Building the expert with RL is also possible <ref type="bibr" target="#b112">[111]</ref>, <ref type="bibr" target="#b113">[112]</ref> but it is more computationally demanding and less interpretable. Our expert policy consists of an A* planner followed by 2 PID controllers (for lateral and longitudinal control). The lateral and longitudinal control tasks are treated independently.</p><p>Lateral control is done by following the path generated by the A* planner. Specifically, we minimize the angle of the car towards the next waypoint in the route, which is at least 3.5 meters away, using a PID controller. Longitudinal control is done using a version of model predictive control and differentiates between 3 target speeds. The standard target speed is 4.0 m/s. When the expert is inside an intersection, the target speed is reduced to 3.0 m/s. Lastly, in case an infraction is predicted the target speed is set to 0.0 m/s bringing the vehicle to a halt. We predict red light infractions by performing intersection tests with trigger boxes that CARLA provides. Collision infractions are predicted by forecasting the oriented bounding boxes of all traffic participants. We forecast in 50 ms discrete steps, for 4 seconds in intersections and 1 second in other areas. The forecasting is done using the pretrained kinematic bicycle model from <ref type="bibr" target="#b111">[110]</ref>. This bicycle model is a simple mathematical model that can predict the position, orientation, and speed of a car after a discrete time step, given its current position orientation speed and the applied control. We forecast all vehicles by iteratively rolling out the bicycle model using its output at time step t as the input for time step t + 1. Since we only know the control input of other traffic participants at the current time step (provided by the simulator), we assume that they will continue to apply the same control at future time steps. For the ego vehicle, we calculate its future steering and throttle by using PID controllers that try to follow the route. The ego brake control is always set to 0 because we want to answer the counterfactual question of whether there will be a collision if we do not brake. We forecast pedestrians analogously but model them as a point with velocity and acceleration. This works well because the movement patterns of pedestrians in CARLA are simple. A collision infraction is detected if there is an intersection of the ego vehicle bounding box at future time step t with the bounding box of another traffic participant at future time step t. A common failure of the action repeat forecast mechanism described above is that it does not anticipate that fast cars approaching the expert from behind will eventually slow down before colliding. To avoid false positives, we do not consider rear-end collisions by only using the front half of the vehicle as its bounding box. For the longitudinal controller, we set K p = 5.0, K i = 0.5, K d = 1.0 and for the lateral controller, we set K p = 1.25, K i = 0.75, K d = 0.3. Both controllers use a buffer of size 40 to approximate the integral term as a running average. An example of the expert performing an unprotected left turn can be seen in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Longest6 Benchmark</head><p>The CARLA simulator provides an official evaluation leaderboard consisting of 100 secret routes. However, teams using this platform are restricted to only 200 hours of evaluation time per month. A single evaluation takes over 100 hours, making the official leaderboard unsuitable for ablation studies or obtaining detailed statistics involving multiple evaluations of each model. Therefore, we propose the Longest6 Benchmark, which shares several similarities to the official leaderboard, but can be used for evaluation on local resources without computational budget restrictions.</p><p>The CARLA leaderboard repository provides a set of 76 routes as a starting point for training and evaluating agents. These routes were originally released along with the 2019 CARLA challenge. They span 6 towns and each of them is defined by a sequence of waypoints. However, there is a large imbalance in the number of routes per town, e.g. Town03 and Town04 have 20 routes each, but Town02 has only 6 routes. To balance the Longest6 driving benchmark across all available towns, we choose the 6 longest routes per town from the set of 76 routes. This results in 36 routes with an average route length of 1.5km, which is similar to the average route length of the official leaderboard (1.7km). We make three other design choices for the Longest6 benchmark, motivated by the official leaderboard. (1) During evaluation, we ensure a high density of dynamic agents by spawning vehicles at every possible spawn point permitted by the CARLA simulator. (2) Following <ref type="bibr" target="#b39">[39]</ref>, each route has a unique environmental condition obtained by combining one of 6 weather conditions (Cloudy, Wet, MidRain, WetCloudy, HardRain, SoftRain) with one of 6 daylight conditions (Night, Twilight, Dawn, Morning, Noon, Sunset).</p><p>(3) We include CARLA's adversarial scenarios in the evaluation, which are spawned at predefined positions along the route. Specifically, we include CARLA's scenarios 1, 3, 4, 7, 8, 9, 10 which are generated based on the NHTSA precrash typology 2 . Visualizations of the routes in the Longest6 benchmark are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Metrics</head><p>We report several metrics to provide a comprehensive understanding of the driving behavior of each agent.</p><p>(1) Route Completion (RC): percentage of route distance completed, R i by the agent in route i, averaged across N routes. However, if an agent drives outside the route lanes for a percentage of the route, then the RC is reduced by a multiplier (1-% off route distance).</p><formula xml:id="formula_5">RC = 1 N N i R i<label>(6)</label></formula><p>(2) Infraction Score (IS): geometric series of infraction penalty coefficients, p j for every instance of infraction j incurred by the agent during the route. Agents start with an ideal 1.0 base score, which is reduced by a penalty coefficient for every infraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IS =</head><p>Ped,Veh,Stat,Red</p><formula xml:id="formula_6">j (p j ) # infractions j<label>(7)</label></formula><p>The penalty coefficient for each infraction is pre-defined and set to 0.50 for collision with a pedestrian, 0.60 for collision with a vehicle, 0.65 for collision with static layout, and 0.7 for red light violations. The official CARLA leaderboard also mentions a penalty for stop sign violations. However, we observe that none of our submissions have any stop sign infractions. Hence, we omit this infraction from our analysis.</p><p>(3) Driving Score (DS): weighted average of the route completion with infraction multiplier P i</p><formula xml:id="formula_7">DS = 1 N N i R i P i<label>(8)</label></formula><p>(4) Infractions per km: the infractions considered are collisions with pedestrians, vehicles, and static elements, running a red light, off-road infractions, route deviations, 2. https://leaderboard.carla.org/scenarios/ timeouts, and vehicle blocked. We report the total number of infractions, normalized by the total number of km driven.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Infractions per km =</head><formula xml:id="formula_8">N i # infractions i N i k i (9)</formula><p>where k i is the driven distance (in km) for route i. The Off-road infraction is slightly different. Instead of the total number of infractions the sum of km driven off-road is used. We multiply by 100 because this metric is a percentage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Baselines</head><p>We compare our TransFuser model to several baselines. (1) WOR <ref type="bibr" target="#b111">[110]</ref>: this is a multi-stage training approach that supervises the driving task with a Q function obtained using dynamic programming. It is the current state-ofthe-art approach on the simpler NoCrash benchmark <ref type="bibr" target="#b38">[38]</ref> for CARLA version 0.9.10. We use the author-provided pretrained model for evaluating this approach. (2) Latent TransFuser: to investigate the importance of the LiDAR input, we implement an auto-regressive waypoint prediction network that has the same architecture as the TransFuser but uses a fixed positional encoding image as input instead of the BEV LiDAR, as described in Section 3.7.</p><p>(3) LAV <ref type="bibr" target="#b46">[46]</ref>: this is a concurrent approach that performs sensor fusion via PointPainting <ref type="bibr" target="#b47">[47]</ref>, which concatenates semantic class information extracted from the RGB image to the LiDAR point cloud, to train a privileged motion planner to predict trajectories of all nearby vehicles in the scene. This privileged planner is then distilled into a policy that drives from raw sensor inputs only. We use the checkpoint publicly released by the authors 3 for our experiments. We note that this published checkpoint is not the exact same model as the one used for LAV's leaderboard entry. (4) Late Fusion: we implement a version of our architecture where the image and the LiDAR features are extracted independently using the same encoders as TransFuser but without the transformers (similar to <ref type="bibr" target="#b35">[35]</ref>). The features from each branch are then fused through element-wise summation and passed to the waypoint prediction network. (5) Geometric Fusion: we implement a multi-scale geometrybased fusion method, inspired by <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, involving both image-to-LiDAR and LiDAR-to-image feature fusion. We unproject each 0.125m ? 0.125m block in our LiDAR BEV representation into 3D space, resulting in a 3D cell. We randomly select 5 points from the LiDAR point cloud lying in this 3D cell and project them into the image space. Then, we aggregate the image features of these points via elementwise summation before passing them to a 3-layer MLP. The output of the MLP is then combined with the LiDAR BEV feature of the corresponding 0.125m ? 0.125m block at multiple resolutions throughout the feature extractor. Similarly, for each image pixel, we aggregate information from the LiDAR BEV features at multiple resolutions. This baseline is equivalent to replacing the transformers in our architecture with projection-based feature fusion. We also report results for the expert used for generating our training data, which defines an upper bound for the performance on the Longest6 evaluation setting. We provide additional details regarding the baselines in the supplementary material.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Implementation Details</head><p>We use 2 sensor modalities, the front-facing cameras and LiDAR point cloud converted to a BEV representation (Section 3.2), i.e., S = 2. The camera inputs are concatenated to a single image and encoded using a RegNetY-3.2GF <ref type="bibr" target="#b114">[113]</ref> which is pretrained on ImageNet <ref type="bibr" target="#b115">[114]</ref>. We use pre-trained models from <ref type="bibr" target="#b117">[115]</ref>. The LiDAR BEV representation is encoded using another RegNetY-3.2GF <ref type="bibr" target="#b114">[113]</ref> which is trained from scratch. Similar to <ref type="bibr" target="#b87">[87]</ref>, we perform angular viewpoint augmentation for the training data, by randomly rotating the sensor inputs by ?20 ? and adjusting the waypoint labels according to this rotation. We use 1 transformer per resolution and 4 attention heads for each transformer. We select D q , D k , D v from {72, 216, 576, 1512} for the 4 transformers corresponding to the feature embedding dimension D f at each resolution. We train the models with 4 RTX 2080Ti GPUs for 41 epochs, with an initial learning rate of 10 ?4 and a batch size of 12. We reduce the learning rate by a factor of 10 after epoch 30 and 40. We evaluate the epochs 31, 33, 35, 37, 39 and 41 closed loop on the validation routes of <ref type="bibr" target="#b39">[39]</ref> for one seed and pick the epoch with the highest driving score. For all models, we use the AdamW optimizer <ref type="bibr" target="#b118">[116]</ref>, which is a variant of Adam. Weight decay is set to 0.01, and Adam beta values to the PyTorch defaults of 0.9 and 0.999.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Longest6 Benchmark Results</head><p>We begin with an analysis of driving performance on CARLA on the new Longest6 evaluation setting <ref type="table" target="#tab_4">(Table 1)</ref>. For each experimental result, the evaluation is repeated 3 times to account for the non-determinism of the CARLA simulator. Furthermore, imitation based methods typically show variation in performance when there is a change in the random initialization and data sampling due to the training seed <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b43">[43]</ref>. To account for the variance observed between different training runs, we use 3 different random seeds for each method, and report the metrics for an ensemble of these 3 training runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent TransFuser as a Strong Baseline:</head><p>In our first experiment, we examine the performance of image-based methods. From the results in <ref type="table" target="#tab_4">Table 1</ref>, we observe that WOR performs poorly on the Longest6 evaluation setting. In particular, we observe that WOR suffers from a poor RC with a much larger number of route deviations (Dev) than the remaining methods. On the other hand, we find that Latent Transfuser obtains the best RC in <ref type="table" target="#tab_4">Table 1</ref>, with zero route deviations. This is likely because Latent TransFuser uses our inverse dynamics model (PID controller) for lowlevel control and represents goal locations in the same BEV coordinate space in which waypoints are predicted. In contrast, WOR uses coarse navigational commands (e.g. follow lane, turn left/right, change lane) to inform the model regarding driver intentions, and chooses an output action from a discrete action space. This result indicates that the TransFuser architecture involving auto-regressive waypoint prediction a strong baseline for the end-to-end driving task, even in the absence of a LiDAR sensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensor Fusion Methods:</head><p>The goal of this experiment is to determine the impact of the LiDAR modality on the driving performance and compare different fusion methods. For this, we compare TransFuser to three baselines, LAV, Late Fusion (LF), Geometric Fusion (GF). LAV performs worse than TransFuser in terms of DS. The main difference arises from the 23% lower RC. Potential reasons could be worse steering as indicated by the higher off-road infractions, or false positives in the modular components which might be the reason for the higher blocked infraction. While LAV obtains a similar IS to TransFuser, upon probing further, we notice that it is only better in terms of avoiding collisions with vehicles and TransFuser performs better with respect to all other infractions. We note that in the Longest6 benchmark, there are a few routes where the vehicle is required to drive in dense traffic on multi-lane highways. TransFuser fails at lane merging in these situations, incurring a large amount of vehicle collisions (&gt;20), strongly affecting its vehicle collision metric. Surprisingly, we observe that LF and GF perform worse than the image-only Latent TransFuser baseline ( <ref type="table" target="#tab_4">Table 1</ref>). The multi-scale geometry-based fusion encoder of GF gives some improvements when compared to LF, however, both LF and GF suffer from a poor IS. We hypothesize that this occurs because they do not incorporate global contextual reasoning which is necessary to safely navigate the intersections, and focus primarily on navigation to the goal at all costs while ignoring obstacles, which leads to several infractions. In contrast, our TransFuser model obtains an absolute improvement 19.98% in terms of DS when compared to GF. It also achieves an 48.59% reduction compared to LF and 47.64% reduction compared to GF in collisions per kilometer (Ped+Veh+Stat), and an absolute improvement of over 0.2 in its IS. These results   indicate that attention is effective in incorporating the global context of the 3D scene, which allows for safer driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations:</head><p>We observe that all methods with a high RC struggle with vehicle collisions (Veh). Avoiding collisions is very challenging in our evaluation setting due to the traffic density being set to the maximum allowed density in CARLA. In particular, TransFuser has around 9? more vehicle collisions per kilometer than the expert. We observe that these collisions primarily occur during unprotected turns and lane changes as is illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runtime:</head><p>We measure the runtime of each method on a single RTX 3090 GPU by averaging over all time-steps of one evaluation route. The runtime considered includes sensor data pre-processing, model inference and PID control. The results are shown in <ref type="table" target="#tab_6">Table 2</ref>. We observe that the transformers in our architecture increase the runtime relative to the LF baseline by 17% for a single model and 28% for an ensemble of three models. However, a single TransFuser model can still be executed in real-time on this hardware. The GF baseline is slower than TransFuser despite its simpler fusion mechanism due to the extra time taken finding correspondences between the image and LiDAR tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Leaderboard Results</head><p>We submit the models from our study to the CARLA autonomous driving leaderboard which contains a secret set of 100 evaluation routes and report the results in   <ref type="table" target="#tab_4">Table 1.</ref> image-based method, GRIAD <ref type="bibr" target="#b119">[117]</ref>. GRIAD builds on top of the Reinforcement Learning (RL) method presented in <ref type="bibr" target="#b110">[109]</ref>. In this approach, an encoder is first trained to predict both the 2D semantics and specific affordances such as the scene traffic light state, and the relative position and orientation between the vehicle and lane. The encoder is then frozen and used to train a value function-based RL method using a replay buffer that is partially filled with expert demonstrations. GRIAD requires 45M samples from the CARLA simulator for training <ref type="bibr" target="#b119">[117]</ref> whereas our training dataset has only 228k frames (200? less than GRIAD  DAgger <ref type="bibr" target="#b123">[121]</ref>, <ref type="bibr" target="#b124">[122]</ref>, adversarial simulation <ref type="bibr" target="#b125">[123]</ref>- <ref type="bibr" target="#b127">[125]</ref>, RLbased fine-tuning <ref type="bibr" target="#b100">[100]</ref> and teacher-student distillation <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b87">[87]</ref>, <ref type="bibr" target="#b111">[110]</ref>, <ref type="bibr" target="#b128">[126]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10">Attention Statistics and Visualizations</head><p>Our architecture <ref type="figure">(Fig. 2)</ref> consists of 4 transformers with 4 attention layers and 4 attention heads in each transformer. In this section, we visualize the attention maps from the final attention layer for each head for each transformer. The transformer takes in 110 image feature tokens and 64 LiDAR feature tokens as input where each token corresponds to a 32?32 patch in the input modality. We consider intersection scenarios from Town03 and Town05 and examine the top-5 attention weights for the 66 tokens in the 2 nd , 3 rd and 4 th rows of the image feature map and the 24 tokens in the 4 th , 5 th and 6 th rows of the LiDAR feature map. We select these tokens since they correspond to the intersection region in the input modality and contain traffic lights and vehicles.</p><p>We compute statistics on cross-modal attention for image and LiDAR feature tokens. Specifically, we report the % of tokens for which at least one of the top-5 attended tokens belong to the other modality for each head of the 4 transformers (T1, T2, T3, T4) in <ref type="table" target="#tab_10">Table 4</ref>. We observe that the earlier transformers have negligible LiDAR to image attention compared to later transformers in which nearly all the LiDAR tokens aggregate information from the image features. Furthermore, different heads of each transformer also show distinctive behavior, e.g. head 3 of T2 has significantly less cross-attention for image tokens than other heads, head 2 of T3 has very little cross-attention whereas head 4 has significantly higher cross-attention for image  tokens compared to other heads. Overall, T4 exhibits extensive cross-attention for both image and LiDAR tokens, which indicates that TransFuser is effective in aggregating information between the two modalities. We show four cross-attention visualizations for T4 in <ref type="figure">Fig. 6</ref>. We observe a common trend in attention maps: TransFuser attends to areas near vehicles and traffic lights at intersections, albeit at a slightly different location in the image and LiDAR feature maps. Additional visualizations for all the transformers are provided in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.11">Global Safety Heuristic</head><p>The primary motivation of the safety heuristic described in Section 3.5 is to prevent collisions during the applied creeping behavior. Therefore, in <ref type="table" target="#tab_4">Tables 1 and 3</ref>, we apply the safety heuristic during creeping only. However, rule-based fallback systems have been shown to improve the safety of IL models <ref type="bibr" target="#b99">[99]</ref>. In this experiment, we investigate the impact of applying the safety heuristic described in Section 3.5 globally, i.e. during both creeping and regular driving. We show results on both the Longest6 benchmark and the CARLA leaderboard. To reduce the computational overhead of this analysis, we evaluate a single model instead of an ensemble of 3 different training runs, which were used in <ref type="table" target="#tab_4">Tables 1 and 3</ref>. However, for clarity, we also report the scores of the ensemble. Additionally, the results shown for Latent TransFuser are from preliminary experiments where we include a LiDAR sensor and use the LiDAR-based safety heuristic instead of the CenterNet based intersection check described in Section 3.7, leading to minor differences when compared to the numbers from <ref type="table" target="#tab_4">Tables 1 and 3</ref>.</p><p>The results are shown in <ref type="table" target="#tab_12">Table 5</ref>. For Latent TransFuser, we observe that the global safety heuristic improves the DS by 8 points on the Longest6 benchmark and 5 points on the leaderboard compared to it being applied during creeping only. In particular, this is due to a large improvement in the IS (e.g. from 0.51 to 0.72 on the leaderboard). Interestingly, we observe a different trend for TransFuser. The global safety heuristic improves the DS by 7 points on the Longest6 benchmark where we tuned the hyper-parameters (i.e., size of the rectangular safety box). However, it leads to a drop of nearly 10 points in DS on the leaderboard. The global safety heuristic leads to reduced route completion for both methods on the CARLA leaderboard. This indicates that the heuristic works well on observed maps, but does not generalize to unknown and potentially unseen road layouts. For TransFuser, which already had a higher infraction score than Latent TransFuser, the improvement in IS does not  make up for the reduction in RC leading to an overall reduction in DS through the safety heuristic.</p><p>When comparing the results of a single model and the corresponding ensemble, we find that ensembling improves the DS for both Latent TransFuser and TransFuser on the leaderboard, in particular for TransFuser which improves by more than 10 points. On the Longest6 routes, the ensembling has a lower impact of 5 points for the TransFuser and even a reduction in performance for Latent TransFuser. The single models reported in <ref type="table" target="#tab_12">Table 5</ref> are the first of three training seeds ( <ref type="table" target="#tab_14">Table 6</ref>). Ensembling might have a larger positive impact on TransFuser because the models had a larger training variance, which we discuss in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Seed Variance:</head><p>We show the impact of training and evaluation seed variance in <ref type="table" target="#tab_14">Table 6</ref>. We train each baseline 3 times and evaluate each model 3 times on the Longest6 routes with the global safety heuristic enabled. We observe that the best achieved score can differ from the worst score by 10-15 points for an individual model, leading to extremely large variance. In particular, for the first seed of TransFuser, the DS ranges from 44.15 to 59.45. For Geometric Fusion, the average DS differs by 12 points between the worst and best training seed. This amount of variance is problematic when trying to analyze or compare different approaches. We would like to emphasize that the variance reported in <ref type="table" target="#tab_14">Table 6</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.12">Ablation Studies</head><p>We now analyze several design choices for TransFuser in a series of ablation studies on the Longest6 benchmark. Since the global safety heuristic leads to consistent and significant improvements for TransFuser on the Longest6 routes <ref type="table" target="#tab_12">(Table 5)</ref>, we use this setting for the ablation studies. The evaluation is repeated 3 times for each experiment, however, we use a single training run for these results instead of an ensemble of 3 different training runs as in <ref type="table" target="#tab_4">Table 1</ref>. To further reduce the computational overhead, we always evaluate epoch 31, as we have observed in preliminary experiments that it is usually close to the best epoch in performance. For the default configuration, we have 3 available training runs. We report the best and worst training seed to account for the randomness due to training. Ablations lying within this interval likely do not have a large impact.</p><p>Auxiliary Tasks: As described in Section 3.6, we consider 4 auxiliary tasks in this work. In <ref type="table" target="#tab_16">Table 7</ref> we show the performance of TransFuser when all these auxiliary losses are removed, as well as the impact of removing each loss independently. We observe that with no auxiliary tasks, there is a steep drop in RC from 92.28 to 78.17. Removing only a single auxiliary task does not have a large impact. All results lie within the range between the best and worst seed of the default configuration in terms of driving score. In <ref type="figure">Fig. 7</ref>, we visualize the predictions made by TransFuser when trained with all 4 auxiliary losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture:</head><p>In <ref type="table" target="#tab_18">Table 8</ref>, we analyze the impact of varying the TransFuser encoder architecture. We study the importance of fusion in both directions by selectively removing the residual output connections from the fusion transformers to the convolutional backbones. Fusion for only the (b) Left to right: LiDAR ground plane channel, bounding box predictions overlaid on LiDAR obstacle channel (points above ground plane), HD map prediction. <ref type="figure">Fig. 7</ref>: Visualization of Auxiliary Tasks. We visualize the inputs and outputs of both the image branch and LiDAR branch for the same driving scene. Further, the input target point is visualized as a red circle and predicted waypoints as blue and white circles on the HD map prediction. The first two waypoints (which are used to obtain the steering angle for our PID controller) are shown as blue circles, and the remaining two waypoints as white circles.</p><p>Camera ? LiDAR or only the LiDAR ? Camera direction gives a slightly lower performance that the default model with bi-directional fusion. Removing the fusion mechanism in the early blocks of the encoder and performing feature fusion at only the deepest 1, 2 and 3 scales also leads to a small drop in performance. For the fusion transformers, we find that 2-8 attention layers give similar performance. The default resolution of the image features is 22?5 and the LiDAR features is 8?8. We observe that using each of these 22?5 + 8?8 features as independent input tokens for the transformer leads to better results when compared to a fusing information across different image and LiDAR resolutions through a reduced image token count of 11?3 or LiDAR token count of 4?4. We also evaluate a version of TransFuser where the input to the MLP and GRU decoder from the final fusion transformer is obtained via a dedicated attention token instead of the default global average pooling. This is a standard idea for attention-based averaging of spatial features, similar to the CLS token of Vision Transformers <ref type="bibr" target="#b37">[37]</ref>. We find that the default architecture with global average pooling is simpler to implement and performs similarly in practice. The most impactful  architecture choice is the backbone architecture for both branches. The default configuration of RegNetY-3.2GF backbones outperforms ConvNeXt-Tiny <ref type="bibr" target="#b129">[127]</ref>, RegNetY-1.6GF and RegNetY-0.8GF based backbones. We also observe a large improvement over the use of a ResNet34 for the image branch and ResNet18 for the BEV branch, as in <ref type="bibr" target="#b41">[41]</ref>, which leads to a model with lower network capacity.</p><p>Model Inputs: As shown in <ref type="table" target="#tab_20">Table 9</ref>, increasing the LiDAR range or reducing the camera FOV from the default configuration leads to a reduced IS and a corresponding drop in DS. Our method works with arbitrary grids as inputs. Therefore, it could benefit from orthogonal improvements in LiDAR encoding. However, we did not observe improvements by using a learned LiDAR encoder <ref type="bibr" target="#b130">[128]</ref>, and hence stick with the simpler voxelization approach. This model was trained with batch size 10 due to its higher memory requirements. Interestingly, despite being useful in our preliminary experiments, removing the rasterized goal location channel from the LiDAR branch, or removing the random rotation of sensor inputs by ?20 ? used during data augmentation show only a small impact on the performance in the final configuration which is unlikely to be significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inertia Problem:</head><p>As we note in Section 3.5, we add creeping to our controller to prevent the agent from being overly cautious. This type of behavior, called the inertia problem <ref type="bibr" target="#b38">[38]</ref> is typically attributed to the spurious correlation that exists between input velocity and output acceleration in an IL dataset. Interestingly, though we do not use velocity as an input to our models, we observe that creeping in the controller increases the RC significantly while maintaining a similar IS <ref type="table" target="#tab_4">(Table 10</ref>). This indicates that a factor besides the velocity input, such as an imbalance in training data distribution, may be a key contributing factor to the inertia problem. We also train a version of TransFuser where we provide the current velocity as input by projecting the   scalar value into the same dimensions as the transformer positional embedding using a linear layer. This velocity embedding is combined with the learnable positional embedding through element-wise summation and fed into the transformer at all 4 stages of the backbone. Including the velocity input leads to a sharp drop in DS, which cannot be recovered through the creeping behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION AND CONCLUSIONS</head><p>In this work, we demonstrate that IL policies based on existing sensor fusion methods suffer from high infraction rates in complex driving scenarios. To overcome this limitation, we present a novel multi-modal fusion transformer (Trans-Fuser) for integrating representations of different modalities. TransFuser uses attention to capture the global 3D scene context and focuses on dynamic agents and traffic lights, resulting in state-of-the-art performance on CARLA with a significant reduction in infractions. Given that our method is simple, flexible and generic, it would be interesting to explore it further with additional sensors, e.g. radar, or apply it to other embodied AI tasks. We hope that the proposed benchmark with long routes and dense traffic will become a suitable option for the community to conduct ablation studies or obtain detailed statistics that are not feasible via the CARLA leaderboard.</p><p>Our study has several limitations. We have provided a simple solution to the inertia problem (creeping), but this deserves more study. Due to the sensor limits of the CARLA leaderboard, our sensor setup does not generate data from the rear of the vehicle, which is relevant in lane change situations. We only investigate single time step input data in this work. Processing temporal inputs is likely necessary to reduce vehicle collisions in intersections by enabling estimation of the acceleration and velocity of other traffic participants. We do not investigate the impact of latency on the final driving performance, which has been shown to be important for real-world applications <ref type="bibr" target="#b131">[129]</ref>. This is because the default configuration of the CARLA simulator waits for the agent to finish its computation before it resumes simulation of the world. Finally, all our experiments are only conducted in simulation. Real-world data is more diverse and can have more challenging noise. We make use of multiple high-quality labels that the CARLA simulator provides, such as dense depth maps. Real-world datasets often do not provide labels of such high quality and might not provide all the types of labels we have used in this work.</p><p>Progress on the CARLA leaderboard has been rapid, with the state-of-the-art scores increasing from the range of 20 to 60 in the short time period since the preliminary version of this work at CVPR 2021. As novel submissions to the leaderboard move towards alternatives to end-to-end IL that involve complex multi-stage training or RL-based objectives, we show that a simple IL training procedure with our proposed architecture is highly competitive. Future works should consider our Latent Transfuser as a standard baseline for image-only IL. Based on our analysis, we believe that overcoming the inertia problem in a principled manner and reducing both training and evaluation variance will be key challenges for IL-based driving.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Auxiliary Loss Functions. Besides the waypoint loss (Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 .</head><label>1</label><figDesc>https://carla.readthedocs.io/en/latest/adv traffic manager/ (a) The expert waits before taking the turn because the trajectory forecasting predicts a collision if the expert would drive. (b) After the oncoming cars have passed, the expert crosses the intersection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Expert performing an unprotected left turn. The black boxes on the street mark the path that the expert has to follow. The predictions of the bicycle model are colored green for the expert and blue for all other vehicles. Red bounding boxes mark predicted collisions. The white box around the car is used to detect the traffic light trigger boxes that are placed on the street (e.g. bottom leftFig. 4a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Lane Change Failures. TransFuser fails at lane changes in dense traffic incurring a high number of consecutive collisions in routes where these situations occur. Two examples are shown in the top and bottom rows. Time goes forward from left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( a )</head><label>a</label><figDesc>Top to bottom: image input, predicted depth, predicted semantics (legend: none, road, lane, sidewalk, vehicle, person).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TransFuser BEV Branch Image Branch</head><label></label><figDesc></figDesc><table><row><cell>RGB</cell><cell>176 x 40 x 72</cell><cell>88 x 20 x 216</cell><cell>44 x 10 x 576</cell><cell>22 x 5 x 1512</cell><cell></cell></row><row><cell>Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H 1</cell><cell>C 1</cell><cell>H 1</cell><cell>C 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>AvgPool + FC</cell><cell>W 1</cell><cell>Self-Attention</cell><cell>W 1</cell><cell>x L</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>C 2</cell><cell>C 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H 2</cell><cell>H 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">W 2</cell><cell>W 2</cell></row><row><cell>LiDAR</cell><cell>Transformer</cell><cell>Transformer</cell><cell>Transformer</cell><cell>Transformer</cell><cell></cell></row><row><cell>BEV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>w 1</cell><cell>w 2</cell><cell>w 3</cell><cell>w 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>AvgPool</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell>MLP</cell><cell>GRU</cell><cell>GRU</cell><cell>GRU</cell><cell>GRU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FC</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell><cell>(0, 0)</cell><cell>w 1</cell><cell>w 2</cell><cell>w 3</cell></row><row><cell></cell><cell>64 x 64 x 72</cell><cell>32 x 32 x 216</cell><cell>16 x 16 x 576</cell><cell>8 x 8 x 1512</cell><cell>512</cell><cell>Goal Location</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Block ? WOR [110] 20.53 ? 3.12 48.47 ? 3.86 0.56 ? 0.03 0.18 1.05 0.37 1.28 0.47 0.88 0.08 0.20 Latent TransFuser (Ours) 37.31 ? 5.35 95.18 ? 0.45 0.38 ? 0.05 0.? 1.45 70.36 ? 3.14 0.51 ? 0.02 0.16 0.83 0.15 0.96 0.42 0.06 0.12 0.45 Late Fusion (LF) 22.47 ? 3.71 83.30 ? 3.04 0.27 ? 0.04 0.05 4.63 0.28 0.11 0.48 0.02 0.11 0.21 Geometric Fusion (GF) 27.32 ? 0.80 91.13 ? 0.95 0.30 ? 0.01 0.06 4.64 0.17 0.13 0.48 0.00 0.05 0.11 TransFuser (Ours) 47.30 ? 5.72 93.38 ? 1.20 0.50 ? 0.06 0.03 2.45 0.07 0.16 0.04 0.00 0.06 0.10 Expert 76.91 ? 2.23 88.67 ? 0.56 0.86 ? 0.03 0.02 0.28 0.01 0.03 0.00 0.00 0.08 0.13</figDesc><table><row><cell>Method</cell><cell>DS ?</cell><cell>RC ?</cell><cell>IS 03 3.66 0.18</cell><cell>0.13 0.04 0.00 0.12</cell><cell>0.05</cell></row><row><cell>LAV [46]</cell><cell>32.74</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? Ped? Veh ? Stat ? Red ? OR ? Dev ? TO ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 1 :</head><label>1</label><figDesc>Longest6 Benchmark Results. We compare our TransFuser model with several baselines in terms of driving performance and infractions incurred. We report the metrics for 3 evaluation runs of each model on the Longest6 evaluation setting. For the primary metrics (DS: Driving Score, RC: Route Completion, IS: Infraction Score) we show the mean and std. For the remaining infractions per km metrics (Ped: Collisions with pedestrians, Veh: Collisions with vehicles, Stat: Collisions with static layout, Red: Red light violation, OR: Off-road driving, Dev: Route deviation, TO: Timeout, Block: Vehicle Blocked) we show only the mean. TransFuser obtains the best DS by a large margin.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table /><note>Runtime. We show the runtime per frame in ms for each method averaged over all timesteps in a single evaluation route. We measure runtimes for both a single model and an ensemble of three models. A single Trans- Fuser model runs in real-time on an RTX 3090 GPU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="2">LiDAR? DS ? RC ? IS ?</cell></row><row><cell>NEAT [39]</cell><cell>-</cell><cell>21.83 41.71 0.65</cell></row><row><cell>MaRLn [109]</cell><cell>-</cell><cell>24.98 46.97 0.52</cell></row><row><cell>WOR [110]</cell><cell>-</cell><cell>31.37 57.65 0.56</cell></row><row><cell>GRIAD [117]</cell><cell>-</cell><cell>36.79 61.86 0.60</cell></row><row><cell>Latent TransFuser (Ours)</cell><cell>-</cell><cell>45.20 66.31 0.72</cell></row><row><cell>Late Fusion (LF)</cell><cell></cell><cell>26.07 64.67 0.47</cell></row><row><cell>Geometric Fusion (GF)</cell><cell></cell><cell>41.70 87.85 0.47</cell></row><row><cell>TransFuser (Ours)</cell><cell></cell><cell>61.18 86.69 0.71</cell></row><row><cell>LAV* [46]</cell><cell></cell><cell>61.85 94.46 0.64</cell></row></table><note>. Among the models that do not use LiDAR inputs, Latent TransFuser achieves the best performance. It obtains a DS of 45.20, which is nearly 10 points better than the next best</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 3 :</head><label>3</label><figDesc>CARLA Leaderboard Evaluation. We report the DS, RC, and IS over the 100 secret routes of the official evaluation server. Latent TransFuser and TransFuser improve the IS by a large margin in comparison to existing methods. *The LAV leaderboard entry uses an updated model different from the public checkpoint in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>For the red query token, we show the top-5 attended tokens in green and highlight the presence of vehicles in the LiDAR point cloud in yellow. Top 2 rows: image to LiDAR, bottom 2 rows: LiDAR to image. TransFuser attends to areas near vehicles and traffic lights at intersections.</figDesc><table><row><cell>Head</cell><cell>T1</cell><cell></cell><cell>T2</cell><cell></cell><cell>T3</cell><cell></cell><cell>T4</cell></row><row><cell></cell><cell>It</cell><cell>Lt</cell><cell>It</cell><cell>Lt</cell><cell>It</cell><cell>Lt</cell><cell>It</cell><cell>Lt</cell></row><row><cell>1</cell><cell cols="8">100.00 0.00 99.83 0.00 44.55 98.69 77.79 89.97</cell></row><row><cell>2</cell><cell cols="8">100.00 0.00 99.99 0.00 07.58 98.98 80.05 95.91</cell></row><row><cell>3</cell><cell cols="8">100.00 0.00 39.71 0.00 27.73 98.09 90.08 99.98</cell></row><row><cell>4</cell><cell cols="8">99.99 1.45 99.99 0.26 99.99 99.98 80.13 99.47</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>).</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>For the LiDAR-based baselines, GF performs better than LF,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>similar to our findings on Longest6. Incorporating global</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>attention via TransFuser leads to further improvements</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>with a state-of-the-art IS. While LAV performs similarly</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>to TransFuser, it is only marginally better in terms of DS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+0.67), which is likely within the evaluation variance. To</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>obtain this marginal improvement, LAV adopts multi-stage</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>training with several pretrained modular components and</cell></row></table><note>a teacher-student distillation framework. In contrast, Trans- Fuser achieves state-of-the-art results with a straightforward single-stage IL training procedure. For further improve- ments, the training procedure of TransFuser can potentially be combined with techniques used in previous work on autonomous driving such as Active Learning [118]-[120], Fig. 6: Attention Maps.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>We report the</cell></row></table><note>Cross-Modal Attention Statistics.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>? 5.49 94.84 ? 1.40 0.44 ? 0.06 42.36 86.67 0.51 Creep Only 36.18 ? 5.36 95.34 ? 2.20 0.37 ? 0.05 45.03 75.37 0.62 TransFuser -Global 49.49 ? 8.63 90.67 ? 4.78 0.55 ? 0.09 41.93 58.55 0.77 -Creep Only 42.51 ? 2.49 91.01 ? 0.83 0.46 ? 0.02 50.57 73.84 0.68 Creep Only 47.30 ? 5.72 93.38 ? 1.20 0.50 ? 0.06 61.18 86.69 0.71</figDesc><table><row><cell>Method</cell><cell cols="2">Ensemble? Safety Heuristic</cell><cell>DS ?</cell><cell>Longest6 RC ?</cell><cell>IS ?</cell><cell>Leaderboard DS ? RC ? IS ?</cell></row><row><cell></cell><cell>-</cell><cell>Global</cell><cell cols="4">50.00 ? 1.13 90.38 ? 3.32 0.56 ? 0.02 47.05 71.66 0.72</cell></row><row><cell>Latent TransFuser</cell><cell>-</cell><cell>Creep Only</cell><cell>42.19</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 5 :</head><label>5</label><figDesc>Impact of Global Safety Heuristic. The heuristic leads to consistent minor improvements for Latent TransFuser. For TransFuser, though the heuristic improves the Longest6 scores, it reduces the performance on the CARLA leaderboard.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 6 :</head><label>6</label><figDesc>Training and evaluation variance. We show the DS of each evaluation on the Longest6 benchmark. We train each baseline 3 times, and perform 3 evaluation runs of each individual trained model. LTF: Latent TransFuser, LF: Late Fusion, GF: Geometric Fusion, TF: TransFuser. All models exhibit large variance in scores.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>comes from two factors. The training variance between different seeds results from different network initializations, data sampling and data augmentations during optimization. The evaluation variance is a result</figDesc><table><row><cell>Auxiliary Losses</cell><cell>DS ?</cell><cell>RC ?</cell><cell>IS ?</cell></row><row><cell>None</cell><cell>44.29</cell><cell>78.17</cell><cell>0.58</cell></row><row><cell>No Depth</cell><cell>56.23</cell><cell>91.80</cell><cell>0.61</cell></row><row><cell>No Semantics</cell><cell>53.76</cell><cell>88.40</cell><cell>0.61</cell></row><row><cell>No HD Map</cell><cell>50.96</cell><cell>89.52</cell><cell>0.58</cell></row><row><cell>No Vehicle Detection</cell><cell>53.43</cell><cell>88.49</cell><cell>0.60</cell></row><row><cell>All Losses (Worst Seed)</cell><cell>49.49</cell><cell>90.67</cell><cell>0.55</cell></row><row><cell>All Losses (Best Seed)</cell><cell>56.68</cell><cell>92.28</cell><cell>0.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 7 :</head><label>7</label><figDesc>Auxiliary Tasks. The results shown are the mean over 3 evaluations on Longest6. Training without auxiliary losses leads to a significant reduction in RC and DS.of the variation in the traffic manager, physics and sensor noise of CARLA 0.9.10. Based on the results observed, the randomness in evaluation is the primary cause of variance, in addition to secondary training seed variance, but both factors are considerable. The existing practice for state-ofthe-art methods on CARLA is to report only the evaluation variance by running multiple evaluations of a single training seed. This may lead to premature conclusions (e.g. when considering only the three evaluations of the first training seed, Latent TransFuser outperforms TransFuser). We argue (given these findings) that future studies should report results by varying the training seed for both the baselines and proposed new methods, in addition to the results of the best seed or ensemble.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 8 :</head><label>8</label><figDesc>Architecture Ablations. The results shown are the mean over 3 evaluations on Longest6. The default configuration fuses in both directions. It uses 4 fusion scales, 4 attention layers, 22?5 + 8?8 tokens, global average pooling, and RegNetY-3.2GF backbones. The encoder backbone has the highest impact on the final driving score.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>RC ? IS ? LiDAR Range 64m ? 32m 49.08 91.10 0.54 64m ? 64m 47.55 90.72 0.52 LiDAR Encoder PointPillars 50.83 91.56 0.55 Camera FOV 120 ? 49.90 90.05 0.56 90 ? 42.18 88.49 0.51 No Rasterized Goal -54.80 91.63 0.60 No Rotation Aug -56.85 92.73 0.61 Default Config Worst Seed 49.49 90.67 0.55 Best Seed 56.68 92.28 0.62</figDesc><table><row><cell>Parameter</cell><cell>Value</cell><cell>DS ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE 9 :</head><label>9</label><figDesc>Model Input Ablations. The results shown are the mean over 3 evaluations on Longest6. The default configuration uses a 32m ? 32m LiDAR range and 132 ? camera FOV. Camera FOV has the largest impact on the DS.</figDesc><table><row><cell cols="3">Velocity Input? Creeping? DS ? RC ? IS ?</cell></row><row><cell>-</cell><cell>-</cell><cell>46.35 78.28 0.63 56.68 92.28 0.62</cell></row><row><cell></cell><cell>-</cell><cell>37.34 64.27 0.65</cell></row><row><cell></cell><cell></cell><cell>45.35 86.22 0.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 10 :</head><label>10</label><figDesc>Inertia Problem. The results shown are the mean over 3 evaluations on Longest6. Creeping improves the RC in both the setting where we input the velocity to our encoder and our default configuration (no velocity input).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. https://github.com/dotchen/LAV</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Katrin Renz is a Ph.D. student with the Autonomous Vision Group led by Prof. Andreas Geiger, part of the Max Planck Institute for Intelligent Systems and University of T?bingen, Germany. She obtained her masters in 2021 from the University of Heilbronn and has spent time as a visiting student in the Visual Geometry Group at the University of Oxford. Andreas Geiger is a professor at the University of T?bingen. Prior to this, he was a visiting professor at ETH Z?rich and a group leader at the Max Planck Institute for Intelligent Systems. He studied at KIT, EPFL and MIT, and received his PhD degree in 2013 from the Karlsruhe Institute of Technology (KIT). His research interests are at the intersection of computer vision, machine learning and robotics, with a particular focus on 3D scene perception, deep representation learning, generative models and sensori-motor control in the context of autonomous systems. He maintains the KITTI and KITTI-360 benchmarks.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep imitative models for flexible inference, planning, and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Can autonomous vehicles identify, recover from, and adapt to distribution shifts?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Filos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tigas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning (ICML)</title>
		<meeting>of the International Conf. on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Perceive, predict, and plan: Safe motion planning through interpretable semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lookout: Diverse multi-future prediction and planning for self-driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page" from="2021" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CARLA: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. on Robot Learning (CoRL)</title>
		<meeting>Conf. on Robot Learning (CoRL)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end learning of driving models from large-scale video datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Waymo open dataset: An autonomous driving dataset</title>
		<idno>2019. 1</idno>
		<ptr target="https://www.waymo.com/open" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BDD100K: A Diverse Driving Video Database with Scalable Annotation Tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>1805.04687</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-view fusion of sensor data for improved perception and prediction in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fadadu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end multi-view fusion for 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. on Robot Learning (CoRL)</title>
		<meeting>Conf. on Robot Learning (CoRL)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mvlidarnet: Real-time multiclass scene understanding for autonomous driving using multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Oldja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smolyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Eden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pehserl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-parametric image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conf. on Intelligent Robots and Systems (IROS)</title>
		<meeting>IEEE International Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sensor fusion for joint 3d object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spagnn: Spatiallyaware graph neural networks for relational behavior forecasting from sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conf. on Robotics and Automation (ICRA)</title>
		<meeting>IEEE International Conf. on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pnpnet: End-to-end perception and prediction with tracking in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stinet: Spatio-temporal-interactive network for pedestrian detection and trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Intentnet: Learning to predict intention from raw sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. on Robot Learning (CoRL)</title>
		<meeting>Conf. on Robot Learning (CoRL)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multixnet: Multiclass multistage multimodal motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wellington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Laserflow: Efficient and probabilistic object detection and motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Wellington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end contextual perception and prediction with interaction transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conf. on Intelligent Robots and Systems (IROS), 2020. 1</title>
		<meeting>IEEE International Conf. on Intelligent Robots and Systems (IROS), 2020. 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Depth completion via inductive fusion of planar LIDAR and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Depth completion from sparse lidar data with depth-normal constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lasernet: An efficient probabilistic 3d object detector for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Wellington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end multi-modal sensors fusion system for urban automated driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sobh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdelkarim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Elmadawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Abdeltawab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Sallab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) Workshops</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploring the limitations of behavior cloning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neat: Neural attention fields for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Carla autonomous driving leaderboard</title>
		<ptr target="https://leaderboard.carla.org/,2020.2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-modal fusion transformer for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multimodal end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gurram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Urfalioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Label efficient visual abstractions for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conf. on Intelligent Robots and Systems (IROS)</title>
		<meeting>IEEE International Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Does computer vision matter for action?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">30</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fully end-to-end autonomous driving with semantic depth cloud mapping and multi-agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Natan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miura</surname></persName>
		</author>
		<idno>2204.05513</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning from all vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pointpainting: Sequential fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Roifusion: 3d object detection from lidar and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Z</forename><surname>Fragonara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tsourdos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mvfusenet: Improving end-to-end object detection and motion forecasting through multi-view fusion of lidar data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palombo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops, 2021</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops, 2021</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sdvtracker: Real-time multi-sensor association and tracking for self-driving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV) Workshops, 2021</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV) Workshops, 2021</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A review of 3d object detection for autonomous driving of electric vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Electric Vehicle Journal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Investigating the effect of sensor modalities in multisensor detection-prediction models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Djuric</surname></persName>
		</author>
		<idno>2101.03279</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Choosing smartly: Adaptive multimodal fusion for object detection in changing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eitel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conf. on Intelligent Robots and Systems (IROS)</title>
		<meeting>IEEE International Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Clocs: Camera-lidar object candidates fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Radha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conf. on Intelligent Robots and Systems (IROS)</title>
		<meeting>IEEE International Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">3d object detection using scale invariant and feature reweighting networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conf. on Artificial Intelligence (AAAI)</title>
		<meeting>of the Conf. on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Transfusion: Robust lidar-camera fusion for 3d object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">FUTR3D: A unified sensor fusion framework for 3d detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<idno>2203.10642</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird&apos;s-Eye View Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno>2205.13542</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Licanext: Incorporating sequential range residuals for additional advancement in joint perception and motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Mouftah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="244" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Interactive multi-scale fusion of 2d and 3d features for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno>2203.16268</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Epnet++: Cascade bi-directional fusion for multi-modal 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="volume">2112</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Cat-det: Contrastively augmented transformer for multi-modal 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Attention-based hierarchical deep reinforcement learning for lane change behaviors in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Palanisamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mudalige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conf. on Intelligent Robots and Systems (IROS)</title>
		<meeting>IEEE International Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Brain inspired cognitive model with attention for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<idno>1702.05596</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Temporalchannel transformer for 3d lidar-based video object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Improving 3d object detection with channel-wise transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Car-net: Clairvoyant attentive recurrent network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Legros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Voisin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Sophie: An attentive GAN for predicting paths compliant to social and physical constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Stgat: Modeling spatial-temporal interactions for human trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Looking to relations for future trajectory forecast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dariush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Social-bigat: Multimodal trajectory forecasting using bicycle-gan and graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The trajectron: Probabilistic multiagent trajectory modeling with dynamic spatiotemporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Perceive, attend, and drive: Learning spatial attention for safe selfdriving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conf. on Robotics and Automation (ICRA)</title>
		<meeting>IEEE International Conf. on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Agentformer: Agentaware transformers for socio-temporal multi-agent forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Scene transformer: A unified architecture for predicting multiple agent trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><forename type="middle">L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno>2106.08417</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Driver attention prediction based on convolution and transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Leveraging smooth attention prior for multi-agent trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Biyik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sadigh</surname></persName>
		</author>
		<idno>2203.04421</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Projecting your view attentively: Monocular road scene layout estimation via cross-view transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Translating Images into Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mendez</forename><surname>Maldonado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conf. on Robotics and Automation (ICRA)</title>
		<meeting>IEEE International Conf. on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Cross-view Transformers for realtime Map-view Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">BEVSegFormer: Bird&apos;s Eye View Semantic Segmentation From Arbitrary Camera Rigs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cheng</surname></persName>
		</author>
		<idno>2203.04050, 2022. 3</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">M 2 BEV: Multi-Camera Joint 3D Detection and Segmentation with Unified Birds-Eye View Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<idno>2204.05088, 2022. 3</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">BEVFormer: Learning Bird&apos;s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno>2203.17270, 2022. 3</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">PRECOG: prediction conditioned on goals in visual multi-agent settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Learning by cheating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. on Robot Learning (CoRL)</title>
		<meeting>Conf. on Robot Learning (CoRL)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Off-road obstacle avoidance through end-to-end learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cosatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Monocular plan view networks for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conf. on Intelligent Robots and Systems (IROS)</title>
		<meeting>IEEE International Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ogale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Robotics: Science and Systems (RSS)</title>
		<meeting>Robotics: Science and Systems (RSS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Fighting copycat agents in behavioral cloning from observation histories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Keyframefocused visual imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<idno>2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning</title>
		<meeting>of the International Conf. on Machine learning</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Adaptive Control Processes -A Guided Tour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Princeton University Press</publisher>
			<biblScope unit="volume">2045</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>2020. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning</title>
		<meeting>of the International Conf. on Machine learning</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sacheti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Safetynet: Safe planning for real-world self-driving vehicles using machinelearned policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vitelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wo?czyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Osi?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grimmett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<idno>2109.13602</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Learning situational driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Multi-modal sensor fusionbased deep neural network for end-to-end autonomous driving with scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Driving among Flatmobiles: Bird-Eye-View occupancy grids from a monocular camera for holistic trajectory planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Probabilistic future prediction for video scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Semantic-aware gradgan for virtual-to-real urban scene adaption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno>1801.01726</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko?eck?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="5632" to="5640" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">3dssd: Point-based 3d single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Towards large-pose face frontalization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">End-to-end modelfree reinforcement learning for urban driving using implicit affordances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toromanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Learning to drive from a world on rails</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">End-toend urban driving by imitating a reinforcement learning coach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liniger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page" from="2021" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Affordance-based reinforcement learning for urban driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<idno>2101.05970, 2021. 7</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
				<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">GRI: general reinforced imitation and its application to visionbased autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chekroun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toromanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hornauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
		<idno>2111.08575</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Large-scale visual active learning with deep probabilistic ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lesnikowski</surname></persName>
		</author>
		<idno>1811.03575</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Training data subset search with ensemble active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haussmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Scalable Active Learning for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haussmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fenzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ivanecky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Koumchatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intelligent Vehicles Symposium (IV)</title>
		<meeting>IEEE Intelligent Vehicles Symposium (IV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Exploring data aggregation in policy learning for vision-based urban autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Advsim: Generating safety-critical scenarios for self-driving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manivasagam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>2021. 11</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Generating useful accident-prone driving scenarios via a learned traffic prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<idno>2021. 11</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">King: Generating safety-critical driving scenarios for robust imitation via kinematics gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hanselmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Renz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno>2204.13683, 2022. 11</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv.org</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Sam: Squeeze-and-mimic networks for conditional visual driving policy learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Broeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. on Robot Learning (CoRL)</title>
		<meeting>Conf. on Robot Learning (CoRL)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno>2201.03545, 2022. 14</idno>
	</analytic>
	<monogr>
		<title level="m">A convnet for the 2020s</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Towards streaming perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno>2020. 15</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
