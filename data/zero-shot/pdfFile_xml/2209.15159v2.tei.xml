<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MOBILEVITV3: MOBILE-FRIENDLY VISION TRANS- FORMER WITH SIMPLE AND EFFECTIVE FUSION OF LOCAL, GLOBAL AND INPUT FEATURES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakti</forename><forename type="middle">N</forename><surname>Wadekar</surname></persName>
							<email>swadekar@purdue.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Chaurasia</surname></persName>
							<email>achaurasia@micron.com</email>
						</author>
						<title level="a" type="main">MOBILEVITV3: MOBILE-FRIENDLY VISION TRANS- FORMER WITH SIMPLE AND EFFECTIVE FUSION OF LOCAL, GLOBAL AND INPUT FEATURES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>MobileViT (MobileViTv1) combines convolutional neural networks (CNNs) and vision transformers (ViTs) to create light-weight models for mobile vision tasks. Though the main MobileViTv1-block helps to achieve competitive state-of-theart results, the fusion block inside MobileViTv1-block, creates scaling challenges and has a complex learning task. We propose changes to the fusion block that are simple and effective to create MobileViTv3-block, which addresses the scaling and simplifies the learning task. Our proposed MobileViTv3-block used to create MobileViTv3-XXS, XS and S models outperform MobileViTv1 on ImageNet-1k, ADE20K, COCO and PascalVOC2012 datasets. On ImageNet-1K, MobileViTv3-XXS and MobileViTv3-XS surpasses MobileViTv1-XXS and MobileViTv1-XS by 2% and 1.9% respectively. Recently published MobileViTv2 architecture removes fusion block and uses linear complexity transformers to perform better than MobileViTv1. We add our proposed fusion block to MobileViTv2 to create MobileViTv3-0.5,0.75 and 1.0 models. These new models give better accuracy numbers on ImageNet-1k, ADE20K, COCO and PascalVOC2012 datasets as compared to MobileViTv2. MobileViTv3-0.5 and MobileViTv3-0.75 outperforms MobileViTv2-0.5 and MobileViTv2-0.75 by 2.1% and 1.0% respectively on ImageNet-1K dataset. For segmentation task, MobileViTv3-1.0 achieves 2.07% and 1.1% better mIOU compared to MobileViTv2-1.0 on ADE20K dataset and PascalVOC2012 dataset respectively. Our code and the trained models are available at https://github.com/micronDLA/MobileViTv3. * This work was completed as an Intern at Micron Technology Inc.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Convolutional Neural Networks(CNNs) [like ResNet <ref type="bibr" target="#b12">(He et al., 2016)</ref>, DenseNet <ref type="bibr" target="#b16">(Huang et al., 2017)</ref> and EfficientNet <ref type="bibr" target="#b32">(Tan &amp; Le, 2019)</ref>] are widely used for vision tasks such classification, detection and segmentation, due to their strong performance on the established benchmark datasets such as Imagenet <ref type="bibr" target="#b29">(Russakovsky et al., 2015)</ref>, COCO <ref type="bibr" target="#b19">(Lin et al., 2014)</ref>, PascalVOC <ref type="bibr" target="#b8">(Everingham et al., 2015)</ref>, ADE20K <ref type="bibr" target="#b47">(Zhou et al., 2017)</ref> and other similar datasets. When deploying CNNs on edge devices like mobile phones which are generally resource constrained, light-weight CNNs suitable for such environments come from family of models of MobileNets (MobileNetv1, MobileNetv2, Mo-bileNetv3) <ref type="bibr" target="#b14">(Howard et al., 2019)</ref>, ShuffleNets (ShuffleNetv1 and ShuffleNetv2) <ref type="bibr" target="#b23">(Ma et al., 2018)</ref> and light weight versions of EfficientNet <ref type="bibr" target="#b32">(Tan &amp; Le, 2019</ref>) (EfficientNet-B0 and EfficientNet-B1). These light-weight models lack in accuracy when compared to models with large parameters and FLOPs. Recently, Vision Transformers(ViTs) have emerged as an strong alternative to CNNs on these vision tasks. CNNs, due to its architecture design, interacts with local neighbouring pixels/feature to produce feature maps which has local information embedded in them. In contrast, self-attention mechanism in ViTs interacts with all parts of the image/feature map to produce features which have global information embedded in them. This has been demonstrated to produce comparable results to CNNs but with large pre-training data and advance data augmentation <ref type="bibr" target="#b7">(Dosovitskiy et al., 2020)</ref>. Also, this global processing comes at a cost of large parameters and FLOPs to match the performance of CNNs as seen in ViT <ref type="bibr" target="#b7">(Dosovitskiy et al., 2020)</ref>, and its different versions such as DeiT . Models of our MobileViTv3 architecture outperforms other models with similar parameter budget of under 2M, 2-4M and 4-8M. Also, they achieve competitive results when compared to the models greater than 8M parameters. , SwinT , MViT , Focal-ViT , PVT , T2T-ViT <ref type="bibr" target="#b43">(Yuan et al., 2021b)</ref>, XCiT <ref type="bibr" target="#b0">(Ali et al., 2021)</ref>.  demonstrates that ViTs suffer from problem of high sensitivity to hyperparameters such as choice of optimizer, learning rate, weight decay and slow convergence. To address these problems  proposes to introduce convolutional layers in ViTs.</p><p>Many recent works have introduced convolutional layers in ViT architecture to form hybrid networks to improve performance, achieve sample efficiency and make the models more efficient in terms of parameters and FLOPs like MobileViTs (MobileViTv1 <ref type="bibr" target="#b25">(Mehta &amp; Rastegari, 2021)</ref>, Mo-bileViTv2 <ref type="bibr" target="#b26">(Mehta &amp; Rastegari, 2022)</ref>), CMT <ref type="bibr" target="#b10">(Guo et al., 2022)</ref>, CvT <ref type="bibr" target="#b36">(Wu et al., 2021)</ref>, PVTv2 , ResT , MobileFormer , CPVT <ref type="bibr" target="#b5">(Chu et al., 2021)</ref>, MiniViT , CoAtNet , CoaT <ref type="bibr" target="#b38">(Xu et al., 2021a)</ref>. Performance of many of these models on ImageNet-1K, with parameters and FLOPs is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Among these models, only MobileViTs and MobileFormer are specifically designed for resource constrained environment such as mobile devices. These two models achieve competitive performance compared to other hybrid networks with less parameters and FLOPs. Even though these small hybrid models are critical for the vision tasks on mobile devices, there is little work done in this area.</p><p>Our work focuses on improving one such light-weight family of models known as MobileViTs (Mo-bileViTv1 <ref type="bibr" target="#b25">(Mehta &amp; Rastegari, 2021)</ref> and MobileViTv2 <ref type="bibr" target="#b26">(Mehta &amp; Rastegari, 2022)</ref>). When compared to the models with parameter budget of 6 million(M) or less, MobileViTs achieve competitive state-of-the-art results with a simple training recipe (basic data augmentation) on classification task. Also it can be used as an efficient backbone across different vision tasks such as detection and segmentation. While focusing on only the models with 6M parameters or less, we pose the question: Is it possible to change the model architecture to improve its performance by maintaining similar parameters and FLOPs? To do so, our work looks into challenges of MobileViT-block architecture and proposes simple and effective way to fuse input, local (CNN) and global (ViT) features which lead to significant performance improvements on Imagenet-1K, ADE20k, PascalVOC and COCO dataset. We propose four main changes to MobileViTv1 block (three changes w.r.t MobileViTv2 block) as shown in figure 2. Three changes are in the fusion block: First, 3x3 convolutional layer is replaced with 1x1 convolutional layer. Second, features of local and global representation blocks are fused together instead of input and global representation blocks. Third, input features are added in the fusion block as a final step before generating the output of MobileViT block. Fourth change is proposed in local representation block where normal 3x3 convolutional layer is replaced by depthwise 3x3 convolutional layer. These changes result in the reduction of parameters and FLOPs of MobileViTv1 block and allow scaling (increasing width of the model) to create a new MobileViTv3-S, XS and XXS architecture, which outperforms MobileViTv1 on classification <ref type="figure" target="#fig_0">(Figure 1</ref>), segmentation and detection tasks. For example, MobileViTv3-XXS and MobileViTv3-XS perform 2% and 1.9% better with similar parameters and FLOPs on ImageNet-1K dataset compared to MobileViTv1-XXS and MobileViTv1-XS respectively. In MobileViTv2, fusion block is absent. Our proposed fusion block is introduced in MobileViTv2 architecture to create MobileViTv3-1.0, 0.75 and 0.5 architectures. MobileViTv3-0.5 and MobileViTv3-0.75 outperforms MobileViTv2-0.5 and MobileViTv2-0.75 by 2.1% and 1.0% respectively with similar parameters and FLOPs on ImageNet-1K dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Vision Transformers: ViT <ref type="bibr" target="#b7">(Dosovitskiy et al., 2020)</ref> introduced the transformer models used for Natural Language Processing tasks to vision domain, specifically for image recognition. Later, its different versions such as DeiT  improved the performance by introducing a novel training technique and reducing the dependency on large pre-training data. Works focusing on improving self-attention mechanism to boost performance include XCiT <ref type="bibr" target="#b0">(Ali et al., 2021)</ref>, SwinT , ViL  and Focal-transformer . XCiT introduces cross-covariance attention where self-attention is operated on feature channels instead of tokens and interactions are based on cross-covariance matrix between keys and queries. SwinT modified the ViT to make it a general purpose architecture which can be used for various vision tasks like classification, detection and segmentation. This was achieved by replacing self-attention mechanism with a shifted-window based self-attention which allows model to be adapted for different input image scales and do it efficiently by achieving a linear computational complexity relation with the input image size. ViL improves ViT by encoding image at multiple scales and uses self-attention mechanism which is a variant of Longformer <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref>. Recent works like T2T-ViT <ref type="bibr" target="#b43">(Yuan et al., 2021b)</ref> and PVT (PVTv1)  also focus on introducing CNN like hierarchical feature learning by reducing spatial resolution or token sizes of output after each layer. T2T-ViT proposes layer-wise token-to-token transformation where neighbouring tokens are recursively aggregated into one token to capture local structure and reduce token lengths. PVT a pyramid vision transformer, successively reduces the feature map resolution size reducing computational complexity and achieving competitive results on ImageNet-1K. Few new architectures like CrossViT <ref type="bibr" target="#b2">(Chen et al., 2021)</ref>, MViT , MViTv2  and Focal-transformer  learn both local features (features learnt specifically from neighbouring pixels/features/patches) and global features (features learnt using all pixels/features/patches). Focal-transformer replaces selfattention with focal-self-attention where each token is able to attend its closest surrounding tokens at fine granularity and also tokens far away at coarse granularity, capturing both short and long-range visual dependencies. CrossViT processes small-patch and large-patch tokens separately and fused together by attention multiple times to complement each other. MViT designed for video and image recognition, learns multi-scale pyramid of features where early layers capture low-level visual information and deeper layers capture complex and high dimensional features. MViTv2 further improves MViT by incorporating positional embedding and residual pooling connections in its architecture.</p><p>CNNs: ResNet <ref type="bibr" target="#b12">(He et al., 2016)</ref> is one of the most widely used general purpose architecture for vision tasks like classification, segmentation and detection. ResNet architecture, due to its residual connections, helps optimization of deeper layers, allowing construction of deep neural networks (deep CNNs). These deep CNNs are able to achieve state of the art results on various benchmarks. DenseNet <ref type="bibr" target="#b16">(Huang et al., 2017)</ref> inspired from ResNet, connects every layer to the next using skip connection in a feed-forward fashion. Other CNNs like ConvNeXt , RegNetY <ref type="bibr" target="#b28">(Radosavovic et al., 2020</ref><ref type="bibr">), SqueezeNet (Iandola et al., 2016</ref> and Inception-v3  also have achieved competitive state-of-the-art performance. But, the best performing CNN models are generally high in number of parameters and FLOPs. Light-weight CNNs that achieve competitive performance with less parameters and FLOPs include EfficientNet <ref type="bibr" target="#b32">(Tan &amp; Le, 2019)</ref>, MobileNetV3 <ref type="bibr" target="#b14">(Howard et al., 2019)</ref>, ShuffleNetv2 <ref type="bibr" target="#b23">(Ma et al., 2018)</ref> and ESPNetv2 <ref type="bibr" target="#b27">(Mehta et al., 2019)</ref>. EfficientNet studied model scaling and developed family of efficientnet models which are still one of the most efficient CNNs in terms of parameters and FLOPs. MobileNetV3 belongs to category of models specifically developed for resource constrained environments such as Mobile phones. Building block of MobileNetV3 architecture uses MobileNetv2 <ref type="bibr" target="#b30">(Sandler et al., 2018)</ref> block and Squeeze-and-Excite <ref type="bibr" target="#b15">(Hu et al., 2018)</ref> network in it. ShuffleNetv2 studies and proposes guidelines for efficient model design and produces shufflenetv2 family of models which also performs competitively with other light-weight CNN models. ESPNetv2 uses depth-wise dilated separable convolution to create EESP (Extremely Efficient Spatial Pyramid) unit which helps to reduce parameters and FLOPs and achieve competitive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrids:</head><p>Recently, many different models are being proposed which combines CNNs and ViTs together in one architecture to capture both long range dependencies using self-attention mechanism of ViT and local information using local kernels in CNNs to improve performance on vision tasks. Mo-bileViT (MobileViTv1, MobileViTv2) <ref type="bibr" target="#b25">(Mehta &amp; Rastegari, 2021)</ref> and MobileFormer  have been specifically designed for constrained environments like mobile devices. Mobile-ViTv1 and MobileViTv2 achieve state-of-the-art results when compared to models with a parameter budget of 6M or less. MobileFormer architecture combines MobileNetv3 and ViT to also achieve competitive results. CMT <ref type="bibr" target="#b10">(Guo et al., 2022)</ref> architecture has convolutional stem, convolutional layer before every transformer block and stacks convolutional layers and transformer layers alternatively. CvT <ref type="bibr" target="#b36">(Wu et al., 2021)</ref> uses convolutional token embedding instead of linear embedding used in ViTs and a convolutional transformer layer block that leverages these convolutional token embeddings to improve performance. PVTv2  in contrast to PVTv1 uses convolutional feed forward layers in transformers, overlapping patch embedding and linear complexity attention layer to gain improvements over PVT. ResT ) uses depth-wise convolution in self-attention (for memory efficiency) and patch embedding as stack of overlapping convolution operation with stride on token map. CoAtNet  unifies depthwise convolution and self-attention using simple relative attention, also vertically stacks convolutional layers and attention layers. PiT's <ref type="bibr" target="#b13">(Heo et al., 2021)</ref> pooling layer uses depthwise convolution to achieve spatial reduction for boosting performance. LVT <ref type="bibr" target="#b40">(Yang et al., 2022)</ref> introduces convolutional self-attention where local self-attention is introduces within a convolutional kernel and also recursive atrous selfattention to encompass multi-scale context to improve performance. ViTAE <ref type="bibr" target="#b39">(Xu et al., 2021b)</ref> has convolution layers in parallel to multi-head self-attention module and both are fused and fed to feedforward network, also ViTAE uses convolutional layers to embed inputs to token. CeiT <ref type="bibr" target="#b42">(Yuan et al., 2021a)</ref> introduces locally enhanced feed-forward by using depth-wise convolution with other changes to achieve competitive results. RVT  uses convolutional stem to generate patch embeddings and uses convolutional feed-forward network in transformer to achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NEW MOBILEVIT ARCHITECTURE</head><p>Our work proposes four design changes to the existing MobileViTv1 block architecture to build MobileViTv3-block as shown in <ref type="figure" target="#fig_1">Figure 2a</ref>. Section 3.1 explains these four changes in MobileViTv3block architecture and compares with MobileViTv1 and MobileViTv2-blocks. Section 3.2 details MobileViTv3-S, XS and XXS architectures and shows how it is scaled compared to MobileViTv1-S, XS and XXS. In recently published MobileViTv2 architecture, changes applied to MobileViTblock are, fusion block is removed, transformer block uses self-attention with linear complexity and depthwise convolutional layer is used in local representation block. We add back the fusion block with our proposed changes to create MobileViTv3-block as shown in <ref type="figure" target="#fig_1">Figure 2b</ref> for MobileViTv3-0.5, 0.75, 1.0 architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MOBILEVITV3 BLOCK</head><p>Replacing 3x3 convolutional layer with 1x1 convolutional layer in fusion block: Two main motivations exist for replacing 3x3 convolutional layer in fusion. First, fuse local and global features independent of other locations in the feature map to simplify the fusion block's learning task. Conceptually, 3x3 convolutional layer is fusing input features, global features, and other location's input and global features which are present in the receptive field, which is a complex task. Fusion block's goal can be simplified by allowing it to fuse input and global features, independent of other locations in feature map. To do so, we use 1x1 convolutional layer in fusion instead of 3x3 convolutional layer. Second, is to remove one of the major constraints in scaling of MobileViTv1 architecture. Scaling MobileViTv1 from XXS to S is done by changing width of the network and keeping depth constant. Changing width (number of input and output channels) of MobileViTv1 block causes large  increase in number of parameters and FLOPs. For example, if the input and output channels are doubled (2x) in MobileViTv1 block, the number of input channels to 3x3 convolutional layer inside fusion block increases by 4x and output channels by 2x, because input to the 3x3 convolutional layer is concatenation of input and global representation block features. This causes a large increase in parameters and FLOPs of MobileViTv1 block. Using 1x1 convolutional layer avoids this large increase in parameters and FLOPs while scaling.</p><p>Local and Global features fusion: In fusion layer, features from local and global representation blocks are concatenated in our proposed MobileViTv3 block instead of input and global representation features. This is because the local representation features are more closely related to the global representation features when compared to the input features. The output channels of the local representation block are slightly higher than the channels in input features. This causes an increase in the number of input feature maps to the fusion block's 1x1 convolutional layer, but the total number of parameters and FLOPs are significantly less than the baseline MobileViTv1 block due to the change of 3x3 convolutional layer to 1x1 convolutional layer.</p><p>Fusing input features: Input features are added to the output of 1x1 convolutional layer in the fusion block. The residual connections in models like ResNet and DenseNet have shown to help the optimization of deeper layers in the architecture. By adding the input features to the output in the fusion block, we introduce this residual connection in new MobileViTv3 architecture. Ablation study results shown in table 6 demonstrates that this residual connection contributes 0.6% accuracy gain.</p><p>Depthwise convolutional layer in local representation block: To further reduce parameters, 3x3 convolutional layer in local representation block is replaced with depthwise 3x3 convolutional layer. As seen in the ablation study results table 6, this change does not have a large impact on the Top-1 ImageNet-1K accuracy gain and provides good parameter and accuracy trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SCALING UP BUILDING BLOCKS</head><p>Applying the changes proposed in section 3.1, allows scaling of our MobileViTv3 architecture by increasing the width (number of channels) of the layers. <ref type="table">Table 1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>Our work shows results on classification task using ImageNet-1K in section 4.1, segmentation task using ADE20K and PASCAL VOC 2012 datasets in section 4.2, detection task using COCO dataset in section 4.3. We also discuss changes to our proposed MobileViTv3 architecture for improving latency and throughput in section 4.4.  <ref type="bibr">(192,</ref><ref type="bibr">192)</ref>, (256,256), (288,288), (320,320)), learning rate increased from 0.0002 to 0.002 for the first 3K iterations and then annealed to 0.0002 using cosine schedule, L2 weight decay of 0.01, basic data augmentation i.e, random resized cropping and horizontal flipping. MobileViTv3-1.0, 0.75 and 0.5: Default hyperparameters used from MobileViTv2 include using AdamW as optimizer, batch-sampler (S = (256,256)), learning rate increased from 1e-6 to 0.002 for the first 20K iterations and then annealed to 0.0002 using cosine schedule, L2 weight decay of 0.05, advanced data augmentation i.e, random resized cropping, horizontal flipping, random augmentation, random erase, mixup and cutmix. Performance is evaluated using single crop top-1 accuracy, for inference an exponential moving average of model weights is used. All the classification models are trained from scratch on the ImageNet-1K classification dataset. This dataset contains 1.28M and 50K images for training and validation respectively.   <ref type="figure" target="#fig_0">Figure 1</ref> compares our proposed MobileViTv3 models performance with other ViT variants and hybrid models. Following MobileViTv1, we mainly compare our models with parameter budget of around 6M or less. Also, when comparing to models greater than 6M parameters, we limit FLOPs budget to ?2 GFLOPs or less because our largest model in this work has ?2 GFLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">IMAGE CLASSIFICATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">COMPARISON WITH MOBILEVITS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">COMPARISON WITH VITS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models under 2 million parameters:</head><p>To the best of our knowledge, only MobileViT variants exist in this range. MobileViTv3-XXS and MobileViTv3-0.5 outperform other MobileViT variants. MobileViTv3-0.5 by far achieves the best accuracy of 72.33 % in 1-2 million parameter budget models (ViT or hybrid).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models between 2-4 million parameters:</head><p>MobileViTv3-XS and MobileViTv3-0.75 outperform all the models in this range. Top-1 accuracy of MobileViTv3-XS on ImageNet-1k is 76.7%, which is 3.9% higher than Mini-DeiT-Ti , 4.5 % higher than XCiT-N12 <ref type="bibr" target="#b0">(Ali et al., 2021)</ref>, and 6.2% higher than PVTv2-B0 . Although Mobile-Former-53M  uses only 53 GFLOPs, it lags in accuracy by a large margin of 12.7% with MobileViTv3-XS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models between 4-8 million parameters:</head><p>MobileViTv3-S attains the highest accuracy in this parameter range. MobileViTv3-S with simple training recipe and 300 epochs is 0.7% better than XCiT-T12 trained using distillation, advanced data augmentation and 400 epochs. It is 1.8%, 2.6% and 2.9% better than Coat-Lite-Tiny <ref type="bibr" target="#b38">(Xu et al., 2021a)</ref>, ViL-Tiny-RPB  and CeiT-Ti <ref type="bibr" target="#b42">(Yuan et al., 2021a)</ref> respectively. MobileViTv3-S is 1% better with 0.5x FLOPs and similar parameters as compared to CoaT-Tiny <ref type="bibr" target="#b38">(Xu et al., 2021a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models greater than 8 million parameters:</head><p>We also compare our designed models with existing models having more than 8M parameters and around 2 GFLOPs. When compared with MobileViTv3-S trained with basic data augmentation and 300 epochs, CoaT-Lite-Mini achieves competitive accuracy of 79.1% with ?2x more parameters, similar FLOPs and advanced data augmentation, MobileFormer-508M achieves similar accuracy of 79.3% with ?2.5x more parameters, ?3.5x less FLOPs, advance data augmentation and 450 training epochs. ResT-Small  achieves similar accuracy of 79.6% with ?2.5x more parameters, similar FLOPs and advanced data augmentation. PVTv2-B1  achieves 78.7% with ?2.3x more parameters, similar FLOPs and advanced data augmentation. CMT-Ti <ref type="bibr" target="#b10">(Guo et al., 2022)</ref> achieves 79.1% with ?1.6x more parameters, ?2.9x less FLOPs (due to input image size of 160x160) and advanced data augmentation. <ref type="figure" target="#fig_2">Figure 3</ref> compares our proposed models with the CNN models which are light-weight with a parameter budget of ?6M or less, similar to MobileViTv1 <ref type="bibr" target="#b25">(Mehta &amp; Rastegari, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">COMPARISON WITH CNNS</head><p>Models in 1-2 million parameters range: MobileViTv3-0.5 and MobileViTv3-XXS with 72.33% and 70.98% respectively are best accuracies in this parameter range. MobileViTv3-0.5 achieves over 2.5% improvement compared to MobileNetv3-small(0.5) <ref type="bibr" target="#b14">(Howard et al., 2019)</ref>, MobileNetv3small(0.75), ShuffleNetv2(0.5) <ref type="bibr" target="#b23">(Ma et al., 2018)</ref>  <ref type="bibr" target="#b8">(Everingham et al., 2015)</ref>. Extra annotations and data is used from <ref type="bibr" target="#b11">(Hariharan et al., 2011)</ref> and <ref type="bibr" target="#b19">(Lin et al., 2014)</ref> respectively, which is a standard practise for training on PascalVOC2012 dataset <ref type="bibr" target="#b3">(Chen et al., 2017)</ref>; <ref type="bibr" target="#b27">(Mehta et al., 2019)</ref>. ADE20K dataset <ref type="bibr" target="#b48">(Zhou et al., 2019)</ref>: Contains total 25K images with 150 semantic categories. Out of 25K images, 20K images are used for training, 3K images for test and 2K images for validation. Same training hyperparameters are used for MobileViTv2 models as MobileViTv3-1.0,0.75 and 0.5 models. Training hyperparameters include using SGD as optimizer, weight decay of 1e-4, momentum of 0.9, cosine learning rate scheduler, 120 epochs training, cross-entropy loss, batch size 16 (4 images per GPU). The segmentation performance is evaluated on the validation set and reported using mean Intersection over Union (mIOU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">RESULTS</head><p>PASCAL VOC 2012 dataset: <ref type="table" target="#tab_7">Table 3a</ref> demonstrates MobileViTv3 models with lower training batch size of 48, outperforming their corresponding counterpart models of MobileViTv1 and Mo-bileViTv2 which are trained on higher batch size of 128. MobileViTv3-1.0 achieves 80.04% mIOU, which outperforms MobileViTv2-1.0 by 1.1%. MobileViTv3-XS is 1.6% better than MobileViTv1-XS and MobileViTv3-0.5 surpasses MobileViTv2-0.5 by 1.41%. ADE20K dataset: <ref type="table" target="#tab_7">Table 3b</ref> shows the results of MobileViTv3-1.0, 0.75 and 0.5 models on ADE20K dataset. MobileViTv3-1.0, 0.75 and 0.5 models outperform MobileViTv2-1.0, 0.75 and 0.5 models by 2.07%, 1.73% and 1.64% respectively.   <ref type="bibr" target="#b20">(Liu et al., 2016)</ref> and the standard convolutions in the SSD head are replaced with separable convolutions to create SS-DLite network. SSDLite had also been used by other light-weight CNNs for evaluating performance on detection task. This SSDLite with pre-trained MobileViTv3-1.0, 0.75 and 0.5 is fine-tuned on MS-COCO dataset. Hyperparameters for training MobileViTv3-1.0, 0.75 and 0.5 are kept same as MobileViTv2-1.0, 0.75 and 0.5. Default hyperparameters include using images of input size 320 x 320, AdamW optimizer, weight decay of 0.05, cosine learning rate scheduler, total batch size of 128 (32 images per GPU), smooth L1 and cross-entropy losses are used for object localization and classification respectively. Training hyperparameters for MobileViTv3-S, XS and XXS are kept same as MobileViTv1-S, XS and XXS. Default hyperparameters include using images of input resolution of 320 x 320, AdamW optimizer, weight decay of 0.01, cosine learning rate scheduler, total batch size of 128 (32 images per GPU), smooth L1 and cross-entropy losses are used for object localization and classification respectively. Performance evaluation is done on validation set using mAP@IoU of 0.50:0.05:0.95 metric.   <ref type="table" target="#tab_11">Table 5</ref> represents number of MobileViTv3-blocks in 'layer4' of MobileViTv3 architectures <ref type="table">(Table 1)</ref>. To improve the latency, we reduce the number of MobileViT-blocks in 'layer4' from 4 to 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">RESULTS</head><p>Results: <ref type="table" target="#tab_11">Table 5</ref> shows the latency and throughput results. MobileViTv3-XXS with similar parameters and FLOPs as the baseline MobileViTv1-XXS, along with 1.98% accuracy improvement achieves similar latency of ?7.1 ms. MobileViTv3-XXS with two MobileViT-blocks instead of four, has 30% less FLOPs and achieves latency of 6.24 ms which is ?1 ms faster than the baseline MobileViTv1-XXS. With similar changes in MobileViTv3-XS and MobileViTv3-S architecture, FLOPs are reduced by 13.5% and 17.82% respectively and latency is reduced by ?1 ms and ?0.7 ms respectively.    <ref type="table">Table 7</ref>: MobileViTv3-S(unscaled), MobileViTv1-S and MobileViTv3-S Top-1 ImageNet-1K accuracy comparisons. With similar parameters and FLOPs after scaling, MobileViTv3-S is able to exhibit better performance than baseline MobileViTv1-S.</p><p>MobileViTv3-S(unscaled) architecture though better than the baseline MobileViTv1-S with training batch size of 192, performs worse than the MobileViTv1-S trained at batch size of 1024. Therefore, MobileViTv3-S, XS and XXS models are scaled to have similar parameters and FLOPs as MobileViTv1-S, XS and XXS and are trained with batch size of 384. <ref type="table">Table 7</ref> demonstrates that after scaling, MobileViTv3-S is able to outperform MobileViTv1-S by achieving 79.3% accuracy with similar parameters and FLOPs. <ref type="table" target="#tab_3">Table 2</ref> shows that MobileViTv3-XS and XXS are also able to surpass MobileViTv3-XS and XXS performance by 1.9% and 2.0% respectively with similar parameters and FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION AND LIMITATIONS</head><p>This work is an effort towards improving performance of models for resource constrained environments like mobile phones. We looked at reducing memory (parameters), computation (FLOPs), latency while boosting accuracy and throughput. With the proposed changes to MobileViT blocks we achieve higher accuracy, with same memory and computation as the baseline MobileViTv1 and v2 as seen in section 4.1. <ref type="table" target="#tab_3">Table 2</ref> shows fine-tuning results which also outperform the fine-tuned MobileViTv2 models. Section 4.4 shows how we can achieve better latency and throughput with minimal impact on the accuracy of the model. While MobileViTv3 has higher accuracy and lower or similar parameters as compared to other mobile-CNNs, it's higher FLOPs can be an issue for edge devices <ref type="figure" target="#fig_2">(Figure 3</ref>). This limitation of MobileViTv3 architecture is inherited from the self-attention module of ViTs. To solve this issue, we will further explore optimization of the self-attention block.  <ref type="figure" target="#fig_4">Figure 4</ref> shows object detection results on COCO validation images using SSD-Lite with MobileViTv3-S as its backbone. <ref type="figure" target="#fig_5">Figure 5</ref> shows object detection results on COCO validation images using SSD-lite with MobileViTv3-1.0 as its backbone. The images shown in <ref type="figure" target="#fig_4">figure 4</ref> include challenging object detection examples (blurred human/person and complex background).</p><p>A.2 SEMANTIC SEGMENTATION ON PASCALVOC2012 DATASET <ref type="figure" target="#fig_6">Figure 6</ref> shows segmentation results on PascalVOC2012 validation images using Deeplabv3 with MobileViTv3-S as its backbone. In <ref type="figure" target="#fig_6">figure 6</ref> and 7, moving from left to right we provide the input image, the corresponding segmentation output, and the overlay of segmentation output on the input image.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparing Top-1 accuracies of MobileViTv3, ViT variants and hybrid models on ImageNet-1K dataset. The area of bubbles correspond to number of FLOPs in the model. The reference FLOP sizes are shown in the bottom right (example, 250M is 250 Mega-FLOPs/Million-FLOPs)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A comparison between: (a) MobileViTv1 and MobileViTv3 modules, and (b) Mobile-ViTv2 and MobileViTv3 modules. The proposed architectural changes are highlighted in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Top-1 accuracy comparison between MobileViTv3 models and existing light-weight CNN models on ImageNet-1K dataset. The bubble size corresponds to the number of FLOPs. The reference FLOP sizes are shown in the bottom right (example, 50M is 50 Mega-FLOPs/Million-FLOPs). Models of our MobileViTv3 architecture outperform other models with similar parameter budget of 1-2M, 2-4M and 4-8M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For MobileViTv3-S, XS and XXS, training hyperparameters are similar to Mobile-ViTv1 except the batch size. Smaller batch size of 48 (12 images per GPU) is used compared to 128 (32 images per GPU) for MobileViTv1. Other default hyperparameters include, using adamw as optimizer, weight decay of 0.01, cosine learning rate scheduler, cross-entropy loss and 50 epochs for training. For MobileViTv3-1.0, 0.75 and 0.5, all the hyperparameters are kept same as used for MobileViTv2-1.0, 0.75 and 0.5 training. Default hyperparameters include using adamw as optimizer, weight decay of 0.05, cosine learning rate scheduler, cross-entropy loss and 50 epochs training. The segmentation performance is evaluated on the validation set and reported using mean Intersection over Union (mIOU).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Object detection results using SSD-Lite model with MobileViTv3-S as its backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Object detection results using SSD-Lite model with MobileViTv3-1.0 as its backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Semantic segmentation results using Deeplabv3 with MobileViTv3-S as its backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Semantic segmentation results using Deeplabv3 with MobileViTv3-1.0 as its backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>shows MobileViTv3-S, XS and XXS architectures with their output channels in each layer, scaling factor, parameters an FLOPs</figDesc><table><row><cell>Layer</cell><cell>Size</cell><cell>Stride</cell><cell>Repeat</cell><cell>XXS</cell><cell>XS</cell><cell>S</cell></row><row><cell>Image</cell><cell>256x256</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conv-3x3, ? 2</cell><cell>128x128</cell><cell>2</cell><cell>1</cell><cell>16</cell><cell>16</cell><cell>16</cell></row><row><cell>MV2</cell><cell>128x128</cell><cell>2</cell><cell>1</cell><cell>16</cell><cell>32</cell><cell>32</cell></row><row><cell>MV2 , ? 2</cell><cell>64x64</cell><cell>4</cell><cell>1</cell><cell>24</cell><cell>48</cell><cell>64</cell></row><row><cell>MV2</cell><cell>64x64</cell><cell>4</cell><cell>2</cell><cell>24</cell><cell>48</cell><cell>64</cell></row><row><cell>MV2 , ? 2</cell><cell>32x32</cell><cell>8</cell><cell>1</cell><cell>64 (1.3x)</cell><cell>96 (1.5x)</cell><cell>128 (1.3x)</cell></row><row><cell>MobileViT block (L=2)</cell><cell>32x32</cell><cell>8</cell><cell>1</cell><cell>64 (1.3x)</cell><cell>96 (1.5x)</cell><cell>128 (1.3x)</cell></row><row><cell>MV2 , ? 2</cell><cell>16x16</cell><cell>16</cell><cell>1</cell><cell>80 (1.3x)</cell><cell>160 (2.0x)</cell><cell>256 (2.0x)</cell></row><row><cell>MobileViT block (L=4)</cell><cell>16x16</cell><cell>16</cell><cell>1</cell><cell>80 (1.3x)</cell><cell>160 (2.0x)</cell><cell>256 (2.0x)</cell></row><row><cell>MV2 , ? 2</cell><cell>8x8</cell><cell>32</cell><cell>1</cell><cell>128 (1.6x)</cell><cell>160 (1.7x)</cell><cell>320 (2.0x)</cell></row><row><cell>MobileViT block (L=3)</cell><cell>8x8</cell><cell>32</cell><cell>1</cell><cell>128 (1.6x)</cell><cell>160 (1.7x)</cell><cell>320 (2.0x)</cell></row><row><cell>Conv-1x1,</cell><cell>8x8</cell><cell>32</cell><cell>1</cell><cell>512 (1.6x)</cell><cell>640 (1.7x)</cell><cell>1280 (2.0x)</cell></row><row><cell>Global pool,</cell><cell>1x1</cell><cell>256</cell><cell>1</cell><cell>512</cell><cell>640</cell><cell>1280</cell></row><row><cell>Linear</cell><cell>1x1</cell><cell>256</cell><cell>1</cell><cell>1000</cell><cell>1000</cell><cell>1000</cell></row><row><cell>Parameters (M)</cell><cell></cell><cell></cell><cell></cell><cell>1.25</cell><cell>2.5</cell><cell>5.8</cell></row><row><cell>FLOPs (M)</cell><cell></cell><cell></cell><cell></cell><cell>289</cell><cell>927</cell><cell>1841</cell></row><row><cell>Top-1 Accuracy (%)</cell><cell></cell><cell></cell><cell></cell><cell>71.0</cell><cell>76.7</cell><cell>79.3</cell></row></table><note>Table 1: MobileViTv3-S, XS and XXS architecture details and comparison with MobileViTv1-S, XS and XXS. Values given in brackets '()' represent scaling factor compared to MobileViTv1 models. MobileViTv3-XXS, XS and S achieve 2%, 1.9% and 0.9% accuracy gain respectively compared to MobileViTv1-XXS, XS and S on ImageNet-1K dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Except for the batch size, hyperparameters used for MobileViTv3-S, XS and XXS are similar to the MobileViTv1 and hyperparameters used for MobileViTv3-1.0, 0.75 and 0.5 are similar to Mo-bileViTv2. Due to resource constraints, we were limited to using a total batch size of 384 (32 images per GPU) for experiments on MobileViTv3-S and XS. To maintain consistency in batch sizes, MobileViTv3-XXS is also trained on batch size of 384. Batch size of 1020 (85 images per GPU) used of MobileViTv3-0.5,0.75 and 1.0 training.</figDesc><table><row><cell>ON IMAGENET-1K</cell></row><row><cell>4.1.1 IMPLEMENTATION DETAILS</cell></row></table><note>MobileViTv3-S, XS and XXS: Default hyperparameters used from MobileViTv1 include us- ing AdamW as optimizer, multi-scale sampler (S = (160,160),</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>demonstrates that performance of all the versions of MobileViTv3 surpass MobileViTv1 and MobileViTv2 versions with similar parameters and FLOPs and smaller training batch size. The effect of training batch size on MobileViTv3 is also shown inTable 2. Increasing total batch size from 192 to 384 improves accuracy of MobileViTv3-S, XS and XXS models. This indicates the potential for further accuracy gains with batch size of 1024 on MobileViTv3-XXS, XS and S models. It is also important to note that MobileViTv3-S, XS and XXS models trained with basic data augmentation not only outperforms MobileViTv1-S, XS, XXS, but also surpasses performance of MobileViTv2-1.0, 0.75 and 0.5 which are trained with advanced data augmentation. Fine-tuned on image size of</figDesc><table><row><cell cols="5">384, MobileViTv3-1.0, 0.75 and 0.5 also outperforms fine-tuned versions of MobileViTv2-1.0, 0.75</cell></row><row><cell>and 0.5.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Training Batch size</cell><cell>FLOPs (M)?</cell><cell># Params. (M)?</cell><cell>Top-1 (%)?</cell></row><row><cell>MobileViTv1-XXS</cell><cell>1024</cell><cell>364</cell><cell>1.3</cell><cell>69.00</cell></row><row><cell>MobileViTv3-XXS</cell><cell>192</cell><cell>289</cell><cell>1.2</cell><cell>70.02 (+1%)</cell></row><row><cell>MobileViTv3-XXS</cell><cell>384</cell><cell>289</cell><cell>1.2</cell><cell>70.98 (+2%)</cell></row><row><cell>MobileViTv2-0.5</cell><cell>1024</cell><cell>466</cell><cell>1.4</cell><cell>70.18</cell></row><row><cell>MobileViTv3-0.5</cell><cell>1020</cell><cell>481</cell><cell>1.4</cell><cell>72.33 (+2.1%)</cell></row><row><cell>MobileViTv2-0.5 (384)</cell><cell>64</cell><cell>1048</cell><cell>1.4</cell><cell>72.14</cell></row><row><cell>MobileViTv3-0.5 (384)</cell><cell>64</cell><cell>1083</cell><cell>1.4</cell><cell>74.01 (+1.87%)</cell></row><row><cell>MobileViTv1-XS</cell><cell>1024</cell><cell>986</cell><cell>2.3</cell><cell>74.8</cell></row><row><cell>MobileViTv3-XS</cell><cell>192</cell><cell>927</cell><cell>2.5</cell><cell>76.3 (+1.5%)</cell></row><row><cell>MobileViTv3-XS</cell><cell>384</cell><cell>927</cell><cell>2.5</cell><cell>76.7 (+1.9%)</cell></row><row><cell>MobileViTv2-0.75</cell><cell>1024</cell><cell>1030</cell><cell>2.9</cell><cell>75.56</cell></row><row><cell>MobileViTv3-0.75</cell><cell>1020</cell><cell>1064</cell><cell>3.0</cell><cell>76.55 (+0.99%)</cell></row><row><cell>MobileViTv2-0.75 (384)</cell><cell>64</cell><cell>2318</cell><cell>2.9</cell><cell>76.98</cell></row><row><cell>MobileViTv3-0.75 (384)</cell><cell>64</cell><cell>2395</cell><cell>3.0</cell><cell>77.81 (+0.83%)</cell></row><row><cell>MobileViTv1-S</cell><cell>1024</cell><cell>2009</cell><cell>5.6</cell><cell>78.4</cell></row><row><cell>MobileViTv3-S</cell><cell>192</cell><cell>1841</cell><cell>5.8</cell><cell>78.8 (+0.4%)</cell></row><row><cell>MobileViTv3-S</cell><cell>384</cell><cell>1841</cell><cell>5.8</cell><cell>79.3 (+0.9%)</cell></row><row><cell>MobileViTv2-1.0</cell><cell>1024</cell><cell>1851</cell><cell>4.9</cell><cell>78.09</cell></row><row><cell>MobileViTv3-1.0</cell><cell>1020</cell><cell>1876</cell><cell>5.1</cell><cell>78.64 (+0.55%)</cell></row><row><cell>MobileViTv2-1.0 (384)</cell><cell>64</cell><cell>4083</cell><cell>4.9</cell><cell>79.68</cell></row><row><cell>MobileViTv3-1.0 (384)</cell><cell>64</cell><cell>4220</cell><cell>5.1</cell><cell>79.74 (+0.06%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>MobileViT V1, V2 and V3 comparison in terms of Top-1 ImageNet-1k accuracy, param- eters and operations. &lt;Model name&gt;(384) indicate that the model was fine-tuned using an image size of 384x384. Models with similar parameters and operations are grouped together for clear comparison.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Comparing MobileViTv3 segmentation task results on PASCAL VOC 2012 and ADE20K datasets. # params of MobileViT models denotes the number of parameters in millions of the encoder/backbone architecture only. 4.3 OBJECT DETECTION 4.3.1 IMPLEMENTATION DETAILS MS-COCO dataset with 117K training and 5K validation images, is used to evaluate the detection performance of MobileViTv3 models. Similar to MobileViTv1, we integrated pretrained Mo-bileViTv3 as a backbone network in Single Shot Detection network (SSD)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4a</head><label>4a</label><figDesc>and 4b show the detection results on COCO dataset. # params of MobileViT models indicates number of parameters of the encoder/backbone architecture only.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MobileViTv3 com-</cell></row><row><cell cols="6">parison with other light-weight CNN models is shown in Table 4a. MobileViTv3-XS outper-</cell></row><row><cell cols="6">forms MobileViTv1-XS by 0.8% and MNASNet by 2.6% mAP. Comparison with heavy-weight</cell></row><row><cell cols="6">CNNs detailed in Table 4b. MobileViTv3-XS and MobileViTv3-1.0 surpasses MobileViTv1-XS</cell></row><row><cell cols="4">and MobileViTv2-1.0 by 0.8% and 0.5% mAP respectively.</cell><cell></cell><cell></cell></row><row><cell>Backbone</cell><cell># Params (M)?</cell><cell>mAP(%) ?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileViTv1-XXS</cell><cell>1.50</cell><cell>18.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileViTv3-XXS</cell><cell>1.53</cell><cell>19.3 (?0.8%)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Backbone</cell><cell># Params (M)?</cell><cell>mAP(%) ?</cell></row><row><cell>MobileViTv2-0.5</cell><cell>2</cell><cell>21.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileViTv3-0.5</cell><cell>2</cell><cell>21.8 (?0.6%)</cell><cell>VGG</cell><cell>35.6</cell><cell>25.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet50</cell><cell>22.9</cell><cell>25.2 (?0.0%)</cell></row><row><cell>MobileViTv2-0.75</cell><cell>3.6</cell><cell>24.6</cell><cell>MobileViTv1-XS</cell><cell>2.7</cell><cell>24.8 (?0.4%)</cell></row><row><cell>MobileViTv3-0.75</cell><cell>3.7</cell><cell>25.0 (?0.4%)</cell><cell>MobileViTv3-XS</cell><cell>2.7</cell><cell>25.6 (?0.4%)</cell></row><row><cell>MobileNetv3 MobileNetv2 MobileNetv1 MixNet</cell><cell>4.9 4.3 5.1 4.5</cell><cell>22.0 22.1 22.2 22.3</cell><cell>MobileViTv2-1.0 MobileViTv3-1.0 MobileViTv1-S MobileViTv3-S</cell><cell>5.6 5.8 5.7 5.5</cell><cell>26.5 (?1.3%) 27.0 (?1.8%) 27.7 (?2.5%) 27.3 (?2.1%)</cell></row><row><cell>MNASNet MobileViTv1-XS</cell><cell>4.9 2.7</cell><cell>23.0 (?0.0%) 24.8 (?1.8%)</cell><cell cols="3">(b) Comparison w/heavy-weight CNNs</cell></row><row><cell>MobileViTv3-XS</cell><cell>2.7</cell><cell>25.6 (?2.6%)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(a) Comparison w/light-weight CNNs</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Comparing MobileViTv3 detection task results on COCO dataset with light-weight and heavy-weight CNNs. # params of MobileViT models denotes the number of parameters of the encoder/backbone architecture only. We use GeForce RTX 2080 Ti GPU for obtaining latency timings. Results are averaged over 10000 iterations. The timing results may vary ?0.1 ms. Throughput for XXS, XS and S are calculated on 1000 iterations with batch size of 100. 'Blocks' in</figDesc><table><row><cell>4.4 IMPROVING LATENCY AND THROUGHPUT</cell></row><row><cell>Implementation details:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5</head><label>5</label><figDesc>Table 6. The baseline MobileViTv1-S, in fusion block, concatenates input features with global representation block features and uses 3x3 convolutional layer. Also, it uses normal 3x3 convolutional layer in the local representation block. This baseline achieves an accuracy of 73.7%. 1% improvement. This result supports the assumption that simplifying the fusion block's task (allowing fusion layer to fuse local and global features independent of the other location's local and global features) should help optimization to attain better performance. Along with 1x1 convolutional layer in fusion block, concatenating local representation features instead of input features results in similar performance gains of 1% compared to concatenating input features. This allows us to incorporate the next change i.e, to add input features to the output of fusion block to create a residual connection for helping optimization of deeper layers in the model. With this change, MobileViTv3-S(unscaled) attains 1.6% accuracy gain over the baseline MobileViTv1-S and 0.6% gain over the last change demonstrating the clear advantage of this residual connection. To further reduce number of parameters and FLOPs in MobileViTv3-block, depth-wise convolutional layer is used instead of normal convolutional layer in the local representation block. MobileViTv3-S(unscaled) maintains high accuracy gains by achieving 1.3% gain over the baseline. 0.3% accuracy drop can be observed when compared to the previous change. We adopt this change since it reduces parameters and FLOPs without significantly impacting performance and helps in scaling of MobileViTv3-block.</figDesc><table><row><cell>: Latency (in milliseconds) and throughput (in Images per sec) comparison between</cell></row><row><cell>MobileViTv3-XXS, XS, and S and MobileViTv1-XXS, XS, and S. While keeping the parameters</cell></row><row><cell>and Top-1 accuracy similar to MobileViTv1, MobileViTv3 with 2 blocks reduces the number of</cell></row><row><cell>FLOPs and improves the throughput and latency.</cell></row><row><cell>4.5 ABLATION STUDY OF OUR PROPOSED MOBILEVITV3 BLOCK</cell></row><row><cell>4.5.1 IMPLEMENTATION DETAILS</cell></row><row><cell>We study the effect of the four proposed changes on MobileViTv1-S block by adding changes</cell></row><row><cell>one by one. The final model with all the four changes is our unscaled version and we name it:</cell></row><row><cell>MobileViTv3-S(unscaled). To match the number of parameters of MobileViTv1 we increase the</cell></row><row><cell>width of MobileViTv3-S(unscaled), giving us MobileViTv3-S. Top-1 accuracy on ImageNet-1K of</cell></row><row><cell>each change is recorded and compared to other proposed changes. In this ablation study we train</cell></row><row><cell>models for 100 epochs, use batch size of 192 (32 images per GPU) and other hyper-parameters</cell></row><row><cell>are default as given in section 4.1.1. All the proposed changes in our work are applied in the</cell></row><row><cell>MobileViTv1-block which is made up of local representation, global representation and fusion</cell></row><row><cell>blocks. In Table 6, 'conv-3x3' represents of 3x3 convolutional layer in fusion block, 'conv-1x1'</cell></row><row><cell>represents of 1x1 convolutional layer in fusion block, 'Input-Concat' represents concatenating in-</cell></row><row><cell>put features with global representation in the fusion block, 'Local-Concat' represents concatenating</cell></row><row><cell>local-representation block output features with global representation in the fusion block, 'Input-</cell></row><row><cell>Add' represents adding input features to the output of fusion block, 'DWConv' represents using</cell></row><row><cell>depthwise convolutional layer in the local representation block instead of a normal convolutional</cell></row><row><cell>layer and 'Top-1' represents Top-1 accuracy on ImageNet-1K dataset.</cell></row><row><cell>4.5.2 WITH 100 TRAINING EPOCHS</cell></row><row><cell>Results are shown in Replacing 3x3 convolution with 1x1 convolutional layer in fusion block, MobileViTv3-</cell></row><row><cell>S(unscaled) achieves 1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Ablation study of MobileViTv3 block. 'unscaled' indicates that the number of channels in the architecture are kept same as the baseline MobileViTv1. represents incorporating the change in the block.4.5.3 WITH 300 TRAINING EPOCHSWhen trained for 300 epochs with the batch size of 192, the baseline MobileViTv1-S achieves Top-1 accuracy of 75.6%, which is lower by 2.8% compared to reported accuracy on MobileViTv1-S trained on 1024 batch size. Results shown inTable 7. With all the four proposed changes implemented in MobileViTv1-S architecture to form MobileViTv3-S(unscaled), the model reaches Top-1 accuracy of 77.5%, which outperforms the baseline by 1.9% with 22.7% and 18.6% less parameters FLOPs respectively.</figDesc><table><row><cell>Model</cell><cell>Training batch size</cell><cell>FLOPs (M)?</cell><cell># Params (M)?</cell><cell>Top-1 (%)?</cell></row><row><cell>MobileViTv1-S</cell><cell>192</cell><cell>2009</cell><cell>5.6</cell><cell>75.6</cell></row><row><cell>MobileViTv3-S(unscaled)</cell><cell>192</cell><cell>1636 (?18.6%)</cell><cell>4.3 (?22.7%)</cell><cell>77.5 (?1.9%)</cell></row><row><cell>MobileViTv1-S</cell><cell>1024</cell><cell>2009</cell><cell>5.6</cell><cell>78.4</cell></row><row><cell>MobileViTv3-S</cell><cell>384</cell><cell>1841 (?8.3%)</cell><cell>5.8 (?3.6%)</cell><cell>79.3 (?0.9%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 2</head><label>2</label><figDesc>shows results on Imagenet-1K. The reported accuracies of MobileViTv3-XXS, XS and S models on Imagenet-1K can potentially be further improved by increasing the training batch size to 1024 similar to the baseline model. The proposed fusion of input features, local features (CNN features) and global features (ViT features) shown in this paper can also be explored in other hybrid architectures.A OBJECT DETECTION AND SEMANTIC SEGMENTATION RESULTSA.1 OBJECT DETECTION ON COCO DATASET</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Micron Deep Learning Accelerator (MDLA) team for support with the training infrastructure, feedback and discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Xcit: Cross-covariance image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu Richard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mobile-former: Bridging mobilenet and transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="5270" to="5279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<title level="m">Conditional positional encodings for vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3965" to="3977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6824" to="6835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cmt: Convolutional neural networks meet vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="12175" to="12185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11936" to="11945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and? 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mvitv2: Improved multiscale vision transformers for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="4804" to="4814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards robust vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gege</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjie</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaokai</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="12042" to="12051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Mobilevit: light-weight, general-purpose, and mobilefriendly vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02178</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Separable self-attention for mobile vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2206.02680" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Espnetv2: A lightweight, power efficient, and general purpose convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9190" to="9200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pvt v2: Improved baselines with pyramid vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="424" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="30392" to="30400" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Co-scale conv-attentional image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9981" to="9990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Vitae: Vision transformer advanced by exploring intrinsic inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="28522" to="28535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lite vision transformer with enhanced self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="11998" to="12008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00641</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="579" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Minivit: Compressing vision transformers with weight multiplexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinnian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="12145" to="12154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2998" to="3008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rest: An efficient transformer for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinglong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Bin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15475" to="15485" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
