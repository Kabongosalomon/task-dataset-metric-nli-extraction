<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deeply-Supervised Nets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
							<email>s9xie@ucsd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallagher</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
							<email>zhang@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of EECS</orgName>
								<orgName type="institution">UCSD</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of CSE and CogSci</orgName>
								<orgName type="institution">UCSD</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dept. of CogSci, UCSD</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Dept. of CogSci, UCSD</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deeply-Supervised Nets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent. We make an attempt to boost the classification performance by studying a new formulation in deep networks. Three aspects in convolutional neural networks (CNN) style architectures are being looked at: (1) transparency of the intermediate layers to the overall classification; (2) discriminativeness and robustness of learned features, especially in the early layers; (3) effectiveness in training due to the presence of the exploding and vanishing gradients. We introduce "companion objective" to the individual hidden layers, in addition to the overall objective at the output layer (a different strategy to layer-wise pre-training). We extend techniques from stochastic gradient methods to analyze our algorithm. The advantage of our method is evident and our experimental result on benchmark datasets shows significant performance gain over existing methods (e.g. all state-of-the-art results on MNIST, CIFAR-10, CIFAR-100, and SVHN). * equal contribution ? Corresponding author. Patent disclosure, UCSD Docket No.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Much attention has been given to a resurgence of neural networks, deep learning (DL) in particular, which can be of unsupervised <ref type="bibr" target="#b9">[10]</ref>, supervised <ref type="bibr" target="#b11">[12]</ref>, or a hybrid form <ref type="bibr" target="#b17">[18]</ref>. Significant performance gain has been observed, especially in the presence of large amount of training data, when deep learning techniques are used for image classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref> and speech recognition <ref type="bibr" target="#b3">[4]</ref>. On the one hand, hierarchical and recursive networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref> have demonstrated great promise in automatically learning thousands or even millions of features for pattern recognition; on the other hand concerns about deep learning have been raised and many fundamental questions remain open. Some potential problems with the current DL frameworks include: reduced transparency and discriminativeness of the features learned at hidden layers <ref type="bibr" target="#b30">[31]</ref>; training difficulty due to exploding and vanishing gradients <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>; lack of a thorough mathematical understanding about the algorithmic behavior, despite of some attempts made on the theoretical side <ref type="bibr" target="#b5">[6]</ref>; dependence on the availability of large amount of training data <ref type="bibr" target="#b10">[11]</ref>; complexity of manual tuning during training <ref type="bibr" target="#b14">[15]</ref>. Nevertheless, DL is capable of automatically learning and fusing rich hierarchical features in an integrated framework. Recent activities in open-sourcing and experience sharing <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2]</ref> have also greatly helped the adopting and advancing of DL in the machine learning community and beyond. Several techniques, such as dropout <ref type="bibr" target="#b10">[11]</ref>, dropconnect <ref type="bibr" target="#b18">[19]</ref>, pre-training <ref type="bibr" target="#b3">[4]</ref>, and data augmentation <ref type="bibr" target="#b23">[24]</ref>, have been proposed to enhance the performance of DL from various angles, in addition to a variety of engineering tricks used to fine-tune feature scale, step size, and convergence rate. Features  learned automatically by the CNN algorithm <ref type="bibr" target="#b11">[12]</ref> are intuitive <ref type="bibr" target="#b30">[31]</ref>. Some portion of features, especially for those in the early layers, also demonstrate certain degree of opacity <ref type="bibr" target="#b30">[31]</ref>. This finding is also consistent with an observation that different initializations of the feature learning at the early layers make negligible difference to the final classification <ref type="bibr" target="#b3">[4]</ref>. In addition, the presence of vanishing gradients also makes the DL training slow and ineffective <ref type="bibr" target="#b7">[8]</ref>. In this paper, we address the feature learning problem in DL by presenting a new algorithm, deeply-supervised nets (DSN), which enforces direct and early supervision for both the hidden layers and the output layer. We introduce companion objective to the individual hidden layers, which is used as an additional constraint (or a new regularization) to the learning process. Our new formulation significantly enhances the performance of existing supervised DL methods. We also make an attempt to provide justification for our formulation using stochastic gradient techniques. We show an improvement of the convergence rate of the proposed method over standard ones, assuming local strong convexity of the optimization function (a very loose assumption but pointing to a promising direction).</p><p>Several existing approaches are particularly worth mentioning and comparing with. In <ref type="bibr" target="#b0">[1]</ref>, layerwise supervised pre-training is performed. Our proposed method does not perform pre-training and it emphasizes the importance of minimizing the output classification error while reducing the prediction error of each individual layer. This is important as the backpropagation is performed altogether in an integrated framework. In <ref type="bibr" target="#b25">[26]</ref>, label information is used for unsupervised learning. Semi-supervised learning is carried in deep learning <ref type="bibr" target="#b29">[30]</ref>. In <ref type="bibr" target="#b27">[28]</ref>, an SVM classifier is used for the output layer, instead of the standard softmax function in the CNN <ref type="bibr" target="#b11">[12]</ref>. Our framework (DSN), with the choice of using SVM, softmax or other classifiers, emphasizes the direct supervision of each intermediate layer.</p><p>In the experiments, we show consistent improvement of DSN-SVM and DSN-Softmax over CNN-SVM and CNN-Softmax respectively. We observe all state-of-the-art results on MNIST, CIFAR-10, CIFAR-100, and SVHN. It is also worth mentioning that our formulation is inclusive to various techniques proposed recently such as averaging <ref type="bibr" target="#b23">[24]</ref>, dropconnect <ref type="bibr" target="#b18">[19]</ref>, and Maxout <ref type="bibr" target="#b8">[9]</ref>. We expect to see more classification error reduction with careful engineering for DSN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deeply-Supervised Nets</head><p>In this section, we give the main formulation of the proposed deeply-supervised nets (DSN). We focus on building our infrastructure around supervised CNN style frameworks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2]</ref> by introducing classifier, e.g. SVM model <ref type="bibr" target="#b28">[29]</ref>, to each layer. An early attempt to combine SVM with DL was made in <ref type="bibr" target="#b27">[28]</ref>, which however has a different motivation with ours and only studies the output layer with some preliminary experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation</head><p>We are motivated by the following simple observation: in general, a discriminative classifier trained on highly discriminative features will display better performance than a discriminative classifier trained on less discriminative features. If the features in question are the hidden layer feature maps of a deep network, this observation means that the performance of a discriminative classifier trained using these hidden layer feature maps can serve as a proxy for the quality/discriminativeness of those hidden layer feature maps, and further to the quality of the upper layer feature maps. By making appropriate use of this feature quality feedback at each hidden layer of the network, we are able to directly influence the hidden layer weight/filter update process to favor highly discriminative feature maps. This is a source of supervision that acts deep within the network at each layer; when our proxy for feature quality is good, we expect to much more rapidly approach the region of good features than would be the case if we had to rely on the gradual backpropagation from the output layer alone. We also expect to alleviate the common problem of having gradients that "explode" or "vanish". One concern with a direct pursuit of feature discriminativeness at all hidden layers is that this might interfere with the overall network performance, since it is ultimately the feature maps at the output layer which are used for the final classification; our experimental results indicate that this is not the case.</p><p>Our basic network architecture will be similar to the standard one used in the CNN framework. Our additional deep feedback is brought in by associating a companion local output with each hidden layer. We may think of this companion local output as analogous to the final output that a truncated network would have produced. Backpropagation of error now proceeds as usual, with the crucial difference that we now backpropagate not only from the final layer but also simultaneously from our local companion output. The empirical result suggests the following main properties of the companion objective: (1) it acts as a kind of feature regularization (although an unusual one), which leads to significant reduction to the testing error but not necessarily to the train error; (2) it results in faster convergence, especially in presence of small training data (see <ref type="figure">Figure (</ref>2) for an illustration on a running example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Formulation</head><p>We focus on the supervised learning case and let S = {(X i , y i ), i = 1..N } be our set of input training data where sample X i ? R n denotes the raw input data and y i ? {1, .., K} is the corresponding groundtruth label for sample X i . We drop i for notational simplicity, since each sample is considered independently. The goal of deep nets, specifically convolutional neural networks (CNN) <ref type="bibr" target="#b11">[12]</ref>, is to learn layers of filters and weights for the minimization of classification error at the output layer. Here, we absorb the bias term into the weight parameters and do not differentiate weights from filters and denote a recursive function for each layer m = 1..M as:</p><formula xml:id="formula_0">Z (m) = f (Q (m) ), and Z (0) ? X,<label>(1)</label></formula><formula xml:id="formula_1">Q (m) = W (m) * Z (m?1) .<label>(2)</label></formula><p>M denotes the total number of layers; W (m) , m = 1..M are the filters/weights to be learned; Z (m?1) is the feature map produced at layer m ? 1; Q (m) refers to the convolved/filtered responses on the previous feature map; f () is a pooling function on Q; Combining all layers of weights gives</p><formula xml:id="formula_2">W = (W (1) , ..., W (M ) ).</formula><p>Now we introduce a set of classifiers, e.g. SVM (other classifiers like Softmax can be applied and we will show results using both SVM and Softmax in the experiments), one for each hidden layer,</p><formula xml:id="formula_3">w = (w (1) , ..., w (M ?1) ),</formula><p>in addition to the W in the standard CNN framework. We denote the w (out) as the SVM weights for the output layer. Thus, we build our overall combined objective function as:</p><formula xml:id="formula_4">w (out) 2 + L(W, w (out) ) + M ?1 m=1 ? m [ w (m) 2 + (W, w (m) ) ? ?] + ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">L(W, w (out) ) = y k =y [1? &lt; w (out) , ?(Z (M ) , y) ? ?(Z (M ) , y k ) &gt;] 2 +<label>(4)</label></formula><p>and</p><formula xml:id="formula_6">(W, w (m) ) = y k =y [1? &lt; w (m) , ?(Z (m) , y) ? ?(Z (m) , y k ) &gt;] 2 +<label>(5)</label></formula><p>We name L(W, w (M ) ) as the overall loss (output layer) and (W, w (m) ) as the companion loss (hidden layers), which are both squared hinge losses of the prediction errors. The above formulation can be understood intuitively: in addition to learning convolution kernels and weights, W , as in the standard CNN model <ref type="bibr" target="#b11">[12]</ref>, enforcing a constraint at each hidden layer for directly making a good label prediction gives a strong push for having discriminative and sensible features at each individual layer. In eqn. <ref type="formula" target="#formula_4">(3)</ref>, w (out) 2 and L(W, w (out) ) are respectively the margin and squared hinge loss of the SVM classifier (L2SVM 1 ) at the output layer (we omit the balance term C in front of the hinge for notational simplicity); in eqn. (4), w (m) 2 and (W, w (m) ) are respectively the margin and squared hinge loss of the SVM classifier at each hidden layer. Note that for each (W, w (m) ), the w (m) directly depends on Z (m) , which is dependent on W 1 , .., W m up to the mth layer. L(W, w (out) ) depends on w (out) , which is decided by the entire W. The second term in eqn.</p><p>(3) often goes to zero during the course of training; this way, the overall goal of producing good classification of the output layer is not altered and the companion objective just acts as a proxy or regularization. This is achieved by having ? as a threshold (a hyper parameter) in the second term of eqn. (3) with a hinge loss: once the overall value of the hidden layer reaches or is below ?, it vanishes and no longer plays role in the learning process. ? m balances the importance of the error in the output objective and the companion objective. In addition, we could use a simple decay function as ? m ? 0.1 ? (1 ? t/N ) ? ? m to enforce the second term to vanish after certain number of iterations, where t is the epoch step and N is the total number of epochs (wheather or not to have the decay on ? m might vary in different experiments although the differences may not be very big).</p><p>To summarize, we describe this optimization problem as follows: we want to learn filters/weights W for the entire network such that an SVM classifier w (out) trained on the output feature maps (that depend on those filters/features) will display good performance. We seek this output performance while also requiring some "satisfactory" level of performance on the part of the hidden layer classifiers. We are saying: restrict attention to the parts of feature space that, when considered at the internal layers, lead to highly discriminative hidden layer feature maps (as measured via our proxy of hidden-layer classifier performance). The main difference between eqn. (3) and previous attempts in layer-wise supervised training is that we perform the optimization altogether with a robust measure (or regularization) of the hidden layer. For example, greedy layer-wise pretraining was performed as either initialization or fine-tuning which results in some overfitting <ref type="bibr" target="#b0">[1]</ref>. The stateof-the-art benchmark results demonstrate the particular advantage of our formulation. As shown in <ref type="figure" target="#fig_3">Figure 2</ref>(c), indeed both CNN and DSN reach training error near zero but DSN demonstrates a clear advantage of having a better generalization capability.</p><p>To train the DSN model using SGD, the gradients of the objective function w.r.t the parameters in the model are:</p><formula xml:id="formula_7">?F ?w (out) =2w (out) ?2 y k =y [?(Z (M ) ,y)??(Z (M ) ,y k )][1?&lt;w (out) ,?(Z (M ) ,y)??(Z (M ) ,y k )&gt;]+ ?F ?w (m) = ?m 2w (m) ?2 y k =y [?(Z (m) ,y)??(Z (m) ,y k )][1?&lt;w (m) ,?(Z (m) ,y)??(Z (m) ,y k )&gt;]+ , otherwise 0, if w (m) 2 + (W,w (m) )??<label>(6)</label></formula><p>The gradient w.r.t W just follows the conventional CNN based model plus the gradient that directly comes from the hidden layer supervision.</p><p>Next, we provide more discussions to and try to understand intuitively about our formulation, eqn.</p><p>(3). For ease of reference, we write this objective function as</p><formula xml:id="formula_8">F (W) ? P(W) + Q(W),<label>(7)</label></formula><p>where P(W) ? w (out) 2 +L(W, w (out) ) and Q(W) ?</p><formula xml:id="formula_9">M ?1 m=1 ? m [ w (m) 2 + (W, w (m) )??] + .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Stochastic Gradient Descent View</head><p>We focus on the convergence advantage of DSN, instead of the regularization to the generalization aspect. In addition to the present problem in CNN where learned features are not always intuitive and discriminative <ref type="bibr" target="#b30">[31]</ref>, the difficulty of training deep neural networks has been discussed <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>As we can observe from eqn. <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref>, the change of the bottom layer weights get propagated through layers of functions, leading to exploding or vanishing gradients <ref type="bibr" target="#b21">[22]</ref>. Various techniques and parameter tuning tricks have been proposed to better train deep neural networks, such as pretraining and dropout <ref type="bibr" target="#b10">[11]</ref>. Here we provide a somewhat loose analysis to our proposed formulation, in a hope to understand its advantage in effectiveness.</p><p>The objective function in deep neural networks is highly non-convex. Here we make the following assumptions/observations: (1) the objective/energy function of DL observes a large "flat" area around the "optimal" solution where any result has a similar performance; locally we still assume a convex (or even ?-strongly convex) function whose optimization is often performed with stochastic gradient descent algorithm <ref type="bibr" target="#b2">[3]</ref>.</p><p>The definition of ?-strongly convex is standard: A function F (W) is ?-strongly convex if ?, W, W ? W and any subgradient g at W,  <ref type="figure" target="#fig_1">Figure (1.b)</ref>. In <ref type="bibr" target="#b20">[21]</ref>, a convergence rate is given for the M-estimators with locally convex function with compositional loss and regularization terms. Both terms in eqn. <ref type="bibr" target="#b7">(8)</ref> here refer to the same class label prediction error, a reason for calling the second term as companion objective. Our motivation is two-fold: (1) encourage the features learned at each layer to be directly discriminative for class label prediction, while keeping the ultimate goal of minimizing class label prediction at the output layer; (2) alleviate the exploding and vanishing gradients problem as each layer now has a direct supervision from the ground truth labels. One might raise a concern that learning highly discriminative intermediate stage filters may not necessarily lead to the best prediction at the output layer. An illustration can been seen in <ref type="figure" target="#fig_1">Figure (1.b</ref>). Next, we give a loose theoretical analysis to our framework, which is also validated by comprehensive experimental studies with overwhelming advantages over the existing methods.</p><formula xml:id="formula_10">F (W ) ? F (W)+ &lt; g, W ? W &gt; + ? 2 W ? W 2 ,<label>(8)</label></formula><p>Definition We name S ? (F ) = {W : F (W) ? ?} as the ?-feasible set for a function F (W) ? P(W) + Q(W).</p><p>First we show that a feasible solution for Q(W) leads to a feasible one to P(W). That is:</p><p>Lemma 1 ?m, m = 1..M ? 1, and m &gt; m if w (m) 2 + ((? <ref type="bibr" target="#b0">(1)</ref> , ..,? (m) ), w (m) ) ? ? then there exists (? (1) , ..,? (m) , ..,? (m ) ) such that w (m ) 2 + ((? <ref type="bibr" target="#b0">(1)</ref> , ..,? (m) ..,? (m ) ), w (m ) ) ? ?. 2</p><p>Proof As we can see from an illustration of our network architecture shown in <ref type="figure" target="#fig_1">fig. (1.a)</ref>, for ? (? <ref type="bibr" target="#b0">(1)</ref> , ..,? (m) ) such that ((? <ref type="bibr" target="#b0">(1)</ref> , ..,? (m) ), w (m) ) ? ?. Then there is a trivial solution for the network for every layer j &gt; m up to m , we let? (j) = I and w (j) = w (m) , meaning that the filters will be identity matrices. This results in ((? <ref type="bibr" target="#b0">(1)</ref> , ..,? (m) ..,? (m ) ), w (m ) ) ? ?.</p><p>Remark Lemma 1 shows that a good solution for Q(W) is also a good one for P(W), but it may not be the case the other way around. </p><formula xml:id="formula_11">W t+1 = ? W (W t ? ? t (?p t +?q t )) where E[?p t ] = gp t and E[?q t ] = gq t . If we use ? t = 1/(? 1 + ? 2 )t, then at time stamp T E[ W T ? W 2 ] ? 12G 2 (? 1 + ? 2 ) 2 T (9)</formula><p>Proof Since F (W) = P(W) + Q(W), it can be directly seen that</p><formula xml:id="formula_12">F (W ) ? F (W)+ &lt; gp + gq, W ? W &gt; + ? 1 + ? 2 2 W ? W 2 .</formula><p>Based on lemma 1 in <ref type="bibr" target="#b22">[23]</ref>, this upper bound directly holds.</p><p>Lemma 3 Following the assumptions in lemma 2, but now we assume ? t = 1/t since ? 1 and ? 2 are not always readily available, then started from</p><formula xml:id="formula_13">W 1 ? W 2 ? D the convergence rate is bounded by E[ W T ? W 2 ] ? e ?2?(ln T +0.578) D + (T ? 1)e ?2? ln(T ?1) G 2 (10) Proof Let ? = ? 1 + ? 2 , we have F (W ) ? F (W t ) ? &lt; gf t , W ? W t &gt; + ? 2 W ? W t 2 , and F (W ) ? F (W t ) ? ? 2 W t ? W 2 .</formula><p>Thus,</p><formula xml:id="formula_14">&lt; gf t , W t ? W &gt; ? ? W t ? W 2 Therefore, with ? t = 1/t, E[ W t+1 ? W 2 ] = E[ ? W (W t ? ? t? f t ) ? W 2 ] ? E[ W t ? ? t? f t ? W 2 ] = E[ W t ? W 2 ] ? 2? t E[&lt; gf t , W t ? W &gt;] + ? t E[ ? f t 2 ] ? (1 ? 2?/t)E[ W t ? W 2 ] + G 2 /t 2<label>(11)</label></formula><p>With 2?/t being small, we have 1 ? 2?/t ? e ?2?/t .</p><formula xml:id="formula_15">E[ W T ? W 2 ] ? e ?2?( 1 1 + 1 2 +,.., 1 T ) D + T ?1 t=1 G 2 t 2 e ?2?( 1 t + 1 t+1 +,.., 1 T ?1 ) = e ?2?(ln T +0.578) D + G 2 T ?1 t=1 e ?2 ln(t)?2?(ln(T ?1)?2? ln(t) ? e ?2?(ln T +0.578) D + (T ? 1)e ?2? ln(T ?1) G 2</formula><p>Theorem 1 Let P(W) be ? 1 -strongly convex and Q(W) be ? 2 -strongly convex near optimal W and denote W </p><formula xml:id="formula_16">T ?W 2 ] E[ W (F ) T ?W 2 ] = ?(1 + ? 2 2 ? 2 1</formula><p>), when ? t = 1/?t, and,</p><formula xml:id="formula_17">E[ W (P) T ?W 2 ] E[ W (F ) T ?W 2 ] =</formula><p>?(e ln(T )?2 ), when ? t = 1/t.</p><p>Proof Lemma 1 shows the compatibility of the companion objective of Q w.r.t the output objective P. The first equation can be directly derived from lemma 2 and the second equation can be seen from lemma 3. In general ? 2 ? 1 which leads to a great improvement in convergence speed and the constraints in each hidden layer also helps to learning filters which are directly discriminative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluate the proposed DSN method on four standard benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and SVHN. We follow a common training protocol used by Krizhevsky et al. <ref type="bibr" target="#b14">[15]</ref> in all experiments. We use SGD solver with mini-batch size of 128 at a fixed constant momentum value of 0.9. Initial value for learning rate and weight decay factor is determined based on the validation set. For a fair comparison and clear illustration of the effectiveness of DSN, we match the complexity of our model with that in network architectures used in <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b8">[9]</ref> to have a comparable number of parameters. We also incorporate two dropout layers with dropout rate at 0.5. Companion objective at the convolutional layers is imposed to backpropagate the classification error guidance to the underlying convolutional layers. Learning rates are annealed during training by a factor of 20 according to an epoch schedule determined on the validation set. The proposed DSN framework is not difficult to train and there are no particular engineering tricks adopted. Our system is built on top of widely used Caffe infrastructure <ref type="bibr" target="#b13">[14]</ref>. For the network architecture setup, we adopted the mlpconv layer and global averaged pooling scheme introduced in <ref type="bibr" target="#b19">[20]</ref>. DSN can be equipped with different types of loss functions, such as Softmax and SVM. We show performance boost of DSN-SVM and DSN-Softmax over CNN-SVM and CNN-Softmax respectively (see <ref type="figure" target="#fig_3">Figure (2.a)</ref>). The performance gain is more evident in presence of small training data (see <ref type="figure">Figure (</ref>2.b)); this might partially ease the burden of requiring large training data for DL. Overall, we observe state-of-the-art classification error in all four datasets (without data augmentation), 0.39% for MINIST, 9.78% for CIFAR-10, 34.57% for CIFAR-100, and 1.92% for SVHN (8.22% for CIFAR-10 with data augmentation). All results are achieved without using averaging <ref type="bibr" target="#b23">[24]</ref>, which is not exclusive to our method. <ref type="figure">Figure (3)</ref> gives an illustration of some learned features. We first validate the effectiveness of the proposed DSN on the MNIST handwritten digits classification task <ref type="bibr" target="#b16">[17]</ref>, a widely and extensively adopted benchmark in machine learning. MNIST dataset consists of images of 10 different classes (0 to 9) of size 28 ? 28 with 60,000 training samples and 10,000 test samples. <ref type="figure" target="#fig_3">Figure 2</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MNIST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CIFAR-10 and CIFAR-100</head><p>CIFAR-10 dataset consists of 32 ? 32 color images. A total number of 60,000 images are split into 50,000 training and 10,000 testing images. The dataset is preprocessed by global contrast normalization. To compare our results with the previous state-of-the-art, in this case, we also augmented the dataset by zero padding 4 pixels on each side, then do corner cropping and random flipping on the fly during training. No model averaging is done at the test phase and we only crop the center of <ref type="table">Table 1</ref>: MNIST classification result (without using data augmentation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Error(%) CNN <ref type="bibr" target="#b12">[13]</ref> 0.53 Stochastic Pooling <ref type="bibr" target="#b31">[32]</ref> 0.47 Network in Network <ref type="bibr" target="#b19">[20]</ref> 0.47 Maxout Networks <ref type="bibr" target="#b8">[9]</ref> 0.45 DSN (ours) 0.39 (a) by DSN (b) by CNN <ref type="figure">Figure 3</ref>: Visualization of the feature maps learned in the convolutional layer. a test sample. <ref type="table">Table (</ref>2) shows our result. Our DSN model achieved an error rates of 9.78% without data augmentation and 8.22% with data agumentation (the best known result to our knowledge).</p><p>DSN also provides added robustness to hyperparameter choice, in that the early layers are guided with direct classification loss, leading to a faster convergence rate and relieved burden on heavy hyperparameter tuning. We also compared the gradients in DSN and those in CNN, observing 4.55 times greater gradient variance of DSN over CNN in the first convolutional layer. This is consistent with an observation in <ref type="bibr" target="#b8">[9]</ref>, and the assumptions and motivations we make in this work. To see what the features have been learned in DSN vs. CNN, we select one example image from each of the ten categories of CIFAR-10 dataset, run one forward pass, and show the feature maps learned from the first (bottom) convolutional layer in <ref type="figure">Figure (3</ref> Method Error(%) Stochastic Pooling <ref type="bibr" target="#b31">[32]</ref> 42.51 Maxout Networks <ref type="bibr" target="#b8">[9]</ref> 38.57 Tree based Priors <ref type="bibr" target="#b26">[27]</ref> 36.85 Network in Network <ref type="bibr" target="#b19">[20]</ref> 35.68 DSN (ours) 34.57 CIFAR-100 dataset is similar to CIFAR-10 dataset, except that it has 100 classes. The number of images for each class is then 500 instead of 5, 000 as in CIFAR-10, which makes the classification task more challenging. We use the same network settings as in CIFAR- <ref type="bibr" target="#b9">10</ref>. <ref type="table">Table (</ref>2) shows previous best results and 34.57% is reported by DSN. The performance boost consistently shown on both CIFAR-10 and CIFAR-100 again demonstrates the advantage of the DSN method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Error(%) Stochastic Pooling <ref type="bibr" target="#b31">[32]</ref> 2.80 Maxout Networks <ref type="bibr" target="#b8">[9]</ref> 2.47 Network in Network <ref type="bibr" target="#b19">[20]</ref> 2.35 Dropconnect <ref type="bibr" target="#b18">[19]</ref> 1.94 DSN (ours) 1.92 <ref type="table">Table 3</ref>: SVHN classification error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Street View House Numbers</head><p>Street View House Numbers (SVHN) dataset consists of 73, 257 digits for training, 26, 032 digits for testing, and 53, 1131 extra training samples on 32 ? 32 color images. We followed the previous works for data preparation, namely: we select 400 samples per class from the training set and 200 samples per class from the extra set. The remaining 598,388 images are used for training. We followed <ref type="bibr" target="#b8">[9]</ref> to preprocess the dataset by Local Contrast Normalization (LCN). We do not do data augmentation in training and use only a single model in testing. <ref type="table">Table 3</ref> shows recent comparable results. Note that Dropconnect <ref type="bibr" target="#b18">[19]</ref> uses data augmentation and multiple model voting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we have presented a new formulation, deeply-supervised nets (DSN), attempting to make a more transparent learning process for deep learning. Evident performance enhancement over existing approaches has been obtained. A stochastic gradient view also sheds light to the understanding of our formulation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Network architecture for the proposed deeply-supervised nets (DSN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>after T iterations when applying SGD on F (W) and P(W) respectively. Then our deeply supervised framework in eqn. (3) improves the the speed over using top layer only by E[ W (P)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Classification error on MNIST test. (a) shows test error of competing methods; (b) shows test error w.r.t. the training sample size. (c) training and testing error comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) and (b) show results from four methods, namely: (1) conventional CNN with softmax loss (CNN-Softmax), (2) the proposed DSN with softmax loss (DSN-Softmax), (3) CNN with max-margin objective (CNN-SVM) , and (4) the proposed DSN with max-margin objective (DSN-SVM). DSN-Softmax and DSN-SVM outperform both their competing CNN algorithms (DSN-SVM shows classification error of 0.39% under a single model without data whitening and augmentation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 (</head><label>2</label><figDesc>b) shows classification error of the competing methods when trained w.r.t. varying sizes of training samples (26% gain of DSN-SVM over CNN-Softmax at 500 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 (</head><label>2</label><figDesc>c) shows a comparison of generalization error between CNN and DSN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>That is: a W that makes P(W) small may not necessarily produce discriminative features for the hidden layers to have a small Q(W). However, Q(W) can be viewed as a regularization term. Since P(W) observes a very flat area near even zero on the training data and it is ultimately the test error that we really care about, we thus only focus on the W, W , which makes both Q(W) and P(W) small. Therefore, it is not unreasonable to assume that + ?1 2 W ? W 2 , where gp and gq are the subgradients for P and Q at W respectively. It can be directly seen that F (W) is also strongly convex and for subgradient gf of F (W) at W, gf = gp+gq. Suppose E[ ?p t 2 ] ? G 2 and E[ ?q t 2 ] ? G 2 , and we use the update rule of</figDesc><table><row><cell>Lemma 2</cell></row></table><note>F (W) ? P(W) + Q(W) and P(W) share the same optimal W . Let P(W)) and P(W)) be strongly convex around W , W ? W 2 ? D and W ? W 2 ? D, with P(W ) ? P(W)+ &lt; gp, W ?W &gt; + ?1 2 W ?W 2 and Q(W ) ? Q(W)+ &lt; gq, W ?W &gt;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). Only the top 30% activations are shown in each of the feature maps. Feature maps learned by DSN show to be more intuitive than those by CNN.</figDesc><table><row><cell cols="2">CIFAR-10 classification error</cell><cell>CIFAR-100 classification error</cell></row><row><cell>Method</cell><cell>Error(%)</cell></row><row><cell>No Data Augmentation</cell><cell></cell></row><row><cell>Stochastic Pooling [32]</cell><cell>15.13</cell></row><row><cell>Maxout Networks [9]</cell><cell>11.68</cell></row><row><cell cols="2">Network in Network [20] 10.41</cell></row><row><cell>DSN (ours)</cell><cell>9.78</cell></row><row><cell>With Data Augmentation</cell><cell></cell></row><row><cell>Maxout Networks [9]</cell><cell>9.38</cell></row><row><cell>DropConnect [19]</cell><cell>9.32</cell></row><row><cell cols="2">Network in Network [20] 8.81</cell></row><row><cell>DSN (ours)</cell><cell>8.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Method comparison on CIFAR-10 and CIFAR-100 test data.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It makes negligible difference between L1SVM and L2SVM.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that we drop the W (j) , j &gt; m since the filters above layer m do not participate in the computation for the objective function of this layer.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgments</head><p>This work is supported by NSF award IIS-1216528 (IIS-1360566) and NSF award IIS-0844566 (IIS-1360568). We thank Min Lin, Naiyan Wang, Baoyuan Wang, Jingdong Wang, Liwei Wang, and David Wipf for help discussions. We are greatful for the generous donation of the GPUs by NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">D</forename><surname>Montral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qubec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Online algorithms and stochastic approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Tran. on Audio, Speech, and Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Understanding deep architectures using a recursive convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.1847v2</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed representations, simple recurrent networks, and grammatical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="195" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTAT</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Maxout networks. In ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
	</analytic>
	<monogr>
		<title level="m">CoRR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale learning with svm and convolutional for generic object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tiled convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Regularized m-estimators with nonconvexity : statistical and algorithmic theory for local optima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1305.2436v1</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5063v2</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonparametric guidance of autoencoder representations using label information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2567" to="2588" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discriminative transfer learning with tree-based priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning using linear support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Representational Learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The nature of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>arXiv 1311.2901</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
