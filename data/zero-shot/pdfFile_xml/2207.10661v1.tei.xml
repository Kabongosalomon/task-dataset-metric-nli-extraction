<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">In Defense of Online Models for Video Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihao</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>3 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>3 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">In Defense of Online Models for Video Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video instance segmentation</term>
					<term>Online model</term>
					<term>Contrastive learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, video instance segmentation (VIS) has been largely advanced by offline models, while online models gradually attracted less attention possibly due to their inferior performance. However, online methods have their inherent advantage in handling long video sequences and ongoing videos while offline models fail due to the limit of computational resources. Therefore, it would be highly desirable if online models can achieve comparable or even better performance than offline models. By dissecting current online models and offline models, we demonstrate that the main cause of the performance gap is the error-prone association between frames caused by the similar appearance among different instances in the feature space. Observing this, we propose an online framework based on contrastive learning that is able to learn more discriminative instance embeddings for association and fully exploit history information for stability. Despite its simplicity, our method outperforms all online and offline methods on three benchmarks. Specifically, we achieve 49.5 AP on YouTube-VIS 2019, a significant improvement of 13.2 AP and 2.1 AP over the prior online and offline art, respectively. Moreover, we achieve 30.2 AP on OVIS, a more challenging dataset with significant crowding and occlusions, surpassing the prior art by 14.8 AP. The proposed method won first place in the video instance segmentation track of the 4th Large-scale Video Object Segmentation Challenge (CVPR2022). We hope the simplicity and effectiveness of our method, as well as our insight on current methods, could shed light on the exploration of VIS models. The code is available at https://github.com/wjf5203/VNext.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video instance segmentation aims at detecting, segmenting, and tracking object instances simultaneously in a given video. It attracted considerable attention after first defined <ref type="bibr" target="#b45">[46]</ref> in 2019 due to the huge challenge and the wide applications in video understanding, video editing, autonomous driving, augmented reality,  <ref type="figure">Fig. 1</ref>. Oracle experiments on SOTA offline methods: We analyze current VIS methods and visualize some results here. More results can be found in Sec. 4.3. 'YTVIS' is short for YouTube-VIS 2019. 'Frame' and 'clip' stand for frame oracle and clip oracle experiments, respectively. For frame oracles, we provide the ground-truth instance ID both within each clip and between adjacent clips. Thus the performance only depends on the quality of the estimated segmentation masks. For clip oracles, we only provide the ground-truth instance ID between adjacent clips, and the method is required to do association within the clips by itself. Therefore the gaps between frame oracles and clip oracles show the effect of the black-box association done within current offline models. When clip length is 1, the method is doing per-frame segmentation.</p><p>etc. Current VIS methods can be categorized as online or offline methods. Online methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b17">18]</ref> take as input a video frame by frame, detecting and segmenting objects per frame while tracking instances and optimizing results across frames. Offline methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">41]</ref>, in contrast, take the whole video as input and generate the instance sequence of the entire video with a single step. Despite high performance, the requirement of full videos limits the application scenarios of offline models, especially for scenarios involving long video sequences (videos exceed 50 frames on GPU of 32G RAM <ref type="bibr" target="#b40">[41]</ref>) and ongoing videos (e.g., videos in autonomous driving and augmented reality). However, online models are usually inferior to the contemporaneous offline models by over 10 AP, which is a huge drawback. Previously, little work tries to explain the performance gap between these two paradigms, or gives insight into the high performance of the offline paradigm. A common attempt of the latter is made from the inherent advantages of offline models being able to skip the erroraccumulating tracking steps <ref type="bibr" target="#b20">[21]</ref> and utilize the richer information provided by multiple frames to improve segmentation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b38">39]</ref>. However, does that really explain the high performance of current offline methods? What's the main problem causing the poor performance of online models? Can online models achieve performance comparable to, or even better than, SOTA offline ones?</p><p>To deeply understand the performance of both online and offline models, we analyze in detail three SOTA methods (offline: IFC <ref type="bibr" target="#b15">[16]</ref> and SeqFormer <ref type="bibr" target="#b40">[41]</ref>, online: CrossVIS <ref type="bibr" target="#b46">[47]</ref>) on two datasets (YouTube-VIS <ref type="bibr" target="#b45">[46]</ref> and OVIS <ref type="bibr" target="#b30">[31]</ref>) that have different difficulty levels. 'Simple video' refers to the video in YouTube-VIS. These videos are much shorter and only contain very slight occlusions, simple motions, and smooth changes in illumination and object shapes. 'Complex video' refers to the video in OVIS. Please see Sec. 4.1 for details. The results ( <ref type="figure">Fig. 1</ref>) of our oracle experiments give us a deep understanding of current SOTA methods:</p><p>From the perspective of instance segmentation, per-clip segmentation doesn't outperform per-frame segmentation a lot in mask quality, and mask quality is also not the reason for the poor performance of online methods: CrossVIS even outperforms its contemporaneous work (i.e. IFC) in frame oracle experiments on both datasets (results in <ref type="table">Table 1</ref>). What's more, per-clip segmentation of current SOTA methods is not effective and robust. Multiple frames do provide more information and improve the mask quality by 3.7 AP for IFC on YouTube-VIS ( <ref type="figure">Fig. 1</ref>). But it only works for some cases: per-clip segmentation doesn't improve the performance of SeqFormer a lot. In addition, when testing on more challenging datasets like OVIS, segmentation on multiple frames even degrades the performance by 1.8 and 2.2 AP on IFC and SeqFormer respectively when clip size becomes longer. Although in theory, per-clip segmentation has its inherent advantage to using multiple frames, it still requires further exploration, especially in how to utilize information in multiple frames and how to handle complex motion patterns, occlusion, and object deformation. Currently, we don't see an obvious gap between the mask qualities of per-clip and per-frame segmentation.</p><p>From the perspective of association, a huge advantage of the offline methods is their ability to avoid the use of hand-designed association modules. It works well on simple cases of the YouTube-VIS dataset. We demonstrate that it is the main reason causing the performance gap between the current online and offline paradigms. However, this black-box association process done within offline models also gets worse rapidly when the video becomes complex (degrades the performance of IFC by 12.3 AP and SeqFormer by 20.9 AP on OVIS). In addition, when handling longer videos, e.g. videos in the real world or from OVIS dataset, offline methods require splitting the input video into clips to avoid exceeding computational limits, and thus hand-designed clip matching is still inevitable, which will further decrease the overall performance. To sum up, matching/association is the main reasoning for the performance gap, and it is still inevitable and of great importance for offline models.</p><p>To improve the matching performance and thus bridge the performance gap, we propose a framework In Defense of OnLine models for video instance segmentation, termed IDOL. The key idea is to ensure, in the embedding space, the similarity of the same instance across frames and the difference of different instances in all frames, even for instances that belong to the same category and have similar appearances. It provides more discriminative instance features with better temporal consistency, which guarantees more accurate association results. However, previous method <ref type="bibr" target="#b29">[30]</ref> selects positive and negative samples by a hand-craft setting, introducing false positives in occlusions and crowded scenes, thus impairing contrastive learning. To address it, we formulate the problem of sample selection as an Optimal Transport problem in Optimization Theory, which reduces false positives and further improves the quality of the embedding. During inference, by using one-to-many temporally weighted softmax, we utilize the learned prior of the embedding to re-identify missing instances caused by occlusions and to enforce the consistency and integrality of associations.</p><p>Our thorough analysis gives us a deep understanding of current online and offline VIS methods. Based on our observation, we bridge the performance gap from the perspective of feature embeddings and propose IDOL. We conduct extensive experiments on YouTube-VIS 2019, YouTube-VIS 2021, and OVIS datasets. Despite its simplicity, our method sets a new state-of-the-art achieving 64.3 AP, 56.1 AP, and 42.6 AP on the validation set of these three benchmarks, respectively. More importantly, compared with previous online methods, we achieve a consistent improvement ranging from 13.2 AP to 14.7 AP on these datasets. We even surpass the previous SOTA offline method by up to 2.1 AP. We believe the simplicity and effectiveness of our method shall benefit further research. In addition, our thorough analysis provides insights for current methods and suggests useful directions for future work in both online and offline VIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Online Video Instance Segmentation. Most online VIS methods are built upon image-level instance segmentation with an additional tracking head to associate instances across the video. The baseline method MaskTrack R-CNN <ref type="bibr" target="#b45">[46]</ref> is built upon Mask R-CNN and proposes to leverage multi cues such as appearance similarity, semantic consistency, spatial correlation, and detection confidence to determine the instance labels. Most online methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b16">17]</ref> follow this pipeline. CrossVIS <ref type="bibr" target="#b46">[47]</ref> proposes a new learning scheme that uses the instance feature in the current frame to segment the same instance in another frame. Multi-Object Tracking and Segmentation (MOTS) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref> aims to simultaneously segment and track all object instances of a given video sequence in real-time, which is similar to online VIS. MOTS methods are usually built upon multiple object trackers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b17">18]</ref>. Track R-CNN <ref type="bibr" target="#b36">[37]</ref> firstly extends the popular task of multi-object tracking to MOTS based on Mask R-CNN <ref type="bibr" target="#b12">[13]</ref>. PointTrack <ref type="bibr" target="#b43">[44]</ref> proposes a new online tracking-by-points paradigm with learning discriminative instance embeddings. Trackformer <ref type="bibr" target="#b25">[26]</ref> adopts the transformer architecture for MOTS and introduces track query embeddings that follow objects through a video sequence. Online models have a wider range of application scenarios, however, they are usually inferior to offline art by over 10 AP. We find that the current SOTA online models fail to achieve accurate associations, causing the performance gap. We aim to tackle this problem in this work. Offline Video Instance Segmentation. Offline methods for VIS take the whole video as input and predict the instance sequence of the entire video (or video clip) with a single step. MaskProp <ref type="bibr" target="#b2">[3]</ref> and Propose-Reduce <ref type="bibr" target="#b20">[21]</ref> perform mask propagation in a video clip to improve mask and association. However, the propagation process is time-consuming, which limits its application. Recently, VisTR <ref type="bibr" target="#b37">[38]</ref> adopts the transformer <ref type="bibr" target="#b35">[36]</ref> to VIS and models the instance queries for the whole video. However, it learns an embedding for each instance of each frame, which makes it hard to apply to longer videos and more complex scenes. IFC <ref type="bibr" target="#b15">[16]</ref> proposes inter-frame communication transformers and significantly reduces computation and memory usage. SeqFormer <ref type="bibr" target="#b40">[41]</ref> dynamically allocates spatial attention on each frame and learns a video-level instance embedding, which greatly improves the performance. We deeply analyze the current SOTA offline models, IFC and SeqFormer, find that their improvement mainly comes from the black-box association between frames, but this advantage is gradually lost in complex scenarios. In contrast, our online method can be applied to both ongoing and long videos and complex scenarios, with more stable association quality and higher performance. Contrastive learning has made significant progress in representation learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b33">34]</ref>. MOCO <ref type="bibr" target="#b11">[12]</ref> and SimCLR <ref type="bibr" target="#b5">[6]</ref> use contrastive learning for image-level self-supervised training and learn strong feature representations for downstream tasks. Some methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref> extend the contrastive learning into multiple positive samples format to obtain better feature representations. We absorb ideas from contrastive learning and propose to learn contrastive embeddings between frames for each instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Given a video clip that consists of multiple image frames, online VIS models <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref> utilize additional association head upon on instance segmentation models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref>. We have already discussed that achieving more stable and discriminative instance embeddings between frames is the key to improve the performance of online models. To achieve this, we propose a contrastive learning framework to extract more discriminative features for instance association. We first introduce the instance segmentation pipeline in Sec. 3.1. Then the details of our contrastive learning framework and the cross-frame instance association strategy are introduced in Sec. 3.2 and Sec. 3.3 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Instance Segmentation.</head><p>For fair comparisons with the state-of-the-art offline method <ref type="bibr" target="#b40">[41]</ref>, we take De-formableDETR <ref type="bibr" target="#b50">[51]</ref> with dynamic mask head <ref type="bibr" target="#b34">[35]</ref> as our instance segmentation pipeline in this paper. Our method can be coupled with other instance segmentation methods with minor modifications.</p><p>Given an input frame x ? R 3?H?W of a video, a CNN backbone extracts multi-scale feature maps. The Deformable DETR module takes the feature maps with additional fixed positional encodings <ref type="bibr" target="#b4">[5]</ref> and N learnable object queries as input. The object queries are first transformed into output embeddings E ? R N ?C by the transformer decoder. After that, they are decoded into box coordinates and class labels by 3-layer feed-forward network (FFN) separately. For per-frame mask generation, we employ an FPN-like <ref type="bibr" target="#b21">[22]</ref> mask branch to make the use of multi-scale feature maps from transformer encoder and generate feature map F mask that are 1/8 resolution of the input frame. Another FFN encode</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Head</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer</head><p>! "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Learning</head><p>Optimal Transport</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask Head</head><p>? Box Head</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Head</head><p>Key Frame</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference Frame</head><p>Object Queries shared <ref type="figure">Fig. 2</ref>. The training pipeline of IDOL. Given a key frame and a reference frame, the shared-weight backbone and transformer predict the instance embeddings on them respectively. The embeddings on the key frame are used to predict masks, boxes, and categories, while the embeddings on the reference frame are selected as positive and negative embeddings dynamically by our optimal transport progress. Embeddings of the same color belong to the same video instance. Best viewed in color.</p><p>outputs embeddings into parameters ? of mask head, which performs three-layer 1 ? 1 convolution on the given feature map F mask :</p><formula xml:id="formula_0">m i = MaskHead(F mask , ? i ).<label>(1)</label></formula><p>Then we calculate pair-wise matching cost which takes into account both the class prediction and the similarity of predicted and ground truth boxes. For each ground truth, we assign multi predictions to it by selecting the top k predictions with the least cost by an optimal transport method <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Finally, the whole model is optimized with a multi-task loss function</p><formula xml:id="formula_1">L = L cls + ? 1 L box + ? 1 L mask + ? 2 L embed ,<label>(2)</label></formula><p>where loss weights ? 1 and ? 2 are set to 2.0 and 1.0 by default. For L box , we use a combination of L 1 loss and the generalized IoU loss <ref type="bibr" target="#b31">[32]</ref>. The L mask is defined as a combination of the Dice loss <ref type="bibr" target="#b27">[28]</ref> and Focal loss <ref type="bibr" target="#b22">[23]</ref>. L embed is the contrastive loss described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contrastive Learning between Frames.</head><p>More discriminative feature embeddings can help distinguish instances on different frames, thereby improving the quality of cross-frame association. To this end, we introduce contrastive learning between frames to make the embedding of the same object instance closer in the embedding space, and the embedding of different object instances farther away. Object queries are used to query the features of instances from each frame in our instance segmentation pipeline. Therefore, the output embeddings can be regarded as features of different instances. We employ an extra light-weighted FFN as a contrastive head to decode the contrastive embeddings from the instance features.</p><p>Given a key frame for instance segmentation training, we select a reference frame from the temporal neighborhood. The instances appearing on the key frame may have different positions and appearances on the reference frame, but their contrastive embeddings should be as close as possible in embedding space. For each instance in the key frame, we send the output embedding with the lowest cost to the contrastive head and get the contrastive embedding v. Different from previous method <ref type="bibr" target="#b29">[30]</ref>, which selects positive and negative samples by a handcraft setting, if the same instance appears on the reference frame, we take top m1 predictions with the least cost as positive and top m2 predictions with the highest cost as negatives. m1 and m2 are calculated dynamically by the optimal transport method <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Please refer to the supplementary for more details. The contrastive loss function for a positive pair of examples is defined as follows:</p><formula xml:id="formula_2">L embed = ? log exp(v ? k + ) exp(v ? k + ) + k ? exp(v ? k ? ) = log[1 + k ? exp(v ? k ? ? v ? k + )],<label>(3)</label></formula><p>where k + and k ? are positive and negative feature embeddings from the reference frame, respectively. We extend Eq. 3 to multiple positive scenarios:</p><formula xml:id="formula_3">L embed = log[1 + k + k ? exp(v ? k ? ? v ? k + )].<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Instance Association.</head><p>Previous online methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref> take semantic consistency, spatial correlation, and detection confidence as cures. They are then leveraged to determine the instance labels. Other clip-based nearly online methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">16]</ref> match instances using the predicted masks of overlapping frames by masking soft IoU metric between clips. However, the online models perform instance segmentation on each frame independently, and therefore, the prediction quality on each frame is unstable. What makes it worse is the complex motion patterns, severe occlusions, false positives, duplicate predictions, error accumulation in long videos, and the frequently disappear and reappear objects, which makes instance association very challenging. Therefore, a strong instance association method should be robust to these cases. To this end, we propose a temporally weighted softmax score for instance matching and a memory bank-based association strategy to address these problems and improve the association quality of the online model. Temporally Weighted Softmax. Considering scenarios with fast motion, occlusion, and crowded objects, box-based matching introduces wrong association due to ambiguous location priors. However, the contrastive embedding learned by our method is able to maintain discriminative in embedding space when the position and shape change. Therefore, the contrastive embeddings are used to calculate the embedding similarity between the instances on the current and the previous frames. Assume there are N predicted instances with N contrastive embeddings d i ? R C , and M instances in the memory bank, each of which has multiple temporal contrastive embeddings {e t j } T t=1 , e t j ? R C from previous T frames. These embeddings are combined by a temporally weighted sum:</p><formula xml:id="formula_4">e j = T t=1 e t j ? (? + T /t) T t=1 ? + T /t .<label>(5)</label></formula><p>Then we compute bi-directional similarity f between predicted instance i and memory instance j by:</p><formula xml:id="formula_5">f(i, j) = [ exp(? j ? d i ) + ? j M k=1 exp(? k ? d i ) + ? k + exp(? j ? d i ) N k=1 exp(? j ? d k ) ]/2,<label>(6)</label></formula><p>where ? j is the existing time of instance j in the memory, it serves as the confidence scores of each instance in the memory. By introducing the temporal contrastive embeddings and the confidence scores determined by the duration of existence, the learned prior information is able to reidentify missing instances caused by occlusions, enforcing the consistency and integrality of associations. Association Strategy. To take full advantage of the learned contrastive embedding, we propose a new association strategy during inference. Given a test video, we initialize an empty memory bank for it and perform instance segmentation on each frame sequentially in an online scheme. For the prediction of each frame, we first perform inter-class duplicate removal by NMS with a threshold of 0.5. Then we compute matching scores f(i, j) between predictions and memory bank by Eq. 6, and search for the best assignment for instance i by:</p><formula xml:id="formula_6">j = arg max f(i, j), ?j ? {1, 2, ..., M }.<label>(7)</label></formula><p>If f(i,?) &gt; 0.5, we assign the instance i on current frame to the memory instanc? j. For the prediction without an assignment but has a high class score, we start a new instance ID in the memory bank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Metrics</head><p>We report our results on YouTube-VIS 2019 <ref type="bibr" target="#b45">[46]</ref>, YouTube-VIS 2021 <ref type="bibr" target="#b42">[43]</ref>, and OVIS <ref type="bibr" target="#b30">[31]</ref>  77s on average, and more importantly, it contains much more videos that record objects with severe occlusion, complex motion patterns, and rapid deformation. All these features make OVIS an ideal dataset to evaluate and analyze different methods. We report standard metrics such as AP, AP 50 , AP 75 , AR 1 , and AR 10 . IoU threshold is used during evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Model settings. We use ResNet-50 <ref type="bibr" target="#b13">[14]</ref> as our backbone unless otherwise specified. For a fair comparison with SOTA offline method, we use the same setting for Deformable DETR and the dynamic mask head following SeqFormer <ref type="bibr" target="#b40">[41]</ref>. For the transformer, we use 6 encoders, 6 decoder layers of width 256 with bounding box refinement mechanism, and the number of object queries is set to 300.</p><p>Training. We use AdamW <ref type="bibr" target="#b24">[25]</ref> optimizer with base learning rate of 1 ? 10 ?4 , and weight decay of 10 ?4 . We first pre-train the model on COCO for instance segmentation following previous work <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b40">41]</ref>. Then we train our model for 12000 iterations, on the corresponding training set and reduce learning rate by a factor of 10 at at the 8000 iterations. For the result with superscript " ?", we randomly and independently crop the image from COCO twice to form a pseudo key-reference frame pair, which is used to pre-train the contrastive embedding of our models before training on video datasets. For YouTube-VIS 2019 and YouTube-VIS 2021, the input frames are downsampled and randomly cropped so that the longest side is at most 768 pixels. For OVIS, we use the same scale augmentation with COCO, resizing the input images so that the shortest side is at least 480 and at most 800 pixels while the longest is at most 1333. The model is trained on 8 V100 GPUs of 32G RAM, with 2 pairs of frames per GPU. In order to provide more positive embeddings for contrastive training, we adopt optimal transport theory <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> to assign a ground truth label to multiple predictions. Please refer to the supplementary for more details about OT.</p><p>Inference. During inference, the input frames are downscaled to 360p for YouTube-VIS 2019 and YouTube-VIS 2021 following previous work, and 720p for OVIS as its videos has a higher resolution. For the hyper-parameters of temporally weighted softmax, we set ? = 0.5 and T = 3 by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of Current SOTA VIS Models</head><p>Since no annotation for the validation set is available, we split the original training set into custom training split and validation split. All models are trained on the training split and evaluated on the validation split. YouTube-VIS 2019 and OVIS are used. We analyze the results as follows: Performance gaps between online and offline models: First, we compare the mask quality of two recent online and offline methods in <ref type="table">Table.</ref> 1. They are both published in 2021 thus can be considered as work in the same period. When ground-truth instance ID is provided (the column of 'frame oracle'), CrossVIS outperforms IFC on both YouTube-VIS and OVIS. However, when the instance ID is not provided (the column of 'predicted'), the methods are required to match the results, and the performance of CrossVIS drops dramatically by 9.4 AP while the performance of IFC drops by 3.3 AP on YouTube-VIS, leading to the poor performance of CrossVIS. Offline methods enable the model to match predicted masks by itself and avoid using hand-designed association modules. It works well on simple datasets and benefits current offline models, but it still fails on challenging datasets like OVIS. <ref type="table">Table 1</ref>. Oracle experiments on association quality. Frame oracle means gt instance ID is provided (same to <ref type="figure">Fig. 1</ref>). We set the clip length of IFC to 10. AP is reported. Analysis of current offline models: In <ref type="table">Table.</ref> 2, we give detailed analyses for SOTA offline models, hoping to provide insights for future research. Compared with online methods, offline models in theory have two inherent advantages: First, as we mentioned above, it avoids hand-designed association. However, this step is very sensitive to the occlusion and the complexity of videos, especially when the clip becomes longer. For example, when clip length is equal to 5, the black-box association degrades the performance of IFC and SeqFormer on OVIS by 3.4 AP (25.9 AP vs. 22.5 AP) and 6.8 AP (31.8 AP vs. 25.0 AP), respectively. When clip length is set to 30, the performance drops by 12.3 AP (22.8 AP vs. 10.5 AP) and 20.9 AP (29.8 AP vs. 8.9 AP), respectively. What's more, even a clip length of 30 still doesn't meet the requirement of real-world application. Clip matching is still inevitable, and it further decreases the overall performance.</p><p>Another inherent advantage of offline models is the ability to use multiple frames for instance segmentation, which provides more information to handle occlusion and optimize the results. However, current models still fail to fully utilize this feature. Currently, it only works for simple videos: compared with per-frame segmentation (clip length=1), it improves the mask quality by at most 3.7 AP for IFC (when clip length=30) and 0.3 AP for SeqFormer (when clip length=3). When testing on OVIS, multiple frames segmentation only improves the mask quality by at most 1.3 AP for IFC (clip length=5) and 0.3 AP for SeqFormer (clip length=3), and even degrades the performance by 1.8 AP for IFC and 2.2 AP for SeqFormer when clip size becomes longer (clip length=30). What's more, the improvement is even less obvious in practice when no groundtruth instance ID is provided, even for simple videos, due to the association problem. It only improves the performance of IFC on YouTube-VIS by 2.0 AP, but degrades the performance in all the other experiments.  <ref type="figure">Fig. 3</ref>. Oracle experiments on IDOL and SeqFormer. To prove the effectiveness of our method, we further analyze IDOL with oracle experiments in <ref type="figure">Fig.3</ref>. Since IDOL is an online model, the results of frame oracles with different clip lengths are the same as per-frame segmentation oracle results. For clip oracles, IDOL is required to do association within the clips by itself. Compared with SeqFormer, the gaps between frame oracles and clip oracles of IDOL are much smaller on OVIS, proving that IDOL performs a more robust association between frames than the offline model on challenging datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>We compare IDOL against current online and offline SOTA methods on YouTube-VIS 2019, YouTube-VIS 2021, and OVIS validation sets. The results are reported in <ref type="table" target="#tab_5">Tables 3, 4</ref>, and 5, respectively. We compare the methods with different backbones <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref> for a fair comparison. Notably, our method significantly surpasses all previous online methods by at least 10.1 AP. In addition, we also outperform all previous offline methods under all evaluation metrics when training on the same data. More importantly, our method achieves an overall first place <ref type="bibr" target="#b39">[40]</ref> in the YouTube-VIS Challenge 2022, which proves our SOTA performance. In <ref type="table">Table 3</ref>. Comparison on YouTube-VIS 2019 val set. The best results with the same backbone are in bold and second underline. 'V' means only YouTube-VIS training set is used. 'V+I' means synthetic videos from COCO with overlapping categories are also used for joint training. Note that offline models take advantage of larger batch sizes thus having much bigger FPS, while online models handle video frame by frame. The result with superscript " ?" is obtained by pre-training on COCO pseudo key-reference frame pairs, and resolution of 480p is used during inference. addition, our method only decreases the inference speed of the adopted instance segmentation pipeline by 1.1 FPS on an RTX-2080Ti, which proves our efficiency. In general, our method is simple and very effective compared with all baseline methods. Qualitative results on sample videos of the challenging OVIS dataset are shown in <ref type="figure" target="#fig_0">Fig. 4</ref>. More qualitative results can be found in the supplementary. We analyze the performance in detail as follows:</p><p>YouTube-VIS 2019: It is the most commonly used dataset. We reported the results in <ref type="table">Table 3</ref>. When using the same backbones, IDOL significantly outperforms previous online methods by 10.1 AP (on ResNet-50) and 10.6 AP (on ResNet-101). Compared with offline methods, we achieve better performance when no extra data is used for training, surpassing previous methods by 1.3 AP and 2.2 AP with ResNet-50 and Swin-L as the backbone, respectively. For the result with superscript " ?", we randomly crop images from COCO twice to form pseudo key-reference frame pairs to pre-train the contrastive embedding part of our model before training on real video datasets. It improves the performance by  <ref type="table">Table 5</ref>. Comparison on OVIS 2021 val set. Best in bold and second underline. The results with superscript " ?" are not reported in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref>. Videos in OVIS are much longer than those in YTVIS, offline models are unable to take the entire video as input due to the limit of computational resources. We split the video into clips of length 10 and perform clip matching provided by <ref type="bibr" target="#b15">[16]</ref> on overlapping frames to get the final results. 3.1 AP on ResNet-50 and 2.8 AP on Swin-L backbone, outperforming previous SOTA models comprehensively. YouTube-VIS 2021: It is an extended version of YouTube-VIS 2019. It contains more videos with a larger number of instances and frames. As shown in <ref type="table" target="#tab_5">Table 4</ref>, we achieve 43.9 AP with a ResNet-50 backbone, surpassing the previous best online method and offline method by 9.7 AP and 3.4 AP, respectively. OVIS: As mentioned before, OVIS contains long video sequences with heavy occlusion and complex motion, thus it is extremely difficult for all algorithms and exceeds the capability of offline methods due to the limit of computational resources. STEm-Seg <ref type="bibr" target="#b0">[1]</ref> is the only offline method that can be directly evaluated on OVIS since its design enables it to run in a nearly online manner. To compare with the SOTA offline methods (e.g. IFC <ref type="bibr" target="#b15">[16]</ref> and SeqFormer <ref type="bibr" target="#b40">[41]</ref>), we split the video into short clips and apply the clipping matching method provided in IFC on these two methods (SeqFormer doesn't provide its matching method). The results are provided in <ref type="table">Table 5</ref>. Note that the previous best method only gains 15.4 AP on the validation set. IDOL with the same ResNet-50 backbone achieves What's more, when using a stronger backbone (i.e., Swin-L) to extract better features, IDOL achieves the state-of-the-art performance of 42.6 AP, which is a huge improvement over previous best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>In this section, we conduct extensive ablation experiments to study the importance of the core factors of our method on YouTube-VIS 2019 and OVIS. Previous SOTA online method <ref type="bibr" target="#b46">[47]</ref> uses an extra M-class classification head where M equals to the number of all instances in the training set, termed as "ID Head" in <ref type="table">Table 6</ref>. and 7. In the "Contrastive" setting, we use box IoU between predictions and ground truth for positive and negative embeddings selection following <ref type="bibr" target="#b29">[30]</ref>. We further evaluate our optimal transport method to dynamically select positive and negative embeddings, termed as "OT". For ablation study on inference strategy, "multi-cues" setting combines semantic consistency, spatial correlation, detection confidence and appearance similarity together to perform association following <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>. Our association strategy is termed "embedding". Contrastive Training. To evaluate the importance of our contrastive embeddings, we apply the same association method on the embeddings predicted by ID Head and contrastive head. As shown in <ref type="table">Table 6</ref>, contrastive training only improves 1.5 AP with the multi-cues association but improves 8.9 AP when it comes to the embedding-based association. Our explanation is that contrastive training provides more discriminative embeddings for instance association, but other cues in multi-cues weaken the role of embeddings. In addition, on the more challenging OVIS dataset, contrastive embeddings increase AP from 11.0 to 18.4, which brings an improvement of 67.3%. This indicates that embedding-based association is more robust in longer videos and complex scenarios. Furthermore, optimal transport matching (OT) improves the results by 2.3 AP on YTVIS and 2.3 AP on OVIS, which indicates that the choice of positive and negative embeddings plays an important role in learning discriminative embeddings. OT provides a better selection of positive and negative embeddings during training, improving the quality of embeddings. We show the visualization of positive and negative embeddings selected by these two strategies in supplementary. Association Strategy and Temporally Weighted Softmax. As shown in <ref type="table">Table 6</ref>, compared with "multi-cues", our embedding association strategy takes advantage of the discriminative embedding learned by contrastive learning, and improves the AP from 31.8 to 42.5 on YouTube-VIS. When it comes to OVIS in <ref type="table">Table 7</ref>, our association strategy also improves the AP from 18.4 to 26.7. In addition, when temporally weighted softmax is added, it can be further improved by 1.6 AP on YouTube-VIS and 1.9 AP on OVIS. Utilizing information and priors from multiple previous frames improve robustness of association. Considering the problem of false positives and disappearing-and-reappearing that the online model needs to deal with, we believe this strategy helps maintain temporal consistency. We provide more visualization results, detailed analysis, and additional ablation experiments on OVIS in supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Online video instance segmentation methods have their inherent advantage in handling long/ongoing videos, but they are inferior to the offline models in performance. In this work, we aim to bridge the performance gap. We first deeply analyze the current online and offline models and find that the gap mainly comes from the error-prone association between frames. Based on this observation, we propose IDOL, which enables models to learn more discriminative and robust instance features for VIS tasks. It significantly outperforms all online and offline methods and achieves new SOTA on three benchmarks. We believe our insights on VIS methods will inspire future work in both online and offline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Qualitative Results</head><p>In this section, we show several qualitative results on the validation sets of YouTube-VIS and OVIS to demonstrate the following advantages of IDOL:</p><p>-For instances that belong to the same category and have very similar appearances, our contrastive learning enables IDOL to segment and track these instances more accurately. (e.g. <ref type="figure">Fig. 5</ref>) -Our method learns embedding with better temporal consistency, benefiting the tracking in videos with high-speed, large, and/or complex motions. (e.g. <ref type="figure">Fig. 6</ref>) -With the help of more stable and discriminative embeddings, as well as our one-to-many temporally weighted softmax during inference, IDOL is more robust when handling crowded scenes with heavy occlusions and frequent position exchanges. (e.g. <ref type="figure">Fig. 7</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Optimal Transport</head><p>Given a ground truth bounding box of an instance, the IoU-based method selects positive and negative samples by a hand-craft IoU threshold setting. A predicted box is defined as positive to an instance if they have an IoU higher than 0.7, or negative if they have an IoU lower than 0.3, which introduces false positives in occlusions and crowded scenes. As shown in <ref type="figure" target="#fig_2">Fig. 8 (a)</ref>, in the case of occlusion between two pandas, IoU-based method would take the boxes belonging to the panda in the back as the positive samples of the front one, which causes false positives. To address it, we formulate the problem of sample selection as an Optimal Transport problem in Optimization Theory, which reduces false positives and further improves the quality of the embedding. For each ground truth, we sum the top 10 IoU values to get m1 and the top 100 IoU values to get m2. Then we take top m1 predictions with the lowest cost as positive and top 300 ? m2 predictions with the highest cost as negatives. As shown in <ref type="figure" target="#fig_2">Fig. 8 (c)</ref>, the optimal transport provides a better selection of positive embeddings during training, and thus improves the quality of the embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Temporally Weighted Softmax</head><p>In <ref type="figure" target="#fig_3">Fig. 9</ref>, we show qualitative results of the temporally weighted softmax in our association strategy. As shown in <ref type="figure" target="#fig_3">Fig. 9</ref> (a) and (b), the bear with 'id:1' in (a) is occluded by another bear in some frames, and without temporally weighted softmax, it is assigned a new id when it reappears. As shown in <ref type="figure" target="#fig_3">Fig. 9 (c)</ref>, the people with 'id:3' and elephant with 'id:0' disappear in the corner of the video, but they swap ids when they reappear after several frames, and this leads to classification errors. However, in <ref type="figure" target="#fig_3">Fig. 9 (d)</ref>, temporally weighted softmax helps maintain temporal consistency of id for the sampe people and elephant.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CrossVIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IDOL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CrossVIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IDOL</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative results on the OVIS validation dataset. IDOL achieves very good results on complex scenes. Please refer to the supplementary for more qualitative results and comparison with other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>48 Fig. 5 .Fig. 6 . 22 Fig. 7 .</head><label>4856227</label><figDesc>Qualitative comparisons on videos with similar instances. Such kind of case is rare in YouTube-VIS, therefore we only select videos from OVIS. All methods use ResNet-50 backbone. Different color represents different instance id. Compare with the previous SOTA method, IDOL is able to segment and track instances with very similar appearances under complex motion and occlusions. Qualitative comparisons on videos with complex motions. We don't show the results of offline methods (IFC, SeqFormer) on OVIS since they do not provide official code/models on OVIS and the clip matching method provided by IFC fails in complex cases. All methods use ResNet-50 backbone. Different color represents different instance id. Compare with the previous SOTA methods, IDOL performs much better on videos with high-speed and large motions (a), and complex motions (b). OVIS / vid-23 CrossVIS (a) YouTube-VIS2019 / vid-Qualitative comparisons on videos with severe occlusions. All methods use ResNet-50 backbone. Different color represents different instance id. Compare with the previous SOTA methods, IDOL is more robust when handling crowded scenes with severe occlusions and frequent position exchanges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 8 .</head><label>8</label><figDesc>Visualization of positive samples selected by IoU-based method (a) and our optimal transport method (c). The panda with red bounding box in (b) is the key instance. The positive samples selected by the IoU-based method are shown in (a), which causes false positives (i.e., the orange bounding box belonging to the panda behind the key instance). The positive samples selected by our method are shown in (c). It gives more accurate samples for positive embeddings and reduces false positives, further improving the quality of the embedding and the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 9 .</head><label>9</label><figDesc>Visualization of association quality with/without temporally weighted softmax (TWS). Each row shows three adjacent frames from the same video. (a) and (c) show the association quality without temporally weighted softmax. (b) and (d) show the association quality with temporally weighted softmax. The bear with 'id:1' in (a) is occluded by another bear in some frames, and it is assigned a new id when it reappears. When the people with 'id:3' and elephant with 'id:0' in (c) disappear in the corner of the video and reappear after several frames, they are also assigned new ids. However, this problem is solved in (b) and (d) by our one-to-many temporally weighted softmax during inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>datasets. YouTube-VIS 2019 is the first and largest dataset for video instance segmentation. It contains 2,238 training, 302 validation, and 343 test high-resolution YouTube video clips, with an average video duration of 4.61s. YouTube-VIS 2021 is an extended version of YouTube-VIS 2019. Both datasets have 40 object categories, but the category label set is slightly different. OVIS dataset is a relatively new and challenging dataset. It consists of 607 training</figDesc><table /><note>videos, 140 validation videos, and 154 test videos. Compared with YouTube-VIS, its videos are much longer and last 12.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Oracle experiments on clip length for offline models. AP is reported.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Method</cell><cell>Publish</cell><cell></cell><cell cols="2">Predicted Frame Oracle</cell></row><row><cell cols="2">YouTube-VIS</cell><cell cols="3">CrossVIS ICCV 2021 IFC NeurIPS 2021</cell><cell>43.4 46.8</cell><cell>52.8 50.1</cell></row><row><cell>OVIS</cell><cell></cell><cell cols="3">CrossVIS ICCV 2021 IFC NeurIPS 2021</cell><cell>10.1 8.7</cell><cell>29.9 25.1</cell></row><row><cell>Dataset</cell><cell cols="2">Method</cell><cell>Oracle Type</cell><cell></cell><cell cols="2">Clip Length (frame)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell></row><row><cell>YouTube-VIS</cell><cell></cell><cell>IFC</cell><cell>frame clip</cell><cell cols="3">48.1 49.5 49.7 50.1 51.3 51.8 48.1 49.1 49.0 48.9 49.7 50.1</cell></row><row><cell></cell><cell cols="2">SeqFormer</cell><cell>frame clip</cell><cell cols="3">57.9 58.2 57.9 57.4 57.6 57.0 57.9 53.8 53.14 52.4 50.2 50.2</cell></row><row><cell>OVIS</cell><cell></cell><cell>IFC</cell><cell>frame clip</cell><cell cols="3">24.6 25.8 25.9 25.1 24.2 22.8 24.6 23.6 22.5 18.8 13.5 10.5</cell></row><row><cell></cell><cell cols="2">SeqFormer</cell><cell>frame clip</cell><cell cols="3">32.0 32.3 31.8 31.5 30.3 29.8 32.0 28.3 25.0 18.9 11.5 8.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparison on YouTube-VIS 2021 val set. Best in bold, second underline.</figDesc><table><row><cell cols="2">Backbone Method</cell><cell cols="3">Type AP AP50 AP75 AR1 AR10</cell></row><row><cell></cell><cell cols="4">MaskTrack R-CNN [46] online 28.6 48.9 29.6 26.5 33.8</cell></row><row><cell></cell><cell>SipMask [4]</cell><cell cols="3">online 31.7 52.5 34.0 30.8 37.8</cell></row><row><cell></cell><cell>STMask [20]</cell><cell cols="3">online 31.1 50.4 33.5 26.9 35.6</cell></row><row><cell>ResNet-50</cell><cell>CrossVIS [47]</cell><cell cols="3">online 34.2 54.4 37.9 30.4 38.2</cell></row><row><cell></cell><cell>IFC [16]</cell><cell>offline 36.6 57.9 39.3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>SeqFormer [41]</cell><cell cols="3">offline 40.5 62.4 43.7 36.1 48.1</cell></row><row><cell></cell><cell>IDOL(ours)</cell><cell cols="3">online 43.9 68.0 49.6 38.0 50.9</cell></row><row><cell>Swin-L</cell><cell>SeqFormer [41] IDOL(ours)</cell><cell cols="3">offline 51.8 74.6 58.2 42.8 58.1 online 56.1 80.8 63.5 45.0 60.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Ablation study on contrastive learning and inference strategies on YTVIS. Ablation study on contrastive learning and inference strategy on OVIS. Medium and heavy denote the AP and AR of objects moderately occluded, and heavily occluded, respectively.2? performance and gains 30.2 AP, surpassing the previous method by 14.8 AP.</figDesc><table><row><cell></cell><cell></cell><cell>Training</cell><cell></cell><cell></cell><cell cols="2">Inference</cell><cell cols="3">AP AP75 AR1</cell></row><row><cell></cell><cell cols="7">ID Head Contrastive OT Matching Temporal</cell><cell></cell><cell></cell></row><row><cell></cell><cell>?</cell><cell>-</cell><cell>-</cell><cell cols="2">multi-cues</cell><cell>-</cell><cell cols="3">30.3 30.5 31.5</cell></row><row><cell></cell><cell>?</cell><cell>-</cell><cell cols="3">-embeddings</cell><cell>-</cell><cell cols="3">33.6 36.8 38.7</cell></row><row><cell></cell><cell>?</cell><cell>-</cell><cell cols="3">-embeddings</cell><cell>?</cell><cell cols="3">34.8 37.6 39.4</cell></row><row><cell></cell><cell>-</cell><cell>?</cell><cell>-</cell><cell cols="2">multi-cues</cell><cell>-</cell><cell cols="3">31.8 35.3 31.9</cell></row><row><cell></cell><cell>-</cell><cell>?</cell><cell cols="3">-embeddings</cell><cell>-</cell><cell cols="3">42.5 45.7 42.2</cell></row><row><cell></cell><cell>-</cell><cell>?</cell><cell cols="3">? embeddings</cell><cell>-</cell><cell cols="3">44.8 49.9 43.4</cell></row><row><cell></cell><cell>-</cell><cell>?</cell><cell cols="3">? embeddings</cell><cell>?</cell><cell cols="3">46.4 51.9 44.8</cell></row><row><cell></cell><cell>Training</cell><cell></cell><cell cols="2">Inference</cell><cell></cell><cell></cell><cell>AP</cell><cell></cell><cell>AR</cell></row><row><cell cols="11">ID Head Contrastive OT Matching Temporal All medium heavy medium heavy</cell></row><row><cell>?</cell><cell>-</cell><cell>-</cell><cell>multi-cues</cell><cell></cell><cell>-</cell><cell>11.0</cell><cell>11.9</cell><cell>2.3</cell><cell>16.4</cell><cell>7.6</cell></row><row><cell>-</cell><cell>?</cell><cell>-</cell><cell>multi-cues</cell><cell></cell><cell>-</cell><cell>18.4</cell><cell>22.5</cell><cell>5.8</cell><cell>34.9</cell><cell>14.9</cell></row><row><cell>-</cell><cell>?</cell><cell cols="3">-embeddings</cell><cell>-</cell><cell>26.7</cell><cell>30.9</cell><cell>9.5</cell><cell>43.2</cell><cell>19.9</cell></row><row><cell>-</cell><cell>?</cell><cell cols="3">? embeddings</cell><cell>-</cell><cell>28.3</cell><cell>34.0</cell><cell>9.8</cell><cell>44.5</cell><cell>20.3</cell></row><row><cell>-</cell><cell>?</cell><cell cols="3">? embeddings</cell><cell>?</cell><cell>30.2</cell><cell>36.5</cell><cell>10.3</cell><cell>46.9</cell><cell>20.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by NSF 1763705. We thank the reviewers for their efforts and valuable feedback to improve our work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stem-seg: Spatiotemporal embeddings for instance segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2020)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classifying, segmenting, and tracking object instances in video with mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2020) 2, 4</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sipmask: Spatial information preservation for fast image and video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2020) 2, 4</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Motchallenge: A benchmark for single-camera multiple target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03400</idno>
		<title level="m">Compfeat: Comprehensive feature aggregation for video instance segmentation</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ota: Optimal transport assignment for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yoshie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021) 6</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08430</idno>
		<title level="m">Yolox: Exceeding yolo series in 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-supervised co-training for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (2020)</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video instance segmentation using interframe communication transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS (2021) 2, 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03747</idno>
		<title level="m">Stc: Spatio-temporal contrastive learning for video instance segmentation</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Prototypical crossattention networks for multiple object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial feature calibration and temporal fusion for effective one-stage video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video instance segmentation with a proposereduce paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2021)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02702</idno>
		<title level="m">TrackFormer: Multiobject tracking with transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Endto-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">fourth international conference on 3D vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<title level="m">Representation learning with contrastive predictive coding. arXiv e-prints pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quasi-dense similarity learning for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021) 3, 4, 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Occluded video instance segmentation: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15460</idno>
		<title level="m">TransTrack: Multiple-object tracking with transformer</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mots: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B G</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021) 2, 4</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient video instance segmentation via tracklet query and proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yarram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eledath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>1st place solution for youtubevos challenge 2022: Video instance segmentation 11</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Seqformer: Sequential transformer for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2022) 2, 5</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://youtube-vos.org/dataset/vis/8" />
		<title level="m">Youtubevis dataset 2021 version</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Segment as points for efficient and effective online multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.07078</idno>
		<title level="m">Towards grand unification of object tracking</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Crossover learning for fast online video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>In: ICCV (2021) 2, 3, 4, 5, 7, 9</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06864</idno>
		<title level="m">Bytetrack: Multi-object tracking by associating every detection box</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">FairMOT: On the fairness of detection and re-identification in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
