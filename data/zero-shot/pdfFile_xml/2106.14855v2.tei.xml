<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">K-Net: Towards Unified Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
							<email>chenkai@sensetime.com</email>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ntu.edu.sgpangjiangmiao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">K-Net: Towards Unified Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic, instance, and panoptic segmentations have been addressed using different and specialized frameworks despite their underlying connections. This paper presents a unified, simple, and effective framework for these essentially similar tasks. The framework, named K-Net, segments both instances and semantic categories consistently by a group of learnable kernels, where each kernel is responsible for generating a mask for either a potential instance or a stuff class. To remedy the difficulties of distinguishing various instances, we propose a kernel update strategy that enables each kernel dynamic and conditional on its meaningful group in the input image. K-Net can be trained in an end-to-end manner with bipartite matching, and its training and inference are naturally NMS-free and box-free. Without bells and whistles, K-Net surpasses all previous published stateof-the-art single-model results of panoptic segmentation on MS COCO test-dev split and semantic segmentation on ADE20K val split with 55.2% PQ and 54.3% mIoU, respectively. Its instance segmentation performance is also on par with Cascade Mask R-CNN on MS COCO with 60%-90% faster inference speeds. Code and models will be released at https://github.com/ZwwWayne/K-Net/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image segmentation aims at finding groups of coherent pixels <ref type="bibr" target="#b47">[48]</ref>. There are different notions in groups, such as semantic categories (e.g., car, dog, cat) or instances (e.g., objects that coexist in the same image). Based on the different segmentation targets, the tasks are termed differently, i.e., semantic and instance segmentation, respectively. There are also pioneer attempts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b63">64]</ref> to joint the two segmentation tasks for more comprehensive scene understanding.</p><p>Grouping pixels according to semantic categories can be formulated as a dense classification problem. As shown in <ref type="figure" target="#fig_0">Fig. 1-(a)</ref>, recent methods directly learn a set of convolutional kernels (namely semantic kernels in this paper) of pre-defined categories and use them to classify pixels <ref type="bibr" target="#b39">[40]</ref> or regions <ref type="bibr" target="#b21">[22]</ref>. Such a framework is elegant and straightforward. However, extending this notion to instance segmentation is non-trivial given the varying number of instances across images. Consequently, instance segmentation is tackled by more complicated frameworks with additional steps such as object detection <ref type="bibr" target="#b21">[22]</ref> or embedding generation <ref type="bibr" target="#b43">[44]</ref>. These methods rely on extra components, which must guarantee the accuracy of extra components to a reasonable extent, or demand complex postprocessing such as Non-Maximum Suppression (NMS) and pixel grouping. Recent approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b55">56]</ref> generate kernels from dense feature grids and then select kernels for segmentation to simplify the frameworks. Nonetheless, since they build upon dense grids to enumerate and select kernels, these methods still rely on hand-crafted post-processing to eliminate masks or kernels of duplicated instances. , and panoptic segmentation (c) tasks are unified by a common framework in this paper. In conventional semantic segmentation methods, each convolutional kernel corresponds to a semantic class. Our framework extends this notion to make each kernel corresponds to either a potential instance or a semantic class.</p><p>In this paper, we make the first attempt to formulate a unified and effective framework to bridge the seemingly different image segmentation tasks (semantic, instance, and panoptic) through the notion of kernels. Our method is dubbed as K-Net ('K' stands for kernels). It begins with a set of convolutional kernels that are randomly initialized, and learns the kernels in accordance to the segmentation targets at hand, namely, semantic kernels for semantic categories and instance kernels for instance identities ( <ref type="figure" target="#fig_0">Fig. 1-(b)</ref>). A simple combination of semantic kernels and instance kernels allows panoptic segmentation naturally ( <ref type="figure" target="#fig_0">Fig. 1-(c)</ref>). In the forward pass, the kernels perform convolution on the image features to obtain the corresponding segmentation predictions.</p><p>The versatility and simplicity of K-Net are made possible through two designs. First, we formulate K-Net so that it dynamically updates the kernels to make them conditional to their activations on the image. Such a content-aware mechanism is crucial to ensure that each kernel, especially an instance kernel, responds accurately to varying objects in an image. Through applying this adaptive kernel update strategy iteratively, K-Net significantly improves the discriminative ability of the kernels and boosts the final segmentation performance. It is noteworthy that this strategy universally applies to kernels for all the segmentation tasks.</p><p>Second, inspired by recent advances in object detection <ref type="bibr" target="#b3">[4]</ref>, we adopt the bipartite matching strategy <ref type="bibr" target="#b46">[47]</ref> to assign learning targets for each kernel. This training approach is advantageous to conventional training strategies <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46]</ref> as it builds a one-to-one mapping between kernels and instances in an image. It thus resolves the problem of dealing with a varying number of instances in an image. In addition, it is purely mask-driven without involving boxes. Hence, K-Net is naturally NMS-free and box-free, which is appealing to real-time applications.</p><p>To show the effectiveness of the proposed unified framework on different segmentation tasks, we conduct extensive experiments on COCO dataset <ref type="bibr" target="#b37">[38]</ref> for panoptic and instance segmentation, and ADE20K dataset <ref type="bibr" target="#b71">[72]</ref> for semantic segmentation. Without bells and whistles, K-Net surpasses all previous state-of-the-art single-model results on panoptic (54.6% PQ) and semantic segmentation benchmarks (54.3% mIoU) and achieves competitive performance compared to the more expensive Cascade Mask R-CNN <ref type="bibr" target="#b2">[3]</ref>. We further analyze the learned kernels and find that instance kernels incline to specialize on objects at specific locations of similar sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semantic Segmentation. Contemporary semantic segmentation approaches typically build upon a fully convolutional network (FCN) <ref type="bibr" target="#b39">[40]</ref> and treat the task as a dense classification problem. Based on this framework, many studies focus on enhancing the feature representation through dilated convolution <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>, pyramid pooling <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b68">69]</ref>, context representations <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67]</ref>, and attention mechanisms <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b69">70]</ref>. Recently, SETR <ref type="bibr" target="#b70">[71]</ref> reformulates the task as a sequence-to-sequence prediction task by using a vision transformer <ref type="bibr" target="#b16">[17]</ref>. Despite the different model architectures, the approaches above share the common notion of making predictions via static semantic kernels. Differently, the proposed K-Net makes the kernels dynamic and conditional on their activations in the image.</p><p>Instance Segmentation. There are two representative frameworks for instance segmentation -'topdown' and 'bottom-up' approaches. 'Top-down' approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref> first detect accurate bounding boxes and generate a mask for each box. Mask R-CNN <ref type="bibr" target="#b21">[22]</ref> simplifies this pipeline by directly adding a FCN <ref type="bibr" target="#b39">[40]</ref> in Faster R-CNN <ref type="bibr" target="#b45">[46]</ref>. Extensions of this framework add a mask scoring branch <ref type="bibr" target="#b23">[24]</ref> or adopt a cascade structure <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. 'Bottom-up' methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> first perform semantic segmentation then group pixels into different instances. These methods usually require a grouping process, and their performance often appears inferior to 'top-down' approaches in popular benchmarks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38]</ref>. Unlike all these works, K-Net performs segmentation and instance separation simultaneously by constraining each kernel to predict one mask at a time for one object. Therefore, K-Net needs neither bounding box detection nor grouping process. It focuses on refining kernels rather than refining bounding boxes, different from previous cascade methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Recent attempts <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b59">60]</ref> perform instance segmentation in one stage without involving detection nor embedding generation. These methods apply dense mask prediction using dense sliding windows <ref type="bibr" target="#b9">[10]</ref> or dense grids <ref type="bibr" target="#b54">[55]</ref>. Some studies explore polar <ref type="bibr" target="#b59">[60]</ref> representation, contour <ref type="bibr" target="#b44">[45]</ref>, and explicit shape representation <ref type="bibr" target="#b61">[62]</ref> of instance masks. These methods all rely on NMS to eliminate duplicated instance masks, which hinders end-to-end training. The heuristic process is also unfavorable for real-time applications. Instance kernels in K-Net are trained in an end-to-end manner with bipartite matching and set prediction loss, thus, our methods does not need NMS.</p><p>Panoptic Segmentation. Panoptic segmentation <ref type="bibr" target="#b28">[29]</ref> combines instance and semantic segmentation to provide a richer understanding of the scene. Different strategies have been proposed to cope with the instance segmentation task. Mainstream frameworks add a semantic segmentation branch <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b60">61]</ref> on an instance segmentation framework or adopt different pixel grouping strategies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b62">63]</ref> based on a semantic segmentation method. Recently, DETR <ref type="bibr" target="#b3">[4]</ref> tries to simplify the framework by transformer <ref type="bibr" target="#b51">[52]</ref> but need to predict boxes around both stuff and things classes in training for assigning learning targets. These methods either need object detection or embedding generation to separate instances, which does not reconcile the instance and semantic segmentation in a unified framework. By contrast, K-Net partitions an image into semantic regions by semantic kernels and object instances by instance kernels through a unified perspective of kernels.</p><p>Concurrent to K-Net, some recent attempts <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b52">53]</ref> apply Transformer <ref type="bibr" target="#b51">[52]</ref> for panoptic segmentation. MaskFormer reformulates semantic segmentation as a mask classification task, which is commonly adopted in instance-level segmentation. From an inverse perspective, K-Net tries to simplify instance and panoptic segmentation by letting a kernel to predict the mask of only one instance or a semantic category, which is the essential design in semantic segmentation. In contrast to K-Net that directly uses learned kernels to predict masks and progressively refines the masks and kernels, MaX-DeepLab <ref type="bibr" target="#b52">[53]</ref> and MaskFormer <ref type="bibr" target="#b11">[12]</ref> rely on queries and Transformer <ref type="bibr" target="#b51">[52]</ref> to produce dynamic kernels for the final mask prediction.</p><p>Dynamic Kernels. Convolution kernels are usually static, i.e., agnostic to the inputs, and thus have limited representation ability. Previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b72">73]</ref> explore different kinds of dynamic kernels to improve the flexibility and performance of models. Some semantic segmentation methods apply dynamic kernels to improve the model representation with enlarged receptive fields <ref type="bibr" target="#b56">[57]</ref> or multi-scales contexts <ref type="bibr" target="#b19">[20]</ref>. Differently, K-Net uses dynamic kernels to improve the discriminative capability of the segmentation kernels more so than the input features of kernels.</p><p>Recent studies apply dynamic kernels to generate instance <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b55">56]</ref> or panoptic <ref type="bibr" target="#b33">[34]</ref> segmentation predictions directly. Because these methods generate kernels from dense feature maps, enumerate kernels of each position, and filter out kernels of background regions, they either still rely on NMS <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b55">56]</ref> or need extra kernel fusion <ref type="bibr" target="#b33">[34]</ref> to eliminate kernels or masks of duplicated objects. Instead of generated from dense grids, the kernels in K-Net are a set of learnable parameters updated by their corresponding contents in the image. K-Net does not need to handle duplicated kernels because its kernels learn to focus on different regions of the image in training, constrained by the bipartite matching strategy that builds a one-to-one mapping between the kernels and instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We consider various segmentation tasks through a unified perspective of kernels. The proposed K-Net uses a set of kernels to assign each pixel to either a potential instance or a semantic class (Sec. 3.1). To enhance the discriminative capability of kernels, we contribute a way to update the static kernels by the contents in their partitioned pixel groups (Sec. 3.2). We adopt the bipartite matching strategy to train instance kernels in an end-to-end manner (Sec. 3.3). K-Net can be applied seamlessly to semantic, instance, and panoptic segmentation as described in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">K-Net</head><p>Despite the different definitions of a 'meaningful group', all segmentation tasks essentially assign each pixel to one of the predefined meaningful groups <ref type="bibr" target="#b47">[48]</ref>. As the number of groups in an image is typically assumed finite, we can set the maximum group number of a segmentation task as N . For example, there are N pre-defined semantic classes for semantic segmentation or at most N objects in an image for instance segmentation. For panoptic segmentation, N is the total number of stuff classes and objects in an image. Therefore, we can use N kernels to partition an image into N groups, where each kernel is responsible to find the pixels belonging to its corresponding group. Specifically, given an input feature map F ? R B?C?H?W of B images, produced by a deep neural network, we only need N kernels K ? R N ?C to perform convolution with F to obtain the corresponding segmentation</p><formula xml:id="formula_0">prediction M ? R B?N ?H?W as M = ?(K * F ),<label>(1)</label></formula><p>where C, H, and W are the number of channels, height, and width of the feature map, respectively. The activation function ? can be softmax function if we want to assign each pixel to only one of the kernels (usually used in semantic segmentation). Sigmoid function can also be used as activation function if we allow one pixel belong to multiple masks, which results on N binary masks by setting a threshold like 0.5 on the activation map (usually used in instance segmentation).</p><p>This formulation has already dominated semantic segmentation for years <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b68">69]</ref>. In semantic segmentation, each kernel is responsible to find all pixels of a similar class across images. Whereas in instance segmentation, each pixel group corresponds to an object. However, previous methods separate instances by extra steps <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44]</ref> instead of by kernels.</p><p>This paper is the first study that explores if the notion of kernels in semantic segmentation is equally applicable to instance segmentation, and more generally panoptic segmentation. To separate instances by kernels, each kernel in K-Net only segments at most one object in an image ( <ref type="figure" target="#fig_0">Fig. 1-(b)</ref>). In this way, K-Net distinguishes instances and performs segmentation simultaneously, achieving instance segmentation in one pass without extra steps. For simplicity, we call these kernels as semantic and instance kernels in this paper for semantic and instance segmentation, respectively. A simple combination of instance kernels and semantic kernels can naturally preform panoptic segmentation that either assigns a pixel to an instance ID or a class of stuff ( <ref type="figure" target="#fig_0">Fig. 1-(c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Group-Aware Kernels</head><p>Kernel Update Head ( ) Despite the simplicity of K-Net, separating instances directly by kernels is non-trivial. Because instance kernels need to discriminate objects that vary in scale and appearance within and across images. Without a common and explicit characteristic like semantic categories, the instance kernels need stronger discriminative ability than static kernels.</p><formula xml:id="formula_1">Adaptive Kernel Update Kernel Interaction Class Predictions !"# ! !"# $ Conv FC Identity Multiplication ? ! Convolution ? ? $ !</formula><p>To overcome this challenge, we contribute an approach to make the kernel conditional on their corresponding pixel groups, through a kernel update head, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The kernel update head f i contains three key steps: group feature assembling, adaptive kernel update, and kernel interaction. Firstly, the group feature F K for each pixel group is assembled using the mask prediction M i?1 . As it is the content of each individual groups that distinguishes them from each other, F K is used to update their corresponding kernel K i?1 adaptively. After that, the kernel interacts with each other to comprehensively model the image context. Finally, the obtained group-aware kernels K i perform convolution over feature map F to obtain more accurate mask prediction M i . As shown in <ref type="figure">Fig. 3</ref>, this process can be conducted iteratively because a finer partition usually reduces the noise in group features, which results in more discriminative kernels. This process is formulated as  <ref type="figure">Figure 3</ref>: K-Net for panoptic segmentaion. A set of learned kernels first performs convolution with the feature map F to predict masks M0. Then the kernel update head takes the mask predictions M0, learned kernels K0, and feature map F as input and produce class predictions, group-aware (dynamic) kernels, and mask predictions. The produced mask prediction, dynamic kernels, and feature map F are sent to the next kernel update head. This process is performed iteratively to progressively refine the kernels and the mask predictions.</p><formula xml:id="formula_2">K i , M i = f i (M i?1 , K i?1 , F ) .<label>(2</label></formula><p>Notably, the kernel update head with the iterative refinement is universal as it does not rely on the characteristic of kernels. Thus, it can enhance not only instance kernels but also semantic kernels. We detail the three steps as follows.</p><p>Group Feature Assembling. The kernel update head first assembles the features of each group, which will be adopted later to make the kernels group-aware. As the mask of each kernel in M i?1 essentially defines whether or not a pixel belongs to the kernel's related group, we can assemble the feature F K for K i?1 by multiplying the feature map F with the M i?1 as</p><formula xml:id="formula_3">F K = H u W v M i?1 (u, v) ? F (u, v), F K ? R B?N ?C ,<label>(3)</label></formula><p>where B is the batch size, N is the number of kernels, and C is the number of channels.</p><p>Adaptive Feature Update. The kernel update head then updates the kernels using the obtained F K to improve the representation ability of kernels. As the mask M i?1 may not be accurate, which is more common the case, the feature of each group may also contain noises introduced by pixels from other groups. To reduce the adverse effect of the noise in group features, we devise an adaptive kernel update strategy. Specifically, we first conduct element-wise multiplication between F K and K i?1 as</p><formula xml:id="formula_4">F G = ? 1 (F K ) ? ? 2 (K i?1 ), F G ? R B?N ?C ,<label>(4)</label></formula><p>where ? 1 and ? 2 are linear transformations. Then the head learns two gates, G F and G K , which adapt the contribution from F K and K i?1 to the updated kernelK, respectively. The formulation is</p><formula xml:id="formula_5">G K = ?(? 1 (F G )), G F = ?(? 2 (F G )), K = G F ? ? 3 (F K ) + G K ? ? 4 (K i?1 ),<label>(5)</label></formula><p>where ? n , n = 1, ..., 4 are different fully connected (FC) layers followed by LayerNorm (LN) and ? is the Sigmoid function.K is then used in kernel interaction.</p><p>The gate learned here plays a role like the self-attention mechanism in Transformer <ref type="bibr" target="#b51">[52]</ref>, whose output is computed as a weighted summation of the values. In Transformer, the weight assigned to each value is usually computed by a compatibility function dot-product of the queries and keys. Similarly, adaptive kernel update essentially performs weighted summation of kernel features K i?1 and group features F G . Their weight G K and G F are computed by element-wise multiplication, which can be regarded as another kind of compatibility function.</p><p>Kernel Interaction. Interaction among kernels is important to inform each kernel with contextual information from other groups. Such information allows the kernel to implicitly model and exploit the relationship between groups of an image. To this end, we add a kernel interaction process to obtain the new kernels K i given the updated kernelsK. Here we simply adopt Multi-Head Attention <ref type="bibr" target="#b51">[52]</ref> followed by a Feed-Forward Neural Network, which has been proven effective in previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b51">52]</ref>. The output K i of kernel interaction is then used to generate a new mask prediction through M i = g i (K i ) * F , where g i is an FC-LN-ReLU layer followed by an FC layer. K i will also be used to predict classification scores in instance and panoptic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Instance Kernels</head><p>While each semantic kernel can be assigned to a constant semantic class, there lacks an explicit rule to assign varying number of targets to instance kernels. In this work, we adopt bipartite matching strategy and set prediction loss <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b46">47]</ref> to train instance kernels in an end-to-end manner. Different from previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b46">47]</ref> that rely on boxes, the learning of instance kernels is purely mask-driven because the inference of K-Net is naturally box-free.</p><p>Loss Functions. The loss function for instance kernels is written as L K = ? cls L cls + ? ce L ce + ? dice L dice , where L cls is Focal loss <ref type="bibr" target="#b36">[37]</ref> for classification, and L ce and L dice are CrossEntropy (CE) loss and Dice loss <ref type="bibr" target="#b41">[42]</ref> for segmentation, respectively. Given that each instance only occupies a small region in an image, CE loss is insufficient to handle the highly imbalanced learning targets of masks. Therefore, we apply Dice loss <ref type="bibr" target="#b41">[42]</ref> to handle this issue following previous works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>.</p><p>Mask-based Hungarian Assignment. We adopt Hungarian assignment strategy used in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b46">47]</ref> for target assignment to train K-Net in an end-to-end manner. It builds a one-to-one mapping between the predicted instance masks and the ground-truth (GT) instances based on the matching costs. The matching cost is calculated between the mask and GT pairs in a similar manner as the training loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Applications to Various Segmentation Tasks</head><p>Panoptic Segmentation. For panoptic segmentation, the kernels are composed of instance kernels K ins 0 and semantic kernels K sem 0 as shown in <ref type="figure">Fig. 3</ref>. We adopt semantic FPN <ref type="bibr" target="#b27">[28]</ref> for producing high resolution feature map F , except that we add positional encoding used in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b73">74</ref>] to enhance the positional information. Specifically, given the feature maps P 2, P 3, P 4, P 5 produced by FPN <ref type="bibr" target="#b35">[36]</ref>, positional encoding is computed based on the feature map size of P 5, and it is added with P 5. Then semantic FPN <ref type="bibr" target="#b27">[28]</ref> is used to produce the final feature map.</p><p>As semantic segmentation mainly relies on semantic information for per-pixel classification, while instance segmentation prefers accurate localization information to separate instances, we use two separate branches to generate the features F ins and F sem to perform convolution with K ins , respectively. Notably, it is unnecessary to produce 'thing' and 'stuff' masks initially from different branches to produce a reasonable performance. Such a design is consistent with previous practices <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b55">56]</ref> and empirically yields better performance (about 1% PQ).</p><p>We then construct M 0 , K 0 , and F as the inputs of kernel update head to dynamically update the kernels and refine the panoptic mask prediction. Because 'things' are already separated by instance masks in M ins 0 , while M sem 0 contains the semantic masks of both 'things' and 'stuff', we select M st 0 , the masks of stuff categories from M sem 0 , and directly concatenate it with M ins 0 to form the panoptic mask prediction M 0 . Due to similar reason, we only select and concatenate the kernels of stuff classes in K sem 0 with K ins 0 to form the panoptic kernels K 0 . To exploit the complementary semantic information in F sem and localization information in F ins , we add them together to obtain F as the input feature map of the kernel update head. With M 0 , K 0 , and F , the kernel update head f 1 can produce group-aware kernels K 1 and mask M 1 . Then kernels and masks are iteratively by S times and finally we can obtain the mask prediction M S .</p><p>To produce the final panoptic segmentation results, we paste thing and stuff masks in a mixed order following MaskFormer <ref type="bibr" target="#b11">[12]</ref>. We also find it necessary in K-Net to firstly sort the pasting order of masks based on their classification scores for further filtering out lower-confident mask predictions. Such a method empirically performs better (about 1% PQ) than the previous strategy that pasting thing and stuff masks separately <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Instance Segmentation. In the similar framework, we simply remove the concatenation process of kernels and masks to perform instance segmentation. We did not remove the semantic segmentation branch as the semantic information is still complementary for instance segmentation. Note that in this case, the semantic segmentation branch does not use extra annotations. The ground truth of semantic segmentation is built by converting instance masks to their corresponding class labels. Semantic Segmentation. As K-Net does not rely on specific architectures of model representation, K-Net can perform semantic segmentation by simply appending its kernel update head to any existing semantic segmentation methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b68">69]</ref> that rely on semantic kernels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Dataset and Metrics. For panoptic and instance segmentation, we perform experiments on the challenging COCO dataset <ref type="bibr" target="#b37">[38]</ref>. All models are trained on the train2017 split and evaluated on the val2017 split. The panoptic segmentation results are evaluated by the PQ metric <ref type="bibr" target="#b28">[29]</ref>. We also report the performance of thing and stuff, noted as PQ Th , PQ St , respectively, for thorough evaluation. The instance segmentation results are evaluated by mask AP <ref type="bibr" target="#b37">[38]</ref>. The AP for small, medium and large objects are noted as AP s , AP m , and AP l , respectively. The AP at mask IoU thresholds 0.5 and 0.75 are also reported as AP 50 and AP 75 , respectively. For semantic segmentation, we conduct experiments on the challenging ADE20K dataset <ref type="bibr" target="#b71">[72]</ref> and report mIoU to evaluate the segmentation quality. All models are trained on the train split and evaluated on the validation split.</p><p>Implementation Details. For panoptic and instance segmentation, we implement K-Net with MMDetection <ref type="bibr" target="#b5">[6]</ref>. In the ablation study, the model is trained with a batch size of 16 for 12 epochs. The learning rate is 0.0001, and it is decreased by 0.1 after 8 and 11 epochs, respectively. We use AdamW <ref type="bibr" target="#b40">[41]</ref> with a weight decay of 0.05. For data augmentation in training, we adopt horizontal flip augmentation with a single scale. The long edge and short edge of images are resized to 1333 and 800, respectively, without changing the aspect ratio. When comparing with other frameworks, we use multi-scale training with a longer schedule (36 epochs) for fair comparisons <ref type="bibr" target="#b5">[6]</ref>. The short edge of images is randomly sampled from [640, 800] <ref type="bibr" target="#b20">[21]</ref>.</p><p>For semantic segmentation, we implement K-Net with MMSegmentation <ref type="bibr" target="#b12">[13]</ref> and train it with 80,000 iterations. As AdamW <ref type="bibr" target="#b40">[41]</ref> empirically works better than SGD, we use AdamW with a weight decay of 0.0005 by default on both the baselines and K-Net for a fair comparison. The initial learning rate is 0.0001, and it is decayed by 0.1 after 60000 and 72000 iterations, respectively. More details are provided in the appendix.</p><p>Model Hyperparameters. In the ablation study, we adopt ResNet-50 <ref type="bibr" target="#b22">[23]</ref> backbone with FPN <ref type="bibr" target="#b35">[36]</ref>. For panoptic and instance segmentation, we use ? cls = 2 for Focal loss following previous methods <ref type="bibr" target="#b73">[74]</ref>, and empirically find ? seg = 1, ? ce = 1, ? dice = 4 work best. For efficiency, the default number of instance kernels is 100. For semantic segmentation, N equals to the number of classes of the dataset, which is 150 in ADE20K and 133 in COCO dataset. The number of rounds of iterative kernel update is set to three by default for all segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benchmark Results</head><p>Panoptic Segmentation. We first benchmark K-Net with other panoptic segmentation frameworks in <ref type="table" target="#tab_1">Table 1</ref>. K-Net surpasses the previous state-of-the-art box-based method <ref type="bibr" target="#b30">[31]</ref> and box/NMS-free method [34] by 1.7 and 1.5 PQ on val split, respectively. On the test-dev split, K-Net with ResNet-   <ref type="bibr" target="#b38">[39]</ref> serving as the backbone.</p><p>We also compare K-Net with concurrent work Max-DeepLab <ref type="bibr" target="#b52">[53]</ref>, MaskFormer <ref type="bibr" target="#b11">[12]</ref>, and Panoptic SegFormer <ref type="bibr" target="#b34">[35]</ref>. K-Net surpasses these methods with the least training epochs <ref type="bibr" target="#b35">(36)</ref>, taking only about 44 GPU days (roughly 2 days and 18 hours with 16 GPUs). Note that only 100 instance kernels and Swin Transformer with window size 7 are used here for efficiency. K-Net could obtain a higher performance with more instance kernels (Sec. 4.2), Swin Transformer with window size 12 (used in MaskFormer <ref type="bibr" target="#b11">[12]</ref>), as well as an extended training schedule with aggressive data augmentation used in previous work <ref type="bibr" target="#b3">[4]</ref>.</p><p>Instance Segmentation. We compare K-Net with other instance segmentation frameworks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b49">50]</ref> in <ref type="table" target="#tab_2">Table 2</ref>. More details are provided in the appendix. As the only box-free and NMS-free method, K-Net achieves better performance and faster inference speed than Mask R-CNN <ref type="bibr" target="#b21">[22]</ref>, SOLO <ref type="bibr" target="#b54">[55]</ref>, SOLOv2 <ref type="bibr" target="#b55">[56]</ref> and CondInst <ref type="bibr" target="#b49">[50]</ref>, indicated by the higher AP and frames per second (FPS). We adopt 256 instance kernels (K-Net-N256 in the On COCO test-dev split, K-Net with ResNet-101-FPN backbone obtains performance that is 0.9 AP better than Mask R-CNN <ref type="bibr" target="#b21">[22]</ref>. It also surpasses previous kernel-based approach CondInst <ref type="bibr" target="#b49">[50]</ref> and  We also compare the number of parameters of these models in <ref type="table" target="#tab_2">Table 2</ref>. Though K-Net does not have the least number of parameters, it is more lightweight than Cascade Mask R-CNN by approximately half number of the parameters (37.3 M vs. 77.1 M).</p><p>Semantic Segmentation. We apply K-Net to existing frameworks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b68">69]</ref> that rely on static semantic kernels in <ref type="table" target="#tab_3">Table 3a</ref>. K-Net consistently improves different frameworks. Notably, K-Net significantly improves FCN (6.6 mIoU). This combination surpasses PSPNet and UperNet by 0.7 and 0.9 mIoU, respectively, and achieves performance comparable with DeepLab v3. Furthermore, the effectiveness of K-Net does not saturate with strong model representation, as it still brings significant improvement (1.4 mIoU) over UperNet with Swin Transformer <ref type="bibr" target="#b38">[39]</ref>. The results suggest the versatility and effectiveness of K-Net for semantic segmentation.</p><p>In <ref type="table" target="#tab_3">Table 3b</ref>, we further compare K-Net with other state-of-the-art methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b70">71]</ref> with test-time augmentation on the validation set. With the input of 512?512, K-Net already achieves state-of-theart performance. With a larger input of 640?640 following previous method <ref type="bibr" target="#b38">[39]</ref> during training and testing, K-Net with UperNet and Swin Transformer achieves new state-of-the-art single model performance, which is 0.8 mIoU higher than the previous one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study on Instance Segmentation</head><p>We conduct an ablation study on COCO instance segmentation dataset to evaluate the effectiveness of K-Net in discriminating instances. The conclusion is also applicable to other segmentation tasks since the design of K-Net is universal to all segmentation tasks.</p><p>Head Architecture. We verify the components in the kernel update head in <ref type="table" target="#tab_5">Table 4a</ref>. The results of without A. K. U. is obtained by updating kernels purely byK = F K + K i?1 followed by an FC-LN-ReLU layer. The results indicates that both adaptive kernel update and kernel interaction are necessary for high performance.</p><p>Positional Information. We study the necessity of positional information in <ref type="table" target="#tab_5">Table 4b</ref>. The results show that positional information is beneficial, and positional encoding <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b51">52]</ref> works slightly better than coordinate convolution. The combination of the two components does not bring additional improvements. The results justify the use of just positional encoding in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Stages.</head><p>We compare different kernel update rounds in <ref type="table" target="#tab_5">Table 4c</ref>. The results show that FPS decreases as the update rounds grow while the performance saturates beyond three stages. Such a conclusion also holds for semantic segmentation as shown in <ref type="table" target="#tab_6">Table 5</ref>. The performance of FCN + K-Net on ADE20K dataset gradually increases as the increase of iteration number but also saturates after four iterations.</p><p>Number of Kernels. We further study the number of kernels in K-Net. The results in <ref type="table" target="#tab_5">Table 4d</ref> reveal that 100 kernels are sufficient to achieve good performance. The observation is expected for COCO dataset because most of the images in the dataset do not contain many objects (7.7 objects per image in average <ref type="bibr" target="#b37">[38]</ref>). K-Net consistently achieves better performance given more instance kernels since they improve the models' capacity in coping with complicated images. However, a larger N may lead (a) Average activation over 5000 images.  to small performance gains and then get saturated (when N = 300, 512, 768, we all get 34.9% mAP). Therefore, we select N = 100 in other experiments for efficiency if without further specification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visual Analysis</head><p>Overall Distribution of Kernels. We carefully analyze the properties of instance kernels learned in K-Net by analyzing the average of mask activations of the 100 instance kernels over the 5000 images in the val split. All the masks are resized to have a similar resolution of 200 ? 200 for the analysis. As shown in <ref type="figure" target="#fig_5">Fig. 4a</ref>, the learned kernels are meaningful. Different kernels specialize on different regions of the image and objects with different sizes, while each kernel attends to objects of similar sizes at close locations across images.</p><p>Masks Refined through Kernel Update. We further analyze how the mask predictions of kernels are refined through the kernel update in <ref type="figure" target="#fig_5">Fig. 4b</ref>. Here we take K-Net for panoptic segmentation to thoroughly analyze both semantic and instance masks. The masks produced by static kernels are incomplete, e.g., the masks of river and building are missed. After kernel update, the contents are thoroughly covered by the segmentation masks, though the boundaries of masks are still unsatisfactory. The boundaries are refined after more kernel update. The classification confidences of instances also increase after kernel update. More results are given in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper explores instance kernels that can learn to separate instances during segmentation. Thus, extra components that previously assist instance segmentation can be replaced by instance kernels, including bounding boxes, embedding generation, and hand-crafted post-processing like NMS, kernel fusion, and pixel grouping. Such an attempt, for the first time, allows different image segmentation tasks to be tackled through a unified framework. The framework, dubbed as K-Net, first partitions an image into different groups by learned static kernels, then iteratively refines these kernels and their partition of the image by the features assembled from their partitioned groups. K-Net obtains new state-of-the-art single-model performance on panoptic and semantic segmentation benchmarks and surpasses the well-developed Cascade Mask R-CNN with the fastest inference speed among the recent instance segmentation frameworks. We wish K-Net and the analysis to pave the way for future research on unified image segmentation frameworks.</p><p>Training details for semantic segmentation. We implement K-Net based on MMSegmentation <ref type="bibr" target="#b12">[13]</ref> for experiments on semantic segmentation. We use AdamW <ref type="bibr" target="#b40">[41]</ref> with a weight decay of 0.0005 and train the model by 80000 iterations by default. The initial learning rate is 0.0001, and it is decayed by 0.1 after 60000 and 72000 iterations, respectively. This is different from the default training setting in MMSegmentation <ref type="bibr" target="#b12">[13]</ref> that uses SGD with momentum by 160000 iterations. But our setting obtains similar performance as the default one. Therefore, we apply AdamW with 80000 iterations to all the models in <ref type="table" target="#tab_3">Table 3a</ref> of the main text for efficiency while keeping fair comparisons. For data augmentation, we follow the default settings in MMSegmentation <ref type="bibr" target="#b12">[13]</ref>. The long edge and short edge of images are resized to 2048 and 512, respectively, without changing the aspect ratio (described as 512 ? 512 in the main text for short). Then random crop, horizontal flip, and photometric distortion augmentations are adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Benchmark Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Instance Segmentation</head><p>Accuracy comparison. In <ref type="table" target="#tab_2">Table 2</ref> of the main text, we compare both accuracy and inference speed of K-Net with previous methods. For fair comparison, we re-implement Mask R-CNN <ref type="bibr" target="#b21">[22]</ref> and Cascade Mask R-CNN <ref type="bibr" target="#b2">[3]</ref> with the multi-scale 3? training schedule <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b57">58]</ref>, and submit their predictions to the evaluation server 1 for obtaining their accuracies on the test-dev split. For SOLO <ref type="bibr" target="#b54">[55]</ref>, SOLOv2 <ref type="bibr" target="#b55">[56]</ref>, and CondInst <ref type="bibr" target="#b49">[50]</ref>, we test and report the accuracies of the models released in their official implementation <ref type="bibr" target="#b48">[49]</ref>, which are trained by multi-scale 3? training schedule. This is because the papers <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref> of SOLO and SOLOv2 only report the results of multi-scale 6? schedule, and the AP s , AP m , and AP l of CondInst <ref type="bibr" target="#b49">[50]</ref> are calculated based on the areas of bounding boxes rather than instance masks due to implementation bug. The performance of TensorMask <ref type="bibr" target="#b9">[10]</ref> is reported from <ref type="table" target="#tab_3">Table 3</ref> of the paper. The results in <ref type="table" target="#tab_2">Table 2</ref> show that K-Net obtains better AP m and AP l but lower AP s than Cascade Mask R-CNN. We hypothesize this is because Cascade Mask R-CNN rescales the regions of small, medium, and large objects to a similar scale of 28 ? 28, and predicts masks on that scale. On the contrary, K-Net predicts all the masks on a high-resolution feature map.</p><p>Inference Speed. We use frames per second (FPS) to benchmark the inference speed of the models. Specifically, we benchmark SOLO <ref type="bibr" target="#b54">[55]</ref>, SOLOv2 <ref type="bibr" target="#b55">[56]</ref>, CondInst <ref type="bibr" target="#b49">[50]</ref>, Mask R-CNN <ref type="bibr" target="#b21">[22]</ref>, Cascade Mask R-CNN <ref type="bibr" target="#b2">[3]</ref> and K-Net with an NVIDIA V100 GPU. We calculate the pure inference speed of the model without counting in the data loading time, because the latency of data loading depends on the storage system of the testing machine and can vary in different environments. The reported FPS is an average FPS obtained in three runs, where each run measures the FPS of a model through 400 iterations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b57">58]</ref>. Note that the inference speed of these models may be updated due to better implementation and specific optimizations. So we present them in <ref type="table" target="#tab_2">Table 2</ref> only to verify that K-Net is fast and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Semantic Segmentation</head><p>In <ref type="table" target="#tab_3">Table 3b</ref> of the main text, we compare K-Net on UperNet <ref type="bibr" target="#b58">[59]</ref> using Swin Transformer <ref type="bibr" target="#b38">[39]</ref> with the previous state-of-the-art obtained by Swin Transformer <ref type="bibr" target="#b38">[39]</ref>. We first directly test the model in the last row of <ref type="table" target="#tab_3">Table 3a</ref> of the main text (52.0 mIoU) with test-time augmentation and obtain 53.3 mIoU, which is on-par with the current state-of-the-art result (53.5 mIoU). Then we follow the setting in Swin Transformer <ref type="bibr" target="#b38">[39]</ref> to train the model with larger scale, which resize the long edge and short edge of images to 2048 and 640, respectively, during training and testing. The model finally obtains 54.3 mIoU on the validation set, which achieves new state-of-the-art performance on ADE20K.  <ref type="figure" target="#fig_0">Figure A1</ref>: Changes of masks before and after kernel updates of a similar model. PQ of each stage on the val split is also reported. Best viewed in color with zoom in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Visual Analysis</head><p>Masks Refined through Kernel Update. We analyze how the mask predictions change before and after each round of kernel update as shown in <ref type="figure" target="#fig_0">Figure A1</ref>. The static kernels have difficulties in handling the boundaries between masks, and the mask prediction cannot cover the whole image. The mask boundaries are gradually refined and the empty holes in big masks are finally filled through kernel updates. Notably, the mask predictions after the second and the third rounds look very similar, which means the discriminative capabilities of kernels start to saturate after the second round kernel update. The visual analysis is consistent with the evaluation metrics of a similar model on the val split, where the static kernels before kernel update only achieve 33.0 PQ, and the dynamic kernels after the first update obtain 41.0 PQ. The dynamic kernels after the second and the third rounds obtain 46.0 PQ and 46.3 PQ, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Prediction Ground Truth (a) Misclassification and inaccurate boundaries between contents that share similar texture appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Prediction Ground Truth (b) Mask prediction on the images that contain crowded instances. <ref type="figure" target="#fig_1">Figure A2</ref>: Failure modes of K-Net. Best viewed in color with zoom in.</p><p>Failure Cases. We also analyze the failure cases and find two typical failure modes of K-Net. First, for the contents that have very similar texture appearance, K-Net sometimes have difficulties to distinguish them from each other and results in inaccurate mask boundaries and misclassification of contents. Second, as shown in <ref type="figure" target="#fig_1">Figure A2b</ref>, in crowded scenarios, it is also challenging for K-Net to recognize and segment all the instances given limited number of instance kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Broader Impact</head><p>Simplicity and effectiveness are two significant properties pursued by computer vision algorithms. Our work pushes the boundary of segmentation algorithms through these two aspects by providing a unified perspective that tackles semantic, instance, and panoptic segmentation tasks consistently. The work could also ease and accelerate the model production and deployment in real-world applications, such as in autonomous driving, robotics, and mobile phones. The model with higher accuracy proposed in this work could also improve the safety of its related applications. However, due to limited resources, we do not evaluate the robustness of the proposed method on corrupted images and adversarial attacks. Therefore, the safety of the applications using this work may not be guaranteed.</p><p>To mitigate that, we plan to analyze and improve the robustness of models in the future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Semantic segmentation (a), instance (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Kernel Update Head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and semantic masks M ins 0 and M sem 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>SOLOv2 [56] by 1.2 AP and 0.6 AP, respectively. With ResNet-101-FPN backbone, K-Net surpasses Cascade Mask R-CNN with 100 and 256 instance kernels in both accuracy and speed by 0.2 AP and 6.7 FPS, and 0.7 AP and 6 FPS, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Mask prediction before and after kernel update.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Visual analysis of kernels and their masks. Best viewed in color and by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparisons with state-of-the-art panoptic segmentation methods on COCO dataset</figDesc><table><row><cell>Framework</cell><cell>Backbone</cell><cell cols="3">Box-free NMS-free Epochs PQ PQ Th PQ St</cell></row><row><cell></cell><cell></cell><cell>val</cell><cell></cell></row><row><cell>Panoptic-DeepLab [11]</cell><cell>Xception-71</cell><cell cols="3">?1000 39.7 43.9 33.2</cell></row><row><cell>Panoptic FPN [28]</cell><cell>R50-FPN</cell><cell>36</cell><cell cols="2">41.5 48.5 31.1</cell></row><row><cell>SOLOv2 [56]</cell><cell>R50-FPN</cell><cell>36</cell><cell cols="2">42.1 49.6 30.7</cell></row><row><cell>DETR [4]  ?</cell><cell>R50</cell><cell cols="3">300 + 25 43.4 48.2 36.3</cell></row><row><cell>Unifying [31]</cell><cell>R50-FPN</cell><cell cols="3">?27 43.4 48.6 35.5</cell></row><row><cell>Panoptic FCN [34]</cell><cell>R50-FPN</cell><cell>36</cell><cell cols="2">43.6 49.3 35.0</cell></row><row><cell>K-Net</cell><cell>R50-FPN</cell><cell>36</cell><cell cols="2">47.1 51.7 40.3</cell></row><row><cell></cell><cell>R101-FPN</cell><cell>36</cell><cell cols="2">49.6 55.1 41.4</cell></row><row><cell>K-Net</cell><cell>R101-FPN-DCN</cell><cell>36</cell><cell cols="2">48.3 54.0 39.7</cell></row><row><cell></cell><cell>Swin-L [39]</cell><cell>36</cell><cell cols="2">54.6 60.2 46.0</cell></row><row><cell></cell><cell cols="2">test-dev</cell><cell></cell></row><row><cell>Panoptic-DeepLab</cell><cell>Xception-71</cell><cell cols="3">?1000 41.4 45.1 35.9</cell></row><row><cell>Panoptic FPN</cell><cell>R101-FPN</cell><cell>36</cell><cell cols="2">43.5 50.8 32.5</cell></row><row><cell>Panoptic FCN</cell><cell>R101-FPN</cell><cell>36</cell><cell cols="2">45.5 51.4 36.4</cell></row><row><cell>DETR</cell><cell>R101</cell><cell cols="2">300 + 25 46.0 -</cell><cell>-</cell></row><row><cell>UPSNet [61]</cell><cell>R101-FPN-DCN</cell><cell>36</cell><cell cols="2">46.6 53.2 36.7</cell></row><row><cell>Unifying [31]</cell><cell>R101-FPN-DCN</cell><cell cols="3">?27 47.2 53.5 37.7</cell></row><row><cell>K-Net</cell><cell>R101-FPN</cell><cell>36</cell><cell cols="2">47.0 52.8 38.2</cell></row><row><cell>K-Net</cell><cell>R101-FPN-DCN</cell><cell>36</cell><cell cols="2">48.3 54.0 39.7</cell></row><row><cell>MaX-DeepLab-L [53]</cell><cell>Max-L</cell><cell>54</cell><cell cols="2">51.3 57.2 42.4</cell></row><row><cell>MaskFormer [12]</cell><cell>Swin-L [39]</cell><cell>300</cell><cell cols="2">53.3 59.1 44.5</cell></row><row><cell cols="2">Panoptic SegfFormer [35] PVTv2-B5 [54]</cell><cell>50</cell><cell cols="2">54.4 61.1 44.3</cell></row><row><cell>K-Net</cell><cell>Swin-L</cell><cell>36</cell><cell cols="2">55.2 61.2 46.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons with state-of-the-art instance segmentation methods on COCO dataset. 'P. (M)' indicates the number of parameters in the model, and the counting unit is million Method Backbone Box-free NMS-free Epochs AP? AP50 AP70 APs APm APl FPS? P. (M) ? val2017 SOLO [55] R-50-FPN 36 35.8 56.7 37.9 14.3 39.3 53.2 12.7 36.08 Mask R-CNN [22] R-50-FPN 36 37.1 58.5 39.7 18.7 39.6 53.9 17.5 44.17 SOLOv2 [56] R-50-FPN 36 37.5 58.2 40.0 15.8 41.4 56.6 17.7 33.89</figDesc><table><row><cell>CondInst [50]</cell><cell>R-50-FPN</cell><cell>36 37.5 58.5 40.1 18.7 41.0 53.3 14.0 46.37</cell></row><row><cell cols="2">Cascade Mask R-CNN [3] R-50-FPN</cell><cell>36 38.5 59.7 41.8 19.3 41.1 55.6 10.3 77.10</cell></row><row><cell>K-Net</cell><cell>R-50-FPN</cell><cell>36 37.8 60.3 39.9 16.9 41.2 57.5 21.2 37.26</cell></row><row><cell>K-Net-N256</cell><cell>R-50-FPN</cell><cell>36 38.6 60.9 41.0 19.1 42.0 57.7 19.8 37.30</cell></row><row><cell></cell><cell></cell><cell>test-dev</cell></row><row><cell>SOLO</cell><cell>R-50-FPN</cell><cell>72 36.8 58.6 39.0 15.9 39.5 52.1 12.7 36.08</cell></row><row><cell>Mask R-CNN</cell><cell>R-50-FPN</cell><cell>36 37.4 59.5 40.1 18.6 39.8 51.6 17.5 44.17</cell></row><row><cell>CondInst</cell><cell>R-50-FPN</cell><cell>36 37.8 59.2 40.4 18.2 40.3 52.7 14.0 46.37</cell></row><row><cell>SOLOv2</cell><cell>R-50-FPN</cell><cell>36 38.2 59.3 40.9 16.0 41.2 55.4 17.7 33.89</cell></row><row><cell cols="2">Cascade Mask R-CNN R-50-FPN</cell><cell>36 38.8 60.4 42.0 19.4 40.9 53.9 10.3 77.10</cell></row><row><cell>K-Net</cell><cell>R-50-FPN</cell><cell>36 38.4 61.2 40.9 17.4 40.7 56.2 21.2 37.26</cell></row><row><cell>K-Net-N256</cell><cell>R-50-FPN</cell><cell>36 39.1 61.7 41.8 18.2 41.4 56.6 19.8 37.30</cell></row><row><cell>SOLO</cell><cell>R-101-FPN</cell><cell>72 37.8 59.5 40.4 16.4 40.6 54.2 10.7 55.07</cell></row><row><cell>Mask R-CNN</cell><cell>R-101-FPN</cell><cell>36 38.8 60.8 41.8 19.1 41.2 54.3 14.3 63.16</cell></row><row><cell>CondInst</cell><cell>R-101-FPN</cell><cell>36 38.9 60.6 41.8 18.8 41.8 54.4 11.0 52.83</cell></row><row><cell>SOLOv2</cell><cell>R-101-FPN</cell><cell>36 39.5 60.8 42.6 16.7 43.0 57.4 14.3 65.36</cell></row><row><cell cols="2">Cascade Mask R-CNN R-101-FPN</cell><cell>36 39.9 61.6 43.3 19.8 42.1 55.7 9.5 96.09</cell></row><row><cell>K-Net</cell><cell>R-101-FPN</cell><cell>36 40.1 62.8 43.1 18.7 42.7 58.8 16.2 56.25</cell></row><row><cell>K-Net-N256</cell><cell>R-101-FPN</cell><cell>36 40.6 63.3 43.7 18.8 43.3 59.0 15.5 56.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">Results of K-Net on ADE20K semantic segmentation dataset</cell><cell></cell></row><row><cell cols="3">(a) Improvements of K-Net on different architec-tures</cell><cell cols="3">(b) Comparisons with state-of-the-art methods. Re-sults marked by  ? use larger image sizes</cell></row><row><cell>Method</cell><cell cols="2">Backbone Val mIoU</cell><cell>Method</cell><cell cols="2">Backbone Val mIoU</cell></row><row><cell>FCN [40]</cell><cell>R50</cell><cell>36.7</cell><cell>OCRNet [66]</cell><cell>HRNet-W48</cell><cell>44.9</cell></row><row><cell>FCN + K-Net</cell><cell>R50</cell><cell>43.3 (+6.6)</cell><cell>PSPNet [69]</cell><cell>R101</cell><cell>45.4</cell></row><row><cell>PSPNet [69]</cell><cell>R50</cell><cell>42.6</cell><cell>PSANet [70]</cell><cell>R101</cell><cell>45.4</cell></row><row><cell>PSPNet + K-Net</cell><cell>R50</cell><cell>43.9 (+1.3)</cell><cell>DNL [65]</cell><cell>R101</cell><cell>45.8</cell></row><row><cell>DLab.v3 [8]</cell><cell>R50</cell><cell>43.5</cell><cell>DLab.v3 [8]</cell><cell>R101</cell><cell>46.7</cell></row><row><cell>DLab.v3 + K-Net</cell><cell>R50</cell><cell>44.6 (+1.1)</cell><cell>DLab.v3+ [9]</cell><cell>S-101 [68]</cell><cell>47.3</cell></row><row><cell>UperNet [59] UperNet + K-Net</cell><cell>R50 R50</cell><cell>42.4 43.6 (+1.2)</cell><cell>SETR [71] UperNet  ?</cell><cell>ViT-L [17] Swin-L</cell><cell>48.6 53.5</cell></row><row><cell>UperNet</cell><cell>Swin-L</cell><cell>50.6</cell><cell>UperNet + K-Net</cell><cell>Swin-L</cell><cell>53.3</cell></row><row><cell cols="3">UperNet + K-Net Swin-L 52 (+1.4)</cell><cell>UperNet + K-Net  ?</cell><cell>Swin-L</cell><cell>54.3</cell></row><row><cell cols="6">101-FPN backbone even obtains better results than that of UPSNet [61], which uses Deformable</cell></row><row><cell cols="6">Convolution Network (DCN) [16]. K-Net equipped with DCN surpasses the previous method [31] by</cell></row><row><cell cols="6">1.1 PQ. Without bells and whistles, K-Net obtains new state-of-the-art single-model performance</cell></row><row><cell cols="2">with Swin Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>table )</head><label>)</label><figDesc></figDesc><table><row><cell>to compare with Cascade Mask R-CNN [3]. The</cell></row><row><cell>performance of K-Net-N256 is on par with Cascade Mask R-CNN [3] but enjoys a 92.2% faster</cell></row><row><cell>inference speed (19.8 v.s 10.3).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies of K-Net on instance segmentation (a) Adaptive Kernel Update (A. K. U.) and Kernel Interaction (K. I.)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(b) Positional Encoding (P. E.) and Coordinate Convo-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>lution (Coors.)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A. K. U.</cell><cell>K. I.</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>Coors.</cell><cell>P. E.</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell></row><row><cell></cell><cell></cell><cell>10.0</cell><cell>18.2</cell><cell>9.6</cell><cell></cell><cell></cell><cell>30.9</cell><cell>51.7</cell><cell>31.6</cell></row><row><cell></cell><cell></cell><cell>22.6</cell><cell>37.3</cell><cell>23.5</cell><cell></cell><cell></cell><cell>34.0</cell><cell>55.4</cell><cell>35.6</cell></row><row><cell></cell><cell></cell><cell>31.2</cell><cell>52.0</cell><cell>32.4</cell><cell></cell><cell></cell><cell>34.1</cell><cell>55.3</cell><cell>35.7</cell></row><row><cell></cell><cell></cell><cell>34.1</cell><cell>55.3</cell><cell>35.7</cell><cell></cell><cell></cell><cell>34.0</cell><cell>55.1</cell><cell>35.8</cell></row><row><cell cols="5">(c) Number of rounds of kernel update</cell><cell cols="5">(d) Numbers of instance kernels</cell></row><row><cell>Stage Number</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>FPS</cell><cell>N</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>FPS</cell></row><row><cell>1</cell><cell>21.8</cell><cell>37.3</cell><cell>22.1</cell><cell>24.0</cell><cell>50</cell><cell>32.7</cell><cell>53.7</cell><cell>34.1</cell><cell>21.6</cell></row><row><cell>2</cell><cell>32.1</cell><cell>52.3</cell><cell>33.5</cell><cell>22.7</cell><cell>64</cell><cell>33.6</cell><cell>54.8</cell><cell>35.1</cell><cell>21.6</cell></row><row><cell>3</cell><cell>34.1</cell><cell>55.3</cell><cell>35.7</cell><cell>21.2</cell><cell>100</cell><cell>34.1</cell><cell>55.3</cell><cell>35.7</cell><cell>21.2</cell></row><row><cell>4</cell><cell>34.5</cell><cell>56.5</cell><cell>35.7</cell><cell>20.1</cell><cell>128</cell><cell>34.3</cell><cell>55.6</cell><cell>35.8</cell><cell>20.7</cell></row><row><cell>5</cell><cell>34.5</cell><cell>56.5</cell><cell>35.9</cell><cell>18.9</cell><cell>256</cell><cell>34.7</cell><cell>56.1</cell><cell>36.3</cell><cell>19.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Numbers of semantic kernels</figDesc><table><row><cell cols="2">Stage Number 0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell>mIoU</cell><cell cols="8">36.7 42.7 43.0 43.3 43.8 44.1 43.1 42.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://competitions.codalab.org/competitions/20796</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This study is supported under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). It is also partially supported by the NTU NAP grant. Jiangmiao Pang and Kai Chen are also supported by the Shanghai Committee of Science and Technology, China (Grant No. 20DZ1100800). The authors would like to thank the valuable suggestions and comments by Jiaqi Wang, Rui Xu, and Xingxing Zou.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We first provide more implementation details for the three segmentation tasks of K-Net (Appendix A). Then we provide more benchmark details and discussion of comparison between K-Net and other methods (Appendix B). We further analyze K-Net about its results of kernel update and failure cases (Appendix C). Last but not the least, we discuss the broader impact of K-Net (Appendix D).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">YOLACT: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wanli Ouyang, Chen Change Loy, and Dahua Lin. Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Panoptic-DeepLab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<idno>abs/2107.06278</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deformable Kernels: Adapting effective receptive fields for object deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Region-based segmentation and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<editor>Yoshua Bengio, Dale Schuurmans, John D. Lafferty, Christopher K. I. Williams, and Aron Culotta</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic multi-scale filters for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rethinking ImageNet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mask Scoring R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep occlusion-aware instance segmentation with overlapping bilayers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Instancecut: From edges to instances with multicut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unifying training and inference for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Expectation-maximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Panoptic segformer. CoRR, abs/2109.03814, 2021</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin Transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">V-Net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Associative Embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep snake for real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaijin</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Computer vision: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">AdelaiDet: A toolbox for instancelevel recognition tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="https://git.io/adelaidet" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Image parsing: Unifying segmentation, detection, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><forename type="middle">Chun</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2012.00759</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">PVTv2: Improved baselines with pyramid vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno>abs/2106.13797</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">SOLO: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">SOLOv2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dynamic filtering with large sampling field for convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandrajit</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">PolarMask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">UPSNet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Explicit shape encoding for real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fubo</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">DeeperLab: Single-shot image parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1902.05093</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">PSANet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-tosequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Semantic understanding of scenes through the ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deformable ConvNets V2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Deformable DETR: deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno>abs/2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
